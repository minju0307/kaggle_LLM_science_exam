{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I previously shared a [notebook](url) (see the discussion [here](https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/441128)) that found a cluster of relevant Wikipedia STEM articles, resulting in around 270K STEM articles for which the resulting dataset is released [here.](https://www.kaggle.com/datasets/mbanaei/stem-wiki-cohere-no-emb)\n\nHowever, due to issues with WikiExtractor, there're cases in which some numbers or even paragraphs are missing from the final Wiki parsing. Therefore,  for the same set of  articles, I used Wiki API to gather the articles' contexts (see discussion [here](https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/442483)), for which the resulting dataset is released [here](https://www.kaggle.com/datasets/mbanaei/all-paraphs-parsed-expanded).\n\nIn order to show that the found articles cover not only the train dataset articles but also a majority of LB gold articles, I release this notebook that uses a simple retrieval model (without any prior indexing) together with a model that is trained only on the RACE dataset. (not fine-tuned on any competition-similar dataset).\n\nThe main design choices for the notebook are:\n- Using a simple TF-IDF to retrieve contexts from both datasets for every given question.\n- Although the majority of high-performing public models use DeBERTa-V3 to do the inference in their pipeline, I used a LongFormer Large model, which enables us to have a much longer prefix context given limited GPU memory. More specifically, as opposed to many public notebooks, there's no splitting to sentence level, and the whole paragraph is retrieved and passed to the classifier as a context (the main reason that we don't get OOM and also have relatively fast inference is that in LongFormer full attention is not computed as opposed to standard models like BERT).\n- I use a fall-back model (based on a public notebook that uses an openbook approach and performs 81.5 on LB) that is used for prediction when there's low confidence in the main model's output for the top choice.\n\nP.S: Although the model's performance is relatively good compared to other public notebooks, many design choices can be revised to improve both inference time and performance. (e.g., currently, context retrieval seems to be the inference bottleneck as no prior indexing is used).","metadata":{}},{"cell_type":"code","source":"!cp /kaggle/input/datasets-wheel/datasets-2.14.4-py3-none-any.whl /kaggle/working\n!pip install  /kaggle/working/datasets-2.14.4-py3-none-any.whl\n!cp /kaggle/input/backup-806/util_openbook.py .","metadata":{"execution":{"iopub.status.busy":"2023-09-29T07:06:07.598878Z","iopub.execute_input":"2023-09-29T07:06:07.599222Z","iopub.status.idle":"2023-09-29T07:06:45.834243Z","shell.execute_reply.started":"2023-09-29T07:06:07.599199Z","shell.execute_reply":"2023-09-29T07:06:45.832997Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Processing ./datasets-2.14.4-py3-none-any.whl\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (1.23.5)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (11.0.0)\nRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (1.5.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (4.65.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (2023.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (3.8.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (6.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.4) (3.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.4) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets==2.14.4) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.4) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.4) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.4) (2023.5.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.4) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.4) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets==2.14.4) (1.16.0)\nInstalling collected packages: datasets\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\nSuccessfully installed datasets-2.14.4\n","output_type":"stream"}]},{"cell_type":"code","source":"# installing offline dependencies\n!pip install -U /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers /kaggle/working/sentence-transformers\n!pip install -U /kaggle/working/sentence-transformers\n!pip install -U /kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\n\n!pip install --no-index --no-deps /kaggle/input/llm-whls/transformers-4.31.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/trl-0.5.0-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2023-09-29T07:06:45.836530Z","iopub.execute_input":"2023-09-29T07:06:45.837490Z","iopub.status.idle":"2023-09-29T07:08:44.435307Z","shell.execute_reply.started":"2023-09-29T07:06:45.837454Z","shell.execute_reply":"2023-09-29T07:08:44.434168Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Processing /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nInstalling collected packages: faiss-gpu\nSuccessfully installed faiss-gpu-1.7.2\nProcessing ./sentence-transformers\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (4.30.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (4.65.0)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.15.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.23.5)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.11.1)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.1.99)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.16.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.12.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2023.6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.6.3)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (21.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2023.6.3)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.3.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->sentence-transformers==2.2.2) (1.16.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==2.2.2) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==2.2.2) (3.1.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence-transformers==2.2.2) (9.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2023.5.7)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\nBuilding wheels for collected packages: sentence-transformers\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=126134 sha256=f038022c7bb1cc8e485eb37e92f1a17a9254b59e57699f69b3917ea8bc398ba2\n  Stored in directory: /root/.cache/pip/wheels/6c/ea/76/d9a930b223b1d3d5d6aff69458725316b0fe205b854faf1812\nSuccessfully built sentence-transformers\nInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-2.2.2\nProcessing /kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\nInstalling collected packages: blingfire\nSuccessfully installed blingfire-0.1.8\nProcessing /kaggle/input/llm-whls/transformers-4.31.0-py3-none-any.whl\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.30.2\n    Uninstalling transformers-4.30.2:\n      Successfully uninstalled transformers-4.30.2\nSuccessfully installed transformers-4.31.0\nProcessing /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl\nInstalling collected packages: peft\nSuccessfully installed peft-0.4.0\nProcessing /kaggle/input/llm-whls/trl-0.5.0-py3-none-any.whl\nInstalling collected packages: trl\nSuccessfully installed trl-0.5.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from util_openbook import get_contexts, generate_openbook_output\nimport pickle\n\nget_contexts()\ngenerate_openbook_output()\n\nimport gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T07:09:05.136117Z","iopub.execute_input":"2023-09-29T07:09:05.136503Z","iopub.status.idle":"2023-09-29T07:19:04.717081Z","shell.execute_reply.started":"2023-09-29T07:09:05.136473Z","shell.execute_reply":"2023-09-29T07:19:04.716102Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n/kaggle/working/util_openbook.py:137: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n  trn = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\").drop(\"id\", 1)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52528956d7bb4f80a24d68111811aa03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08b2ac2c453846d8beb2bf979916f4ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/28 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb3f8ab0e6dc42df9b715568f0c49f2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3545 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d933bd1c3984690aa6952a463ef7bc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3545 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2121557ace6e4f78a47fd816fc4566f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/10454 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6496be7542944b85b37569c4a97cb2de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f6bcf1c82cb46e19616c598e5de7675"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4f4830c9dc24d7b95804a95db7381d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb0d995a4bdc4e8fb1c272f8b3f4bfd8"}},"metadata":{}},{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\nYou're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"22"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nbackup_model_predictions = pd.read_csv(\"submission_backup.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-09-29T07:19:04.719000Z","iopub.execute_input":"2023-09-29T07:19:04.719918Z","iopub.status.idle":"2023-09-29T07:19:04.726975Z","shell.execute_reply.started":"2023-09-29T07:19:04.719884Z","shell.execute_reply":"2023-09-29T07:19:04.726003Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nfrom datasets import load_dataset, load_from_disk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport torch\nfrom transformers import LongformerTokenizer, LongformerForMultipleChoice\nimport transformers\nimport pandas as pd\nimport pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport unicodedata\n\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-29T07:19:04.728233Z","iopub.execute_input":"2023-09-29T07:19:04.729127Z","iopub.status.idle":"2023-09-29T07:19:04.746308Z","shell.execute_reply.started":"2023-09-29T07:19:04.729093Z","shell.execute_reply":"2023-09-29T07:19:04.745395Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"!cp -r /kaggle/input/stem-wiki-cohere-no-emb /kaggle/working\n!cp -r /kaggle/input/all-paraphs-parsed-expanded /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2023-09-29T07:19:04.748162Z","iopub.execute_input":"2023-09-29T07:19:04.748740Z","iopub.status.idle":"2023-09-29T07:19:31.263353Z","shell.execute_reply.started":"2023-09-29T07:19:04.748709Z","shell.execute_reply":"2023-09-29T07:19:31.261332Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"def SplitList(mylist, chunk_size):\n    return [mylist[offs:offs+chunk_size] for offs in range(0, len(mylist), chunk_size)]\n\n## 270K 데이터셋에서 뽑아오는 함수 \ndef get_relevant_documents_parsed(df_valid):\n    df_chunk_size=600\n    paraphs_parsed_dataset = load_from_disk(\"/kaggle/working/all-paraphs-parsed-expanded\")\n    ## 위키 데이터셋 가져오기 \n    modified_texts = paraphs_parsed_dataset.map(lambda example:\n                                             {'temp_text':\n                                              f\"{example['title']} {example['section']} {example['text']}\".replace('\\n',\" \").replace(\"'\",\"\")},\n                                             num_proc=2)[\"temp_text\"]\n    \n    all_articles_indices = []\n    all_articles_values = []\n    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]\n    \n        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts)\n        all_articles_indices.append(articles_indices)\n        all_articles_values.append(merged_top_scores)\n        \n    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n    \n    top_per_query = article_indices_array.shape[1]\n    articles_flatten = [(\n                         articles_values_array[index],\n                         paraphs_parsed_dataset[idx.item()][\"title\"],\n                         paraphs_parsed_dataset[idx.item()][\"text\"],\n                        )\n                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n    retrieved_articles = SplitList(articles_flatten, top_per_query)\n    return retrieved_articles\n\n\n## cohere no emb 데이터셋에서 가져오는 함수 \ndef get_relevant_documents(df_valid):\n    df_chunk_size=800\n    \n    cohere_dataset_filtered = load_from_disk(\"/kaggle/working/stem-wiki-cohere-no-emb\")\n    modified_texts = cohere_dataset_filtered.map(lambda example:\n                                             {'temp_text':\n                                              unicodedata.normalize(\"NFKD\", f\"{example['title']} {example['text']}\").replace('\"',\"\")},\n                                             num_proc=2)[\"temp_text\"]\n    \n    all_articles_indices = []\n    all_articles_values = []\n    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]\n    \n        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts)\n        all_articles_indices.append(articles_indices)\n        all_articles_values.append(merged_top_scores)\n        \n    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n    \n    top_per_query = article_indices_array.shape[1]\n    articles_flatten = [(\n                         articles_values_array[index],\n                         cohere_dataset_filtered[idx.item()][\"title\"],\n                         unicodedata.normalize(\"NFKD\", cohere_dataset_filtered[idx.item()][\"text\"]),\n                        )\n                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n    retrieved_articles = SplitList(articles_flatten, top_per_query)\n    return retrieved_articles\n\n\n## 검색 알고리즘 \ndef retrieval(df_valid, modified_texts):\n    \n    ## question 부터 ABCDE 답변들까지 전부다 합쳐서 query로 이용 \n    corpus_df_valid = df_valid.apply(lambda row:\n                                     f'{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"A\"]}\\n{row[\"B\"]}\\n{row[\"C\"]}\\n{row[\"D\"]}\\n{row[\"E\"]}',\n                                     axis=1).values\n    vectorizer1 = TfidfVectorizer(ngram_range=(1,2),\n                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n                                 stop_words=stop_words)\n    vectorizer1.fit(corpus_df_valid)\n    vocab_df_valid = vectorizer1.get_feature_names_out()\n    vectorizer = TfidfVectorizer(ngram_range=(1,2),\n                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n                                 stop_words=stop_words,\n                                 vocabulary=vocab_df_valid)\n    vectorizer.fit(modified_texts[:500000])\n    ## 쿼리의 tf-idf vector를 생성\n    corpus_tf_idf = vectorizer.transform(corpus_df_valid)\n    \n    print(f\"length of vectorizer vocab is {len(vectorizer.get_feature_names_out())}\")\n\n    chunk_size = 100000\n    top_per_chunk = 10\n    top_per_query = 10\n\n    all_chunk_top_indices = []\n    all_chunk_top_values = []\n\n    for idx in tqdm(range(0, len(modified_texts), chunk_size)):\n        wiki_vectors = vectorizer.transform(modified_texts[idx: idx+chunk_size]) \n        temp_scores = (corpus_tf_idf * wiki_vectors.T).toarray()\n        chunk_top_indices = temp_scores.argpartition(-top_per_chunk, axis=1)[:, -top_per_chunk:]\n        chunk_top_values = temp_scores[np.arange(temp_scores.shape[0])[:, np.newaxis], chunk_top_indices]\n\n        all_chunk_top_indices.append(chunk_top_indices + idx)\n        all_chunk_top_values.append(chunk_top_values)\n\n    top_indices_array = np.concatenate(all_chunk_top_indices, axis=1)\n    top_values_array = np.concatenate(all_chunk_top_values, axis=1)\n    \n    merged_top_scores = np.sort(top_values_array, axis=1)[:,-top_per_query:]\n    merged_top_indices = top_values_array.argsort(axis=1)[:,-top_per_query:] ## top 10개를 뽑아냄. 관련있는 context.\n    articles_indices = top_indices_array[np.arange(top_indices_array.shape[0])[:, np.newaxis], merged_top_indices]\n    \n    return articles_indices, merged_top_scores\n\n\ndef prepare_answering_input(\n        tokenizer, \n        question,  \n        options,   \n        context,   \n        max_seq_length=4096,\n    ):\n    c_plus_q   = context + ' ' + tokenizer.bos_token + ' ' + question\n    c_plus_q_4 = [c_plus_q] * len(options)\n    tokenized_examples = tokenizer(\n        c_plus_q_4, options,\n        max_length=max_seq_length,\n        padding=\"longest\",\n        truncation=False,\n        return_tensors=\"pt\",\n    )\n    input_ids = tokenized_examples['input_ids'].unsqueeze(0)\n    attention_mask = tokenized_examples['attention_mask'].unsqueeze(0)\n    example_encoded = {\n        \"input_ids\": input_ids.to(model.device.index),\n        \"attention_mask\": attention_mask.to(model.device.index),\n    }\n    return example_encoded\n","metadata":{"execution":{"iopub.status.busy":"2023-09-29T07:21:03.344455Z","iopub.execute_input":"2023-09-29T07:21:03.344816Z","iopub.status.idle":"2023-09-29T07:21:03.368318Z","shell.execute_reply.started":"2023-09-29T07:21:03.344782Z","shell.execute_reply":"2023-09-29T07:21:03.367181Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"stop_words = ['each', 'you', 'the', 'use', 'used',\n                  'where', 'themselves', 'nor', \"it's\", 'how', \"don't\", 'just', 'your',\n                  'about', 'himself', 'with', \"weren't\", 'hers', \"wouldn't\", 'more', 'its', 'were',\n                  'his', 'their', 'then', 'been', 'myself', 're', 'not',\n                  'ours', 'will', 'needn', 'which', 'here', 'hadn', 'it', 'our', 'there', 'than',\n                  'most', \"couldn't\", 'both', 'some', 'for', 'up', 'couldn', \"that'll\",\n                  \"she's\", 'over', 'this', 'now', 'until', 'these', 'few', 'haven',\n                  'of', 'wouldn', 'into', 'too', 'to', 'very', 'shan', 'before', 'the', 'they',\n                  'between', \"doesn't\", 'are', 'was', 'out', 'we', 'me',\n                  'after', 'has', \"isn't\", 'have', 'such', 'should', 'yourselves', 'or', 'during', 'herself',\n                  'doing', 'in', \"shouldn't\", \"won't\", 'when', 'do', 'through', 'she',\n                  'having', 'him', \"haven't\", 'against', 'itself', 'that',\n                  'did', 'theirs', 'can', 'those',\n                  'own', 'so', 'and', 'who', \"you've\", 'yourself', 'her', 'he', 'only',\n                  'what', 'ourselves', 'again', 'had', \"you'd\", 'is', 'other',\n                  'why', 'while', 'from', 'them', 'if', 'above', 'does', 'whom',\n                  'yours', 'but', 'being', \"wasn't\", 'be']","metadata":{"execution":{"iopub.status.busy":"2023-09-29T07:21:01.192488Z","iopub.execute_input":"2023-09-29T07:21:01.192998Z","iopub.status.idle":"2023-09-29T07:21:01.212032Z","shell.execute_reply.started":"2023-09-29T07:21:01.192959Z","shell.execute_reply":"2023-09-29T07:21:01.209310Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df_valid = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-09-29T07:21:13.510675Z","iopub.execute_input":"2023-09-29T07:21:13.511209Z","iopub.status.idle":"2023-09-29T07:21:13.529689Z","shell.execute_reply.started":"2023-09-29T07:21:13.511166Z","shell.execute_reply":"2023-09-29T07:21:13.528660Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"retrieved_articles_parsed = get_relevant_documents_parsed(df_valid)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T07:21:16.703305Z","iopub.execute_input":"2023-09-29T07:21:16.703665Z","iopub.status.idle":"2023-09-29T07:28:48.196800Z","shell.execute_reply.started":"2023-09-29T07:21:16.703618Z","shell.execute_reply":"2023-09-29T07:28:48.195675Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/2101279 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcefcf257a5247438e81d41b80818371"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 0/1 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'd', 'doesn', 'don', 'isn', 'll', 's', 'shouldn', 't', 've', 'wasn', 'weren', 'won'] not in stop_words.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"length of vectorizer vocab is 11222\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n  5%|▍         | 1/22 [00:12<04:24, 12.59s/it]\u001b[A\n  9%|▉         | 2/22 [00:25<04:14, 12.74s/it]\u001b[A\n 14%|█▎        | 3/22 [00:38<04:09, 13.12s/it]\u001b[A\n 18%|█▊        | 4/22 [00:51<03:52, 12.89s/it]\u001b[A\n 23%|██▎       | 5/22 [01:05<03:42, 13.10s/it]\u001b[A\n 27%|██▋       | 6/22 [01:18<03:32, 13.25s/it]\u001b[A\n 32%|███▏      | 7/22 [01:31<03:18, 13.25s/it]\u001b[A\n 36%|███▋      | 8/22 [01:44<03:01, 12.97s/it]\u001b[A\n 41%|████      | 9/22 [01:56<02:47, 12.86s/it]\u001b[A\n 45%|████▌     | 10/22 [02:10<02:36, 13.02s/it]\u001b[A\n 50%|█████     | 11/22 [02:22<02:21, 12.82s/it]\u001b[A\n 55%|█████▍    | 12/22 [02:35<02:09, 12.95s/it]\u001b[A\n 59%|█████▉    | 13/22 [02:48<01:55, 12.78s/it]\u001b[A\n 64%|██████▎   | 14/22 [03:00<01:41, 12.69s/it]\u001b[A\n 68%|██████▊   | 15/22 [03:13<01:30, 12.87s/it]\u001b[A\n 73%|███████▎  | 16/22 [03:26<01:16, 12.81s/it]\u001b[A\n 77%|███████▋  | 17/22 [03:40<01:04, 12.99s/it]\u001b[A\n 82%|████████▏ | 18/22 [03:52<00:51, 12.83s/it]\u001b[A\n 86%|████████▋ | 19/22 [04:04<00:38, 12.70s/it]\u001b[A\n 91%|█████████ | 20/22 [04:18<00:25, 12.87s/it]\u001b[A\n 95%|█████████▌| 21/22 [04:30<00:12, 12.74s/it]\u001b[A\n100%|██████████| 22/22 [04:30<00:00, 12.31s/it]\u001b[A\n100%|██████████| 1/1 [05:33<00:00, 333.38s/it]\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"18"},"metadata":{}}]},{"cell_type":"code","source":"retrieved_articles = get_relevant_documents(df_valid)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T07:30:43.728737Z","iopub.execute_input":"2023-09-29T07:30:43.729108Z","iopub.status.idle":"2023-09-29T07:40:00.123230Z","shell.execute_reply.started":"2023-09-29T07:30:43.729082Z","shell.execute_reply":"2023-09-29T07:40:00.122060Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/2781652 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30d61f4b1166434e8e108828000f515e"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 0/1 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'd', 'doesn', 'don', 'isn', 'll', 's', 'shouldn', 't', 've', 'wasn', 'weren', 'won'] not in stop_words.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"length of vectorizer vocab is 11222\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n  4%|▎         | 1/28 [00:08<03:57,  8.79s/it]\u001b[A\n  7%|▋         | 2/28 [00:18<03:58,  9.19s/it]\u001b[A\n 11%|█         | 3/28 [00:26<03:41,  8.87s/it]\u001b[A\n 14%|█▍        | 4/28 [00:35<03:32,  8.84s/it]\u001b[A\n 18%|█▊        | 5/28 [00:44<03:27,  9.01s/it]\u001b[A\n 21%|██▏       | 6/28 [00:55<03:26,  9.40s/it]\u001b[A\n 25%|██▌       | 7/28 [01:03<03:09,  9.04s/it]\u001b[A\n 29%|██▊       | 8/28 [01:11<02:55,  8.76s/it]\u001b[A\n 32%|███▏      | 9/28 [01:20<02:47,  8.80s/it]\u001b[A\n 36%|███▌      | 10/28 [01:28<02:33,  8.55s/it]\u001b[A\n 39%|███▉      | 11/28 [01:36<02:22,  8.38s/it]\u001b[A\n 43%|████▎     | 12/28 [01:44<02:11,  8.21s/it]\u001b[A\n 46%|████▋     | 13/28 [01:52<02:04,  8.33s/it]\u001b[A\n 50%|█████     | 14/28 [02:00<01:55,  8.28s/it]\u001b[A\n 54%|█████▎    | 15/28 [02:08<01:46,  8.20s/it]\u001b[A\n 57%|█████▋    | 16/28 [02:16<01:36,  8.06s/it]\u001b[A\n 61%|██████    | 17/28 [02:25<01:30,  8.24s/it]\u001b[A\n 64%|██████▍   | 18/28 [02:32<01:20,  8.05s/it]\u001b[A\n 68%|██████▊   | 19/28 [02:40<01:10,  7.89s/it]\u001b[A\n 71%|███████▏  | 20/28 [02:47<01:02,  7.77s/it]\u001b[A\n 75%|███████▌  | 21/28 [02:56<00:55,  7.98s/it]\u001b[A\n 79%|███████▊  | 22/28 [03:04<00:47,  7.92s/it]\u001b[A\n 82%|████████▏ | 23/28 [03:12<00:39,  7.98s/it]\u001b[A\n 86%|████████▌ | 24/28 [03:19<00:31,  7.85s/it]\u001b[A\n 89%|████████▉ | 25/28 [03:28<00:23,  7.97s/it]\u001b[A\n 93%|█████████▎| 26/28 [03:35<00:15,  7.87s/it]\u001b[A\n 96%|█████████▋| 27/28 [03:42<00:07,  7.61s/it]\u001b[A\n100%|██████████| 28/28 [03:48<00:00,  8.15s/it]\u001b[A\n100%|██████████| 1/1 [04:28<00:00, 268.69s/it]\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"18"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = LongformerTokenizer.from_pretrained(\"/kaggle/input/longformer-race-model/longformer_qa_model\")\nmodel = LongformerForMultipleChoice.from_pretrained(\"/kaggle/input/longformer-race-model/longformer_qa_model\").cuda()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T07:41:04.115305Z","iopub.execute_input":"2023-09-29T07:41:04.115910Z","iopub.status.idle":"2023-09-29T07:41:22.712150Z","shell.execute_reply.started":"2023-09-29T07:41:04.115867Z","shell.execute_reply":"2023-09-29T07:41:22.711151Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"predictions = []\nsubmit_ids = []\n\nfor index in tqdm(range(df_valid.shape[0])):\n    columns = df_valid.iloc[index].values\n    submit_ids.append(columns[0])\n    question = columns[1]\n    options = [columns[2], columns[3], columns[4], columns[5], columns[6]]\n    context1 = f\"{retrieved_articles[index][-4][2]}\\n{retrieved_articles[index][-3][2]}\\n{retrieved_articles[index][-2][2]}\\n{retrieved_articles[index][-1][2]}\"\n    context2 = f\"{retrieved_articles_parsed[index][-3][2]}\\n{retrieved_articles_parsed[index][-2][2]}\\n{retrieved_articles_parsed[index][-1][2]}\"\n    inputs1 = prepare_answering_input(\n        tokenizer=tokenizer, question=question,\n        options=options, context=context1,\n        )\n    inputs2 = prepare_answering_input(\n        tokenizer=tokenizer, question=question,\n        options=options, context=context2,\n        )\n    \n    with torch.no_grad():\n        outputs1 = model(**inputs1)    \n        losses1 = -outputs1.logits[0].detach().cpu().numpy()\n        probability1 = torch.softmax(torch.tensor(-losses1), dim=-1)\n        \n    with torch.no_grad():\n        outputs2 = model(**inputs2)\n        losses2 = -outputs2.logits[0].detach().cpu().numpy()\n        probability2 = torch.softmax(torch.tensor(-losses2), dim=-1)\n        \n    probability_ = (probability1 + probability2)/2 ## 2가지 input을 모두 사용한 다음에 probablity를 더해서 나눈 값을 사용함 (일종의 앙상블이라고 볼 수 있음!)\n\n    if probability_.max() > 0.4:\n        predict = np.array(list(\"ABCDE\"))[np.argsort(probability_)][-3:].tolist()[::-1]\n    else:\n        predict = backup_model_predictions.iloc[index].prediction.replace(\" \",\"\") ## confidence가 높지 않을 때에는 backupmodel로 prediction 했던 것을 가져온다. \n    predictions.append(predict)\n\npredictions = [\" \".join(i) for i in predictions]","metadata":{"execution":{"iopub.status.busy":"2023-09-29T07:41:49.044420Z","iopub.execute_input":"2023-09-29T07:41:49.044800Z","iopub.status.idle":"2023-09-29T07:46:34.406039Z","shell.execute_reply.started":"2023-09-29T07:41:49.044771Z","shell.execute_reply":"2023-09-29T07:46:34.405080Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"100%|██████████| 200/200 [04:45<00:00,  1.43s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"pd.DataFrame({'id':submit_ids,'prediction':predictions}).to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T07:48:36.657079Z","iopub.execute_input":"2023-09-29T07:48:36.657451Z","iopub.status.idle":"2023-09-29T07:48:36.669034Z","shell.execute_reply.started":"2023-09-29T07:48:36.657421Z","shell.execute_reply":"2023-09-29T07:48:36.668128Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}