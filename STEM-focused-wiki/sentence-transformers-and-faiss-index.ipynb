{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# Installing offline dependencies\n",
    "!pip install -U --no-deps /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!pip install -U --no-deps /kaggle/input/datasets-214/datasets-2.14.5-py3-none-any.whl"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-09-29T17:12:59.515900Z",
     "iopub.execute_input": "2023-09-29T17:12:59.516271Z",
     "iopub.status.idle": "2023-09-29T17:13:06.000776Z",
     "shell.execute_reply.started": "2023-09-29T17:12:59.516234Z",
     "shell.execute_reply": "2023-09-29T17:13:05.999410Z"
    },
    "trusted": true
   },
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "text": "Processing /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nInstalling collected packages: faiss-gpu\nSuccessfully installed faiss-gpu-1.7.2\nProcessing /kaggle/input/datasets-214/datasets-2.14.5-py3-none-any.whl\nInstalling collected packages: datasets\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.14.4\n    Uninstalling datasets-2.14.4:\n      Successfully uninstalled datasets-2.14.4\nSuccessfully installed datasets-2.14.5\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!cp /kaggle/input/86-2-with-only-270k-articles/datasets-2.14.4-py3-none-any.whl /kaggle/working\n",
    "!pip install  /kaggle/working/datasets-2.14.4-py3-none-any.whl\n",
    "!cp /kaggle/input/backup-806/util_openbook.py ."
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T17:03:02.198506Z",
     "iopub.execute_input": "2023-09-29T17:03:02.198890Z",
     "iopub.status.idle": "2023-09-29T17:03:14.790684Z",
     "shell.execute_reply.started": "2023-09-29T17:03:02.198857Z",
     "shell.execute_reply": "2023-09-29T17:03:14.789366Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": "Processing ./datasets-2.14.4-py3-none-any.whl\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (1.23.5)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (11.0.0)\nRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (2.0.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (3.3.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (2023.9.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (3.8.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (6.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.4) (3.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.4) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets==2.14.4) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.4) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.4) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.4) (2023.7.22)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.4) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.4) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.4) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.14.4) (1.16.0)\nInstalling collected packages: datasets\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\nSuccessfully installed datasets-2.14.4\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -U sentence_transformers"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### sentence transformer 저장하기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T17:05:34.819211Z",
     "iopub.execute_input": "2023-09-29T17:05:34.820658Z",
     "iopub.status.idle": "2023-09-29T17:05:41.487823Z",
     "shell.execute_reply.started": "2023-09-29T17:05:34.820608Z",
     "shell.execute_reply": "2023-09-29T17:05:41.486776Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "## sentence-transformers/all-MiniLM-L6-v2 모델 불러오기 \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T17:05:51.551054Z",
     "iopub.execute_input": "2023-09-29T17:05:51.551595Z",
     "iopub.status.idle": "2023-09-29T17:06:07.395473Z",
     "shell.execute_reply.started": "2023-09-29T17:05:51.551567Z",
     "shell.execute_reply": "2023-09-29T17:06:07.394468Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0a1088a4519c451ca1928ef46a57abae"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e6cacf694d14a2686e0f3f6952f0d7a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c603f9d003774c3ea7c41cde3945abcc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f80622c6487343789b12ca27f5f74f09"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4692b91cebd54ea2b21e133147680ead"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40ed4862182c413ebec67091c60341df"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "## sentence-transformers/all-MiniLM-L6-v2 모델 저장하기\n",
    "\n",
    "tokenizer.save_pretrained(\"/kaggle/working/all-MiniLM-L6-v2\")\n",
    "model.save_pretrained(\"/kaggle/working/all-MiniLM-L6-v2\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T17:06:07.397190Z",
     "iopub.execute_input": "2023-09-29T17:06:07.398037Z",
     "iopub.status.idle": "2023-09-29T17:06:07.568861Z",
     "shell.execute_reply.started": "2023-09-29T17:06:07.398002Z",
     "shell.execute_reply": "2023-09-29T17:06:07.567888Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### wiki dataset 불러오기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "!cp -r /kaggle/input/all-paraphs-parsed-expanded /kaggle/working/"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T17:07:51.673832Z",
     "iopub.execute_input": "2023-09-29T17:07:51.674949Z",
     "iopub.status.idle": "2023-09-29T17:08:06.752412Z",
     "shell.execute_reply.started": "2023-09-29T17:07:51.674899Z",
     "shell.execute_reply": "2023-09-29T17:08:06.751135Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "## wiki dataset 불러오기 \n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "paraphs_parsed_dataset = load_from_disk(\"/kaggle/working/all-paraphs-parsed-expanded\")\n",
    "modified_texts = paraphs_parsed_dataset.map(lambda example:\n",
    "                                             {'temp_text':\n",
    "                                              f\"{example['title']} {example['section']} {example['text']}\".replace('\\n',\" \").replace(\"'\",\"\")},\n",
    "                                             num_proc=2)[\"temp_text\"]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T17:08:06.754877Z",
     "iopub.execute_input": "2023-09-29T17:08:06.755839Z",
     "iopub.status.idle": "2023-09-29T17:10:51.058838Z",
     "shell.execute_reply.started": "2023-09-29T17:08:06.755803Z",
     "shell.execute_reply": "2023-09-29T17:10:51.056616Z"
    },
    "trusted": true
   },
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "text": "   ",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "#0:   0%|          | 0/1050640 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "76ba59f33cf24736b7aa6d3d180f5d9b"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": " ",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "#1:   0%|          | 0/1050639 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "63574e34a3f7410cb30db7fa969537d8"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "len(modified_texts)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T17:11:47.734664Z",
     "iopub.execute_input": "2023-09-29T17:11:47.735154Z",
     "iopub.status.idle": "2023-09-29T17:11:47.740480Z",
     "shell.execute_reply.started": "2023-09-29T17:11:47.735116Z",
     "shell.execute_reply": "2023-09-29T17:11:47.739574Z"
    },
    "trusted": true
   },
   "execution_count": 19,
   "outputs": [
    {
     "execution_count": 19,
     "output_type": "execute_result",
     "data": {
      "text/plain": "2101279"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "modified_texts[444]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T17:12:09.616587Z",
     "iopub.execute_input": "2023-09-29T17:12:09.616954Z",
     "iopub.status.idle": "2023-09-29T17:12:09.622636Z",
     "shell.execute_reply.started": "2023-09-29T17:12:09.616924Z",
     "shell.execute_reply": "2023-09-29T17:12:09.621798Z"
    },
    "trusted": true
   },
   "execution_count": 21,
   "outputs": [
    {
     "execution_count": 21,
     "output_type": "execute_result",
     "data": {
      "text/plain": "'Cetrimonium bromide Nanoparticle synthesis Surfactants play a key role in nanoparticle synthesis by adsorbing to the surface of the forming nanoparticle and lowering its surface energy. Surfactants also help to prevent aggregation (e.g. via DLVO mechanisms).'"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Faiss index 만들기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import faiss\n",
    "from faiss import write_index, read_index"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T17:13:16.908734Z",
     "iopub.execute_input": "2023-09-29T17:13:16.910032Z",
     "iopub.status.idle": "2023-09-29T17:13:17.021422Z",
     "shell.execute_reply.started": "2023-09-29T17:13:16.909982Z",
     "shell.execute_reply": "2023-09-29T17:13:17.020518Z"
    },
    "trusted": true
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "encoder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "vectors = encoder.encode(modified_texts)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T17:18:48.228390Z",
     "iopub.execute_input": "2023-09-29T17:18:48.228840Z",
     "iopub.status.idle": "2023-09-29T17:21:38.183442Z",
     "shell.execute_reply.started": "2023-09-29T17:18:48.228809Z",
     "shell.execute_reply": "2023-09-29T17:21:38.181910Z"
    },
    "trusted": true
   },
   "execution_count": 47,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Batches:   0%|          | 0/65665 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9eec061ccef744348121547a7133615a"
      }
     },
     "metadata": {}
    },
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[47], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m encoder \u001B[38;5;241m=\u001B[39m SentenceTransformer(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentence-transformers/all-MiniLM-L6-v2\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m vectors \u001B[38;5;241m=\u001B[39m \u001B[43mencoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodified_texts\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:161\u001B[0m, in \u001B[0;36mSentenceTransformer.encode\u001B[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001B[0m\n\u001B[1;32m    159\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m start_index \u001B[38;5;129;01min\u001B[39;00m trange(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(sentences), batch_size, desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBatches\u001B[39m\u001B[38;5;124m\"\u001B[39m, disable\u001B[38;5;241m=\u001B[39m\u001B[38;5;129;01mnot\u001B[39;00m show_progress_bar):\n\u001B[1;32m    160\u001B[0m     sentences_batch \u001B[38;5;241m=\u001B[39m sentences_sorted[start_index:start_index\u001B[38;5;241m+\u001B[39mbatch_size]\n\u001B[0;32m--> 161\u001B[0m     features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43msentences_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    162\u001B[0m     features \u001B[38;5;241m=\u001B[39m batch_to_device(features, device)\n\u001B[1;32m    164\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:319\u001B[0m, in \u001B[0;36mSentenceTransformer.tokenize\u001B[0;34m(self, texts)\u001B[0m\n\u001B[1;32m    315\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtokenize\u001B[39m(\u001B[38;5;28mself\u001B[39m, texts: Union[List[\u001B[38;5;28mstr\u001B[39m], List[Dict], List[Tuple[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]]]):\n\u001B[1;32m    316\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;124;03m    Tokenizes the texts\u001B[39;00m\n\u001B[1;32m    318\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 319\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_first_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:113\u001B[0m, in \u001B[0;36mTransformer.tokenize\u001B[0;34m(self, texts)\u001B[0m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdo_lower_case:\n\u001B[1;32m    111\u001B[0m     to_tokenize \u001B[38;5;241m=\u001B[39m [[s\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m col] \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m to_tokenize]\n\u001B[0;32m--> 113\u001B[0m output\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mto_tokenize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtruncation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlongest_first\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_seq_length\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2602\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.__call__\u001B[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m   2600\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_in_target_context_manager:\n\u001B[1;32m   2601\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_switch_to_input_mode()\n\u001B[0;32m-> 2602\u001B[0m     encodings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_one\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext_pair\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext_pair\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mall_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2603\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m text_target \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2604\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_switch_to_target_mode()\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2688\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._call_one\u001B[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m   2683\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   2684\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch length of `text`: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(text)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not match batch length of `text_pair`:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2685\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(text_pair)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2686\u001B[0m         )\n\u001B[1;32m   2687\u001B[0m     batch_text_or_text_pairs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(text, text_pair)) \u001B[38;5;28;01mif\u001B[39;00m text_pair \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m text\n\u001B[0;32m-> 2688\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_encode_plus\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2689\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch_text_or_text_pairs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_text_or_text_pairs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2690\u001B[0m \u001B[43m        \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2691\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2692\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtruncation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtruncation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2693\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2694\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstride\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2695\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2696\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2697\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2698\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2699\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2700\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2701\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2702\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2703\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2704\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2705\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2706\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2707\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2708\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencode_plus(\n\u001B[1;32m   2709\u001B[0m         text\u001B[38;5;241m=\u001B[39mtext,\n\u001B[1;32m   2710\u001B[0m         text_pair\u001B[38;5;241m=\u001B[39mtext_pair,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2726\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   2727\u001B[0m     )\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2879\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001B[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m   2869\u001B[0m \u001B[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001B[39;00m\n\u001B[1;32m   2870\u001B[0m padding_strategy, truncation_strategy, max_length, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_padding_truncation_strategies(\n\u001B[1;32m   2871\u001B[0m     padding\u001B[38;5;241m=\u001B[39mpadding,\n\u001B[1;32m   2872\u001B[0m     truncation\u001B[38;5;241m=\u001B[39mtruncation,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2876\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   2877\u001B[0m )\n\u001B[0;32m-> 2879\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_batch_encode_plus\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2880\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_text_or_text_pairs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_text_or_text_pairs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2881\u001B[0m \u001B[43m    \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2882\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpadding_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2883\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtruncation_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtruncation_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2884\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2885\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstride\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2886\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2887\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2888\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2889\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2890\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2891\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2892\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2893\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2894\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2895\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2896\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2897\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:452\u001B[0m, in \u001B[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001B[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001B[0m\n\u001B[1;32m    443\u001B[0m \u001B[38;5;66;03m# Set the truncation and padding strategy and restore the initial configuration\u001B[39;00m\n\u001B[1;32m    444\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_truncation_and_padding(\n\u001B[1;32m    445\u001B[0m     padding_strategy\u001B[38;5;241m=\u001B[39mpadding_strategy,\n\u001B[1;32m    446\u001B[0m     truncation_strategy\u001B[38;5;241m=\u001B[39mtruncation_strategy,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    449\u001B[0m     pad_to_multiple_of\u001B[38;5;241m=\u001B[39mpad_to_multiple_of,\n\u001B[1;32m    450\u001B[0m )\n\u001B[0;32m--> 452\u001B[0m encodings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode_batch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    453\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_text_or_text_pairs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    454\u001B[0m \u001B[43m    \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    455\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_pretokenized\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    456\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    458\u001B[0m \u001B[38;5;66;03m# Convert encoding to dict\u001B[39;00m\n\u001B[1;32m    459\u001B[0m \u001B[38;5;66;03m# `Tokens` has type: Tuple[\u001B[39;00m\n\u001B[1;32m    460\u001B[0m \u001B[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001B[39;00m\n\u001B[1;32m    461\u001B[0m \u001B[38;5;66;03m#                       List[EncodingFast]\u001B[39;00m\n\u001B[1;32m    462\u001B[0m \u001B[38;5;66;03m#                    ]\u001B[39;00m\n\u001B[1;32m    463\u001B[0m \u001B[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001B[39;00m\n\u001B[1;32m    464\u001B[0m tokens_and_encodings \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    465\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_encoding(\n\u001B[1;32m    466\u001B[0m         encoding\u001B[38;5;241m=\u001B[39mencoding,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    475\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m encoding \u001B[38;5;129;01min\u001B[39;00m encodings\n\u001B[1;32m    476\u001B[0m ]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "vector_dimension = vectors.shape[1]\n",
    "index = faiss.IndexFlatL2(vector_dimension)\n",
    "faiss.normalize_L2(vectors)\n",
    "index.add(vectors)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T17:17:47.453414Z",
     "iopub.execute_input": "2023-09-29T17:17:47.453980Z",
     "iopub.status.idle": "2023-09-29T17:17:47.459143Z",
     "shell.execute_reply.started": "2023-09-29T17:17:47.453946Z",
     "shell.execute_reply": "2023-09-29T17:17:47.458251Z"
    },
    "trusted": true
   },
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "write_index(index, \"/kaggle/working/all-MiniLM-L6-v2/faiss.index\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### index search 해보기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "search_text = 'natural language processing'\n",
    "search_vector = encoder.encode(search_text)\n",
    "_vector = np.array([search_vector])\n",
    "faiss.normalize_L2(_vector)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T17:17:47.460610Z",
     "iopub.execute_input": "2023-09-29T17:17:47.461225Z",
     "iopub.status.idle": "2023-09-29T17:17:47.509548Z",
     "shell.execute_reply.started": "2023-09-29T17:17:47.461193Z",
     "shell.execute_reply": "2023-09-29T17:17:47.508699Z"
    },
    "trusted": true
   },
   "execution_count": 38,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fb60b4df97c7469eac63c9379c0e32c6"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "distances, idx = index.search(_vector, k=3)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T17:18:18.998709Z",
     "iopub.execute_input": "2023-09-29T17:18:18.999089Z",
     "iopub.status.idle": "2023-09-29T17:18:19.003575Z",
     "shell.execute_reply.started": "2023-09-29T17:18:18.999062Z",
     "shell.execute_reply": "2023-09-29T17:18:19.002644Z"
    },
    "trusted": true
   },
   "execution_count": 43,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "idx"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T17:18:19.783499Z",
     "iopub.execute_input": "2023-09-29T17:18:19.783883Z",
     "iopub.status.idle": "2023-09-29T17:18:19.790255Z",
     "shell.execute_reply.started": "2023-09-29T17:18:19.783854Z",
     "shell.execute_reply": "2023-09-29T17:18:19.789311Z"
    },
    "trusted": true
   },
   "execution_count": 44,
   "outputs": [
    {
     "execution_count": 44,
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[92, 89,  3]])"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
