{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51caf4b8-a6eb-48d9-9090-8d4dd30c7cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import logging\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import ctypes\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# For RAG\n",
    "import faiss\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_from_disk, Dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# For LLM\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "from accelerate import init_empty_weights\n",
    "from accelerate.utils.modeling import set_module_tensor_to_device\n",
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de59813d-4fc8-4cfa-a2d8-eff04dfa3b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean RAM & vRAM\n",
    "def clean_memory():\n",
    "    gc.collect()\n",
    "    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3645e6b-320a-4c60-89a5-9f949fae34fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_SentenceTransformer:\n",
    "    def __init__(self, checkpoint, device=\"cuda:0\"):\n",
    "        self.device = device\n",
    "        self.checkpoint = checkpoint\n",
    "        self.model = AutoModel.from_pretrained(checkpoint).to(self.device).half()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "    def transform(self, batch):\n",
    "        tokens = self.tokenizer(batch[\"text\"], truncation=True, padding=True, return_tensors=\"pt\", max_length=MAX_SEQ_LEN)\n",
    "        return tokens.to(self.device)  \n",
    "\n",
    "    def get_dataloader(self, sentences, batch_size=32):\n",
    "        sentences = [\"Represent this sentence for searching relevant passages: \" + x for x in sentences]\n",
    "        dataset = Dataset.from_dict({\"text\": sentences})\n",
    "        dataset.set_transform(self.transform)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        return dataloader\n",
    "\n",
    "    def encode(self, sentences, show_progress_bar=False, batch_size=32):\n",
    "        dataloader = self.get_dataloader(sentences, batch_size=batch_size)\n",
    "        pbar = tqdm(dataloader) if show_progress_bar else dataloader\n",
    "\n",
    "        embeddings = []\n",
    "        for batch in pbar:\n",
    "            with torch.no_grad():\n",
    "                e = self.model(**batch).pooler_output\n",
    "                e = F.normalize(e, p=2, dim=1)\n",
    "                embeddings.append(e.detach().cpu().numpy())\n",
    "        embeddings = np.concatenate(embeddings, axis=0)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e5dd39-51de-415e-a604-7d68666a7c9a",
   "metadata": {},
   "source": [
    "### bge-small-faiss embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbecaf34-5015-4058-82f6-e3056ab97072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prompt embedding, t=0.0s\n",
      "Loading faiss index, t=2.6s\n",
      "Starting text search, t=13.1s\n",
      "Starting context extraction, t=13.2s\n"
     ]
    }
   ],
   "source": [
    "## bge-small-faiss embedding \n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"../dataset/test.csv\", index_col=\"id\")\n",
    "\n",
    "NUM_TITLES = 5\n",
    "MAX_SEQ_LEN = 512\n",
    "MODEL_PATH = \"output/bge-small-faiss/\"\n",
    "\n",
    "## load embedding model\n",
    "start = time()\n",
    "print(f\"Starting prompt embedding, t={time() - start :.1f}s\")\n",
    "model = my_SentenceTransformer(MODEL_PATH, device=\"cuda:2\") ## 직접 정의한 sentencetransformer 사용\n",
    "\n",
    "## Get query embedding\n",
    "f = lambda row : \" \".join([row[\"prompt\"], row[\"A\"], row[\"B\"], row[\"C\"], row[\"D\"], row[\"E\"]])\n",
    "inputs = df.apply(f, axis=1).values # better results than prompt only\n",
    "prompt_embeddings = model.encode(inputs, show_progress_bar=False)\n",
    "\n",
    "## faiss wikipedia index 불러오기\n",
    "print(f\"Loading faiss index, t={time() - start :.1f}s\")\n",
    "faiss_index = faiss.read_index(MODEL_PATH + '/faiss.index')\n",
    "faiss_index = faiss.index_cpu_to_all_gpus(faiss_index) # OOM이 일어날 때는 지우기 \n",
    "\n",
    "## top-5의 관련있는 인덱스 가져오기 \n",
    "print(f\"Starting text search, t={time() - start :.1f}s\")\n",
    "search_index = faiss_index.search(np.float32(prompt_embeddings), NUM_TITLES)[1]\n",
    "\n",
    "## 인덱스를 찾아서 실제 문서 가져오기 \n",
    "print(f\"Starting context extraction, t={time() - start :.1f}s\")\n",
    "dataset = load_from_disk(\"dataset/all-paraphs-parsed-expanded\")\n",
    "for i in range(len(df)):\n",
    "    df.loc[i, \"context\"] = \"-\" + \"\\n-\".join([dataset[int(j)][\"text\"] for j in search_index[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95ee8864-0403-4114-b690-0687cef716d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which of the following statements accurately d...</td>\n",
       "      <td>MOND is a theory that reduces the observed mis...</td>\n",
       "      <td>MOND is a theory that increases the discrepanc...</td>\n",
       "      <td>MOND is a theory that explains the missing bar...</td>\n",
       "      <td>MOND is a theory that reduces the discrepancy ...</td>\n",
       "      <td>MOND is a theory that eliminates the observed ...</td>\n",
       "      <td>-MOND is an example of a class of theories kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which of the following is an accurate definiti...</td>\n",
       "      <td>Dynamic scaling refers to the evolution of sel...</td>\n",
       "      <td>Dynamic scaling refers to the non-evolution of...</td>\n",
       "      <td>Dynamic scaling refers to the evolution of sel...</td>\n",
       "      <td>Dynamic scaling refers to the non-evolution of...</td>\n",
       "      <td>Dynamic scaling refers to the evolution of sel...</td>\n",
       "      <td>-In such systems we can define a certain time-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which of the following statements accurately d...</td>\n",
       "      <td>The triskeles symbol was reconstructed as a fe...</td>\n",
       "      <td>The triskeles symbol is a representation of th...</td>\n",
       "      <td>The triskeles symbol is a representation of a ...</td>\n",
       "      <td>The triskeles symbol represents three interloc...</td>\n",
       "      <td>The triskeles symbol is a representation of th...</td>\n",
       "      <td>-Classical Antiquity The triskeles proper, com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the significance of regularization in ...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>-Regularization: Classical physics theory brea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which of the following statements accurately d...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>-Several qualitative observations can be made ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Which of the following statements accurately d...</td>\n",
       "      <td>Gauss's law holds only for situations involvin...</td>\n",
       "      <td>Gauss's law holds in all cases, but it is most...</td>\n",
       "      <td>Gauss's law, which applies equally to all elec...</td>\n",
       "      <td>Gauss's law only holds for electric fields wit...</td>\n",
       "      <td>Gauss's law, which holds for all situations, i...</td>\n",
       "      <td>-While the electric flux is not affected by ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Which of the following statements accurately d...</td>\n",
       "      <td>The dimension of an object in a CW complex is ...</td>\n",
       "      <td>The dimension of an object in a CW complex is ...</td>\n",
       "      <td>The dimension of an object in a CW complex is ...</td>\n",
       "      <td>The dimension of an object in a CW complex is ...</td>\n",
       "      <td>The dimension of an object in a CW complex dep...</td>\n",
       "      <td>-An inductive dimension may be defined inducti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Which of the following statements accurately d...</td>\n",
       "      <td>The blocking temperature of an antiferromagnet...</td>\n",
       "      <td>The blocking temperature of an antiferromagnet...</td>\n",
       "      <td>The blocking temperature of an antiferromagnet...</td>\n",
       "      <td>The blocking temperature of an antiferromagnet...</td>\n",
       "      <td>The blocking temperature of an antiferromagnet...</td>\n",
       "      <td>-Magnetic blocking temperature The so-called m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is the term used in astrophysics to descr...</td>\n",
       "      <td>Blueshifting</td>\n",
       "      <td>Redshifting</td>\n",
       "      <td>Reddening</td>\n",
       "      <td>Whitening</td>\n",
       "      <td>Yellowing</td>\n",
       "      <td>-The interactions and phenomena summarized in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What is the role of axioms in a formal theory?</td>\n",
       "      <td>Basis statements called axioms form the founda...</td>\n",
       "      <td>Axioms are supplementary statements added to a...</td>\n",
       "      <td>Axioms are redundant statements that can be de...</td>\n",
       "      <td>The axioms in a theory are used for experiment...</td>\n",
       "      <td>The axioms in a formal theory are added to pro...</td>\n",
       "      <td>-Thus, an axiom is an elementary basis for a f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               prompt  \\\n",
       "id                                                      \n",
       "0   Which of the following statements accurately d...   \n",
       "1   Which of the following is an accurate definiti...   \n",
       "2   Which of the following statements accurately d...   \n",
       "3   What is the significance of regularization in ...   \n",
       "4   Which of the following statements accurately d...   \n",
       "5   Which of the following statements accurately d...   \n",
       "6   Which of the following statements accurately d...   \n",
       "7   Which of the following statements accurately d...   \n",
       "8   What is the term used in astrophysics to descr...   \n",
       "9      What is the role of axioms in a formal theory?   \n",
       "\n",
       "                                                    A  \\\n",
       "id                                                      \n",
       "0   MOND is a theory that reduces the observed mis...   \n",
       "1   Dynamic scaling refers to the evolution of sel...   \n",
       "2   The triskeles symbol was reconstructed as a fe...   \n",
       "3   Regularizing the mass-energy of an electron wi...   \n",
       "4   The angular spacing of features in the diffrac...   \n",
       "5   Gauss's law holds only for situations involvin...   \n",
       "6   The dimension of an object in a CW complex is ...   \n",
       "7   The blocking temperature of an antiferromagnet...   \n",
       "8                                        Blueshifting   \n",
       "9   Basis statements called axioms form the founda...   \n",
       "\n",
       "                                                    B  \\\n",
       "id                                                      \n",
       "0   MOND is a theory that increases the discrepanc...   \n",
       "1   Dynamic scaling refers to the non-evolution of...   \n",
       "2   The triskeles symbol is a representation of th...   \n",
       "3   Regularizing the mass-energy of an electron wi...   \n",
       "4   The angular spacing of features in the diffrac...   \n",
       "5   Gauss's law holds in all cases, but it is most...   \n",
       "6   The dimension of an object in a CW complex is ...   \n",
       "7   The blocking temperature of an antiferromagnet...   \n",
       "8                                         Redshifting   \n",
       "9   Axioms are supplementary statements added to a...   \n",
       "\n",
       "                                                    C  \\\n",
       "id                                                      \n",
       "0   MOND is a theory that explains the missing bar...   \n",
       "1   Dynamic scaling refers to the evolution of sel...   \n",
       "2   The triskeles symbol is a representation of a ...   \n",
       "3   Regularizing the mass-energy of an electron wi...   \n",
       "4   The angular spacing of features in the diffrac...   \n",
       "5   Gauss's law, which applies equally to all elec...   \n",
       "6   The dimension of an object in a CW complex is ...   \n",
       "7   The blocking temperature of an antiferromagnet...   \n",
       "8                                           Reddening   \n",
       "9   Axioms are redundant statements that can be de...   \n",
       "\n",
       "                                                    D  \\\n",
       "id                                                      \n",
       "0   MOND is a theory that reduces the discrepancy ...   \n",
       "1   Dynamic scaling refers to the non-evolution of...   \n",
       "2   The triskeles symbol represents three interloc...   \n",
       "3   Regularizing the mass-energy of an electron wi...   \n",
       "4   The angular spacing of features in the diffrac...   \n",
       "5   Gauss's law only holds for electric fields wit...   \n",
       "6   The dimension of an object in a CW complex is ...   \n",
       "7   The blocking temperature of an antiferromagnet...   \n",
       "8                                           Whitening   \n",
       "9   The axioms in a theory are used for experiment...   \n",
       "\n",
       "                                                    E  \\\n",
       "id                                                      \n",
       "0   MOND is a theory that eliminates the observed ...   \n",
       "1   Dynamic scaling refers to the evolution of sel...   \n",
       "2   The triskeles symbol is a representation of th...   \n",
       "3   Regularizing the mass-energy of an electron wi...   \n",
       "4   The angular spacing of features in the diffrac...   \n",
       "5   Gauss's law, which holds for all situations, i...   \n",
       "6   The dimension of an object in a CW complex dep...   \n",
       "7   The blocking temperature of an antiferromagnet...   \n",
       "8                                           Yellowing   \n",
       "9   The axioms in a formal theory are added to pro...   \n",
       "\n",
       "                                              context  \n",
       "id                                                     \n",
       "0   -MOND is an example of a class of theories kno...  \n",
       "1   -In such systems we can define a certain time-...  \n",
       "2   -Classical Antiquity The triskeles proper, com...  \n",
       "3   -Regularization: Classical physics theory brea...  \n",
       "4   -Several qualitative observations can be made ...  \n",
       "5   -While the electric flux is not affected by ch...  \n",
       "6   -An inductive dimension may be defined inducti...  \n",
       "7   -Magnetic blocking temperature The so-called m...  \n",
       "8   -The interactions and phenomena summarized in ...  \n",
       "9   -Thus, an axiom is an elementary basis for a f...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d23ccf5-7e84-4248-b739-27774031c271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed \"missing baryonic mass\" discrepancy in galaxy clusters?\n",
      "-MOND is an example of a class of theories known as modified gravity, and is an alternative to the hypothesis that the dynamics of galaxies are determined by massive, invisible dark matter halos. Since Milgrom's original proposal, proponents of MOND have claimed to successfully predict a variety of galactic phenomena that they state are difficult to understand as consequences of dark matter.Though MOND explains the anomalously great rotational velocities of galaxies at their perimeters, it does not fully explain the velocity dispersions of individual galaxies within galaxy clusters. MOND reduces the discrepancy between the velocity dispersions and clusters' observed missing baryonic mass from a factor of around 10 to a factor of about 2. However, the residual discrepancy cannot be accounted for by MOND, requiring that other explanations close the gap such as the presence of as-yet undetected missing baryonic matter.The accurate measurement of the speed of gravitational waves compared to the speed of light in 2017 ruled out a certain class of modified gravity theories but concluded that other MOND theories that dispense with the need for dark matter remained viable. Two years later, theories put forth by Constantinos Skordis and Tom Zlosnik were consistent with gravitational waves that always travel at the speed of light. Later still in 2021, Skordis and Zlosnik developed a subclass of their theory called \"RMOND\", for \"relativistic MOND\", which had \"been shown to reproduce in great detail the main observations in cosmology, including the cosmic-microwave-background power spectrum, and the matter structure power spectrum.\" \n",
      "-Modified Newtonian dynamics (MOND) is a hypothesis that proposes a modification of Newton's law of universal gravitation to account for observed properties of galaxies. It is an alternative to the hypothesis of dark matter in terms of explaining why galaxies do not appear to obey the currently understood laws of physics.\n",
      "-MOND Modified Newtonian Dynamics (MOND) is a relatively modern proposal to explain the galaxy rotation problem based on a variation of Newton's Second Law of Dynamics at low accelerations. This would produce a large-scale variation of Newton's universal theory of gravity. A modification of Newton's theory would also imply a modification of general relativistic cosmology in as much as Newtonian cosmology is the limit of Friedman cosmology. While almost all astrophysicists today reject MOND in favor of dark matter, a small number of researchers continue to enhance it, recently incorporating Brans–Dicke theories into treatments that attempt to account for cosmological observations.\n",
      "-There have been a number of attempts to solve the problem of galaxy rotation by modifying gravity without invoking dark matter. One of the most discussed is modified Newtonian dynamics (MOND), originally proposed by Mordehai Milgrom in 1983, which modifies the Newtonian force law at low accelerations to enhance the effective gravitational attraction. MOND has had a considerable amount of success in predicting the rotation curves of low-surface-brightness galaxies, matching the baryonic Tully–Fisher relation, and the velocity dispersions of the small satellite galaxies of the Local Group.Using data from the Spitzer Photometry and Accurate Rotation Curves (SPARC) database, a group has found that the radial acceleration traced by rotation curves could be predicted just from the observed baryon distribution (that is, including stars and gas but not dark matter). The same relation provided a good fit for 2693 samples in 153 rotating galaxies, with diverse shapes, masses, sizes, and gas fractions. Brightness in the near infrared, where the more stable light from red giants dominates, was used to estimate the density contribution due to stars more consistently. The results are consistent with MOND, and place limits on alternative explanations involving dark matter alone. However, cosmological simulations within a Lambda-CDM framework that include baryonic feedback effects reproduce the same relation, without the need to invoke new dynamics (such as MOND). Thus, a contribution due to dark matter itself can be fully predictable from that of the baryons, once the feedback effects due to the dissipative collapse of baryons are taken into account. MOND is not a relativistic theory, although relativistic theories which reduce to MOND have been proposed, such as tensor–vector–scalar gravity (TeVeS), scalar–tensor–vector gravity (STVG), the f(R) theory of Capozziello and De Laurentis, not to mention a version of Superfluid Vacuum theory based on the Logarithmic Schrödinger equation.A model of galaxy based on a general relativity metric was also proposed, showing that the rotation curves for the Milky Way, NGC 3031, NGC 3198 and NGC 7331 are consistent with the mass density distributions of the visible matter, avoiding the need for a massive halo of exotic dark matter.According to a 2020 analysis of the data produced by the Gaia spacecraft, it would seem possible to explain at least the Milky Way's rotation curve without requiring any dark matter if instead of a Newtonian approximation the entire set of equations of general relativity is adopted.In March 2021, Gerson Otto Ludwig published a model based on general relativity that explains galaxy rotation curves with gravitoelectromagnetism.\n",
      "-Outstanding problems for MOND The most serious problem facing Milgrom's law is that it cannot eliminate the need for dark matter in all astrophysical systems: galaxy clusters show a residual mass discrepancy even when analyzed using MOND. The fact that some form of unseen mass must exist in these systems detracts from the adequacy of MOND as a solution to the missing mass problem, although the amount of extra mass required is a fifth that of a Newtonian analysis, and there is no requirement that the missing mass be non-baryonic. It has been speculated that 2 eV neutrinos could account for the cluster observations in MOND while preserving the hypothesis's successes at the galaxy scale. Indeed, analysis of sharp lensing data for the galaxy cluster Abell 1689 shows that MOND only becomes distinctive at Mpc distance from the center, so that Zwicky's conundrum remains, and 1.8 eV neutrinos are needed in clusters.The 2006 observation of a pair of colliding galaxy clusters known as the \"Bullet Cluster\", poses a significant challenge for all theories proposing a modified gravity solution to the missing mass problem, including MOND. Astronomers measured the distribution of stellar and gas mass in the clusters using visible and X-ray light, respectively, and in addition mapped the inferred dark matter density using gravitational lensing. In MOND, one would expect the \"missing mass\" to be centred on regions of visible mass which experience accelerations lower than a0 (assuming the external field effect is negligible). In ΛCDM, on the other hand, one would expect the dark matter to be significantly offset from the visible mass because the halos of the two colliding clusters would pass through each other (assuming, as is conventional, that dark matter is collisionless), whilst the cluster gas would interact and end up at the centre. An offset is clearly seen in the observations. It has been suggested, however, that MOND-based models may be able to generate such an offset in strongly non-spherically symmetric systems, such as the Bullet Cluster.A significant piece of evidence in favor of standard dark matter is the observed anisotropies in the cosmic microwave background. While ΛCDM is able to explain the observed angular power spectrum, MOND has a much harder time, though recently it has been shown that MOND can fit the observations too. MOND also encounters difficulties explaining structure formation, with density perturbations in MOND perhaps growing so rapidly that too much structure is formed by the present epoch. However, forming galaxies more rapidly than in ΛCDM can be a good thing to some extent.Several other studies have noted observational difficulties with MOND. For example, it has been claimed that MOND offers a poor fit to the velocity dispersion profile of globular clusters and the temperature profile of galaxy clusters, that different values of a0 are required for agreement with different galaxies' rotation curves, and that MOND is naturally unsuited to forming the basis of cosmology. Furthermore, many versions of MOND predict that the speed of light is different from the speed of gravity, but in 2017 the speed of gravitational waves was measured to be equal to the speed of light to high precision. This is well understood in modern relativistic theories of MOND, with the constraint from gravitational waves actually helping by substantially restricting how a covariant theory might be constructed.Besides these observational issues, MOND and its relativistic generalizations are plagued by theoretical difficulties. Several ad hoc and inelegant additions to general relativity are required to create a theory compatible with a non-Newtonian non-relativistic limit, though the predictions in this limit are rather clear. This is the case for the more commonly used modified gravity versions of MOND, but some formulations (most prominently those based on modified inertia) have long suffered from poor compatibility with cherished physical principles such as conservation laws. Researchers working on MOND generally do not interpret it as a modification of inertia, with only very limited work done on this area.\n",
      "\n",
      "\n",
      "\n",
      "Which of the following is an accurate definition of dynamic scaling in self-similar systems?\n",
      "-In such systems we can define a certain time-dependent stochastic variable  x . We are interested in computing the probability distribution of  x at various instants of time i.e.  f(x,t) . The numerical value of  f and the typical or mean value of  x generally changes over time. The question is: what happens to the corresponding dimensionless variables? If the numerical values of the dimensional quantities change, but corresponding dimensionless quantities remain invariant then we can argue that snapshots of the system at different times are similar. When this happens we say that the system is self-similar.  One way of verifying dynamic scaling is to plot dimensionless variables  f/tθ as a function of  x/tz of the data extracted at various different time. Then if all the plots of  f vs  x obtained at different times collapse onto a single universal curve then it is said that the systems at different time are similar and it obeys dynamic scaling. The idea of data collapse is deeply rooted to the Buckingham Pi theorem. Essentially such systems can be termed as temporal self-similarity since the same system is similar at different times.\n",
      "-Dynamic scaling (sometimes known as Family-Vicsek scaling) is a litmus test that shows whether an evolving system exhibits self-similarity. In general a function is said to exhibit dynamic scaling if it satisfies: f(x,t)∼tθφ(xtz).\n",
      "-In computer networks, self-similarity is a feature of network data transfer dynamics. When modeling network data dynamics the traditional time series models, such as an autoregressive moving average model are not appropriate. This is because these models only provide a finite number of parameters in the model and thus interaction in a finite time window, but the network data usually have a long-range dependent temporal structure. A self-similar process is one way of modeling network data dynamics with such a long range correlation. This article defines and describes network data transfer dynamics in the context of a self-similar process. Properties of the process are shown and methods are given for graphing and estimating parameters modeling the self-similarity of network data.\n",
      "-Many phenomena investigated by physicists are not static but evolve probabilistically with time (i.e. Stochastic process). The universe itself is perhaps one of the best examples. It has been expanding ever since the Big Bang. Similarly, growth of networks like the Internet are also ever growing systems. Another example is polymer degradation where degradation does not occur in a blink of an eye but rather over quite a long time. Spread of biological and computer viruses too does not happen over night.  Many other seemingly disparate systems which are found to exhibit dynamic scaling. For example: kinetics of aggregation described by Smoluchowski coagulation equation,  complex networks described by Barabasi–Albert model, the kinetic and stochastic Cantor set, the growth model within the Kardar–Parisi–Zhang (KPZ) universality class; one find that the width of the surface  W(L,t) exhibits dynamic scaling.\n",
      "-Time-varying networks are inherently dynamic, and used for modeling spreading processes on networks. Whether using time-varying networks will be worth the added complexity depends on the relative time scales in question. Time-varying networks are most useful in describing systems where the spreading process on a network and the network itself evolve at similar timescales.Let the characteristic timescale for the evolution of the network be  tN , and the characteristic timescale for the evolution of the spreading process be  tP . A process on a network will fall into one of three categories: Static approximation – where  tN≫tP . The network evolves relatively slowly, so the dynamics of the process can be approximated using a static version of the network.\n",
      "\n",
      "\n",
      "\n",
      "Which of the following statements accurately describes the origin and significance of the triskeles symbol?\n",
      "-Classical Antiquity The triskeles proper, composed of three human legs, is younger than the triple spiral, found in decorations on Greek pottery especially as a design shown on hoplite shields, and later also minted on Greek and Anatolian coinage.\n",
      "An early example is found on the shield of Achilles in an Attic hydria of the late 6th century BCE.\n",
      "It is found on coinage in Lycia, and on staters of Pamphylia (at Aspendos, 370–333 BCE) and Pisidia. The meaning of the Greek triskeles is not recorded directly.\n",
      "The Duc de Luynes in his 1835 study noted the co-occurrence of the symbol with the eagle, the cockerel, the head of Medusa, Perseus, three crescent moons, three ears of corn, and three grains of corn.\n",
      "From this, he reconstructed feminine divine triad which he identified with the \"triple goddess\" Hecate.\n",
      "The triskeles was adopted as emblem by the rulers of Syracuse. It is possible that this usage is related with the Greek name of the island of Sicily, Trinacria (Τρινακρία 'having three headlands').\n",
      "The Sicilian triskeles is shown with the head of Medusa at the center.\n",
      "The ancient symbol has been re-introduced in modern flags of Sicily since 1848. The oldest find of a triskeles in Sicily is a vase dated to 700 BCE, for which researchers assume a Minoan-Mycenaean origin.\n",
      "Roman period and Late Antiquity Late examples of the triple spiral symbols are found in Iron Age Europe, e.g. carved in rock in Castro Culture settlement in Galicia, Asturias and Northern Portugal.\n",
      "In Ireland before the 5th century, in Celtic Christianity the symbol took on new meaning, as a symbol of the Trinity (Father, Son, and Holy Spirit).\n",
      "-A triskelion or triskeles is an ancient motif consisting of a triple spiral exhibiting rotational symmetry or other patterns in triplicate that emanate from a common center.\n",
      "The spiral design can be based on interlocking Archimedean spirals, or represent three bent human legs. It is found in artifacts of the European Neolithic and Bronze Age with continuation into the Iron Age especially in the context of the La Tène culture and related Celtic traditions.\n",
      "The actual triskeles symbol of three human legs is found especially in Greek antiquity, beginning in archaic pottery and continued in coinage of the classical period.\n",
      "In the Hellenistic period, the symbol becomes associated with the island of Sicily, appearing on coins minted under Dionysius I of Syracuse beginning in c. 382 BCE.\n",
      "It later appears in heraldry, and, other than in the flag of Sicily, came to be used in the flag of the Isle of Man (known as ny tree cassyn 'the three legs').Greek τρισκελής (triskelḗs) means 'three-legged'.\n",
      "While the Greek adjective τρισκελής 'three-legged (e.g., of a table)' is ancient, use of the term for the symbol is modern, introduced in 1835 by Honoré Théodoric d'Albert de Luynes as French triskèle, and adopted in the spelling triskeles following Otto Olshausen (1886).\n",
      "The form triskelion (as it were Greek τρισκέλιον) is a diminutive which entered English usage in numismatics in the late 19th century.\n",
      "The form consisting of three human legs (as opposed to the triple spiral) has also been called a \"triquetra of legs\", also triskelos or triskel.\n",
      "-The triskeles was included in the design of the Army Gold Medal awarded to British Army majors and above who had taken a key part in the Battle of Maida (1806).\n",
      "An early flag of Sicily, proposed in 1848, included the Sicilian triskeles or \"Trinacria symbol\".\n",
      "Later versions of Sicilian flags have retained the emblem, including the one officially adopted in 2000.\n",
      "The Flag of the Isle of Man (1932) shows a heraldic design of a triskeles of three armoured legs.\n",
      "-The spiral and triple spiral motif is a Neolithic symbol in Europe (Megalithic Temples of Malta). The Celtic symbol the triple spiral is in fact a pre-Celtic symbol. It is carved into the rock of a stone lozenge near the main entrance of the prehistoric Newgrange monument in County Meath, Ireland. Newgrange was built around 3200 BCE predating the Celts and the triple spirals were carved at least 2,500 years before the Celts reached Ireland but has long since been incorporated into Celtic culture. The triskelion symbol, consisting of three interlocked spirals or three bent human legs, appears in many early cultures, including Mycenaean vessels, on coinage in Lycia, on staters of Pamphylia (at Aspendos, 370–333 BC) and Pisidia, as well as on the heraldic emblem on warriors' shields depicted on Greek pottery.Spirals can be found throughout pre-Columbian art in Latin and Central America. The more than 1,400 petroglyphs (rock engravings) in Las Plazuelas, Guanajuato Mexico, dating 750-1200 AD, predominantly depict spirals, dot figures and scale models. In Colombia monkeys, frog and lizard like figures depicted in petroglyphs or as gold offering figures frequently includes spirals, for example on the palms of hands. In Lower Central America spirals along with circles, wavy lines, crosses and points are universal petroglyphs characters. Spirals can also be found among the Nazca Lines in the coastal desert of Peru, dating from 200 BC to 500 AD. The geoglyphs number in the thousands and depict animals, plants and geometric motifs, including spirals.Spiral shapes, including the swastika, triskele, etc., have often been interpreted as solar symbols.\n",
      "-In the Bavarian town of Füssen, Germany the flag and coat of arms of the town contains a triskele, as does the flag of the Russian autonomous region of Ust-Orda Buryat Okrug.The spiral is used by some polytheistic reconstructionist or neopagan groups. As a \"Celtic symbol\", it is used primarily by groups with a Celtic cultural orientation and, less frequently, can also be found in use by various eclectic or syncretic traditions such as Neopaganism. The spiral triskele is one of the primary symbols of Celtic Reconstructionist Paganism, used to represent a variety of triplicities in cosmology and theology; it is also a favored symbol due to its association with the god Manannán mac Lir.Other uses of triskelion-like emblems include the logo for the Trisquel Linux distribution and the seal of the United States Department of Transportation.A specific version of the triskele featuring a conglomeration composed of three sevens has been adopted by neo-nazis, having been originally used (although without the sevens) by the 27th SS Volunteer Division Langemarck on their shoulder strap. In South Africa the Afrikaner Weerstandsbeweging (AWB), an Afrikaner nationalist, neo-Nazi organization and political party (founded 1973), uses a triskele composed of three sevens as its symbol in place of a swastika. The Blood & Honour group also utilises it. Usage of the symbol can be a prosecutable offence under German law, depending on the context in which the symbol is used.\n",
      "\n",
      "\n",
      "\n",
      "What is the significance of regularization in terms of renormalization problems in physics?\n",
      "-Regularization: Classical physics theory breaks down at small scales, e.g., the difference between an electron and a point particle shown above. Addressing this problem requires new kinds of additional physical constraints. For instance, in this case, assuming a finite electron radius (i.e., regularizing the electron mass-energy) suffices to explain the system below a certain size. Similar regularization arguments work in other renormalization problems. For example, a theory may hold under one narrow set of conditions, but due to calculations involving infinities or singularities, it may breakdown under other conditions or scales. In the case of the electron, another way to avoid infinite mass-energy while retaining the point nature of the particle is to postulate tiny additional dimensions over which the particle could 'spread out' rather than restrict its motion solely over 3D space. This is precisely the motivation behind string theory and other multi-dimensional models including multiple time dimensions. Rather than the existence of unknown new physics, assuming the existence of particle interactions with other surrounding particles in the environment, renormalization offers an alternatively strategy to resolve infinities in such classical problems.\n",
      "-However, the result usually includes terms proportional to expressions like  1/ϵ which are not well-defined in the limit  ϵ→0 . Regularization is the first step towards obtaining a completely finite and meaningful result; in quantum field theory it must be usually followed by a related, but independent technique called renormalization. Renormalization is based on the requirement that some physical quantities — expressed by seemingly divergent expressions such as  1/ϵ — are equal to the observed values. Such a constraint allows one to calculate a finite value for many other quantities that looked divergent.\n",
      "-Renormalization is distinct from regularization, another technique to control infinities by assuming the existence of new unknown physics at new scales.\n",
      "-Conceptual problem Perturbative predictions by quantum field theory about quantum scattering of elementary particles, implied by a corresponding Lagrangian density, are computed using the Feynman rules, a regularization method to circumvent ultraviolet divergences so as to obtain finite results for Feynman diagrams containing loops, and a renormalization scheme. Regularization method results in regularized n-point Green's functions (propagators), and a suitable limiting procedure (a renormalization scheme) then leads to perturbative S-matrix elements. These are independent of the particular regularization method used, and enable one to model perturbatively the measurable physical processes (cross sections, probability amplitudes, decay widths and lifetimes of excited states). However, so far no known regularized n-point Green's functions can be regarded as being based on a physically realistic theory of quantum-scattering since the derivation of each disregards some of the basic tenets of conventional physics (e.g., by not being Lorentz-invariant, by introducing either unphysical particles with a negative metric or wrong statistics, or discrete space-time, or lowering the dimensionality of space-time, or some combination thereof). So the available regularization methods are understood as formalistic technical devices, devoid of any direct physical meaning. In addition, there are qualms about renormalization. For a history and comments on this more than half-a-century old open conceptual problem, see e.g.\n",
      "-Regularization The metrics is often called image energy; people usually add energy that comes from mechanics assumptions as the Laplacian of displacement (a special case of Tikhonov regularization ) or even finite element problems. As one decided not to solve the Gauss-Newton problem for most of cases this solution is far from being CPU efficient. Cachier et al. demonstrated that the problem of minimizing image and mechanical energy can be reformulated in solving the energy image then applying a Gaussian filter at each iteration. We use this strategy in Yadics and we add the median filter as it is massively used in PIV. One notes that the median filter avoids local minima while preserving discontinuities.\n",
      "\n",
      "\n",
      "\n",
      "Which of the following statements accurately describes the relationship between the dimensions of a diffracting object and the angular spacing of features in the diffraction pattern?\n",
      "-Several qualitative observations can be made of diffraction in general: The angular spacing of the features in the diffraction pattern is inversely proportional to the dimensions of the object causing the diffraction. In other words: the smaller the diffracting object, the wider the resulting diffraction pattern, and vice versa. (More precisely, this is true of the sines of the angles.) The diffraction angles are invariant under scaling; that is, they depend only on the ratio of the wavelength to the size of the diffracting object.\n",
      "-Several qualitative observations can be made of diffraction in general: The angular spacing of the features in the diffraction pattern is inversely proportional to the dimensions of the object causing the diffraction. In other words: The smaller the diffracting object, the 'wider' the resulting diffraction pattern, and vice versa. (More precisely, this is true of the sines of the angles.) The diffraction angles are invariant under scaling; that is, they depend only on the ratio of the wavelength to the size of the diffracting object.\n",
      "-A grating whose elements are separated by S diffracts a normally incident beam of light into a set of beams, at angles θn given by: This is known as the grating equation. The finer the grating spacing, the greater the angular separation of the diffracted beams.\n",
      "If the light is incident at an angle θ0, the grating equation is: The detailed structure of the repeating pattern determines the form of the individual diffracted beams, as well as their relative intensity while the grating spacing always determines the angles of the diffracted beams.\n",
      "The image on the right shows a laser beam diffracted by a grating into n = 0, and ±1 beams. The angles of the first order beams are about 20°; if we assume the wavelength of the laser beam is 600 nm, we can infer that the grating spacing is about 1.8 μm.\n",
      "-In practice, all slits are of finite size so produce diffraction on the both transverse directions, along the x (width W defined) and y (height H defined) axes. If the height H of the slit is much greater than its width W, then the spacing of the vertical (along the height or the y axis) diffraction fringes is much less than the spacing of the horizontal (along the width or x axis) fringes. If the vertical fringe spacing is so less by a relatively so large H, then the observation of the vertical fringes is so hard that a person observing the diffracted wave intensity pattern on the plane of observation or the image plane recognizes only the horizontal fringes with their narrow height. This is the reason why a height-long slit or slit array such as a diffraction grating is typically analyzed only in the dimension along the width. If the illuminating beam does not illuminate the whole height of the slit, then the spacing of the vertical fringes is determined by the dimension of the laser beam along the slit height. Close examination of the two-slit pattern below shows that there are very fine vertical diffraction fringes above and below the main spots, as well as the more obvious horizontal fringes.\n",
      "-The interference fringe maxima occur at angles where λ is the wavelength of the light. The angular spacing of the fringes is given by When the distance between the slits and the viewing plane is z, the spacing of the fringes is equal to zθ and is the same as above: Diffraction by a grating A grating is defined in Born and Wolf as \"any arrangement which imposes on an incident wave a periodic variation of amplitude or phase, or both\".\n",
      "\n",
      "\n",
      "\n",
      "Which of the following statements accurately depicts the relationship between Gauss's law, electric flux, electric field, and symmetry in electric fields?\n",
      "-While the electric flux is not affected by charges that are not within the closed surface, the net electric field, E can be affected by charges that lie outside the closed surface. While Gauss's law holds for all situations, it is most useful for \"by hand\" calculations when high degrees of symmetry exist in the electric field. Examples include spherical and cylindrical symmetry.\n",
      "-For a closed Gaussian surface, electric flux is given by: where E is the electric field, S is any closed surface, Q is the total electric charge inside the surface S, ε0 is the electric constant (a universal constant, also called the \"permittivity of free space\") (ε0 ≈ 8.854187817×10−12 F/m)This relation is known as Gauss' law for electric fields in its integral form and it is one of Maxwell's equations.\n",
      "-In terms of fields of force Gauss's theorem can be interpreted in terms of the lines of force of the field as follows: The flux through a closed surface is dependent upon both the magnitude and direction of the electric field lines penetrating the surface. In general a positive flux is defined by these lines leaving the surface and negative flux by lines entering this surface. This results in positive charges causing a positive flux and negative charges creating a negative flux. These electric field lines will extend to infinity decreasing in strength by a factor of one over the distance from the source of the charge squared. The larger the number of field lines emanating from a charge the larger the magnitude of the charge is, and the closer together the field lines are the greater the magnitude of the electric field. This has the natural result of the electric field becoming weaker as one moves away from a charged particle, but the surface area also increases so that the net electric field exiting this particle will stay the same. In other words the closed integral of the electric field and the dot product of the derivative of the area will equal the net charge enclosed divided by permittivity of free space.\n",
      "-Integral form Gauss's law may be expressed as: where ΦE is the electric flux through a closed surface S enclosing any volume V, Q is the total charge enclosed within V, and ε0 is the electric constant. The electric flux ΦE is defined as a surface integral of the electric field: ΦE= S E⋅dA where E is the electric field, dA is a vector representing an infinitesimal element of area of the surface, and · represents the dot product of two vectors.\n",
      "-By way of contrast, Gauss's law for electric fields, another of Maxwell's equations, is ΦE= S E⋅dS=Qε0 where E is the electric field, S is any closed surface, Q is the total electric charge inside the surface S, ε0 is the electric constant (a universal constant, also called the \"permittivity of free space\").The flux of E through a closed surface is not always zero; this indicates the presence of \"electric monopoles\", that is, free positive or negative charges.\n",
      "\n",
      "\n",
      "\n",
      "Which of the following statements accurately describes the dimension of an object in a CW complex?\n",
      "-An inductive dimension may be defined inductively as follows. Consider a discrete set of points (such as a finite collection of points) to be 0-dimensional. By dragging a 0-dimensional object in some direction, one obtains a 1-dimensional object. By dragging a 1-dimensional object in a new direction, one obtains a 2-dimensional object. In general one obtains an (n + 1)-dimensional object by dragging an n-dimensional object in a new direction. The inductive dimension of a topological space may refer to the small inductive dimension or the large inductive dimension, and is based on the analogy that, in the case of metric spaces, (n + 1)-dimensional balls have n-dimensional boundaries, permitting an inductive definition based on the dimension of the boundaries of open sets. Moreover, the boundary of a discrete set of points is the empty set, and therefore the empty set can be taken to have dimension -1.Similarly, for the class of CW complexes, the dimension of an object is the largest n for which the n-skeleton is nontrivial. Intuitively, this can be described as follows: if the original space can be continuously deformed into a collection of higher-dimensional triangles joined at their faces with a complicated surface, then the dimension of the object is the dimension of those triangles.\n",
      "-A 1-dimensional CW complex is constructed by taking the disjoint union of a 0-dimensional CW complex with one or more copies of the unit interval. For each copy, there is a map that \"glues\" its boundary (its two endpoints) to elements of the 0-dimensional complex (the points). The topology of the CW complex is the topology of the quotient space defined by these gluing maps.\n",
      "-The construction, in words The CW complex construction is a straightforward generalization of the following process: A 0-dimensional CW complex is just a set of zero or more discrete points (with the discrete topology).\n",
      "-The first analysis of contraction hierarchy performance relies in part on a quantity known as the highway dimension. While the definition of this quantity is technical, intuitively a graph has a small highway dimension if for every  r>0 there is a sparse set of vertices  Sr such that every shortest path of length greater than  r includes a vertex from  Sr . Calculating the exact value of the highway dimension is NP-hard and W[1]-hard, but for grids it is known that the highway dimension  h∈Θ(n) .An alternative analysis was presented in the Customizable Contraction Hierarchy line of work. Query running times can be bounded by  O(td2) . As the tree-depth can be bounded in terms of the tree-width,  log ⁡n)2) is also a valid upper bound. The main source is but the consequences for the worst case running times are better detailed in.\n",
      "-In the notation of topological space operators, the matrix elements can be expressed also as The dimension of empty sets (∅) are denoted as −1 or F (false). The dimension of non-empty sets (¬∅) are denoted with the maximum number of dimensions of the intersection, specifically 0 for points, 1 for lines, 2 for areas. Then, the domain of the model is {0,1,2,F}.\n",
      "\n",
      "\n",
      "\n",
      "Which of the following statements accurately describes the blocking temperature of an antiferromagnetic layer in a spin valve?\n",
      "-Magnetic blocking temperature The so-called magnetic blocking temperature, TB, is defined as the temperature below which the relaxation of the magnetization becomes slow compared to the time scale of a particular investigation technique. Historically, the blocking temperature for single-molecule magnets has been defined as the temperature at which the molecule's magnetic relaxation time, τ, is 100 seconds. This definition is the current standard for comparison of single-molecule magnet properties, but otherwise is not technologically significant. There is typically a correlation between increasing an SMM's blocking temperature and energy barrier. The average blocking temperature for SMMs is 4K. Dy-metallocenium salts are the most recent SMM to achieve the highest temperature of magnetic hysteresis, greater than that of liquid nitrogen.\n",
      "-Antiferromagnets can couple to ferromagnets, for instance, through a mechanism known as exchange bias, in which the ferromagnetic film is either grown upon the antiferromagnet or annealed in an aligning magnetic field, causing the surface atoms of the ferromagnet to align with the surface atoms of the antiferromagnet. This provides the ability to \"pin\" the orientation of a ferromagnetic film, which provides one of the main uses in so-called spin valves, which are the basis of magnetic sensors including modern hard disk drive read heads. The temperature at or above which an antiferromagnetic layer loses its ability to \"pin\" the magnetization direction of an adjacent ferromagnetic layer is called the blocking temperature of that layer and is usually lower than the Néel temperature.\n",
      "-Equivalently, blocking temperature is the temperature below which a material shows slow relaxation of magnetization.\n",
      "-Blocking temperature The reason the characteristics of the field are conserved comes from the concept of blocking temperature (also known as closure temperature in geochronology). This temperature is where the system becomes blocked against thermal agitation at lower temperatures. Therefore, some minerals exhibit remnant magnetization. One problem that arises in the determination of remnant (or fossil) magnetization is that if the temperature rises above this point, the magnetic history is destroyed. However, in theory it should be possible to relate the magnetic blocking temperature to the isotopic closure temperature, such that it could be checked whether or not a sample can be used.\n",
      "-The state of the nanoparticle (superparamagnetic or blocked) depends on the measurement time. A transition between superparamagnetism and blocked state occurs when  τm=τN . In several experiments, the measurement time is kept constant but the temperature is varied, so the transition between superparamagnetism and blocked state is seen as a function of the temperature. The temperature for which  τm=τN is called the blocking temperature: ln ⁡(τmτ0) For typical laboratory measurements, the value of the logarithm in the previous equation is in the order of 20–25.\n",
      "\n",
      "\n",
      "\n",
      "What is the term used in astrophysics to describe light-matter interactions resulting in energy shifts in the radiation field?\n",
      "-The interactions and phenomena summarized in the subjects of radiative transfer and physical optics can result in shifts in the wavelength and frequency of electromagnetic radiation. In such cases, the shifts correspond to a physical energy transfer to matter or other photons rather than being by a transformation between reference frames. Such shifts can be from such physical phenomena as coherence effects or the scattering of electromagnetic radiation whether from charged elementary particles, from particulates, or from fluctuations of the index of refraction in a dielectric medium as occurs in the radio phenomenon of radio whistlers. While such phenomena are sometimes referred to as \"redshifts\" and \"blueshifts\", in astrophysics light-matter interactions that result in energy shifts in the radiation field are generally referred to as \"reddening\" rather than \"redshifting\" which, as a term, is normally reserved for the effects discussed above.In many circumstances scattering causes radiation to redden because entropy results in the predominance of many low-energy photons over few high-energy ones (while conserving total energy). Except possibly under carefully controlled conditions, scattering does not produce the same relative change in wavelength across the whole spectrum; that is, any calculated z is generally a function of wavelength. Furthermore, scattering from random media generally occurs at many angles, and z is a function of the scattering angle. If multiple scattering occurs, or the scattering particles have relative motion, then there is generally distortion of spectral lines as well.In interstellar astronomy, visible spectra can appear redder due to scattering processes in a phenomenon referred to as interstellar reddening—similarly Rayleigh scattering causes the atmospheric reddening of the Sun seen in the sunrise or sunset and causes the rest of the sky to have a blue color. This phenomenon is distinct from redshifting because the spectroscopic lines are not shifted to other wavelengths in reddened objects and there is an additional dimming and distortion associated with the phenomenon due to photons being scattered in and out of the line of sight.\n",
      "-When a photon is the incident particle, there is an inelastic scattering process called Raman scattering. In this scattering process, the incident photon interacts with matter (gas, liquid, and solid) and the frequency of the photon is shifted towards red or blue. A red shift can be observed when part of the energy of the photon is transferred to the interacting matter, where it adds to its internal energy in a process called Stokes Raman scattering. The blue shift can be observed when internal energy of the matter is transferred to the photon; this process is called anti-Stokes Raman scattering.\n",
      "-The term calorescence is rarely seen in use today, whereas the term fluorescence is common. One reason is that there isn't a physical explanation for calorescence that's specific to calorescence. Relatedly, the physical explanations for some types of fluorescence behavior are also explanations for calorescence and the word fluorescence has been preferred and expanded in customary usage to include calorescence. Another reason is that there isn't a widely used practical application attached to the word calorescence, whereas there is for fluorescence. A related item of physics terminology today is the so-called \"Anti-Stokes Shift\". A Stokes shift refers to molecular absorptions of radiant energy of higher frequencies followed by emissions of lower frequencies; and an anti-Stokes shift refers to absorptions of lower frequencies followed by emissions of higher frequencies. With this terminology, practical applications are attached to the term \"anti-Stokes photoluminescence\" in materials science including semiconductors (see examples). Equal terminology in use in laser science is \"infrared upconversion\", \"upconversion luminescence\", or simply \"upconversion\" (see examples). This terminology is usually contemplating luminescence, as opposed to incandescence, whereas the word Calorescence belongs to the 19th century when the only known upconversion methods were of the incandescent kind.\n",
      "-Dark radiation (also dark electromagnetism) is a postulated type of radiation that mediates interactions of dark matter.\n",
      "By analogy to the way photons mediate electromagnetic interactions between particles in the Standard Model (called baryonic matter in cosmology), dark radiation is proposed to mediate interactions between dark matter particles. Similar to dark matter particles, the hypothetical dark radiation does not interact with Standard Model particles.\n",
      "-Photons have the largest range of energy and central in a variety of energy conversions. Photons interact with electric and magnetic entities. For example, electric dipole which in turn are excited by optical phonons or fluid particle vibration, or transition dipole moments of electronic transitions. In heat transfer physics, the interaction kinetics of phonon is treated using the perturbation theory (the Fermi golden rule) and the interaction Hamiltonian. The photon-electron interaction is where pe is the dipole moment vector and a† and a are the creation and annihilation of internal motion of electron. Photons also participate in ternary interactions, e.g., phonon-assisted photon absorption/emission (transition of electron energy level). The vibrational mode in fluid particles can decay or become excited by emitting or absorbing photons. Examples are solid and molecular gas laser cooling.Using ab initio calculations based on the first principles along with EM theory, various radiative properties such as dielectric function (electrical permittivity, εe,ω), spectral absorption coefficient (σph,ω), and the complex refraction index (mω), are calculated for various interactions between photons and electric/magnetic entities in matter. For example, the imaginary part (εe,c,ω) of complex dielectric function (εe,ω = εe,r,ω + i εe,c,ω) for electronic transition across a bandgap is  where V is the unit-cell volume, VB and CB denote the valence and conduction bands, wκ is the weight associated with a κ-point, and pij is the transition momentum matrix element.\n",
      "\n",
      "\n",
      "\n",
      "What is the role of axioms in a formal theory?\n",
      "-Thus, an axiom is an elementary basis for a formal logic system that together with the rules of inference define a deductive system.\n",
      "Examples This section gives examples of mathematical theories that are developed entirely from a set of non-logical axioms (axioms, henceforth). A rigorous treatment of any of these topics begins with a specification of these axioms.\n",
      "-In mathematical logic, the concepts of theorems and proofs have been formalized in order to allow mathematical reasoning about them. In this context, statements become well-formed formulas of some formal language. A theory consists of some basis statements called axioms, and some deducing rules (sometimes included in the axioms). The theorems of the theory are the statements that can be derived from the axioms by using the deducing rules. This formalization led to proof theory, which allows proving general theorems about theorems and proofs. In particular, Gödel's incompleteness theorems show that every consistent theory containing the natural numbers has true statements on natural numbers that are not theorems of the theory (that is they cannot be proved inside the theory).\n",
      "-An axiom is called independent if it can not be proved or disproved from the other axioms of the axiomatic system. An axiomatic system is said to be independent if each of its axioms is independent. If a true statement is a logical consequence of an axiomatic system, then it will be a true statement in every model of that system. To prove that an axiom is independent of the remaining axioms of the system, it is sufficient to find two models of the remaining axioms, for which the axiom is a true statement in one and a false statement in the other. Independence is not always a desirable property from a pedagogical viewpoint.\n",
      "-Axioms are statements about these primitives; for example, any two points are together incident with just one line (i.e. that for any two points, there is just one line which passes through both of them). Axioms are assumed true, and not proven. They are the building blocks of geometric concepts, since they specify the properties that the primitives have.From a given set of axioms, synthesis proceeds as a carefully constructed logical argument. When a significant result is proved rigorously, it becomes a theorem.\n",
      "-Axioms (or postulates) are statements about these primitives; for example, any two points are together incident with just one line (i.e. that for any two points, there is just one line which passes through both of them). Axioms are assumed true, and not proven. They are the building blocks of geometric concepts, since they specify the properties that the primitives have.\n",
      "\n",
      "\n",
      "\n",
      "What did Fresnel predict and verify with regards to total internal reflections?\n",
      "-Similarly, Fresnel calculated and verified the angle of incidence that would give a 90° phase difference after three reflections at the same angle, and four reflections at the same angle. In each case there were two solutions, and in each case he reported that the larger angle of incidence gave an accurate circular polarization (for an initial linear polarization at 45° to the plane of reflection). For the case of three reflections he also tested the smaller angle, but found that it gave some coloration due to the proximity of the critical angle and its slight dependence on wavelength. (Compare Fig. 2 above, which shows that the phase difference δ is more sensitive to the refractive index for smaller angles of incidence.) For added confidence, Fresnel predicted and verified that four total internal reflections at 68°27' would give an accurate circular polarization if two of the reflections had water as the external medium while the other two had air, but not if the reflecting surfaces were all wet or all dry.\n",
      "-For glass with a refractive index of 1.51, Fresnel calculated that a 45° phase difference between the two reflection coefficients (hence a 90° difference after two reflections) required an angle of incidence of 48°37' or 54°37'. He cut a rhomb to the latter angle and found that it performed as expected. Thus the specification of the Fresnel rhomb was completed. Similarly, Fresnel calculated and verified the angle of incidence that would give a 90° phase difference after three reflections at the same angle, and four reflections at the same angle. In each case there were two solutions, and in each case he reported that the larger angle of incidence gave an accurate circular polarization (for an initial linear polarization at 45° to the plane of reflection). For the case of three reflections he also tested the smaller angle, but found that it gave some coloration due to the proximity of the critical angle and its slight dependence on wavelength. (Compare Fig. 13 above, which shows that the phase difference δ is more sensitive to the refractive index for smaller angles of incidence.) For added confidence, Fresnel predicted and verified that four total internal reflections at 68°27' would give an accurate circular polarization if two of the reflections had water as the external medium while the other two had air, but not if the reflecting surfaces were all wet or all dry.Fresnel's deduction of the phase shift in TIR is thought to have been the first occasion on which a physical meaning was attached to the argument of a complex number. Although this reasoning was applied without the benefit of knowing that light waves were electromagnetic, it passed the test of experiment, and survived remarkably intact after James Clerk Maxwell changed the presumed nature of the waves. Meanwhile, Fresnel's success inspired James MacCullagh and Augustin-Louis Cauchy, beginning in 1836, to analyze reflection from metals by using the Fresnel equations with a complex refractive index. The imaginary part of the complex index represents absorption.The term critical angle, used for convenience in the above narrative, is anachronistic: it apparently dates from 1873.In the 20th century, quantum electrodynamics reinterpreted the amplitude of an electromagnetic wave in terms of the probability of finding a photon. In this framework, partial transmission and frustrated TIR concern the probability of a photon crossing a boundary, and attenuated total reflectance concerns the probability of a photon being absorbed on the other side.\n",
      "-In 1816, Fresnel offered his first attempt at a wave-based theory of chromatic polarization. Without (yet) explicitly invoking transverse waves, his theory treated the light as consisting of two perpendicularly polarized components. In 1817 he noticed that plane-polarized light seemed to be partly depolarized by total internal reflection, if initially polarized at an acute angle to the plane of incidence. By including total internal reflection in a chromatic-polarization experiment, he found that the apparently depolarized light was a mixture of components polarized parallel and perpendicular to the plane of incidence, and that the total reflection introduced a phase difference between them. Choosing an appropriate angle of incidence (not yet exactly specified) gave a phase difference of 1/8 of a cycle. Two such reflections from the \"parallel faces\" of \"two coupled prisms\" gave a phase difference of 1/4 of a cycle. In that case, if the light was initially polarized at 45° to the plane of incidence and reflection, it appeared to be completely depolarized after the two reflections. These findings were reported in a memoir submitted and read to the French Academy of Sciences in November 1817.In 1821, Fresnel derived formulae equivalent to his sine and tangent laws (Eqs. (19) and (20), above)  by modeling light waves as transverse elastic waves with vibrations perpendicular to what had previously been called the plane of polarization. Using old experimental data, he promptly confirmed that the equations correctly predicted the direction of polarization of the reflected beam when the incident beam was polarized at 45° to the plane of incidence, for light incident from air onto glass or water. The experimental confirmation was reported in a \"postscript\" to the work in which Fresnel expounded his mature theory of chromatic polarization, introducing transverse waves. Details of the derivation were given later, in a memoir read to the academy in January 1823. The derivation combined conservation of energy with continuity of the tangential vibration at the interface, but failed to allow for any condition on the normal component of vibration.Meanwhile, in a memoir submitted in December 1822, Fresnel coined the terms linear polarization, circular polarization, and elliptical polarization. For circular polarization, the two perpendicular components were a quarter-cycle (±90°) out of phase.\n",
      "-Total internal reflection alters only the mutual phase between s- and p-polarized light. Under well chosen angle of incidence, this phase is close to  π/4 .  Fresnel rhomb uses this effect to achieve conversion between circular and linear polarisation. This phase difference is not explicitly dependent on wavelength, but only on refractive index, so Fresnel rhombs made of low-dispersion glasses achieve much broader spectral range than quarter-wave plates. They displace the beam, however.\n",
      "-Between 1817 and 1823, Augustin-Jean Fresnel discovered that total internal reflection is accompanied by a non-trivial phase shift (that is, a phase shift that is not restricted to 0° or 180°), as the Fresnel reflection coefficient acquires a non-zero imaginary part. We shall now explain this effect for electromagnetic waves in the case of linear, homogeneous, isotropic, non-magnetic media. The phase shift turns out to be an advance, which grows as the incidence angle increases beyond the critical angle, but which depends on the polarization of the incident wave.\n",
      "\n",
      "\n",
      "\n",
      "What is the relationship between the Wigner function and the density matrix operator?\n",
      "-The density matrix operator may also be realized in phase space. Under the Wigner map, the density matrix transforms into the equivalent Wigner function, W(x,p)=def1πℏ∫−∞∞ψ∗(x+y)ψ(x−y)e2ipy/ℏdy.\n",
      "The equation for the time evolution of the Wigner function, known as Moyal equation, is then the Wigner-transform of the above von Neumann equation, ∂W(x,p,t)∂t=−{{W(x,p,t),H(x,p)}}, where  H(x,p) is the Hamiltonian, and  {{⋅,⋅}} is the Moyal bracket, the transform of the quantum commutator.\n",
      "The evolution equation for the Wigner function is then analogous to that of its classical limit, the Liouville equation of classical physics. In the limit of vanishing Planck's constant  ℏ ,  W(x,p,t) reduces to the classical Liouville probability density function in phase space.\n",
      "-It is symmetric in x and p: W(x,p)=1πℏ∫−∞∞φ∗(p+q)φ(p−q)e−2ixq/ℏdq, where φ is the normalized momentum-space wave function, proportional to the Fourier transform of ψ.\n",
      "In 3D, W(r→,p→)=1(2π)3∫ψ∗(r→+ℏs→/2)ψ(r→−ℏs→/2)eip→⋅s→d3s.\n",
      "In the general case, which includes mixed states, it is the Wigner transform of the density matrix: where ⟨x|ψ⟩ = ψ(x). This Wigner transformation (or map) is the inverse of the Weyl transform, which maps phase-space functions to Hilbert-space operators, in Weyl quantization.\n",
      "Thus, the Wigner function is the cornerstone of quantum mechanics in phase space.\n",
      "In 1949, José Enrique Moyal elucidated how the Wigner function provides the integration measure (analogous to a probability density function) in phase space, to yield expectation values from phase-space c-number functions g(x, p) uniquely associated to suitably ordered operators Ĝ through Weyl's transform (see Wigner–Weyl transform and property 7 below), in a manner evocative of classical probability theory.\n",
      "Specifically, an operator's Ĝ expectation value is a \"phase-space average\" of the Wigner transform of that operator: \n",
      "-The Wigner transformation is a general invertible transformation of an operator Ĝ on a Hilbert space to a function g(x, p) on phase space and is given by g(x,p)=∫−∞∞dseips/ℏ⟨x−s2|G^|x+s2⟩.\n",
      "Hermitian operators map to real functions. The inverse of this transformation, from phase space to Hilbert space, is called the Weyl transformation: ⟨x|G^|y⟩=∫−∞∞dpheip(x−y)/ℏg(x+y2,p) (not to be confused with the distinct Weyl transformation in differential geometry).\n",
      "The Wigner function W(x, p) discussed here is thus seen to be the Wigner transform of the density matrix operator ρ̂. Thus the trace of an operator with the density matrix Wigner-transforms to the equivalent phase-space integral overlap of g(x, p) with the Wigner function.\n",
      "-The Liouville theorem of classical mechanics fails, to the extent that, locally, the phase space volume is not preserved in time.  In fact, the quantum phase flow does not preserve all differential forms  ω2s defined by exterior powers of  ω2=Ikldξk⋏dξl .  The Wigner function represents a quantum system in a more general form than the wave function. Wave functions describe pure states, while the Wigner function characterizes ensembles of quantum states. Any Hermitian operator can be diagonalized:  f^=∑sλs|s⟩⟨s| .Those operators whose eigenvalues  λs are non-negative and sum to a finite number can be mapped to density matrices, i.e., to some physical states. The Wigner function is an image of the density matrix, so the Wigner functions admit a similar decomposition: W(ξ)=∑sλsWs(ξ), with  λs≥0 and  Ws(ξ)⋆Wr(ξ)=δsrWs(ξ) Quantum Hamilton's equations The Quantum Hamilton's equations can be obtained applying the Wigner transform to the evolution equations for Heisenberg operators of canonical coordinates and momenta, ∂∂τqi(ξ,τ)={ζi,H(ζ)}|ζ=⋆q(ξ,τ).\n",
      "-Wave functions are not always the most convenient way to describe quantum systems and their behavior. When the preparation of a system is only imperfectly known, or when the system under investigation is a part of a larger whole, density matrices may be used instead.: 74  A density matrix is a positive semi-definite operator whose trace is equal to 1. (The term \"density operator\" is also used, particularly when the underlying Hilbert space is infinite-dimensional.) The set of all density matrices is convex, and the extreme points are the operators that project onto vectors in the Hilbert space. These are the density-matrix representations of wave functions; in Dirac notation, they are written  The density-matrix analogue of the Schrödinger equation for wave functions is where the brackets denote a commutator. This is variously known as the von Neumann equation, the Liouville–von Neumann equation, or just the Schrödinger equation for density matrices.: 312  If the Hamiltonian is time-independent, this equation can be easily solved to yield More generally, if the unitary operator  U^(t) describes wave function evolution over some time interval, then the time evolution of a density matrix over that same interval is given by Unitary evolution of a density matrix conserves its von Neumann entropy.: 267 \n",
      "\n",
      "\n",
      "\n",
      "What is one of the examples of the models proposed by cosmologists and theoretical physicists without the cosmological or Copernican principles that can be used to address specific issues in the Lambda-CDM model and distinguish between current models and other possible models?\n",
      "-The standard model of cosmology, the Lambda-CDM model, assumes the Copernican principle and the more general cosmological principle. Some cosmologists and theoretical physicists have created models without the cosmological or Copernican principles to constrain the values of observational results, to address specific known issues in the Lambda-CDM model, and to propose tests to distinguish between current models and other possible models.\n",
      "-A prominent example in this context is inhomogeneous cosmology, to model the observed accelerating universe and cosmological constant. Instead of using the current accepted idea of dark energy, this model proposes the universe is much more inhomogeneous than currently assumed, and instead, we are in an extremely large low-density void. To match observations we would have to be very close to the centre of this void, immediately contradicting the Copernican principle.\n",
      "-The Copernican principle has never been proven, and in the most general sense cannot be proven, but it is implicit in many modern theories of physics. Cosmological models are often derived with reference to the cosmological principle, slightly more general than the Copernican principle, and many tests of these models can be considered tests of the Copernican principle.\n",
      "-In physical cosmology, the Copernican principle states that humans, on the Earth or in the Solar System, are not privileged observers of the universe, that observations from the Earth are representative of observations from the average position in the universe. Named for Copernican heliocentrism, it is a working assumption that arises from a modified cosmological extension of Copernicus' argument of a moving Earth.\n",
      "-Copernican and cosmological principles The Copernican principle, named after Nicolaus Copernicus, states that the Earth is not in a central, specially favored position. Hermann Bondi named the principle after Copernicus in the mid-20th century, although the principle itself dates back to the 16th-17th century paradigm shift away from the geocentric Ptolemaic system.\n",
      "The cosmological principle is an extension of the Copernican principle which states that the Universe is homogeneous (the same observational evidence is available to observers at different locations in the Universe) and isotropic (the same observational evidence is available by looking in any direction in the Universe). A homogeneous, isotropic Universe does not have a center.\n",
      "\n",
      "\n",
      "\n",
      "What is the Roche limit?\n",
      "-In celestial mechanics, the Roche limit, also called Roche radius, is the distance from a celestial body within which a second celestial body, held together only by its own force of gravity, will disintegrate because the first body's tidal forces exceed the second body's self-gravitation. Inside the Roche limit, orbiting material disperses and forms rings, whereas outside the limit, material tends to coalesce. The Roche radius depends on the radius of the first body and on the ratio of the bodies' densities.\n",
      "-But note that, as defined above, the Roche limit refers to a body held together solely by the gravitational forces which cause otherwise unconnected particles to coalesce, thus forming the body in question. The Roche limit is also usually calculated for the case of a circular orbit, although it is straightforward to modify the calculation to apply to the case (for example) of a body passing the primary on a parabolic or hyperbolic trajectory.\n",
      "-The Roche limit typically applies to a satellite's disintegrating due to tidal forces induced by its primary, the body around which it orbits. Parts of the satellite that are closer to the primary are attracted more strongly by gravity from the primary than parts that are farther away; this disparity effectively pulls the near and far parts of the satellite apart from each other, and if the disparity (combined with any centrifugal effects due to the object's spin) is larger than the force of gravity holding the satellite together, it can pull the satellite apart. Some real satellites, both natural and artificial, can orbit within their Roche limits because they are held together by forces other than gravitation. Objects resting on the surface of such a satellite would be lifted away by tidal forces. A weaker satellite, such as a comet, could be broken up when it passes within its Roche limit.\n",
      "-When a body (body 1) is acted on by the gravity of another body (body 2), the field can vary significantly on body 1 between the side of the body facing body 2 and the side facing away from body 2. Figure 4 shows the differential force of gravity on a spherical body (body 1) exerted by another body (body 2). These so-called tidal forces cause strains on both bodies and may distort them or even, in extreme cases, break one or the other apart. The Roche limit is the distance from a planet at which tidal effects would cause an object to disintegrate because the differential force of gravity from the planet overcomes the attraction of the parts of the object for one another. These strains would not occur if the gravitational field were uniform, because a uniform field only causes the entire body to accelerate together in the same direction and at the same rate.\n",
      "-The Roche limit for a rigid spherical satellite is the distance,  d , from the primary at which the gravitational force on a test mass at the surface of the object is exactly equal to the tidal force pulling the mass away from the object: d=RM(2ρMρm)13 where  RM is the radius of the primary,  ρM is the density of the primary, and  ρm is the density of the satellite. This can be equivalently written as d=Rm(2MMMm)13 where  Rm is the radius of the secondary,  MM is the mass of the primary, and  Mm is the mass of the secondary.\n",
      "\n",
      "\n",
      "\n",
      "What is Martin Heidegger's view on the relationship between time and human existence?\n",
      "-Henri Bergson believed that time was neither a real homogeneous medium nor a mental construct, but possesses what he referred to as Duration. Duration, in Bergson's view, was creativity and memory as an essential component of reality.According to Martin Heidegger we do not exist inside time, we are time. Hence, the relationship to the past is a present awareness of having been, which allows the past to exist in the present. The relationship to the future is the state of anticipating a potential possibility, task, or engagement. It is related to the human propensity for caring and being concerned, which causes \"being ahead of oneself\" when thinking of a pending occurrence. Therefore, this concern for a potential occurrence also allows the future to exist in the present. The present becomes an experience, which is qualitative instead of quantitative. Heidegger seems to think this is the way that a linear relationship with time, or temporal existence, is broken or transcended.\n",
      "-In Being and Time (1927; transl. 1962), Martin Heidegger argues that the concept of time prevalent in all Western thought has largely remained unchanged since the definition offered by Aristotle in the Physics. Heidegger says, \"Aristotle's essay on time is the first detailed Interpretation of this phenomenon [time] which has come down to us. Every subsequent account of time, including Henri Bergson's, has been essentially determined by it.\" Aristotle defined time as \"the number of movement in respect of before and after\". By defining time in this way Aristotle privileges what is present-at-hand, namely the \"presence\" of time. Heidegger argues in response that \"entities are grasped in their Being as 'presence'; this means that they are understood with regard to a definite mode of time – the 'Present'\". Central to Heidegger's own philosophical project is the attempt to gain a more authentic understanding of time. Heidegger considers time to be the unity of three ecstases: the past, the present, and the future.\n",
      "-The presence to which Heidegger refers is both a presence as in a \"now\" and also a presence as in an eternal present, as one might associate with God or the \"eternal\" laws of science. This hypostatized (underlying) belief in presence is undermined by novel phenomenological ideas, such that presence itself does not subsist, but comes about primordially through the action of our futural projection, our realization of finitude and the reception or rejection of the traditions of our time.In his short work Intuition of the Instant, Gaston Bachelard attempts to navigate beyond, or parallel to, the Western concept of 'time as duration' – as the imagined trajectorial space of movement. He distinguishes between two foundations of time: time viewed as a duration, and time viewed as an instant. Bachelard then follows this second phenomenon of time and concludes that time as a duration does not exist, but is created as a necessary mediation for increasingly complex beings to persist. The reality of time for existence, though, is in fact a reprisal of the instant, the gestation of all existence every instant, the eternal death that gives life.\n",
      "-Heidegger's being-for-death The German philosopher Martin Heidegger wrote about death as something conclusively determined, in the sense that it is inevitable for every human being, while on the other hand, it unmasks its indeterminate nature via the truth that one never knows when or how death is going to come. Heidegger does not engage in speculation about whether being after death is possible. He argues that all human existence is embedded in time: past, present, future, and when considering the future, we encounter the notion of death. This then creates angst. Angst can create a clear understanding in one that death is a possible mode of existence, which Heidegger described as \"clearing\". Thus, angst can lead to a freedom about existence, but only if people can stop denying their mortality (as expressed in Heidegger's terminology as \"stop denying being-for-death\").\n",
      "-Heidegger Martin Heidegger, meanwhile, argued that \"the surrounding world is different for each of us, and notwithstanding that we move about in a common world\". The world, for Heidegger, was that into which we are always already \"thrown\" and with which we, as beings-in-the-world, must come to terms. His conception of \"world disclosure\" was most notably elaborated in his 1927 work Being and Time.\n",
      "\n",
      "\n",
      "\n",
      "What is the \"ultraviolet catastrophe\"?\n",
      "-This formula is obtained from the equipartition theorem of classical statistical mechanics which states that all harmonic oscillator modes (degrees of freedom) of a system at equilibrium have an average energy of  kBT The \"ultraviolet catastrophe\" is the expression of the fact that the formula misbehaves at higher frequencies, i.e.  Bν(T)→∞ as  ν→∞ An example, from Mason's A History of the Sciences, illustrates multi-mode vibration via a piece of string. As a natural vibrator, the string will oscillate with specific modes (the standing waves of a string in harmonic resonance), dependent on the length of the string. In classical physics, a radiator of energy will act as a natural vibrator. Additionally, since each mode will have the same energy, most of the energy in a natural vibrator will be in the smaller wavelengths and higher frequencies, where most of the modes are.\n",
      "-In the longer wavelengths this deviation is not so noticeable, as  hν and  nhν are very small. In the shorter wavelengths of the ultraviolet range, however, classical theory predicts the energy emitted tends to infinity, hence the ultraviolet catastrophe. The theory even predicted that all bodies would emit most of their energy in the ultraviolet range, clearly contradicted by the experimental data which showed a different peak wavelength at different temperatures (see also Wien's law). Instead, in the quantum treatment of this problem, the numbers of the energy modes are quantized, attenuating the spectrum at high frequency in agreement with experimental observation and resolving the catastrophe. The modes that had more energy than the thermal energy of the substance itself were not considered, and because of quantization modes having infinitesimally little energy were excluded.\n",
      "-The ultraviolet catastrophe, also called the Rayleigh–Jeans catastrophe, was the prediction of late 19th century/early 20th century classical physics that an ideal black body at thermal equilibrium would emit an unbounded quantity of energy as wavelength decreased into the ultraviolet range.: 6–7 The term \"ultraviolet catastrophe\" was first used in 1911 by Paul Ehrenfest, but the concept originated with the 1900 statistical derivation of the Rayleigh–Jeans law.  The phrase refers to the fact that the empirically derived Rayleigh–Jeans law, which accurately predicted experimental results at large wavelengths, failed to do so for short wavelengths. (See the image for further elaboration.) As the theory diverged from empirical observations when these frequencies reached the ultraviolet region of the electromagnetic spectrum, there was a problem. This problem was later found to be due to a property of quanta as proposed by Max Planck: There could be no fraction of a discrete energy package already carrying minimal energy.\n",
      "-The optical breakdown is a very \"violent\" phenomenon and changes drastically the structure of the surrounding medium. To the naked eye, optical breakdown looks like a spark and if the event happens in air or some other fluid, it is even possible to hear a short noise (burst) caused by the explosive plasma expansion.\n",
      "-Ultraviolet (UV) is a form of electromagnetic radiation with wavelength shorter than that of visible light, but longer than X-rays. UV radiation is present in sunlight, and constitutes about 10% of the total electromagnetic radiation output from the Sun. It is also produced by electric arcs; Cherenkov radiation; and specialized lights; such as mercury-vapor lamps, tanning lamps, and black lights. Although long-wavelength ultraviolet is not considered an ionizing radiation because its photons lack the energy to ionize atoms, it can cause chemical reactions and causes many substances to glow or fluoresce. Many practical applications, including chemical and biological effects, derive from the way that UV radiation can interact with organic molecules. These interactions can involve absorption or adjusting energy states in molecules, but do not necessarily involve heating.Short-wave ultraviolet light damages DNA and sterilizes surfaces with which it comes into contact. For humans, suntan and sunburn are familiar effects of exposure of the skin to UV light, along with an increased risk of skin cancer. The amount of UV light produced by the Sun means that the Earth would not be able to sustain life on dry land if most of that light were not filtered out by the atmosphere. More energetic, shorter-wavelength \"extreme\" UV below 121 nm ionizes air so strongly that it is absorbed before it reaches the ground. However, ultraviolet light (specifically, UVB) is also responsible for the formation of vitamin D in most land vertebrates, including humans. The UV spectrum, thus, has effects both beneficial and harmful to life.\n",
      "\n",
      "\n",
      "\n",
      "What is the most popular explanation for the shower-curtain effect?\n",
      "-Bernoulli effect hypothesis The most popular explanation given for the shower-curtain effect is Bernoulli's principle. Bernoulli's principle states that an increase in velocity results in a decrease in pressure. This theory presumes that the water flowing out of a shower head causes the air through which the water moves to start flowing in the same direction as the water. This movement would be parallel to the plane of the shower curtain. If air is moving across the inside surface of the shower curtain, Bernoulli's principle says the air pressure there will drop. This would result in a pressure differential between the inside and outside, causing the curtain to move inward. It would be strongest when the gap between the bather and the curtain is smallest, resulting in the curtain attaching to the bather.\n",
      "-Often Bernoulli's principle is used to explain the topspin effect, as the difference in speed between ball surface and air is greater on the top of the ball. For example, if the air flowing past the bottom of the ball is moving faster than the air flowing past the top then Bernoulli's principle implies that the pressure on the surfaces of the ball will be lower below than above. In other words, since there is more air friction occurring on the top surface of the ball compared to the bottom, this differential causes a greater pressure to be applied on the top of the ball, resulting in the ball being pushed down.\n",
      "-A correct explanation of why the paper rises would observe that the plume follows the curve of the paper and that a curved streamline will develop a pressure gradient perpendicular to the direction of flow, with the lower pressure on the inside of the curve. Bernoulli's principle predicts that the decrease in pressure is associated with an increase in speed; in other words, as the air passes over the paper, it speeds up and moves faster than it was moving when it left the demonstrator's mouth. But this is not apparent from the demonstration.Other common classroom demonstrations, such as blowing between two suspended spheres, inflating a large bag, or suspending a ball in an airstream are sometimes explained in a similarly misleading manner by saying \"faster moving air has lower pressure\".\n",
      "-Suction is the result of air pressure differential between areas.  Removing air from a space results in a pressure differential. Suction pressure is therefore limited by external air pressure. Even a perfect vacuum cannot suck with more pressure than is available in the surrounding environment. Suctions can form on the sea, for example, when a ship founders.\n",
      "-Coandă effect The Coandă effect, also known as \"boundary layer attachment\", is the tendency of a moving fluid to adhere to an adjacent wall.\n",
      "Condensation A hot shower will produce steam that condenses on the shower side of the curtain, lowering the pressure there. In a steady state the steam will be replaced by new steam delivered by the shower but in reality the water temperature will fluctuate and lead to times when the net steam production is negative.\n",
      "Air pressure Colder dense air outside and hot less dense air inside causes higher air pressure on the outside to force the shower curtain inwards to equalise the air pressure, this can be observed simply when the bathroom door is open allowing cold air into the bathroom.\n",
      "\n",
      "\n",
      "\n",
      "What is the butterfly effect?\n",
      "-A related way to interpret the butterfly effect is to see it as highlighting the difference between the application of the notion of causality in physics and a more general use of causality as represented by Mackie's INUS conditions. In classical (Newtonian) physics, in general, only those conditions are (explicitly) taken into account, that are both necessary and sufficient. For instance, when a massive sphere is caused to roll down a slope starting from a point of unstable equilibrium, then its velocity is assumed to be caused by the force of gravity accelerating it; the small push that was needed to set it into motion is not explicitly dealt with as a cause. In order to be a physical cause there must be a certain proportionality with the ensuing effect. A distinction is drawn between triggering and causation of the ball's motion. By the same token the butterfly can be seen as triggering a tornado, its cause being assumed to be seated in the atmospherical energies already present beforehand, rather than in the movements of a butterfly.\n",
      "-Butterfly effect The butterfly effect is the notion that small events can have large, widespread consequences. The term describes events observed in chaos theory where a very small change in initial conditions results in vastly different outcomes. The term was coined by mathematician Edward Lorenz years after the phenomenon was first described.The butterfly effect has found its way into popular imagination. For example, in Ray Bradbury's 1952 short story A Sound of Thunder, the killing of a single insect millions of years in the past drastically changes the world, and in the 2004 film The Butterfly Effect, the protagonist's small changes to his past results in extreme changes.\n",
      "-In chaos theory, the butterfly effect is the sensitive dependence on initial conditions in which a small change in one state of a deterministic nonlinear system can result in large differences in a later state.\n",
      "-Theories in physics like the butterfly effect from chaos theory open up the possibility of a type of distributed parameter systems in causality. The butterfly effect theory proposes: \"Small variations of the initial condition of a nonlinear dynamical system may produce large variations in the long term behavior of the system.\" This opens up the opportunity to understand a distributed causality.\n",
      "-The butterfly effect describes a phenomenon in chaos theory whereby a minor change in circumstances can cause a large change in outcome. The scientific concept is attributed to Edward Lorenz, a mathematician and meteorologist who used the metaphor to describe his research findings related to chaos theory and weather prediction, initially in a 1972 paper titled \"Predictability: Does the Flap of a Butterfly's Wings in Brazil Set Off a Tornado in Texas?\" The butterfly metaphor is attributed to the 1952 Ray Bradbury short story \"A Sound of Thunder\".The concept has been widely adopted by popular culture, and interpreted to mean that small events have a rippling effect that cause much larger events to occur, and has become a common reference.\n",
      "\n",
      "\n",
      "\n",
      "What is the 'reactive Leidenfrost effect' observed in non-volatile materials?\n",
      "-Non-volatile materials were discovered in 2015 to also exhibit a 'reactive Leidenfrost effect', whereby solid particles were observed to float above hot surfaces and skitter around erratically. Detailed characterization of the reactive Leidenfrost effect was completed for small particles of cellulose (~0.5 mm) on high temperature polished surfaces by high speed photography. Cellulose was shown to decompose to short-chain oligomers which melt and wet smooth surfaces with increasing heat transfer associated with increasing surface temperature. Above 675 °C (1,247 °F), cellulose was observed to exhibit transition boiling with violent bubbling and associated reduction in heat transfer. Liftoff of the cellulose droplet (depicted at the right) was observed to occur above about 750 °C (1,380 °F), associated with a dramatic reduction in heat transfer.High speed photography of the reactive Leidenfrost effect of cellulose on porous surfaces (macroporous alumina) was also shown to suppress the reactive Leidenfrost effect and enhance overall heat transfer rates to the particle from the surface. The new phenomenon of a 'reactive Leidenfrost (RL) effect' was characterized by a dimensionless quantity, (φRL= τconv/τrxn), which relates the time constant of solid particle heat transfer to the time constant of particle reaction, with the reactive Leidenfrost effect occurring for 10−1< φRL< 10+1. The reactive Leidenfrost effect with cellulose will occur in numerous high temperature applications with carbohydrate polymers, including biomass conversion to biofuels, preparation and cooking of food, and tobacco use.The Leidenfrost effect has also been used as a means to promote chemical change of various organic liquids through their conversion by thermal decomposition into various products. Examples include decomposition of ethanol, diethyl carbonate, and glycerol.\n",
      "-The Leidenfrost effect is a physical phenomenon in which a liquid, close to a surface that is significantly hotter than the liquid's boiling point, produces an insulating vapor layer that keeps the liquid from boiling rapidly. Because of this repulsive force, a droplet hovers over the surface, rather than making physical contact with it. The effect is named after the German doctor Johann Gottlob Leidenfrost, who described it in A Tract About Some Qualities of Common Water.\n",
      "-The Leidenfrost point may also be taken to be the temperature for which the hovering droplet lasts longest.It has been demonstrated that it is possible to stabilize the Leidenfrost vapor layer of water by exploiting superhydrophobic surfaces. In this case, once the vapor layer is established, cooling never collapses the layer, and no nucleate boiling occurs; the layer instead slowly relaxes until the surface is cooled.Droplets of different liquids with different boiling temperatures will also exhibit a Leidenfrost effect with respect to each other and repel each other.The Leidenfrost effect has been used for the development of high sensitivity ambient mass spectrometry. Under the influence of the Leidenfrost condition, the levitating droplet does not release molecules, and the molecules are enriched inside the droplet. At the last moment of droplet evaporation, all the enriched molecules release in a short time period and thereby increase the sensitivity.A heat engine based on the Leidenfrost effect has been prototyped; it has the advantage of extremely low friction.The effect also applies when the surface is at room temperature but the liquid is cryogenic, allowing liquid nitrogen droplets to harmlessly roll off exposed skin. Conversely, the inverse Leidenfrost effect lets drops of relatively warm liquid levitate on a bath of liquid nitrogen.\n",
      "-Henry developed a model for Leidenfrost phenomenon which includes transient wetting and microlayer evaporation. Since the Leidenfrost phenomenon is a special case of film boiling, the Leidenfrost temperature is related to the minimum film boiling temperature via a relation which factors in the properties of the solid being used. While the Leidenfrost temperature is not directly related to the surface tension of the fluid, it is indirectly dependent on it through the film boiling temperature. For fluids with similar thermophysical properties, the one with higher surface tension usually has a higher Leidenfrost temperature.\n",
      "-The Leidenfrost point signifies the onset of stable film boiling. It represents the point on the boiling curve where the heat flux is at the minimum and the surface is completely covered by a vapor blanket. Heat transfer from the surface to the liquid occurs by conduction and radiation through the vapour. In 1756, Leidenfrost observed that water droplets supported by the vapor film slowly evaporate as they move about on the hot surface. As the surface temperature is increased, radiation through the vapor film becomes more significant and the heat flux increases with increasing excess temperature.\n",
      "\n",
      "\n",
      "\n",
      "What is reciprocal length or inverse length?\n",
      "-Reciprocal length or inverse length is a quantity or measurement used in several branches of science and mathematics. As the reciprocal of length, common units used for this measurement include the reciprocal metre or inverse metre (symbol: m−1), the reciprocal centimetre or inverse centimetre (symbol: cm−1).\n",
      "-The inverse second or reciprocal second (s−1), also called per second, is a unit defined as the multiplicative inverse of the second (a unit of time). It is applicable for physical quantities of dimension reciprocal time, such as frequency and strain rate.\n",
      "-The inverse minute or reciprocal minute (min−1), also called per minute, is 60−1 s−1, as 1 min = 60 s; it is used in quantities of type \"counts per minute\", such as: Actions per minute Beats per minute Counts per minute Revolutions per minute (rpm) Words per minute \n",
      "-The energy is inversely proportional to the size of the unit of which the reciprocal is used, and is proportional to the number of reciprocal length units. For example, in terms of energy, one reciprocal metre equals 10−2 (one hundredth) as much as a reciprocal centimetre. Five reciprocal metres are five times as much energy as one reciprocal metre.\n",
      "-Quantities measured in reciprocal length include: absorption coefficient or attenuation coefficient, in materials science curvature of a line, in mathematics gain, in laser physics magnitude of vectors in reciprocal space, in crystallography more generally any spatial frequency e.g. in cycles per unit length optical power of a lens, in optics rotational constant of a rigid rotor, in quantum mechanics wavenumber, or magnitude of a wavevector, in spectroscopy density of a linear feature in hydrology and other fields; see kilometre per square kilometre surface area to volume ratioIn optics, the dioptre is a unit equivalent to reciprocal metre.\n",
      "\n",
      "\n",
      "\n",
      "Which of the following statements is true about the categorization of planetary systems according to their orbital dynamics?\n",
      "-Orbital dynamics Planetary systems can be categorized according to their orbital dynamics as resonant, non-resonant-interacting, hierarchical, or some combination of these. In resonant systems the orbital periods of the planets are in integer ratios. The Kepler-223 system contains four planets in an 8:6:4:3 orbital resonance.\n",
      "Giant planets are found in mean-motion resonances more often than smaller planets.\n",
      "In interacting systems the planets orbits are close enough together that they perturb the orbital parameters. The Solar System could be described as weakly interacting. In strongly interacting systems Kepler's laws do not hold.\n",
      "In hierarchical systems the planets are arranged so that the system can be gravitationally considered as a nested system of two-bodies, e.g. in a star with a close-in hot jupiter with another gas giant much further out, the star and hot jupiter form a pair that appears as a single object to another planet that is far enough out.\n",
      "Other, as yet unobserved, orbital possibilities include: double planets; various co-orbital planets such as quasi-satellites, trojans and exchange orbits; and interlocking orbits maintained by precessing orbital planes.\n",
      "-The classes of TNO have no universally agreed precise definitions, the boundaries are often unclear and the notion of resonance is not defined precisely. The Deep Ecliptic Survey introduced formally defined dynamical classes based on long-term forward integration of orbits under the combined perturbations from all four giant planets. (see also formal definition of classical KBO) In general, the mean-motion resonance may involve not only orbital periods of the form  p⋅λ−q⋅λN where p and q are small integers, λ and λN are respectively the mean longitudes of the object and Neptune, but can also involve the longitude of the perihelion and the longitudes of the nodes (see orbital resonance, for elementary examples) An object is resonant if for some small integers (p,q,n,m,r,s), the argument (angle) defined below is librating (i.e. is bounded): ϕ=p⋅λ−q⋅λN−m⋅ϖ−n⋅Ω−r⋅ϖN−s⋅ΩN where the  ϖ are the longitudes of perihelia and the  Ω are the longitudes of the ascending nodes, for Neptune (with subscripts \"N\") and the resonant object (no subscripts).\n",
      "-Orbital In celestial mechanics, an orbital resonance occurs when two orbiting bodies exert a regular, periodic gravitational influence on each other, usually due to their orbital periods being related by a ratio of two small integers. Orbital resonances greatly enhance the mutual gravitational influence of the bodies. In most cases, this results in an unstable interaction, in which the bodies exchange momentum and shift orbits until the resonance no longer exists. Under some circumstances, a resonant system can be stable and self-correcting, so that the bodies remain in resonance. Examples are the 1:2:4 resonance of Jupiter's moons Ganymede, Europa, and Io, and the 2:3 resonance between Pluto and Neptune. Unstable resonances with Saturn's inner moons give rise to gaps in the rings of Saturn. The special case of 1:1 resonance (between bodies with similar orbital radii) causes large Solar System bodies to clear the neighborhood around their orbits by ejecting nearly everything else around them; this effect is used in the current definition of a planet.\n",
      "-In celestial mechanics, orbital resonance occurs when orbiting bodies exert regular, periodic gravitational influence on each other, usually because their orbital periods are related by a ratio of small integers. Most commonly, this relationship is found between a pair of objects (binary resonance). The physical principle behind orbital resonance is similar in concept to pushing a child on a swing, whereby the orbit and the swing both have a natural frequency, and the body doing the \"pushing\" will act in periodic repetition to have a cumulative effect on the motion. Orbital resonances greatly enhance the mutual gravitational influence of the bodies (i.e., their ability to alter or constrain each other's orbits). In most cases, this results in an unstable interaction, in which the bodies exchange momentum and shift orbits until the resonance no longer exists. Under some circumstances, a resonant system can be self-correcting and thus stable. Examples are the 1:2:4 resonance of Jupiter's moons Ganymede, Europa and Io, and the 2:3 resonance between Neptune and Pluto. Unstable resonances with Saturn's inner moons give rise to gaps in the rings of Saturn. The special case of 1:1 resonance between bodies with similar orbital radii causes large planetary system bodies to eject most other bodies sharing their orbits; this is part of the much more extensive process of clearing the neighbourhood, an effect that is used in the current definition of a planet.A binary resonance ratio in this article should be interpreted as the ratio of number of orbits completed in the same time interval, rather than as the ratio of orbital periods, which would be the inverse ratio. Thus, the 2:3 ratio above means that Pluto completes two orbits in the time it takes Neptune to complete three. In the case of resonance relationships among three or more bodies, either type of ratio may be used (whereby the smallest whole-integer ratio sequences are not necessarily reversals of each other), and the type of ratio will be specified.\n",
      "-The category dwarf planet arose from a conflict between dynamical and geophysical ideas of what a useful conception of a planet would be. In terms of the dynamics of the Solar System, the major distinction is between bodies that gravitationally dominate their neighbourhood (Mercury through Neptune) and those that do not (such as the asteroids and Kuiper belt objects). A celestial body may have a dynamic (planetary) geology at approximately the mass required for its mantle to become plastic under its own weight, which results in the body acquiring a round shape. Because this requires a much lower mass than gravitationally dominating the region of space near their orbit, there are a population of objects that are massive enough to have a world-like appearance and planetary geology, but not massive enough to clear their neighborhood. Examples are Ceres in the asteroid belt and Pluto in the Kuiper belt.Dynamicists usually prefer using gravitational dominance as the threshold for planethood, because from their perspective smaller bodies are better grouped with their neighbours, e.g. Ceres as simply a large asteroid and Pluto as a large Kuiper belt object. Geoscientists usually prefer roundness as the threshold, because from their perspective the internally driven geology of a body like Ceres makes it more similar to a classical planet like Mars, than to a small asteroid that lacks internally driven geology. This necessitated the creation of the category of dwarf planets to describe this intermediate class.\n",
      "\n",
      "\n",
      "\n",
      "What is the propagation constant in sinusoidal waves?\n",
      "-Propagation constant The propagation constant of the sinusoidal electromagnetic wave is a measure of the change undergone by the amplitude and phase of the wave as it propagates in a given direction. The quantity being measured can be the voltage, the current in a circuit, or a field vector such as electric field strength or flux density. The propagation constant itself measures the change per unit length, but it is otherwise dimensionless. In the context of two-port networks and their cascades, propagation constant measures the change undergone by the source quantity as it propagates from one port to the next.\n",
      "-The propagation constant of a sinusoidal electromagnetic wave is a measure of the change undergone by the amplitude and phase of the wave as it propagates in a given direction. The quantity being measured can be the voltage, the current in a circuit, or a field vector such as electric field strength or flux density. The propagation constant itself measures the change per unit length, but it is otherwise dimensionless. In the context of two-port networks and their cascades, propagation constant measures the change undergone by the source quantity as it propagates from one port to the next.\n",
      "-The propagation constant's value is expressed logarithmically, almost universally to the base e, rather than the more usual base 10 that is used in telecommunications in other situations. The quantity measured, such as voltage, is expressed as a sinusoidal phasor. The phase of the sinusoid varies with distance which results in the propagation constant being a complex number, the imaginary part being caused by the phase change.\n",
      "-The term propagation constant or propagation function is applied to filters and other two-port networks used for signal processing. In these cases, however, the attenuation and phase coefficients are expressed in terms of nepers and radians per network section rather than per unit length. Some authors make a distinction between per unit length measures (for which \"constant\" is used) and per section measures (for which \"function\" is used).\n",
      "-Equation (1.1) has an analytical solution given by exp 1.2 ) Where k is the wave number. When the wave propagates in inhomogeneous seismic media the propagation constant k must be a complex value that includes not only an imaginary part, the frequency-dependent attenuation coefficient, but also a real part, the dispersive wavenumber. We can call this K(w) a propagation constant in line with Futterman.\n",
      "\n",
      "\n",
      "\n",
      "What is the gravitomagnetic interaction?\n",
      "-Some higher-order gravitomagnetic effects can reproduce effects reminiscent of the interactions of more conventional polarized charges. For instance, if two wheels are spun on a common axis, the mutual gravitational attraction between the two wheels will be greater if they spin in opposite directions than in the same direction. This can be expressed as an attractive or repulsive gravitomagnetic component.\n",
      "-The test particle is not drawn to the bottom stream because of a velocity-dependent force that serves to repel a particle that is moving in the same direction as the bottom stream. This velocity-dependent gravitational effect is gravitomagnetism.: 245–253 Matter in motion through a gravitomagnetic field is hence subject to so-called frame-dragging effects analogous to electromagnetic induction. It has been proposed that such gravitomagnetic forces underlie the generation of the relativistic jets (Fig. 5-8) ejected by some rotating supermassive black holes.\n",
      "-Some scientists hypothesize that a fifth force might exist, but these hypotheses remain speculative.Each of the known fundamental interactions can be described mathematically as a field. The gravitational force is attributed to the curvature of spacetime, described by Einstein's general theory of relativity. The other three are discrete quantum fields, and their interactions are mediated by elementary particles described by the Standard Model of particle physics.Within the Standard Model, the strong interaction is carried by a particle called the gluon and is responsible for quarks binding together to form hadrons, such as protons and neutrons. As a residual effect, it creates the nuclear force that binds the latter particles to form atomic nuclei. The weak interaction is carried by particles called W and Z bosons, and also acts on the nucleus of atoms, mediating radioactive decay. The electromagnetic force, carried by the photon, creates electric and magnetic fields, which are responsible for the attraction between orbital electrons and atomic nuclei which holds atoms together, as well as chemical bonding and electromagnetic waves, including visible light, and forms the basis for electrical technology. Although the electromagnetic force is far stronger than gravity, it tends to cancel itself out within large objects, so over large (astronomical) distances gravity tends to be the dominant force, and is responsible for holding together the large scale structures in the universe, such as planets, stars, and galaxies.\n",
      "-Electromagnetic interaction: the familiar interaction that acts on electrically charged particles. The photon is the exchange particle for this force.\n",
      "Weak interaction: a short-range interaction responsible for some forms of radioactivity, that acts on electrons, neutrinos, and quarks. It is mediated by the W and Z bosons.\n",
      "Gravitational interaction: a long-range attractive interaction that acts on all particles. The postulated exchange particle has been named the graviton.Modern unified field theory attempts to bring these four forces and matter together into a single framework.\n",
      "-Just as electric charge and current multipoles contribute to the electromagnetic field, mass and mass-current multipoles contribute to the gravitational field in general relativity, causing the so-called gravitomagnetic effects. Changing mass-current multipoles can also give off gravitational radiation. However, contributions from the current multipoles will typically be much smaller than that of the mass quadrupole.\n",
      "\n",
      "\n",
      "\n",
      "What did Newton's manuscripts of the 1660s show?\n",
      "-In regard to evidence that still survives of the earlier history, manuscripts written by Newton in the 1660s show that Newton himself had, by 1669, arrived at proofs that in a circular case of planetary motion, \"endeavour to recede\" (what was later called centrifugal force) had an inverse-square relation with distance from the center. After his 1679–1680 correspondence with Hooke, Newton adopted the language of inward or centripetal force. According to Newton scholar J. Bruce Brackenridge, although much has been made of the change in language and difference of point of view, as between centrifugal or centripetal forces, the actual computations and proofs remained the same either way. They also involved the combination of tangential and radial displacements, which Newton was making in the 1660s. The lesson offered by Hooke to Newton here, although significant, was one of perspective and did not change the analysis. This background shows there was basis for Newton to deny deriving the inverse square law from Hooke.\n",
      "-Newton's early work on motion In the 1660s Newton studied the motion of colliding bodies and deduced that the centre of mass of two colliding bodies remains in uniform motion. Surviving manuscripts of the 1660s also show Newton's interest in planetary motion and that by 1669 he had shown, for a circular case of planetary motion, that the force he called \"endeavour to recede\" (now called centrifugal force) had an inverse-square relation with distance from the center. After his 1679–1680 correspondence with Hooke, described below, Newton adopted the language of inward or centripetal force. According to Newton scholar J. Bruce Brackenridge, although much has been made of the change in language and difference of point of view, as between centrifugal or centripetal forces, the actual computations and proofs remained the same either way. They also involved the combination of tangential and radial displacements, which Newton was making in the 1660s. The difference between the centrifugal and centripetal points of view, though a significant change of perspective, did not change the analysis. Newton also clearly expressed the concept of linear inertia in the 1660s: for this Newton was indebted to Descartes' work published 1644.\n",
      "-Newton's acknowledgment On the other hand, Newton did accept and acknowledge, in all editions of the Principia, that Hooke (but not exclusively Hooke) had separately appreciated the inverse square law in the solar system. Newton acknowledged Wren, Hooke, and Halley in this connection in the Scholium to Proposition 4 in Book 1. Newton also acknowledged to Halley that his correspondence with Hooke in 1679–80 had reawakened his dormant interest in astronomical matters, but that did not mean, according to Newton, that Hooke had told Newton anything new or original: \"yet am I not beholden to him for any light into that business but only for the diversion he gave me from my other studies to think on these things & for his dogmaticalness in writing as if he had found the motion in the Ellipsis, which inclined me to try it ...\" Modern priority controversy Since the time of Newton and Hooke, scholarly discussion has also touched on the question of whether Hooke's 1679 mention of 'compounding the motions' provided Newton with something new and valuable, even though that was not a claim actually voiced by Hooke at the time. As described above, Newton's manuscripts of the 1660s do show him actually combining tangential motion with the effects of radially directed force or endeavour, for example in his derivation of the inverse square relation for the circular case. They also show Newton clearly expressing the concept of linear inertia—for which he was indebted to Descartes' work, published in 1644 (as Hooke probably was). These matters do not appear to have been learned by Newton from Hooke.\n",
      "-Later, in 1686, when Newton's Principia had been presented to the Royal Society, Hooke claimed from this correspondence the credit for some of Newton's content in the Principia, and said Newton owed the idea of an inverse-square law of attraction to him – although at the same time, Hooke disclaimed any credit for the curves and trajectories that Newton had demonstrated on the basis of the inverse square law.Newton, who heard of this from Halley, rebutted Hooke's claim in letters to Halley, acknowledging only an occasion of reawakened interest. Newton did acknowledge some prior work of others, including Ismaël Bullialdus, who suggested (but without demonstration) that there was an attractive force from the Sun in the inverse square proportion to the distance, and Giovanni Alfonso Borelli, who suggested (again without demonstration) that there was a tendency towards the Sun like gravity or magnetism that would make the planets move in ellipses; but that the elements Hooke claimed were due either to Newton himself, or to other predecessors of them both such as Bullialdus and Borelli, but not Hooke. Wren and Halley were both skeptical of Hooke's claims, recalling an occasion when Hooke had claimed to have a derivation of planetary motions under an inverse square law, but had failed to produce it even under the incentive of a prize.There has been scholarly controversy over exactly what if anything Newton really gained from Hooke, apart from the stimulus that Newton acknowledged.About thirty years after Newton's death in 1727, Alexis Clairaut, one of Newton's early and eminent successors in the field of gravitational studies, wrote after reviewing Hooke's work that it showed \"what a distance there is between a truth that is glimpsed and a truth that is demonstrated\".\n",
      "-Newton's work and claims Newton, faced in May 1686 with Hooke's claim on the inverse square law, denied that Hooke was to be credited as author of the idea. Among the reasons, Newton recalled that the idea had been discussed with Sir Christopher Wren previous to Hooke's 1679 letter. Newton also pointed out and acknowledged prior work of others, including Bullialdus, (who suggested, but without demonstration, that there was an attractive force from the Sun in the inverse square proportion to the distance), and Borelli (who suggested, also without demonstration, that there was a centrifugal tendency in counterbalance with a gravitational attraction towards the Sun so as to make the planets move in ellipses). D T Whiteside has described the contribution to Newton's thinking that came from Borelli's book, a copy of which was in Newton's library at his death.Newton further defended his work by saying that had he first heard of the inverse square proportion from Hooke, he would still have some rights to it in view of his demonstrations of its accuracy. Hooke, without evidence in favor of the supposition, could only guess that the inverse square law was approximately valid at great distances from the center. According to Newton, while the 'Principia' was still at pre-publication stage, there were so many a priori reasons to doubt the accuracy of the inverse-square law (especially close to an attracting sphere) that \"without my (Newton's) Demonstrations, to which Mr Hooke is yet a stranger, it cannot believed by a judicious Philosopher to be any where accurate.\"This remark refers among other things to Newton's finding, supported by mathematical demonstration, that if the inverse square law applies to tiny particles, then even a large spherically symmetrical mass also attracts masses external to its surface, even close up, exactly as if all its own mass were concentrated at its center. Thus Newton gave a justification, otherwise lacking, for applying the inverse square law to large spherical planetary masses as if they were tiny particles. In addition, Newton had formulated, in Propositions 43–45 of Book 1 and associated sections of Book 3, a sensitive test of the accuracy of the inverse square law, in which he showed that only where the law of force is calculated as the inverse square of the distance will the directions of orientation of the planets' orbital ellipses stay constant as they are observed to do apart from small effects attributable to inter-planetary perturbations.\n",
      "\n",
      "\n",
      "\n",
      "What is the decay energy for the free neutron decay process?\n",
      "-For the free neutron the decay energy for this process (based on the masses of the neutron, proton, and electron) is 0.782343 MeV. The maximal energy of the beta decay electron (in the process wherein the neutrino receives a vanishingly small amount of kinetic energy) has been measured at 0.782±0.013 MeV. The latter number is not well-enough measured to determine the comparatively tiny rest mass of the neutrino (which must in theory be subtracted from the maximal electron kinetic energy) as well as neutrino mass is constrained by many other methods.\n",
      "-For the free neutron, the decay energy for this process (based on the rest masses of the neutron, proton and electron) is 0.782343 MeV. That is the difference between the rest mass of the neutron and the sum of the rest masses of the products. That difference has to be carried away as kinetic energy. The maximal energy of the beta decay electron (in the process wherein the neutrino receives a vanishingly small amount of kinetic energy) has been measured at 0.782±0.013 MeV. The latter number is not well-enough measured to determine the comparatively tiny rest mass of the neutrino (which must in theory be subtracted from the maximal electron kinetic energy); furthermore, neutrino mass is constrained by many other methods.\n",
      "-Free neutron decay Outside the nucleus, free neutrons are unstable and have a mean lifetime of 879.6±0.8 s (about 14 minutes, 40 seconds); therefore the half-life for this process (which differs from the mean lifetime by a factor of ln(2) = 0.693) is 610.1±0.7 s (about 10 minutes, 10 seconds). This decay is only possible because the mass of the proton is less than that of the neutron. By the mass-energy equivalence, when a neutron decays to a proton this way, a lower energy state is attained.\n",
      "-In a typical nuclear fission reaction, 187 MeV of energy are released instantaneously in the form of kinetic energy from the fission products, kinetic energy from the fission neutrons, instantaneous gamma rays, or gamma rays from the capture of neutrons. An additional 23 MeV of energy are released at some time after fission from the beta decay of fission products. About 10 MeV of the energy released from the beta decay of fission products is in the form of neutrinos, and since neutrinos are very weakly interacting, this 10 MeV of energy will not be deposited in the reactor core. This results in 13 MeV (6.5% of the total fission energy) being deposited in the reactor core from delayed beta decay of fission products, at some time after any given fission reaction has occurred. In a steady state, this heat from delayed fission product beta decay contributes 6.5% of the normal reactor heat output.\n",
      "-Outside the nucleus, free neutrons are unstable and have a mean lifetime of 14 minutes, 42 seconds. Free neutrons decay by emission of an electron and an electron antineutrino to become a proton, a process known as beta decay:In the adjacent diagram, a neutron collides with a proton of the target material, and then becomes a fast recoil proton that ionizes in turn. At the end of its path, the neutron is captured by a nucleus in an (n,γ)-reaction that leads to the emission of a neutron capture photon. Such photons always have enough energy to qualify as ionizing radiation.\n",
      "\n",
      "\n",
      "\n",
      "What is Hesse's principle of transfer in geometry?\n",
      "-In geometry, Hesse's principle of transfer (German: Übertragungsprinzip) states that if the points of the projective line P1 are depicted by a rational normal curve in Pn, then the group of the projective transformations of Pn that preserve the curve is isomorphic to the group of the projective transformations of P1 (this is a generalization of the original Hesse's principle, in a form suggested by Wilhelm Franz Meyer). It was originally introduced by Otto Hesse in 1866, in a more restricted form. It influenced Felix Klein in the development of the Erlangen program. Since its original conception, it was generalized by many mathematicians, including Klein, Fano, and Cartan.\n",
      "-In model theory, a transfer principle states that all statements of some language that are true for some structure are true for another structure. One of the first examples was the Lefschetz principle, which states that any sentence in the first-order language of fields that is true for the complex numbers is also true for any algebraically closed field of characteristic 0.\n",
      "-In chemistry, transfer hydrogenation is a chemical reaction involving the addition of hydrogen to a compound from a source other than molecular H2. It is applied in laboratory and industrial organic synthesis to saturate organic compounds and reduce ketones to alcohols, and imines to amines. It avoids the need for high-pressure molecular H2 used in conventional hydrogenation. Transfer hydrogenation usually occurs at mild temperature and pressure conditions using organic or organometallic catalysts, many of which are chiral, allowing efficient asymmetric synthesis. It uses hydrogen donor compounds such as formic acid, isopropanol or dihydroanthracene, dehydrogenating them to CO2, acetone, or anthracene respectively. Often, the donor molecules also function as solvents for the reaction. A large scale application of transfer hydrogenation is coal liquefaction using \"donor solvents\" such as tetralin.\n",
      "-The transfer principle concerns the logical relation between the properties of the real numbers R, and the properties of a larger field denoted *R called the hyperreal numbers. The field *R includes, in particular, infinitesimal (\"infinitely small\") numbers, providing a rigorous mathematical realisation of a project initiated by Leibniz.\n",
      "-Mass transfer is the net movement of mass from one location (usually meaning stream, phase, fraction or component) to another. Mass transfer occurs in many processes, such as absorption, evaporation, drying, precipitation, membrane filtration, and distillation. Mass transfer is used by different scientific disciplines for different processes and mechanisms. The phrase is commonly used in engineering for physical processes that involve diffusive and convective transport of chemical species within physical systems.\n",
      "\n",
      "\n",
      "\n",
      "What is the relationship between the Cauchy momentum equation and the Navier-Stokes equation?\n",
      "-All non-relativistic momentum conservation equations, such as the Navier–Stokes equation, can be derived by beginning with the Cauchy momentum equation and specifying the stress tensor through a constitutive relation. By expressing the shear tensor in terms of viscosity and fluid velocity, and assuming constant density and viscosity, the Cauchy momentum equation will lead to the Navier–Stokes equations. By assuming inviscid flow, the Navier–Stokes equations can further simplify to the Euler equations.\n",
      "-All non-relativistic balance equations, such as the Navier–Stokes equations, can be derived by beginning with the Cauchy equations and specifying the stress tensor through a constitutive relation. By expressing the deviatoric (shear) stress tensor in terms of viscosity and the fluid velocity gradient, and assuming constant viscosity, the above Cauchy equations will lead to the Navier–Stokes equations below.\n",
      "-The classic Navier-Stokes equation is the balance equation for momentum density for an isotropic, compressional and viscous fluid that is used in fluid mechanics in general and fluid dynamics in particular: ρ[∂u∂t+u⋅∇u]=−∇P+∇[ζ(∇⋅u)]+∇⋅[η(∇u+(∇u)T−23(∇⋅u)I)]+ρg On the right hand side is (the divergence of) the total stress tensor  σ which consists of a pressure tensor  (−PI) and a dissipative (or viscous or deviatoric) stress tensor  τd . The dissipative stress consists of a compression stress tensor  τc (term no. 2) and a shear stress tensor  τs (term no. 3). The rightmost term  ρg is the gravitational force which is the body force contribution, and  ρ is the mass density, and  u is the fluid velocity.\n",
      "-The Navier–Stokes momentum equation can be derived as a particular form of the Cauchy momentum equation, whose general convective form is By setting the Cauchy stress tensor  σ {\\textstyle {\\boldsymbol {\\sigma }}} to be the sum of a viscosity term  τ {\\textstyle {\\boldsymbol {\\tau }}} (the deviatoric stress) and a pressure term  − p I {\\textstyle -p\\mathbf {I} } (volumetric stress), we arrive at where D D t {\\textstyle {\\frac {\\mathrm {D} }{\\mathrm {D} t}}} is the material derivative, defined as  ∂ ∂ t + u ⋅ ∇ {\\textstyle {\\frac {\\partial }{\\partial t}}+\\mathbf {u} \\cdot \\nabla } , ρ {\\textstyle \\rho } is the (mass) density, u {\\textstyle \\mathbf {u} } is the flow velocity, ∇ ⋅ {\\textstyle \\nabla \\cdot \\,} is the divergence, p {\\textstyle p} is the pressure, t {\\textstyle t} is time, τ {\\textstyle {\\boldsymbol {\\tau }}} is the deviatoric stress tensor, which has order 2, g {\\textstyle \\mathbf {g} } represents body accelerations acting on the continuum, for example gravity, inertial accelerations, electrostatic accelerations, and so on.In this form, it is apparent that in the assumption of an inviscid fluid – no deviatoric stress – Cauchy equations reduce to the Euler equations.\n",
      "-It is closely related to the Navier–Stokes equations, because the flow of momentum in a fluid is mathematically similar to the flow of mass or energy. The correspondence is clearest in the case of an incompressible Newtonian fluid, in which case the Navier–Stokes equation is: where M is the momentum of the fluid (per unit volume) at each point (equal to the density ρ multiplied by the velocity v), μ is viscosity, P is fluid pressure, and f is any other body force such as gravity. In this equation, the term on the left-hand side describes the change in momentum at a given point; the first term on the right describes the diffusion of momentum by viscosity; the second term on the right describes the advective flow of momentum; and the last two terms on the right describes the external and internal forces which can act as sources or sinks of momentum.\n",
      "\n",
      "\n",
      "\n",
      "What is X-ray pulsar-based navigation (XNAV)?\n",
      "-X-ray pulsar-based navigation and timing (XNAV) or simply pulsar navigation is a navigation technique whereby the periodic X-ray signals emitted from pulsars are used to determine the location of a vehicle, such as a spacecraft in deep space. A vehicle using XNAV would compare received X-ray signals with a database of known pulsar frequencies and locations. Similar to GPS, this comparison would allow the vehicle to calculate its position accurately (±5 km). The advantage of using X-ray signals over radio waves is that X-ray telescopes can be made smaller and lighter. Experimental demonstrations have been reported in 2018.\n",
      "-Pulsar navigation X-ray pulsar-based navigation and timing (XNAV) or simply pulsar navigation is a navigation technique whereby the periodic X-ray signals emitted from pulsars are used to determine the location of a vehicle, such as a spacecraft in deep space. A vehicle using XNAV would compare received X-ray signals with a database of known pulsar frequencies and locations. Similar to GPS, this comparison would allow the vehicle to calculate its position accurately (±5 km). The advantage of using X-ray signals over radio waves is that X-ray telescopes can be made smaller and lighter. Experimental demonstrations have been reported in 2018.\n",
      "-X-ray pulsar-based navigation and timing (XNAV) is an experimental navigation technique whereby the periodic X-ray signals emitted from pulsars are used to determine the location of a vehicle, such as a spacecraft in deep space. A vehicle using XNAV would compare received X-ray signals with a database of known pulsar frequencies and locations. Similar to GNSS, this comparison would allow the vehicle to triangulate its position accurately (±5 km). The advantage of using X-ray signals over radio waves is that X-ray telescopes can be made smaller and lighter. On 9 November 2016 the Chinese Academy of Sciences launched an experimental pulsar navigation satellite called XPNAV 1. SEXTANT (Station Explorer for X-ray Timing and Navigation Technology) is a NASA-funded project developed at the Goddard Space Flight Center that is testing XNAV on-orbit on board the International Space Station in connection with the NICER project, launched on 3 June 2017 on the SpaceX CRS-11 ISS resupply mission.\n",
      "-Experiments XPNAV 1 On 9 November 2016, the Chinese Academy of Sciences launched an experimental pulsar navigation satellite called XPNAV 1. XPNAV-1 has a mass of 240 kg, and is in a 493 km × 512 km, 97.41° orbit. XPNAV-1 will characterize 26 nearby pulsars for their pulse frequency and intensity to create a navigation database that could be used by future operational missions. The satellite is expected to operate for five to ten years. XPNAV-1 is the first pulsar navigation mission launched into orbit.SEXTANT SEXTANT (Station Explorer for X-ray Timing and Navigation Technology) is a NASA-funded project developed at the Goddard Space Flight Center that is testing XNAV on-orbit on board the International Space Station in connection with the NICER project, launched on 3 June 2017 on the SpaceX CRS-11 ISS resupply mission. If this is successful, XNAV may be used as secondary navigation technology for the planned Orion missions. In January 2018, X-ray navigation feasibility was demonstrated using NICER/SEXTANT on ISS. It reported a 7 km accuracy (in 2 days).\n",
      "-Studies The Advanced Concepts Team of ESA studied in 2003 the feasibility of x-ray pulsar navigation in collaboration with the Universitat Politecnica de Catalunya in Spain. After the study, the interest in the XNAV technology within the European Space Agency was consolidated leading, in 2012, to two different and more detailed studies performed by GMV AEROSPACE AND DEFENCE (ES) and the National Physical Laboratory (UK).\n",
      "\n",
      "\n",
      "\n",
      "What is the evidence for the existence of a supermassive black hole at the center of the Milky Way galaxy?\n",
      "-In the Milky Way Evidence indicates that the Milky Way galaxy has a supermassive black hole at its center, 26,000 light-years from the Solar System, in a region called Sagittarius A* because: The star S2 follows an elliptical orbit with a period of 15.2 years and a pericenter (closest distance) of 17 light-hours (1.8×1013 m or 120 AU) from the center of the central object.\n",
      "-It is thought that supermassive black holes like these do not form immediately from the singular collapse of a cluster of stars. Instead they may begin life as smaller, stellar-sized black holes and grow larger by the accretion of matter, or even of other black holes.The Schwarzschild radius of the supermassive black hole at the Galactic Center of the Milky Way is approximately 12 million kilometres. Its mass is about 4.1 million M☉.\n",
      "-Outside the Milky Way Unambiguous dynamical evidence for supermassive black holes exists only for a handful of galaxies; these include the Milky Way, the Local Group galaxies M31 and M32, and a few galaxies beyond the Local Group, such as NGC 4395. In these galaxies, the root mean square (or rms) velocities of the stars or gas rises proportionally to 1/r near the center, indicating a central point mass. In all other galaxies observed to date, the rms velocities are flat, or even falling, toward the center, making it impossible to state with certainty that a supermassive black hole is present. Nevertheless, it is commonly accepted that the center of nearly every galaxy contains a supermassive black hole. The reason for this assumption is the M–sigma relation, a tight (low scatter) relation between the mass of the hole in the 10 or so galaxies with secure detections, and the velocity dispersion of the stars in the bulges of those galaxies. This correlation, although based on just a handful of galaxies, suggests to many astronomers a strong connection between the formation of the black hole and the galaxy itself.On March 28, 2011, a supermassive black hole was seen tearing a mid-size star apart. That is the only likely explanation of the observations that day of sudden X-ray radiation and the follow-up broad-band observations. The source was previously an inactive galactic nucleus, and from study of the outburst the galactic nucleus is estimated to be a SMBH with mass of the order of a million M☉. This rare event is assumed to be a relativistic outflow (material being emitted in a jet at a significant fraction of the speed of light) from a star tidally disrupted by the SMBH. A significant fraction of a solar mass of material is expected to have accreted onto the SMBH. Subsequent long-term observation will allow this assumption to be confirmed if the emission from the jet decays at the expected rate for mass accretion onto a SMBH.\n",
      "-Many bulges are thought to host a supermassive black hole at their centers. In our own galaxy, for instance, the object called Sagittarius A* is believed to be a supermassive black hole. There are many lines of evidence for the existence of black holes in spiral galaxy centers, including the presence of active nuclei in some spiral galaxies, and dynamical measurements that find large compact central masses in galaxies such as Messier 106.\n",
      "-Proper motions of stars orbiting Sagittarius A* The proper motions of stars near the centre of our own Milky Way provide strong observational evidence that these stars are orbiting a supermassive black hole. Since 1995, astronomers have tracked the motions of 90 stars orbiting an invisible object coincident with the radio source Sagittarius A*. By fitting their motions to Keplerian orbits, the astronomers were able to infer, in 1998, that a 2.6×106 M☉ object must be contained in a volume with a radius of 0.02 light-years to cause the motions of those stars. Since then, one of the stars—called S2—has completed a full orbit. From the orbital data, astronomers were able to refine the calculations of the mass to 4.3×106 M☉ and a radius of less than 0.002 light-years for the object causing the orbital motion of those stars. The upper limit on the object's size is still too large to test whether it is smaller than its Schwarzschild radius; nevertheless, these observations strongly suggest that the central object is a supermassive black hole as there are no other plausible scenarios for confining so much invisible mass into such a small volume. Additionally, there is some observational evidence that this object might possess an event horizon, a feature unique to black holes.\n",
      "\n",
      "\n",
      "\n",
      "What is the function of the fibrous cardiac skeleton?\n",
      "-In cardiology, the cardiac skeleton, also known as the fibrous skeleton of the heart, is a high-density homogeneous structure of connective tissue that forms and anchors the valves of the heart, and influences the forces exerted by and through them. The cardiac skeleton separates and partitions the atria (the smaller, upper two chambers) from the ventricles (the larger, lower two chambers).The heart's cardiac skeleton comprises four dense connective tissue rings that encircle the mitral and tricuspid atrioventricular (AV) canals and extend to the origins of the pulmonary trunk and aorta. This provides crucial support and structure to the heart while also serving to electrically isolate the atria from the ventricles.The unique matrix of connective tissue within the cardiac skeleton isolates electrical influence within these defined chambers. In normal anatomy, there is only one conduit for electrical conduction from the upper chambers to the lower chambers, known as the atrioventricular node. The physiologic cardiac skeleton forms a firewall governing autonomic/electrical influence until bordering the bundle of His which further governs autonomic flow to the bundle branches of the ventricles. Understood as such, the cardiac skeleton efficiently centers and robustly funnels electrical energy from the atria to the ventricles.\n",
      "-Cardiac The collagenous cardiac skeleton which includes the four heart valve rings, is histologically, elastically and uniquely bound to cardiac muscle. The cardiac skeleton also includes the separating septa of the heart chambers – the interventricular septum and the atrioventricular septum. Collagen contribution to the measure of cardiac performance summarily represents a continuous torsional force opposed to the fluid mechanics of blood pressure emitted from the heart. The collagenous structure that divides the upper chambers of the heart from the lower chambers is an impermeable membrane that excludes both blood and electrical impulses through typical physiological means. With support from collagen, atrial fibrillation never deteriorates to ventricular fibrillation. Collagen is layered in variable densities with smooth muscle mass. The mass, distribution, age, and density of collagen all contribute to the compliance required to move blood back and forth. Individual cardiac valvular leaflets are folded into shape by specialized collagen under variable pressure. Gradual calcium deposition within collagen occurs as a natural function of aging. Calcified points within collagen matrices show contrast in a moving display of blood and muscle, enabling methods of cardiac imaging technology to arrive at ratios essentially stating blood in (cardiac input) and blood out (cardiac output). Pathology of the collagen underpinning of the heart is understood within the category of connective tissue disease.\n",
      "-The four cardiac valves are kept in their place partly because of the fibrous skeleton of the heart, which is a collection of connective tissue. It consists of the right fibrous trigone (which along with the membranous septum forms the central fibrous body), the left right fibrous trigone, and the conus tendon. The right fibrous trigone is the strongest part of the skeleton. It lies to the right of the aortic valve and connects it with the mitral and tricuspid valves. It is pierced by the Bundle of His. Lastly, the aortomitral curtain is also a part of the fibrous skeleton; it is formed by fibrous tissue connecting two of three of the aortic valve leaflets (the right and non-coronary leaflet) with anterior leaflet of the mitral valve.\n",
      "-In humans, the axial skeleton serves to protect the brain, spinal cord, heart, and lungs. It also serves as the attachment site for muscles that move the head, neck, and back, and for muscles that act across the shoulder and hip joints to move their corresponding limbs.\n",
      "-The structure of the components of the heart has become an area of increasing interest. The cardiac skeleton binds several bands of dense connective tissue, as collagen, that encircle the bases of the pulmonary trunk, aorta, and all four heart valves. While not a traditionally or \"true\" or rigid skeleton, it does provide structure and support for the heart, as well as isolate the atria from the ventricles. This is why atrial fibrillation almost never degrades to ventricular fibrillation. In youth, this collagen structure is free of calcium adhesions and is quite flexible. With aging, calcium and other mineral accumulation occur within this skeleton. Distensibility of the ventricles is tied to variable accumulation of minerals which also contributes to the delay of the depolarization wave in geriatric patients that can take place from the AV node and the bundle of His.\n",
      "\n",
      "\n",
      "\n",
      "What is the Carnot engine?\n",
      "-Carnot's principle The historical origin of the second law of thermodynamics was in Sadi Carnot's theoretical analysis of the flow of heat in steam engines (1824). The centerpiece of that analysis, now known as a Carnot engine, is an ideal heat engine fictively operated in the limiting mode of extreme slowness known as quasi-static, so that the heat and work transfers are between subsystems that are always in their own internal states of thermodynamic equilibrium. It represents the theoretical maximum efficiency of a heat engine operating between any two given thermal or heat reservoirs at different temperatures. Carnot's principle was recognized by Carnot at a time when the caloric theory represented the dominant understanding of the nature of heat, before the recognition of the first law of thermodynamics, and before the mathematical expression of the concept of entropy. Interpreted in the light of the first law, Carnot's analysis is physically equivalent to the second law of thermodynamics, and remains valid today. Some samples from his book are: ...wherever there exists a difference of temperature, motive power can be produced.The production of motive power is then due in steam engines not to an actual consumption of caloric, but to its transportation from a warm body to a cold body ...The motive power of heat is independent of the agents employed to realize it; its quantity is fixed solely by the temperatures of the bodies between which is effected, finally, the transfer of caloric.In modern terms, Carnot's principle may be stated more precisely: The efficiency of a quasi-static or reversible Carnot cycle depends only on the temperatures of the two heat reservoirs, and is the same, whatever the working substance. A Carnot engine operated in this way is the most efficient possible heat engine using those two temperatures.\n",
      "-Carnot efficiency The second law of thermodynamics puts a fundamental limit on the thermal efficiency of all heat engines. Even an ideal, frictionless engine can't convert anywhere near 100% of its input heat into work. The limiting factors are the temperature at which the heat enters the engine,  TH , and the temperature of the environment into which the engine exhausts its waste heat,  TC , measured in an absolute scale, such as the Kelvin or Rankine scale. From Carnot's theorem, for any engine working between these two temperatures: ηth≤1−TCTH This limiting value is called the Carnot cycle efficiency because it is the efficiency of an unattainable, ideal, reversible engine cycle called the Carnot cycle. No device converting heat into mechanical energy, regardless of its construction, can exceed this efficiency.\n",
      "-Due to the other causes detailed below, practical engines have efficiencies far below the Carnot limit. For example, the average automobile engine is less than 35% efficient.\n",
      "Carnot's theorem applies to thermodynamic cycles, where thermal energy is converted to mechanical work. Devices that convert a fuel's chemical energy directly into electrical work, such as fuel cells, can exceed the Carnot efficiency.\n",
      "-A Carnot heat engine is a heat engine performing a Carnot cycle, and its realization on a macroscopic scale is impractical. For example, for the isothermal expansion part of the Carnot cycle, the following conditions must be satisfied simultaneously at every step in the expansion: The hot reservoir temperature TH is infinitesimally higher than the system gas temperature T so heat flow (energy transfer) from the hot reservoir to the gas is made without increasing T (via infinitesimal work on the surroundings by the gas as another energy transfer); if TH is significantly higher than T, then T may be not uniform through the gas so the system would deviate from thermal equilibrium as well as not being a reversible process (i.e. not a Carnot cycle) or T might increase noticeably so it would not be an isothermal process.\n",
      "-A quantum Carnot engine is one in which the atoms in the heat bath are given a small bit of quantum coherence. The phase of the atomic coherence provides a new control parameter.The deep physics behind the second law of thermodynamics is not violated; nevertheless, the quantum Carnot engine has certain features that are not possible in a classical engine.\n",
      "\n",
      "\n",
      "\n",
      "Which mathematical function is commonly used to characterize linear time-invariant systems?\n",
      "-A continuous time-invariant linear state-space model is observable if and only if rank ⁡[CCA⋮CAn−1]=n.\n",
      "Transfer function The \"transfer function\" of a continuous time-invariant linear state-space model can be derived in the following way: First, taking the Laplace transform of  x˙(t)=Ax(t)+Bu(t) yields sX(s)−x(0)=AX(s)+BU(s).\n",
      "Next, we simplify for  X(s) , giving (sI−A)X(s)=x(0)+BU(s) and thus X(s)=(sI−A)−1x(0)+(sI−A)−1BU(s).\n",
      "Substituting for  X(s) in the output equation Y(s)=CX(s)+DU(s), giving Y(s)=C((sI−A)−1x(0)+(sI−A)−1BU(s))+DU(s).\n",
      "-Mason introduced both nonlinear and linear flow graphs. To clarify this point, Mason wrote : \"A linear flow graph is one whose associated equations are linear.\" Examples of nonlinear branch functions It we denote by xj the signal at node j, the following are examples of node functions that do not pertain to a linear time-invariant system: log ,where  represents time Examples of nonlinear signal-flow graph models Although they generally can't be transformed between time domain and frequency domain representations for classical control theory analysis, nonlinear signal-flow graphs can be found in electrical engineering literature.\n",
      "-Generalizing resonance and antiresonance for linear systems Next consider an arbitrary linear system with multiple inputs and outputs. For example, in state-space representation a third order linear time-invariant system with three inputs and two outputs might be written as where ui(t) are the inputs, xi(t) are the state variables, yi(t) are the outputs, and A, B, C, and D are matrices describing the dynamics between the variables.\n",
      "-If we assume the controller C, the plant P, and the sensor F are linear and time-invariant (i.e., elements of their transfer function C(s), P(s), and F(s) do not depend on time), the systems above can be analysed using the Laplace transform on the variables. This gives the following relations: Y(s)=P(s)U(s) U(s)=C(s)E(s) E(s)=R(s)−F(s)Y(s).\n",
      "Solving for Y(s) in terms of R(s) gives Y(s)=(P(s)C(s)1+F(s)P(s)C(s))R(s)=H(s)R(s).\n",
      "-So  zn is an eigenfunction of an LTI system because the system response is the same as the input times the constant  H(z) Z and discrete-time Fourier transforms The eigenfunction property of exponentials is very useful for both analysis and insight into LTI systems. The Z transform is exactly the way to get the eigenvalues from the impulse response. Of particular interest are pure sinusoids; i.e. exponentials of the form  ejωn , where  ω∈R . These can also be written as  zn with  z=ejω . The discrete-time Fourier transform (DTFT)  H(ejω)=F{h[n]} gives the eigenvalues of pure sinusoids. Both of  H(z) and  H(ejω) are called the system function, system response, or transfer function.\n",
      "\n",
      "\n",
      "\n",
      "What is the second law of thermodynamics?\n",
      "-The second law of thermodynamics is a physical law based on universal experience concerning heat and energy interconversions. One simple statement of the law is that heat always moves from hotter objects to colder objects (or \"downhill\"), unless energy in some form is supplied to reverse the direction of heat flow. Another definition is: \"Not all heat energy can be converted into work in a cyclic process.\"The second law of thermodynamics in other versions establishes the concept of entropy as a physical property of a thermodynamic system. It can be used to predict whether processes are forbidden despite obeying the requirement of conservation of energy as expressed in the first law of thermodynamics and provides necessary criteria for spontaneous processes. The second law may be formulated by the observation that the entropy of isolated systems left to spontaneous evolution cannot decrease, as they always arrive at a state of thermodynamic equilibrium where the entropy is highest at the given internal energy. An increase in the combined entropy of system and surroundings accounts for the irreversibility of natural processes, often referred to in the concept of the arrow of time.Historically, the second law was an empirical finding that was accepted as an axiom of thermodynamic theory. Statistical mechanics provides a microscopic explanation of the law in terms of probability distributions of the states of large assemblies of atoms or molecules. The second law has been expressed in many ways. Its first formulation, which preceded the proper definition of entropy and was based on caloric theory, is Carnot's theorem, formulated by the French scientist Sadi Carnot, who in 1824 showed that the efficiency of conversion of heat to work in a heat engine has an upper limit. The first rigorous definition of the second law based on the concept of entropy came from German scientist Rudolf Clausius in the 1850s and included his statement that heat can never pass from a colder to a warmer body without some other change, connected therewith, occurring at the same time.\n",
      "-The first law of thermodynamics states that, when energy passes into or out of a system (as work, heat, or matter), the system's internal energy changes in accordance with the law of conservation of energy.\n",
      "The second law of thermodynamics states that in a natural thermodynamic process, the sum of the entropies of the interacting thermodynamic systems never decreases. A common corollary of the statement is that heat does not spontaneously pass from a colder body to a warmer body.\n",
      "-Second law of thermodynamics As an alternative to considering or defining the zeroth law of thermodynamics, it was the historical development in thermodynamics to define temperature in terms of the second law of thermodynamics which deals with entropy. The second law states that any process will result in either no change or a net increase in the entropy of the universe. This can be understood in terms of probability.\n",
      "-Second law of thermodynamics The second law of thermodynamics conceptualizes that the entropy of a closed system can never decrease. As the law relates to power plants, it dictates that heat is to flow from a body at high temperature to a body at low temperature (the device in which electricity is being generated). This law is particularly pertinent to thermal power plants which derive their energy from the combustion of a fuel source.\n",
      "-Second law A traditional version of the second law of thermodynamics states: Heat does not spontaneously flow from a colder body to a hotter body.\n",
      "\n",
      "\n",
      "\n",
      "What are amorphous ferromagnetic metallic alloys, and what are their advantages?\n",
      "-Amorphous (non-crystalline) ferromagnetic metallic alloys can be made by very rapid quenching (cooling) of an alloy. These have the advantage that their properties are nearly isotropic (not aligned along a crystal axis); this results in low coercivity, low hysteresis loss, high permeability, and high electrical resistivity. One such typical material is a transition metal-metalloid alloy, made from about 80% transition metal (usually Fe, Co, or Ni) and a metalloid component (B, C, Si, P, or Al) that lowers the melting point.\n",
      "-Amorphous metal is usually an alloy rather than a pure metal. The alloys contain atoms of significantly different sizes, leading to low free volume (and therefore up to orders of magnitude higher viscosity than other metals and alloys) in molten state. The viscosity prevents the atoms moving enough to form an ordered lattice. The material structure also results in low shrinkage during cooling, and resistance to plastic deformation. The absence of grain boundaries, the weak spots of crystalline materials, leads to better resistance to wear and corrosion. Amorphous metals, while technically glasses, are also much tougher and less brittle than oxide glasses and ceramics. Amorphous metals can be grouped in two categories, as either non-ferromagnetic, if they are composed of Ln, Mg, Zr, Ti, Pd, Ca, Cu, Pt and Au, or ferromagnetic alloys, if they are composed of Fe, Co, and Ni.Thermal conductivity of amorphous materials is lower than that of crystalline metal. As formation of amorphous structure relies on fast cooling, this limits the maximum achievable thickness of amorphous structures. To achieve formation of amorphous structure even during slower cooling, the alloy has to be made of three or more components, leading to complex crystal units with higher potential energy and lower chance of formation. The atomic radius of the components has to be significantly different (over 12%), to achieve high packing density and low free volume. The combination of components should have negative heat of mixing, inhibiting crystal nucleation and prolonging the time the molten metal stays in supercooled state.\n",
      "-Electric and Magnetic Properties The amorphous material produced by melt spinning is considered a soft magnet. That is to say that their natural coercivity is less than 1000 Am-1, which means that the metal's magnetism is more responsive to outside influences and as a result can be easily switched on and off. This makes amorphous metals particularly useful in applications requiring the repeated magnetization and demagnetization of a material in order to function. Certain amorphous alloys also provide the ability to enhance and or channel flux created by electrical currents, making them useful for magnetic shielding and insulation.\n",
      "-Amorphous steel This material is a metallic glass prepared by pouring molten alloy onto a rotating cooled wheel, which cools the metal at a rate of about one megakelvin per second, so fast that crystals do not form. Amorphous steel is limited to foils of about 50 µm thickness. The mechanical properties of amorphous steel make stamping laminations for electric motors difficult. Since amorphous ribbon can be cast to any specific width under roughly 13 inches and can be sheared with relative ease, it is a suitable material for wound electrical transformer cores. In 2019 the price of amorphous steel outside the US is approximately $.95/pound compared to HiB grain-oriented steel which costs approximately $.86/pound. Transformers with amorphous steel cores can have core losses of one-third that of conventional electrical steels.\n",
      "-Vitreous metal Amorphous metal is a variety of alloys (e.g. Metglas) that are non-crystalline or glassy. These are being used to create high-efficiency transformers. The materials can be highly responsive to magnetic fields for low hysteresis losses, and they can also have lower conductivity to reduce eddy current losses. Power utilities are currently making widespread use of these transformers for new installations. High mechanical strength and corrosion resistance are also common properties of metallic glasses which are positive for this application.\n",
      "\n",
      "\n",
      "\n",
      "What is the Penrose process?\n",
      "-The Penrose process (also called Penrose mechanism) is theorised by Sir Roger Penrose as a means whereby energy can be extracted from a rotating black hole. The process takes advantage of the ergosphere – a region of spacetime around the black hole dragged by its rotation faster than the speed of light, meaning that from the point of an outside observer any matter inside is forced to move in the direction of the rotation of the black hole.\n",
      "-This process of removing energy from a rotating black hole was proposed by the mathematician Roger Penrose in 1969 and is called the Penrose process. The maximal amount of energy gain possible for a single particle via this process is 20.7% in terms of its mass equivalence, and if this process is repeated by the same mass, the theoretical maximal energy gain approaches 29% of its original mass-energy equivalent. As this energy is removed, the black hole loses angular momentum, and thus the limit of zero rotation is approached as spacetime dragging is reduced. In the limit, the ergosphere no longer exists. This process is considered a possible explanation for a source of energy of such energetic phenomena as gamma-ray bursts. Results from computer models show that the Penrose process is capable of producing the high-energy particles that are observed being emitted from quasars and other active galactic nuclei.\n",
      "-The region outside the event horizon but inside the surface where the rotational velocity is the speed of light, is called the ergosphere (from Greek ergon meaning work). Particles falling within the ergosphere are forced to rotate faster and thereby gain energy. Because they are still outside the event horizon, they may escape the black hole. The net process is that the rotating black hole emits energetic particles at the cost of its own total energy. The possibility of extracting spin energy from a rotating black hole was first proposed by the mathematician Roger Penrose in 1969 and is thus called the Penrose process. Rotating black holes in astrophysics are a potential source of large amounts of energy and are used to explain energetic phenomena, such as gamma-ray bursts.\n",
      "-Ergosphere Rotating black holes are surrounded by a region of spacetime in which it is impossible to stand still, called the ergosphere. This is the result of a process known as frame-dragging; general relativity predicts that any rotating mass will tend to slightly \"drag\" along the spacetime immediately surrounding it. Any object near the rotating mass will tend to start moving in the direction of rotation. For a rotating black hole, this effect is so strong near the event horizon that an object would have to move faster than the speed of light in the opposite direction to just stand still.The ergosphere of a black hole is a volume bounded by the black hole's event horizon and the ergosurface, which coincides with the event horizon at the poles but is at a much greater distance around the equator.Objects and radiation can escape normally from the ergosphere. Through the Penrose process, objects can emerge from the ergosphere with more energy than they entered with. The extra energy is taken from the rotational energy of the black hole. Thereby the rotation of the black hole slows down. A variation of the Penrose process in the presence of strong magnetic fields, the Blandford–Znajek process is considered a likely mechanism for the enormous luminosity and relativistic jets of quasars and other active galactic nuclei.\n",
      "-A rotating black hole can produce large amounts of energy at the expense of its rotational energy. This happens through the Penrose process in the black hole's ergosphere, an area just outside its event horizon. In that case a rotating black hole gradually reduces to a Schwarzschild black hole, the minimum configuration from which no further energy can be extracted, although the Kerr black hole's rotation velocity will never quite reach zero.\n",
      "\n",
      "\n",
      "\n",
      "What was the aim of the Gravity Probe B (GP-B) mission?\n",
      "-Gravitomagnetism The existence of gravitomagnetism was proven by Gravity Probe B (GP-B), a satellite-based mission which launched on 20 April 2004. The spaceflight phase lasted until 2005. The mission aim was to measure spacetime curvature near Earth, with particular emphasis on gravitomagnetism.\n",
      "-The geodetic effect (also known as geodetic precession, de Sitter precession or de Sitter effect) represents the effect of the curvature of spacetime, predicted by general relativity, on a vector carried along with an orbiting body. For example, the vector could be the angular momentum of a gyroscope orbiting the Earth, as carried out by the Gravity Probe B experiment. The geodetic effect was first predicted by Willem de Sitter in 1916, who provided relativistic corrections to the Earth–Moon system's motion. De Sitter's work was extended in 1918 by Jan Schouten and in 1920 by Adriaan Fokker. It can also be applied to a particular secular precession of astronomical orbits, equivalent to the rotation of the Laplace–Runge–Lenz vector.The term geodetic effect has two slightly different meanings as the moving body may be spinning or non-spinning. Non-spinning bodies move in geodesics, whereas spinning bodies move in slightly different orbits.The difference between de Sitter precession and Lense–Thirring precession (frame dragging) is that the de Sitter effect is due simply to the presence of a central mass, whereas Lense–Thirring precession is due to the rotation of the central mass. The total precession is calculated by combining the de Sitter precession with the Lense–Thirring precession.\n",
      "-General relativityGeneral relativity is governed by the Einstein field equations, which describe the curvature of space-time due to mass–energy equivalent to the gravitational field. Solving the equation for the geometry of space warped due to the mass distribution gives the metric tensor. Using the geodesic equation, the motion of masses falling along the geodesics can be calculated.\n",
      "GravitomagnetismIn a relatively flat spacetime due to weak gravitational fields, gravitational analogues of Maxwell's equations can be found; the GEM equations, to describe an analogous gravitomagnetic field. They are well established by the theory, and experimental tests form ongoing research.\n",
      "Classical laws Kepler's Laws, though originally discovered from planetary observations (also due to Tycho Brahe), are true for any central forces.\n",
      "-Also with implications for the physical exploration of the Earth's interior is the gravitational field, which is the net effect of gravitation (due to mass attraction) and centrifugal force (due to rotation). It can be measured very accurately at the surface and remotely by satellites. True vertical generally does not correspond to theoretical vertical (deflection ranges up to 50\") because topography and all geological masses disturb the gravitational field. Therefore, the gross structure of the Earth's crust and mantle can be determined by geodetic-geophysical models of the subsurface.\n",
      "-The simplest (and historically the first) way of defining an asymptotically flat spacetime assumes that we have a coordinate chart, with coordinates  t,x,y,z , which far from the origin behaves much like a Cartesian chart on Minkowski spacetime, in the following sense. Write the metric tensor as the sum of a (physically unobservable) Minkowski background plus a perturbation tensor,  gab=ηab+hab , and set  r2=x2+y2+z2 . Then we require: lim r→∞hab=O(1/r) lim r→∞hab,p=O(1/r2) lim r→∞hab,pq=O(1/r3) One reason why we require the partial derivatives of the perturbation to decay so quickly is that these conditions turn out to imply that the gravitational field energy density (to the extent that this somewhat nebulous notion makes sense in a metric theory of gravitation) decays like  O(1/r4) , which would be physically sensible. (In classical electromagnetism, the energy of the electromagnetic field of a point charge decays like  O(1/r4) .) \n",
      "\n",
      "\n",
      "\n",
      "What was Pierre de Fermat's solution to the problem of refraction?\n",
      "-Fermat vs. the Cartesians In 1657, Pierre de Fermat received from Marin Cureau de la Chambre a copy of newly published treatise, in which La Chambre noted Hero's principle and complained that it did not work for refraction.Fermat replied that refraction might be brought into the same framework by supposing that light took the path of least resistance, and that different media offered different resistances. His eventual solution, described in a letter to La Chambre dated 1 January 1662, construed \"resistance\" as inversely proportional to speed, so that light took the path of least time. That premise yielded the ordinary law of refraction, provided that light traveled more slowly in the optically denser medium.Fermat's solution was a landmark in that it unified the then-known laws of geometrical optics under a variational principle or action principle, setting the precedent for the principle of least action in classical mechanics and the corresponding principles in other fields (see History of variational principles in physics). It was the more notable because it used the method of adequality, which may be understood in retrospect as finding the point where the slope of an infinitesimally short chord is zero, without the intermediate step of finding a general expression for the slope (the derivative).\n",
      "-It was also immediately controversial. The ordinary law of refraction was at that time attributed to René Descartes (d. 1650), who had tried to explain it by supposing that light was a force that propagated instantaneously, or that light was analogous to a tennis ball that traveled faster in the denser medium, either premise being inconsistent with Fermat's. Descartes' most prominent defender, Claude Clerselier, criticized Fermat for apparently ascribing knowledge and intent to nature, and for failing to explain why nature should prefer to economize on time rather than distance. Clerselier wrote in part: 1. The principle that you take as the basis of your demonstration, namely that nature always acts in the shortest and simplest ways, is merely a moral principle and not a physical one; it is not, and cannot be, the cause of any effect in nature.... For otherwise we would attribute knowledge to nature; but here, by \"nature\", we understand only this order and this law established in the world as it is, which acts without foresight, without choice, and by a necessary determination.\n",
      "-Optics The earlier ideas of variational principles in optics were generalized to refraction by Pierre de Fermat, who, in the 17th century, refined the principle to \"light travels between two given points along the path of shortest time\"; now known as the principle of least time or Fermat's principle.\n",
      "-If a ray follows a straight line, it obviously takes the path of least length. Hero of Alexandria, in his Catoptrics (1st century CE), showed that the ordinary law of reflection off a plane surface follows from the premise that the total length of the ray path is a minimum. Ibn al-Haytham, an 11th century polymaths later extended this principle to refraction, hence giving an early version of the Fermat's principle.\n",
      "-The laws of reflection and refraction can be derived from Fermat's principle which states that the path taken between two points by a ray of light is the path that can be traversed in the least time.\n",
      "\n",
      "\n",
      "\n",
      "What is the reason behind the adoption of a logarithmic scale of 5√100 ≈ 2.512 between magnitudes in astronomy?\n",
      "-Thus in 1856 Norman Pogson of Oxford proposed that a logarithmic scale of 5√100 ≈ 2.512 be adopted between magnitudes, so five magnitude steps corresponded precisely to a factor of 100 in brightness. Every interval of one magnitude equates to a variation in brightness of 5√100 or roughly 2.512 times. Consequently, a magnitude 1 star is about 2.5 times brighter than a magnitude 2 star, about 2.52 times brighter than a magnitude 3 star, about 2.53 times brighter than a magnitude 4 star, and so on.\n",
      "-Both the apparent and absolute magnitude scales are logarithmic units: one whole number difference in magnitude is equal to a brightness variation of about 2.5 times (the 5th root of 100 or approximately 2.512). This means that a first magnitude star (+1.00) is about 2.5 times brighter than a second magnitude (+2.00) star, and about 100 times brighter than a sixth magnitude star (+6.00). The faintest stars visible to the naked eye under good seeing conditions are about magnitude +6.On both apparent and absolute magnitude scales, the smaller the magnitude number, the brighter the star; the larger the magnitude number, the fainter the star. The brightest stars, on either scale, have negative magnitude numbers. The variation in brightness (ΔL) between two stars is calculated by subtracting the magnitude number of the brighter star (mb) from the magnitude number of the fainter star (mf), then using the difference as an exponent for the base number 2.512; that is to say: Δm=mf−mb 2.512 Δm=ΔL Relative to both luminosity and distance from Earth, a star's absolute magnitude (M) and apparent magnitude (m) are not equivalent; for example, the bright star Sirius has an apparent magnitude of −1.44, but it has an absolute magnitude of +1.41.\n",
      "-Magnitude values do not have a unit. The scale is logarithmic and defined such that a magnitude 1 star is exactly 100 times brighter than a magnitude 6 star. Thus each step of one magnitude is  100 2.512 times brighter than the magnitude 1 higher. The brighter an object appears, the lower the value of its magnitude, with the brightest objects reaching negative values.\n",
      "-The scale is reverse logarithmic: the brighter an object is, the lower its magnitude number. A difference of 1.0 in magnitude corresponds to a brightness ratio of  100 5 , or about 2.512. For example, a star of magnitude 2.0 is 2.512 times as bright as a star of magnitude 3.0, 6.31 times as bright as a star of magnitude 4.0, and 100 times as bright as one of magnitude 7.0.\n",
      "-The magnitude scale is a reverse logarithmic scale. A common misconception is that the logarithmic nature of the scale is because the human eye itself has a logarithmic response. In Pogson's time this was thought to be true (see Weber–Fechner law), but it is now believed that the response is a power law (see Stevens' power law).Magnitude is complicated by the fact that light is not monochromatic. The sensitivity of a light detector varies according to the wavelength of the light, and the way it varies depends on the type of light detector. For this reason, it is necessary to specify how the magnitude is measured for the value to be meaningful. For this purpose the UBV system is widely used, in which the magnitude is measured in three different wavelength bands: U (centred at about 350 nm, in the near ultraviolet), B (about 435 nm, in the blue region) and V (about 555 nm, in the middle of the human visual range in daylight). The V band was chosen for spectral purposes and gives magnitudes closely corresponding to those seen by the human eye. When an apparent magnitude is discussed without further qualification, the V magnitude is generally understood.Because cooler stars, such as red giants and red dwarfs, emit little energy in the blue and UV regions of the spectrum, their power is often under-represented by the UBV scale. Indeed, some L and T class stars have an estimated magnitude of well over 100, because they emit extremely little visible light, but are strongest in infrared.Measures of magnitude need cautious treatment and it is extremely important to measure like with like. On early 20th century and older orthochromatic (blue-sensitive) photographic film, the relative brightnesses of the blue supergiant Rigel and the red supergiant Betelgeuse irregular variable star (at maximum) are reversed compared to what human eyes perceive, because this archaic film is more sensitive to blue light than it is to red light. Magnitudes obtained from this method are known as photographic magnitudes, and are now considered obsolete.For objects within the Milky Way with a given absolute magnitude, 5 is added to the apparent magnitude for every tenfold increase in the distance to the object. For objects at very great distances (far beyond the Milky Way), this relationship must be adjusted for redshifts and for non-Euclidean distance measures due to general relativity.For planets and other Solar System bodies, the apparent magnitude is derived from its phase curve and the distances to the Sun and observer.\n",
      "\n",
      "\n",
      "\n",
      "What is the spin quantum number?\n",
      "-In physics, the spin quantum number is a quantum number (designated s) that describes the intrinsic angular momentum (or spin angular momentum, or simply spin) of an electron or other particle. It has the same value for all particles of the same type, such as s = 1/2 for all electrons. It is an integer for all bosons, such as photons, and a half-odd-integer for all fermions, such as electrons and protons. The component of the spin along a specified axis is given by the spin magnetic quantum number, conventionally written ms. The value of ms is the component of spin angular momentum, in units of the reduced Planck constant ħ, parallel to a given direction (conventionally labelled the z–axis). It can take values ranging from +s to −s in integer increments. For an electron, ms can be either ++1/2 or −+1/2 .\n",
      "-Spin quantum number The spin quantum number describes the intrinsic spin angular momentum of the electron within each orbital and gives the projection of the spin angular momentum S along the specified axis: Sz = ms ħ.In general, the values of ms range from −s to s, where s is the spin quantum number, associated with the particle's intrinsic spin angular momentum: ms = −s, −s + 1, −s + 2, ..., s − 2, s − 1, s.An electron has spin number s = 1/2, consequently ms will be ±1/2, referring to \"spin up\" and \"spin down\" states. Each electron in any individual orbital must have different quantum numbers because of the Pauli exclusion principle, therefore an orbital never contains more than two electrons.\n",
      "-The phrase spin quantum number was originally used to describe the fourth of a set of quantum numbers (the principal quantum number n, the azimuthal quantum number ℓ, the magnetic quantum number m, and the spin magnetic quantum number ms), which completely describe the quantum state of an electron in an atom.  Some introductory chemistry textbooks describe ms as the spin quantum number, and s is not mentioned since its value 1/2 is a fixed property of the electron, sometimes using the variable s in place of ms. Some authors discourage this usage as it causes confusion. At a more advanced level where quantum mechanical operators or coupled spins are introduced, s is referred to as the spin quantum number, and ms is described as the spin magnetic quantum number or as the z-component of spin sz.Spin quantum numbers apply also to systems of coupled spins, such as atoms that may contain more than one electron. Capitalized symbols are used: S for the total electronic spin, and mS or MS for the z-axis component. A pair of electrons in a spin singlet state has S = 0, and a pair in the triplet state has S = 1, with mS = −1, 0, or +1. Nuclear-spin quantum numbers are conventionally written I for spin, and mI or MI for the z-axis component.\n",
      "-In atomic physics, a magnetic quantum number is a quantum number used to distinguish quantum states of an electron or other particle according to its angular momentum along a given axis in space. The orbital magnetic quantum number (ml or m) distinguishes the orbitals available within a given subshell of an atom. It specifies the component of the orbital angular momentum that lies along a given axis, conventionally called the z-axis, so it describes the orientation of the orbital in space. The spin magnetic quantum number ms specifies the z-axis component of the spin angular momentum for a particle having spin quantum number s. For an electron, s is 1⁄2, and ms is either +1⁄2 or −1⁄2, often called \"spin-up\" and \"spin-down\", or α and β. The term magnetic in the name refers to the magnetic dipole moment associated with each type of angular momentum, so states having different magnetic quantum numbers shift in energy in a magnetic field according to the Zeeman effect.The four quantum numbers conventionally used to describe the quantum state of an electron in an atom are the principal quantum number n, the azimuthal (orbital) quantum number  ℓ , and the magnetic quantum numbers ml and ms. Electrons in a given subshell of an atom (such as s, p, d, or f) are defined by values of  ℓ (0, 1, 2, or 3). The orbital magnetic quantum number takes integer values in the range from  −ℓ to  +ℓ , including zero. Thus the s, p, d, and f subshells contain 1, 3, 5, and 7 orbitals each, with values of ml within the ranges 0, ±1, ±2, ±3 respectively. Each of these orbitals can accommodate up to two electrons (with opposite spins), forming the basis of the periodic table.\n",
      "-In quantum mechanics, spin is an intrinsic property of all elementary particles. All known fermions, the particles that constitute ordinary matter, have a spin of 1/2. The spin number describes how many symmetrical facets a particle has in one full rotation; a spin of 1/2 means that the particle must be rotated by two full turns (through 720°) before it has the same configuration as when it started.\n",
      "\n",
      "\n",
      "\n",
      "What is the synapstor or synapse transistor?\n",
      "-In July 2008, Erokhin and Fontana claimed to have developed a polymeric memristor before the more recently announced titanium dioxide memristor.In 2010, Alibart, Gamrat, Vuillaume et al. introduced a new hybrid organic/nanoparticle device (the NOMFET : Nanoparticle Organic Memory Field Effect Transistor), which behaves as a memristor and which exhibits the main behavior of a biological spiking synapse. This device, also called a synapstor (synapse transistor), was used to demonstrate a neuro-inspired circuit (associative memory showing a pavlovian learning).In 2012, Crupi, Pradhan and Tozer described a proof of concept design to create neural synaptic memory circuits using organic ion-based memristors. The synapse circuit demonstrated long-term potentiation for learning as well as inactivity based forgetting. Using a grid of circuits, a pattern of light was stored and later recalled. This mimics the behavior of the V1 neurons in the primary visual cortex that act as spatiotemporal filters that process visual signals such as edges and moving lines.\n",
      "-The model for gated synapses was originally derived from the model electronic circuit, in which the gatekeeper serves as a transistor in a circuit. In a circuit, a transistor can act as a switch that turns an electrical signal on or off. In addition, a transistor can serve to amplify an existing current in a circuit. In effect, the gatekeeper neuron acts as the transistor of a gated synapse by modulating the transmission of the signal between the pre-synaptic and post-synaptic neurons.\n",
      "-A synaptic transistor has a traditional immediate response whose amount of current that passes between the source and drain contacts varies with voltage applied to the gate electrode. It also produces a much slower learned response such that the conductivity of the SNO layer varies in response to the transistor's STDP history, essentially by shuttling oxygen ions between the SNO and the ionic liquid.The analog of strengthening a synapse is to increase the SNO's conductivity, which essentially increases gain. Similarly, weakening a synapse is analogous to decreasing the SNO's conductivity, lowering the gain.The input and output of the synaptic transistor are continuous analog values, rather than digital on-off signals. While the physical structure of the device has the potential to learn from history, it contains no way to bias the transistor to control the memory effect. An external supervisory circuit converts the time delay between input and output into a voltage applied to the ionic liquid that either drives ions into the SNO or removes them.A network of such devices can learn particular responses to \"sensory inputs\", with those responses being learned through experience rather than explicitly programmed.\n",
      "-These types of devices would allow for a synapse model that could realise a learning rule, by which the synaptic efficacy is altered by voltages applied to the terminals of the device. An example of such a learning rule is spike-timing-dependant-plasticty by which the weight of the synapse, in this case the conductivity, could be modulated based on the timing of pre and post synaptic spikes arriving at each terminal. The advantage of this approach over two terminal memristive devices is that read and write protocols have the possibility to occur simultaneously and distinctly.\n",
      "-Research in 2004 has shown that synapses do not strengthen or weaken on a sliding scale. There are discrete states that synapses move between. These states are active, silent, recently silent, potentiated, and depressed. The states which they can move to are dependent on the state that they are in at the moment. Thus, the future state is determined by the state gained by previous activity. For instance, silent (but not recently silent) synapses can be converted to active via the insertion of AMPARs in the postsynaptic membrane. Active synapses can move to either potentiated or depressed via LTP or LTD respectively. Prolonged low-frequency stimulation (5 Hz, the method used to induce LTD) can move an active synapse to depressed and then silent. However, synapses that have just become active cannot be depressed or silenced. Thus there is state-machine-like behavior at the synapse when it comes to transitions. However, the states themselves can have varying degrees of intensity. One active-state synapse can be stronger than another active-state synapse. This is, in theory, how you can have a strong memory vs. a weak memory. The strong memories are the ones with very heavily populated active synapses, while weak memories may still be active but poorly populated with AMPARs. The same research has shown that NMDA receptors themselves, once thought to be the control mechanism behind AMPA receptor organization, can be regulated by synaptic activity. This regulation of the regulation mechanism itself adds another layer of complexity to the biology of the brain.\n",
      "\n",
      "\n",
      "\n",
      "What is spontaneous symmetry breaking?\n",
      "-Showing that a system admits spontaneous symmetry breaking requires introducing a weak external source field that breaks the symmetry and gives rise to a preferred ground state. The system is then taken to the thermodynamic limit after which the external source field is switched off. If the vacuum expectation value of symmetry non-invariant operators is nonzero in this limit then there is spontaneous symmetry breaking. Physically it means that the system never leaves the original ground state into which it was placed by the external field. For global symmetries this occurs because the energy barrier between the various ground states is proportional to the volume, so in the thermodynamic limit this diverges, locking the system into the ground state. Local symmetries get around this construction because the energy barrier between two ground states depends only on local features so transitions to different gauge related ground states can occur locally and does not require the field to change everywhere at the same time as it does for global symmetries.\n",
      "-Spontaneous symmetry breaking is a spontaneous process of symmetry breaking, by which a physical system in a symmetric state spontaneously ends up in an asymmetric state. In particular, it can describe systems where the equations of motion or the Lagrangian obey symmetries, but the lowest-energy vacuum solutions do not exhibit that same symmetry. When the system goes to one of those vacuum solutions, the symmetry is broken for perturbations around that vacuum even though the entire Lagrangian retains that symmetry.\n",
      "-A field theory admits numerous types of symmetries, with the two most common ones being global and local symmetries. Global symmetries are fields transformations acting the same way everywhere while local symmetries act on fields in a position dependent way. The latter correspond to redundancies in the description of the system. This is a consequence of Noether's second theorem which shows that each gauge symmetry degree of freedom corresponds to a relation among the Euler–Lagrange equations, making the system underdetermined. Underdeterminacy requires gauge fixing of the non-propagating components so that the equations of motion admits a unique solution.Spontaneous symmetry breaking occurs when the action of a theory has a symmetry but the vacuum state violates this symmetry. In that case there will exist a local operator that is non-invariant under the symmetry giving it a nonzero vacuum expectation value. Such non-invariant local operators always have vanishing vacuum expectation values for finite size systems prohibiting spontaneous symmetry breaking. This occurs because over large timescales, finite systems always transition between all its possible ground states, averaging away the expectation value to zero.While spontaneous symmetry breaking can occur for global symmetries, Elitzur's theorem states that the same is not the case for gauge symmetries; all vacuum expectation values of gauge non-invariant operators are vanishing, even in systems of infinite size. On the lattice this follows from the fact that integrating gauge non-invariant observables over a group measure always yields zero for compact gauge groups. Positivity of the measure and gauge invariance are sufficient to prove the theorem. This is also an explanation for why gauge symmetries are mere redundancies in lattice field theories, where the equations of motion need not define a well-posed problem as they do not need to be solved. Instead, Elitzur's theorem shows that any observable that is not invariant under the symmetry has a vanishing expectation value making it unobservable and thus redundant.\n",
      "-Spontaneous symmetry breaking When the Hamiltonian of a system (or the Lagrangian) has a certain symmetry, but the vacuum does not, then one says that spontaneous symmetry breaking (SSB) has taken place.\n",
      "-An interesting feature can occur if m2 turns negative, but with λ still positive. In this case, the vacuum consists of two lowest-energy states, each of which spontaneously breaks the Z2 global symmetry of the original theory. This leads to the appearance of interesting collective states like domain walls. In the O(2) theory, the vacua would lie on a circle, and the choice of one would spontaneously break the O(2) symmetry. A continuous broken symmetry leads to a Goldstone boson. This type of spontaneous symmetry breaking is the essential component of the Higgs mechanism.\n",
      "\n",
      "\n",
      "\n",
      "What is the proper distance for a redshift of 8.2?\n",
      "-The most distant astronomical object identified (as of 2022) is a galaxy classified as HD1, with a redshift of 13.27, corresponding to a distance of about 33.4 billion light years. In 2009, a gamma ray burst, GRB 090423, was found to have a redshift of 8.2, which indicates that the collapsing star that caused it exploded when the universe was only 630 million years old. The burst happened approximately 13 billion years ago, so a distance of about 13 billion light-years was widely quoted in the media (or sometimes a more precise figure of 13.035 billion light-years) - however, this would be the \"light travel distance\" (see Distance measures (cosmology)) rather than the \"proper distance\" used in both Hubble's law and in defining the size of the observable universe, and cosmologist Ned Wright argues against using this measure. The proper distance for a redshift of 8.2 would be about 9.2 Gpc, or about 30 billion light-years.\n",
      "-The decoupling, or the last scattering, is thought to have occurred about 300,000 years after the Big Bang, or at a redshift of about  1100 . We can determine both the approximate angular diameter of the universe and the physical size of the particle horizon that had existed at this time.  The angular diameter distance, in terms of redshift  z , is described by  dA(z)=r(z)/(1+z) . If we assume a flat cosmology then,  r(z)=∫temt0dta(t)=∫aem1daa2H(a)=∫0zdzH(z).\n",
      "-There are a few different definitions of \"distance\" in cosmology which are all asymptotic one to another for small redshifts. The expressions for these distances are most practical when written as functions of redshift  z , since redshift is always the observable. They can also be written as functions of scale factor  a=1/(1+z).\n",
      "In the remainder of this article, the peculiar velocity is assumed to be negligible unless specified otherwise.\n",
      "We first give formulas for several distance measures, and then describe them in more detail further down. Defining the \"Hubble distance\" as 3000 Mpc 9.26 10 25 h−1m where  c is the speed of light,  H0 is the Hubble parameter today, and h is the dimensionless Hubble constant, all the distances are asymptotic to  z⋅dH for small z.\n",
      "-The proper distance—the distance as would be measured at a specific time, including the present—between Earth and the edge of the observable universe is 46 billion light-years (14 billion parsecs), making the diameter of the observable universe about 93 billion light-years (28 billion parsecs). The distance the light from the edge of the observable universe has travelled is very close to the age of the universe times the speed of light, 13.8 billion light-years (4.2×10^9 pc), but this does not represent the distance at any given time because the edge of the observable universe and the Earth have since moved further apart. For comparison, the diameter of a typical galaxy is 30,000 light-years (9,198 parsecs), and the typical distance between two neighboring galaxies is 3 million light-years (919.8 kiloparsecs). As an example, the Milky Way is roughly 100,000–180,000 light-years in diameter, and the nearest sister galaxy to the Milky Way, the Andromeda Galaxy, is located roughly 2.5 million light-years away.Because humans cannot observe space beyond the edge of the observable universe, it is unknown whether the size of the universe in its totality is finite or infinite. Estimates suggest that the whole universe, if finite, must be more than 250 times larger than a Hubble sphere. Some disputed estimates for the total size of the universe, if finite, reach as high as  10 10 10 122 megaparsecs, as implied by a suggested resolution of the No-Boundary Proposal.\n",
      "-The \"distance\" of a far away galaxy depends on how it is measured. With a redshift of 1, light from this galaxy is estimated to have taken around 7.7 billion years to reach Earth. However, since this galaxy is receding from Earth, the present comoving distance is estimated to be around 10 billion light-years away. In context, Hubble is observing this galaxy as it appeared when the Universe was around 5.9 billion years old.\n",
      "\n",
      "\n",
      "\n",
      "Who was the first to determine the velocity of a star moving away from the Earth using the Doppler effect?\n",
      "-The history of the subject began with the development in the 19th century of classical wave mechanics and the exploration of phenomena associated with the Doppler effect. The effect is named after Christian Doppler, who offered the first known physical explanation for the phenomenon in 1842. The hypothesis was tested and confirmed for sound waves by the Dutch scientist Christophorus Buys Ballot in 1845. Doppler correctly predicted that the phenomenon should apply to all waves, and in particular suggested that the varying colors of stars could be attributed to their motion with respect to the Earth. Before this was verified, however, it was found that stellar colors were primarily due to a star's temperature, not motion. Only later was Doppler vindicated by verified redshift observations.The first Doppler redshift was described by French physicist Hippolyte Fizeau in 1848, who pointed to the shift in spectral lines seen in stars as being due to the Doppler effect. The effect is sometimes called the \"Doppler–Fizeau effect\". In 1868, British astronomer William Huggins was the first to determine the velocity of a star moving away from the Earth by this method. In 1871, optical redshift was confirmed when the phenomenon was observed in Fraunhofer lines using solar rotation, about 0.1 Å in the red. In 1887, Vogel and Scheiner discovered the annual Doppler effect, the yearly change in the Doppler shift of stars located near the ecliptic due to the orbital velocity of the Earth. In 1901, Aristarkh Belopolsky verified optical redshift in the laboratory using a system of rotating mirrors.Arthur Eddington used the term red shift as early as 1923. The word does not appear unhyphenated until about 1934 by Willem de Sitter.Beginning with observations in 1912, Vesto Slipher discovered that most spiral galaxies, then mostly thought to be spiral nebulae, had considerable redshifts. Slipher first reports on his measurement in the inaugural volume of the Lowell Observatory Bulletin. Three years later, he wrote a review in the journal Popular Astronomy. In it he states that \"the early discovery that the great Andromeda spiral had the quite exceptional velocity of –300 km(/s) showed the means then available, capable of investigating not only the spectra of the spirals but their velocities as well.\" Slipher reported the velocities for 15 spiral nebulae spread across the entire celestial sphere, all but three having observable \"positive\" (that is recessional) velocities. Subsequently, Edwin Hubble discovered an approximate relationship between the redshifts of such \"nebulae\" and the distances to them with the formulation of his eponymous Hubble's law. These observations corroborated Alexander Friedmann's 1922 work, in which he derived the Friedmann–Lemaître equations. In the present day they are considered strong evidence for an expanding universe and the Big Bang theory.\n",
      "-Doppler first proposed this effect in 1842 in his treatise \"Über das farbige Licht der Doppelsterne und einiger anderer Gestirne des Himmels\" (On the coloured light of the binary stars and some other stars of the heavens). The hypothesis was tested for sound waves by Buys Ballot in 1845. He confirmed that the sound's pitch was higher than the emitted frequency when the sound source approached him, and lower than the emitted frequency when the sound source receded from him. Hippolyte Fizeau discovered independently the same phenomenon on electromagnetic waves in 1848 (in France, the effect is sometimes called \"effet Doppler-Fizeau\" but that name was not adopted by the rest of the world as Fizeau's discovery was six years after Doppler's proposal). In Britain, John Scott Russell made an experimental study of the Doppler effect (1848).\n",
      "-The first measurements of the speed of light using completely terrestrial apparatus were published in 1849 by Hippolyte Fizeau (1819–96). Compared to values accepted today, Fizeau's result (about 313,000 kilometres per second) was too high, and less accurate than those obtained by Rømer's method. It would be another thirty years before A. A. Michelson in the United States published his more precise results (299,910±50 km/s) and Simon Newcomb confirmed the agreement with astronomical measurements, almost exactly two centuries after Rømer's announcement.\n",
      "-1832–1838 – Following over 100 years of unsuccessful attempts, Thomas Henderson, Friedrich Bessel, and Otto Struve measure the parallax of a few nearby stars; these are the first measurements of any distances outside the Solar System.1842 – Christian Doppler proposes the redshift and blueshift effects, based on an analog effect found in sound. Hippolyte Fizeau discovered independently the same phenomenon on electromagnetic waves in 1848.\n",
      "-A typical method to determine proper motion is to measure the position of a star relative to a limited, selected set of very distant objects that exhibit no mutual movement, and that, because of their distance, are assumed to have very small proper motion. Another approach is to compare photographs of a star at different times against a large background of more distant objects. The star with the largest known proper motion is Barnard's Star.Radial velocity of stars, and other deep-space objects, can be revealed spectroscopically thru the Doppler-Fizeau effect, by which the frequency of the received light decreases for objects that were receding (redshift) and increases for objects that were approaching (blueshift), when compared to the light emitted by a stationary object. William Huggins ventured in 1868 to estimate the radial velocity of Sirius with respect to the Sun, based on observed redshift of the star's light.The phrase \"fixed star\" is technically incorrect, but nonetheless it is used in an historical context, and in classical mechanics. When used as a visual reference for observations, they usually are called background stars or simply distant stars, still retaining the intuitive meaning of they being \"fixed\" in some practical sense.\n",
      "\n",
      "\n",
      "\n",
      "What is the information loss paradox in black holes?\n",
      "-Information loss paradox Because a black hole has only a few internal parameters, most of the information about the matter that went into forming the black hole is lost. Regardless of the type of matter which goes into a black hole, it appears that only information concerning the total mass, charge, and angular momentum are conserved. As long as black holes were thought to persist forever this information loss is not that problematic, as the information can be thought of as existing inside the black hole, inaccessible from the outside, but represented on the event horizon in accordance with the holographic principle. However, black holes slowly evaporate by emitting Hawking radiation. This radiation does not appear to carry any additional information about the matter that formed the black hole, meaning that this information appears to be gone forever.The question whether information is truly lost in black holes (the black hole information paradox) has divided the theoretical physics community. In quantum mechanics, loss of information corresponds to the violation of a property called unitarity, and it has been argued that loss of unitarity would also imply violation of conservation of energy, though this has also been disputed. Over recent years evidence has been building that indeed information and unitarity are preserved in a full quantum gravitational treatment of the problem.One attempt to resolve the black hole information paradox is known as black hole complementarity. In 2012, the \"firewall paradox\" was introduced with the goal of demonstrating that black hole complementarity fails to solve the information paradox. According to quantum field theory in curved spacetime, a single emission of Hawking radiation involves two mutually entangled particles. The outgoing particle escapes and is emitted as a quantum of Hawking radiation; the infalling particle is swallowed by the black hole. Assume a black hole formed a finite time in the past and will fully evaporate away in some finite time in the future. Then, it will emit only a finite amount of information encoded within its Hawking radiation. According to research by physicists like Don Page and Leonard Susskind, there will eventually be a time by which an outgoing particle must be entangled with all the Hawking radiation the black hole has previously emitted. This seemingly creates a paradox: a principle called \"monogamy of entanglement\" requires that, like any quantum system, the outgoing particle cannot be fully entangled with two other systems at the same time; yet here the outgoing particle appears to be entangled both with the infalling particle and, independently, with past Hawking radiation. In order to resolve this contradiction, physicists may eventually be forced to give up one of three time-tested principles: Einstein's equivalence principle, unitarity, or local quantum field theory. One possible solution, which violates the equivalence principle, is that a \"firewall\" destroys incoming particles at the event horizon. In general, which—if any—of these assumptions should be abandoned remains a topic of debate.\n",
      "-The simplest models of black hole evaporation lead to the black hole information paradox. The information content of a black hole appears to be lost when it dissipates, as under these models the Hawking radiation is random (it has no relation to the original information). A number of solutions to this problem have been proposed, including suggestions that Hawking radiation is perturbed to contain the missing information, that the Hawking evaporation leaves some form of remnant particle containing the missing information, and that information is allowed to be lost under these conditions.\n",
      "-The information paradox appears when one considers a process in which a black hole is formed through a physical process and then evaporates away entirely through Hawking radiation. Hawking's calculation suggests that the final state of radiation would retain information only about the total mass, electric charge and angular momentum of the initial state. Since many different states can have the same mass, charge and angular momentum, this suggests that many initial physical states could evolve into the same final state. Therefore, information about the details of the initial state would be permanently lost; however, this violates a core precept of both classical and quantum physics—that, in principle, the state of a system at one point in time should determine its value at any other time. Specifically, in quantum mechanics the state of the system is encoded by its wave function. The evolution of the wave function is determined by a unitary operator, and unitarity implies that the wave function at any instant of time can be used to determine the wave function either in the past or the future. In 1993, Don Page argued that if a black hole starts in a pure quantum state and evaporates completely by a unitary process, the von Neumann entropy of the Hawking radiation initially increases and then decreases back to zero when the black hole has disappeared. This is called the Page curve.It is now generally believed that information is preserved in black-hole evaporation. For many researchers, deriving the Page curve is synonymous with solving the black hole information puzzle.: 291  However, views differ as to how, precisely, Hawking's original semi-classical calculation should be corrected. In recent years, several extensions of the original paradox have been explored. Taken together these puzzles about black hole evaporation have implications for how gravity and quantum mechanics must be combined, leading to the information paradox remaining an active field of research within quantum gravity.\n",
      "-Information is stored in a large remnantThis idea suggests that Hawking radiation stops before the black hole reaches the Planck size. Since the black hole never evaporates, information about its initial state can remain inside the black hole and the paradox disappears. However, there is no accepted mechanism that would allow Hawking radiation to stop while the black hole remains macroscopic.\n",
      "-Information is irretrievably lost A minority view within the theoretical physics community is that information is genuinely lost when black holes form and evaporate.\n",
      "This conclusion follows if one assumes that the predictions of semiclassical gravity and the causal structure of the black-hole spacetime are exact.\n",
      "\n",
      "\n",
      "\n",
      "What is the Kutta condition?\n",
      "-Kutta condition – is a principle in steady-flow fluid dynamics, especially aerodynamics, that is applicable to solid bodies with sharp corners, such as the trailing edges of airfoils. It is named for German mathematician and aerodynamicist Martin Kutta.Kuethe and Schetzer state the Kutta condition as follows:: § 4.11 A body with a sharp trailing edge which is moving through a fluid will create about itself a circulation of sufficient strength to hold the rear stagnation point at the trailing edge.In fluid flow around a body with a sharp corner, the Kutta condition refers to the flow pattern in which fluid approaches the corner from above and below, meets at the corner, and then flows away from the body. None of the fluid flows around the sharp corner.The Kutta condition is significant when using the Kutta–Joukowski theorem to calculate the lift created by an airfoil with a sharp trailing edge. The value of circulation of the flow around the airfoil must be that value that would cause the Kutta condition to exist.Kutta–Joukowski theorem – is a fundamental theorem in aerodynamics used for the calculation of lift of an airfoil and any two-dimensional bodies including circular cylinders translating into a uniform fluid at a constant speed large enough so that the flow seen in the body-fixed frame is steady and unseparated. The theorem relates the lift generated by an airfoil to the speed of the airfoil through the fluid, the density of the fluid and the circulation around the airfoil. The circulation is defined as the line integral around a closed-loop enclosing the airfoil of the component of the velocity of the fluid tangent to the loop. It is named after Martin Kutta and Nikolai Zhukovsky (or Joukowski) who first developed its key ideas in the early 20th century. Kutta–Joukowski theorem is an inviscid theory, but it is a good approximation for real viscous flow in typical aerodynamic applications.\n",
      "-The Kutta condition is a principle in steady-flow fluid dynamics, especially aerodynamics, that is applicable to solid bodies with sharp corners, such as the trailing edges of airfoils. It is named for German mathematician and aerodynamicist Martin Kutta.\n",
      "Kuethe and Schetzer state the Kutta condition as follows:: § 4.11 A body with a sharp trailing edge which is moving through a fluid will create about itself a circulation of sufficient strength to hold the rear stagnation point at the trailing edge.\n",
      "In fluid flow around a body with a sharp corner, the Kutta condition refers to the flow pattern in which fluid approaches the corner from above and below, meets at the corner, and then flows away from the body. None of the fluid flows around the sharp corner.\n",
      "The Kutta condition is significant when using the Kutta–Joukowski theorem to calculate the lift created by an airfoil with a sharp trailing edge. The value of circulation of the flow around the airfoil must be that value which would cause the Kutta condition to exist.\n",
      "-The Kutta condition allows an aerodynamicist to incorporate a significant effect of viscosity while neglecting viscous effects in the underlying conservation of momentum equation. It is important in the practical calculation of lift on a wing.\n",
      "-Any real fluid is viscous, which implies that the fluid velocity vanishes on the airfoil. Prandtl showed that for large Reynolds number, defined as  Re =ρV∞cAμ , and small angle of attack, the flow around a thin airfoil is composed of a narrow viscous region called the boundary layer near the body and an inviscid flow region outside. In applying the Kutta-Joukowski theorem, the loop must be chosen outside this boundary layer. (For example, the circulation calculated using the loop corresponding to the surface of the airfoil would be zero for a viscous fluid.) The sharp trailing edge requirement corresponds physically to a flow in which the fluid moving along the lower and upper surfaces of the airfoil meet smoothly, with no fluid moving around the trailing edge of the airfoil. This is known as the Kutta condition.\n",
      "-In irrotational, inviscid, incompressible flow (potential flow) over an airfoil, the Kutta condition can be implemented by calculating the stream function over the airfoil surface.\n",
      "The same Kutta condition implementation method is also used for solving two dimensional subsonic (subcritical) inviscid steady compressible flows over isolated airfoils.\n",
      "The viscous correction for the Kutta condition can be found in some of the recent studies.\n",
      "\n",
      "\n",
      "\n",
      "What is classical mechanics?\n",
      "-Classical mechanics is a physical theory describing the motion of macroscopic objects, from projectiles to parts of machinery and astronomical objects, such as spacecraft, planets, stars, and galaxies. For objects governed by classical mechanics, if the present state is known, it is possible to predict how it will move in the future (determinism), and how it has moved in the past (reversibility).\n",
      "-Classical Classical physics includes the traditional branches and topics that were recognized and well-developed before the beginning of the 20th century—classical mechanics, acoustics, optics, thermodynamics, and electromagnetism. Classical mechanics is concerned with bodies acted on by forces and bodies in motion and may be divided into statics (study of the forces on a body or bodies not subject to an acceleration), kinematics (study of motion without regard to its causes), and dynamics (study of motion and the forces that affect it); mechanics may also be divided into solid mechanics and fluid mechanics (known together as continuum mechanics), the latter include such branches as hydrostatics, hydrodynamics, aerodynamics, and pneumatics. Acoustics is the study of how sound is produced, controlled, transmitted and received. Important modern branches of acoustics include ultrasonics, the study of sound waves of very high frequency beyond the range of human hearing; bioacoustics, the physics of animal calls and hearing, and electroacoustics, the manipulation of audible sound waves using electronics.Optics, the study of light, is concerned not only with visible light but also with infrared and ultraviolet radiation, which exhibit all of the phenomena of visible light except visibility, e.g., reflection, refraction, interference, diffraction, dispersion, and polarization of light. Heat is a form of energy, the internal energy possessed by the particles of which a substance is composed; thermodynamics deals with the relationships between heat and other forms of energy. Electricity and magnetism have been studied as a single branch of physics since the intimate connection between them was discovered in the early 19th century; an electric current gives rise to a magnetic field, and a changing magnetic field induces an electric current. Electrostatics deals with electric charges at rest, electrodynamics with moving charges, and magnetostatics with magnetic poles at rest.\n",
      "-There are many branches of classical mechanics, such as: statics, dynamics, kinematics, continuum mechanics (which includes fluid mechanics), statistical mechanics, etc.\n",
      "Mechanics: A branch of physics in which we study the object and properties of an object in form of a motion under the action of the force.\n",
      "-Mechanics (from Ancient Greek: μηχανική, mēkhanikḗ, lit. \"of machines\") is the area of mathematics and physics concerned with the relationships between force, matter, and motion among physical objects. Forces applied to objects result in displacements or changes of an object's position relative to its environment.\n",
      "Theoretical expositions of this branch of physics has its origins in Ancient Greece, for instance, in the writings of Aristotle and Archimedes (see History of classical mechanics and Timeline of classical mechanics). During the early modern period, scientists such as Galileo, Kepler, Huygens, and Newton laid the foundation for what is now known as classical mechanics.\n",
      "As a branch of classical physics, mechanics deals with bodies that are either at rest or are moving with velocities significantly less than the speed of light. It can also be defined as the physical science that deals with the motion of and forces on bodies not in the quantum realm.\n",
      "-Classical mechanics – describes the motion of macroscopic objects, from projectiles to parts of machinery, and astronomical objects, such as spacecraft, planets, stars and galaxies.\n",
      "\n",
      "\n",
      "\n",
      "Who shared the other half of the Nobel Prize with Yoichiro Nambu for discovering the origin of the explicit breaking of CP symmetry in the weak interactions?\n",
      "-On October 7, 2008, the Royal Swedish Academy of Sciences awarded the 2008 Nobel Prize in Physics to three scientists for their work in subatomic physics symmetry breaking. Yoichiro Nambu, of the University of Chicago, won half of the prize for the discovery of the mechanism of spontaneous broken symmetry in the context of the strong interactions, specifically chiral symmetry breaking. Physicists Makoto Kobayashi and Toshihide Maskawa, of Kyoto University, shared the other half of the prize for discovering the origin of the explicit breaking of CP symmetry in the weak interactions. This origin is ultimately reliant on the Higgs mechanism, but, so far understood as a \"just so\" feature of Higgs couplings, not a spontaneously broken symmetry phenomenon.\n",
      "-Modern progress In 1963, American physicist Sheldon Glashow proposed that the weak nuclear force, electricity, and magnetism could arise from a partially unified electroweak theory. In 1967, Pakistani Abdus Salam and American Steven Weinberg independently revised Glashow's theory by having the masses for the W particle and Z particle arise through spontaneous symmetry breaking with the Higgs mechanism. This unified theory modeled the electroweak interaction as a force mediated by four particles: the photon for the electromagnetic aspect, and a neutral Z particle, and two charged W particles for the weak aspect. As a result of the spontaneous symmetry breaking, the weak force becomes short-range and the W and Z bosons acquire masses of 80.4 and 91.2 GeV/c2, respectively. Their theory was first given experimental support by the discovery of weak neutral currents in 1973. In 1983, the Z and W bosons were first produced at CERN by Carlo Rubbia's team. For their insights, Glashow, Salam, and Weinberg were awarded the Nobel Prize in Physics in 1979. Carlo Rubbia and Simon van der Meer received the Prize in 1984.\n",
      "-However, this theory allowed a compound symmetry CP to be conserved. CP combines parity P (switching left to right) with charge conjugation C (switching particles with antiparticles). Physicists were again surprised when in 1964, James Cronin and Val Fitch provided clear evidence in kaon decays that CP symmetry could be broken too, winning them the 1980 Nobel Prize in Physics. In 1973, Makoto Kobayashi and Toshihide Maskawa showed that CP violation in the weak interaction required more than two generations of particles, effectively predicting the existence of a then unknown third generation. This discovery earned them half of the 2008 Nobel Prize in Physics.Unlike parity violation, CP violation occurs only in rare circumstances. Despite its limited occurrence under present conditions, it is widely believed to be the reason that there is much more matter than antimatter in the universe, and thus forms one of Andrei Sakharov's three conditions for baryogenesis.\n",
      "-Charge violation was confirmed in the Wu experiment and in experiments performed by Valentine Telegdi and Jerome Friedman and Garwin and Lederman who observed parity non-conservation in pion and muon decay and found that C is also violated. Charge violation was more explicitly shown in experiments done by John Riley Holt at the University of Liverpool.Oehme then wrote a paper with Lee and Yang in which they discussed the interplay of non-invariance under P, C and T. The same result was also independently obtained by Ioffe, Okun and Rudik. Both groups also discussed possible CP violations in neutral kaon decays.Lev Landau proposed in 1957 CP-symmetry, often called just CP as the true symmetry between matter and antimatter. CP-symmetry is the product of two transformations: C for charge conjugation and P for parity. In other words, a process in which all particles are exchanged with their antiparticles was assumed to be equivalent to the mirror image of the original process and so the combined CP-symmetry would be conserved in the weak interaction.\n",
      "-Following the success of quantum electrodynamics in the 1950s, attempts were undertaken to formulate a similar theory of the weak nuclear force. This culminated around 1968 in a unified theory of electromagnetism and weak interactions by Sheldon Glashow, Steven Weinberg, and Abdus Salam, for which they shared the 1979 Nobel Prize in Physics. Their electroweak theory postulated not only the W bosons necessary to explain beta decay, but also a new Z boson that had never been observed.\n",
      "\n",
      "\n",
      "\n",
      "What are some models that attempt to account for all observations without invoking supplemental non-baryonic matter?\n",
      "-Photons, W bosons, and Z bosons, excitations of the electroweak gauge fields.\n",
      "Higgs bosons, excitations of one component of the Higgs field, which gives mass to fundamental particles.In addition, composite particles such as mesons, as well as quasiparticles, can be described as excitations of an effective field.\n",
      "Gravity is not a part of the Standard Model, but it is thought that there may be particles called gravitons which are the excitations of gravitational waves. The status of this particle is still tentative, because the theory is incomplete and because the interactions of single gravitons may be too weak to be detected.\n",
      "-The Higgs boson plays a unique role in the Standard Model, by explaining why the other elementary particles, except the photon and gluon, are massive. In particular, the Higgs boson explains why the photon has no mass, while the W and Z bosons are very heavy. Elementary-particle masses and the differences between electromagnetism (mediated by the photon) and the weak force (mediated by the W and Z bosons) are critical to many aspects of the structure of microscopic (and hence macroscopic) matter. In electroweak theory, the Higgs boson generates the masses of the leptons (electron, muon, and tau) and quarks. As the Higgs boson is massive, it must interact with itself.\n",
      "-Their high masses limit the range of the weak interaction. By way of contrast, the photon is the force carrier of the electromagnetic force and has zero mass, consistent with the infinite range of electromagnetism; the hypothetical graviton is also expected to have zero mass. (Although gluons are also presumed to have zero mass, the range of the color force is limited for different reasons; see color confinement.) All three bosons have particle spin s = 1. The emission of a W+ or W− boson either lowers or raises the electric charge of the emitting particle by one unit, and also alters the spin by one unit. At the same time, the emission or absorption of a W± boson can change the type of the particle – for example changing a strange quark into an up quark. The neutral Z boson cannot change the electric charge of any particle, nor can it change any other of the so-called \"charges\" (such as strangeness, baryon number, charm, etc.). The emission or absorption of a Z0 boson can only change the spin, momentum, and energy of the other particle. (See also Weak neutral current.) \n",
      "-For the derivation of Einstein's equations from an entropic gravity perspective, Tower Wang shows that the inclusion of energy-momentum conservation and cosmological homogeneity and isotropy requirements severely restricts a wide class of potential modifications of entropic gravity, some of which have been used to generalize entropic gravity beyond the singular case of an entropic model of Einstein's equations. Wang asserts that: As indicated by our results, the modified entropic gravity models of form (2), if not killed, should live in a very narrow room to assure the energy-momentum conservation and to accommodate a homogeneous isotropic universe.\n",
      "-Higgs boson Although the weak and electromagnetic forces appear quite different to us at everyday energies, the two forces are theorized to unify as a single electroweak force at high energies. This prediction was clearly confirmed by measurements of cross-sections for high-energy electron-proton scattering at the HERA collider at DESY. The differences at low energies is a consequence of the high masses of the W and Z bosons, which in turn are a consequence of the Higgs mechanism. Through the process of spontaneous symmetry breaking, the Higgs selects a special direction in electroweak space that causes three electroweak particles to become very heavy (the weak bosons) and one to remain with an undefined rest mass as it is always in motion (the photon). On 4 July 2012, after many years of experimentally searching for evidence of its existence, the Higgs boson was announced to have been observed at CERN's Large Hadron Collider. Peter Higgs who first posited the existence of the Higgs boson was present at the announcement. The Higgs boson is believed to have a mass of approximately 125 GeV. The statistical significance of this discovery was reported as 5 sigma, which implies a certainty of roughly 99.99994%. In particle physics, this is the level of significance required to officially label experimental observations as a discovery. Research into the properties of the newly discovered particle continues.\n",
      "\n",
      "\n",
      "\n",
      "What is the purpose of the proximity-focusing design in a RICH detector?\n",
      "-The most advanced type of a detector is the RICH, or ring-imaging Cherenkov detector, developed in the 1980s. In a RICH detector, a cone of Cherenkov light is produced when a high-speed charged particle traverses a suitable medium, often called radiator. This light cone is detected on a position sensitive planar photon detector, which allows reconstructing a ring or disc, whose radius is a measure for the Cherenkov emission angle. Both focusing and proximity-focusing detectors are in use. In a focusing RICH detector, the photons are collected by a spherical mirror and focused onto the photon detector placed at the focal plane. The result is a circle with a radius independent of the emission point along the particle track. This scheme is suitable for low refractive index radiators—i.e. gases—due to the larger radiator length needed to create enough photons. In the more compact proximity-focusing design, a thin radiator volume emits a cone of Cherenkov light which traverses a small distance—the proximity gap—and is detected on the photon detector plane. The image is a ring of light whose radius is defined by the Cherenkov emission angle and the proximity gap. The ring thickness is determined by the thickness of the radiator. An example of a proximity gap RICH detector is the High Momentum Particle Identification Detector (HMPID), a detector currently under construction for ALICE (A Large Ion Collider Experiment), one of the six experiments at the LHC (Large Hadron Collider) at CERN.\n",
      "-In the more compact proximity-focusing design a thin radiator volume emits a cone of Cherenkov light which traverses a small distance, the proximity gap, and is detected on the photon detector plane. The image is a ring of light the radius of which is defined by the Cherenkov emission angle and the proximity gap. The ring thickness is mainly determined by the thickness of the radiator. An example of a proximity gap RICH detector is the High Momentum Particle Identification (HMPID), one of the detectors of ALICE (A Large Ion Collider Experiment), which is one of the five experiments at the LHC (Large Hadron Collider) at CERN.\n",
      "-If a dense medium (large refractive index) is used, only a thin radiator layer of the order of a few centimetres is required to emit a sufficient number of Cherenkov photons. The photon detector is then located at some distance (usually about 10 cm) behind the radiator, allowing the cone of light to expand and form the characteristic ring-shaped image. Such a proximity-focusing RICH is installed in the ALICE experiment.\n",
      "-In a RICH detector the photons within this light-cone pass through an optical system and impinge upon a position sensitive photon detector. With a suitably focusing optical system this allows reconstruction of a ring, similar to that above, the radius of which gives a measure of the Cherenkov emission angle  θc . The resolving power of this method is illustrated by comparing the Cherenkov angle per photon, see the first plot above, with the mean Cherenkov angle per particle (averaged over all photons emitted by that particle) obtained by ring-imaging, shown below; the greatly enhanced separation between particle types is very clear: This ability of a RICH system to successfully resolve different hypotheses for the particle type depends on two principal factors, which in turn depend upon the listed sub-factors; The effective angular resolution per photon,  σ Chromatic dispersion in the radiator ( n varies with photon frequency) Aberrations in the optical system Position resolution of the photon detector The maximum number of detected photons in the ring-image,  Nc The length of radiator through which the particle travels Photon transmission through the radiator material Photon transmission through the optical system Quantum efficiency of the photon detectors σ is a measure of the intrinsic optical precision of the RICH detector.  Nc is a measure of the optical response of the RICH; it can be thought of as the limiting case of the number of actually detected photons produced by a particle whose velocity approaches that of light, averaged over all relevant particle trajectories in the RICH detector. The average number of Cherenkov photons detected, for a slower particle, of charge  q (normally ±1), emitting photons at angle  θc is then sin 2⁡(θc)1−1n2 and the precision with which the mean Cherenkov angle can be determined with these photons is approximately σm=σN to which the angular precision of the emitting particle's measured direction must be added in quadrature, if it is not negligible compared to  σm Given the known momentum of the emitting particle and the refractive index of the radiator, the expected Cherenkov angle for each particle type can be predicted, and its difference from the observed mean Cherenkov angle calculated. Dividing this difference by  σm then gives a measure of the 'number of sigma' deviation of the hypothesis from the observation, which can be used in computing a probability or likelihood for each possible hypothesis. The following figure shows the 'number of sigma' deviation of the kaon hypothesis from a true pion ring image (π not k) and of the pion hypothesis from a true kaon ring image (k not π), as a function of momentum, for a RICH with  n = 1.0005,  Nc = 25,  σ = 0.64 milliradians; Also shown are the average number of detected photons from pions(Ngπ) or from kaons(Ngk). One can see that the RICH's ability to separate the two particle types exceeds 4-sigma everywhere between threshold and 80 GeV/c, finally dropping below 3-sigma at about 100 GeV. It is important to note that this result is for an 'ideal' detector, with homogeneous acceptance and efficiency, normal error distributions and zero background. No such detector exists, of course, and in a real experiment much more sophisticated procedures are actually used to account for those effects; position dependent acceptance and efficiency; non-Gaussian error distributions; non negligible and variable event-dependent backgrounds.In practice, for the multi-particle final states produced in a typical collider experiment, separation of kaons from other final state hadrons, mainly pions, is the most important purpose of the RICH. In that context the two most vital RICH functions, which maximise signal and minimise combinatorial backgrounds, are its ability to correctly identify a kaon as a kaon and its ability not to misidentify a pion as a kaon. The related probabilities, which are the usual measures of signal detection and background rejection in real data, are plotted below to show their variation with momentum (simulation with 10% random background); Note that the ~30% π → k misidentification rate at 100 GeV is, for the most part, due to the presence of 10% background hits (faking photons) in the simulated detector; the 3-sigma separation in the mean Cherenkov angle (shown in the 4th plot above) would, by itself, only account for about 6% misidentification. More detailed analyses of the above type, for operational RICH detectors, can be found in the published literature.\n",
      "-RICH Types Both focusing and proximity-focusing detectors are in use. In a focusing RICH detector, the photons are collected by a spherical mirror with focal length  f and focused onto the photon detector placed at the focal plane. The result is a circle with a radius  r=fθc , independent of the emission point along the particle's track ( θc≪1 ). This scheme is suitable for low refractive index radiators (i.e., gases) with their larger radiator length needed to create enough photons.\n",
      "\n",
      "\n",
      "\n",
      "What is a light-year?\n",
      "-A light-year, alternatively spelled light year, is a unit of length used to express astronomical distances and is equivalent to about 9.46 trillion kilometers (9.46×1012 km), or 5.88 trillion miles (5.88×1012 mi). As defined by the International Astronomical Union (IAU), a light-year is the distance that light travels in a vacuum in one Julian year (365.25 days). Because it includes the word \"year\", the term is sometimes misinterpreted as a unit of time.The light-year is most often used when expressing distances to stars and other distances on a galactic scale, especially in non-specialist contexts and popular science publications. The unit most commonly used in professional astronomy is the parsec (symbol: pc, about 3.26 light-years) which derives from astrometry; it is the distance at which one astronomical unit (au) subtends an angle of one second of arc.\n",
      "-light-year (ly) A unit of length used to express astronomical distances that is equivalent to the distance that an object moving at the speed of light in vacuum would travel in one Julian year: approximately 9.46 trillion kilometres (9.46×1012 km) or 5.88 trillion miles (5.88×1012 mi). Though the light-year is often used to measure galactic-scale distances in non-specialist publications, the unit of length most commonly used in professional astrometry is the parsec.\n",
      "-Distances between objects within a star system tend to be small fractions of a light-year, and are usually expressed in astronomical units. However, smaller units of length can similarly be formed usefully by multiplying units of time by the speed of light. For example, the light-second, useful in astronomy, telecommunications and relativistic physics, is exactly 299792458 metres or 1⁄31557600 of a light-year. Units such as the light-minute, light-hour and light-day are sometimes used in popular science publications. The light-month, roughly one-twelfth of a light-year, is also used occasionally for approximate measures. The Hayden Planetarium specifies the light month more precisely as 30 days of light travel time.Light travels approximately one foot in a nanosecond; the term \"light-foot\" is sometimes used as an informal measure of time.\n",
      "-Subsequent explorations of the Solar System by space probes made it possible to obtain precise measurements of the relative positions of the inner planets and other objects by means of radar and telemetry. As with all radar measurements, these rely on measuring the time taken for photons to be reflected from an object. Because all photons move at the speed of light in vacuum, a fundamental constant of the universe, the distance of an object from the probe is calculated as the product of the speed of light and the measured time. However, for precision the calculations require adjustment for things such as the motions of the probe and object while the photons are transiting. In addition, the measurement of the time itself must be translated to a standard scale that accounts for relativistic time dilation. Comparison of the ephemeris positions with time measurements expressed in Barycentric Dynamical Time (TDB) leads to a value for the speed of light in astronomical units per day (of 86400 s). By 2009, the IAU had updated its standard measures to reflect improvements, and calculated the speed of light at 173.1446326847(69) au/d (TDB).In 1983, the CIPM modified the International System of Units (SI) to make the metre defined as the distance travelled in a vacuum by light in 1 / 299792458 second. This replaced the previous definition, valid between 1960 and 1983, which was that the metre equalled a certain number of wavelengths of a certain emission line of krypton-86. (The reason for the change was an improved method of measuring the speed of light.) The speed of light could then be expressed exactly as c0 = 299792458 m/s, a standard also adopted by the IERS numerical standards. From this definition and the 2009 IAU standard, the time for light to traverse an astronomical unit is found to be τA = 499.0047838061±0.00000001 s, which is slightly more than 8 minutes 19 seconds. By multiplication, the best IAU 2009 estimate was A = c0τA = 149597870700±3 m, based on a comparison of Jet Propulsion Laboratory and IAA–RAS ephemerides.In 2006, the BIPM reported a value of the astronomical unit as 1.49597870691(6)×1011 m. In the 2014 revision of the SI Brochure, the BIPM recognised the IAU's 2012 redefinition of the astronomical unit as 149597870700 m.This estimate was still derived from observation and measurements subject to error, and based on techniques that did not yet standardize all relativistic effects, and thus were not constant for all observers. In 2012, finding that the equalization of relativity alone would make the definition overly complex, the IAU simply used the 2009 estimate to redefine the astronomical unit as a conventional unit of length directly tied to the metre (exactly 149597870700 m). The new definition also recognizes as a consequence that the astronomical unit is now to play a role of reduced importance, limited in its use to that of a convenience in some applications.\n",
      "-An equivalent formulation of the old definition of the astronomical unit is the radius of an unperturbed circular Newtonian orbit about the Sun of a particle having infinitesimal mass, moving with a mean motion of 0.01720209895 radians per day.  The speed of light in IAU is the defined value c0 = 299792458 m/s of the SI units. In terms of this speed, the old definition of the astronomical unit of length had the accepted value: 1 au = c0τA = (149597870700±3) m, where τA is the transit time of light across the astronomical unit. The astronomical unit of length was determined by the condition that the measured data in the ephemeris match observations, and that in turn decides the transit time τA.\n",
      "\n",
      "\n",
      "\n",
      "What is the main advantage of ferroelectric memristors?\n",
      "-Ferroelectric memristor The ferroelectric memristor is based on a thin ferroelectric barrier sandwiched between two metallic electrodes. Switching the polarization of the ferroelectric material by applying a positive or negative voltage across the junction can lead to a two order of magnitude resistance variation: ROFF ≫ RON (an effect called Tunnel Electro-Resistance). In general, the polarization does not switch abruptly. The reversal occurs gradually through the nucleation and growth of ferroelectric domains with opposite polarization. During this process, the resistance is neither RON or ROFF, but in between. When the voltage is cycled, the ferroelectric domain configuration evolves, allowing a fine tuning of the resistance value. The ferroelectric memristor's main advantages are that ferroelectric domain dynamics can be tuned, offering a way to engineer the memristor response, and that the resistance variations are due to purely electronic phenomena, aiding device reliability, as no deep change to the material structure is involved.\n",
      "-Atomristor Atomristor is defined as the electrical devices showing memristive behavior in atomically thin nanomaterials or atomic sheets. In 2018, Ge and Wu et al. in the Akinwande group at the University of Texas, first reported a universal memristive effect in single-layer TMD (MX2, M = Mo, W; and X = S, Se) atomic sheets based on vertical metal-insulator-metal (MIM) device structure. The work was later extended to monolayer hexagonal boron nitride, which is the thinnest memory material of around 0.33 nm. These atomristors offer forming-free switching and both unipolar and bipolar operation. The switching behavior is found in single-crystalline and poly-crystalline films, with various conducting electrodes (gold, silver and graphene). Atomically thin TMD sheets are prepared via CVD/MOCVD, enabling low-cost fabrication. Afterwards, taking advantage of the low \"on\" resistance and large on/off ratio, a high-performance zero-power RF switch is proved based on MoS2 or h-BN atomristors, indicating a new application of memristors for 5G, 6G and THz communication and connectivity systems. In 2020, atomistic understanding of the conductive virtual point mechanism was elucidated in an article in nature nanotechnology.\n",
      "-Memristive networks are a particular type of physical neural network that have very similar properties to (Little-)Hopfield networks, as they have a continuous dynamics, have a limited memory capacity and they natural relax via the minimization of a function which is asymptotic to the Ising model. In this sense, the dynamics of a memristive circuit has the advantage compared to a Resistor-Capacitor network to have a more interesting non-linear behavior. From this point of view, engineering an analog memristive networks accounts to a peculiar type of neuromorphic engineering in which the device behavior depends on the circuit wiring, or topology.\n",
      "-Multiferroicity and magnetic-ferroelectric crossover. The PJTE theory of ferroelectricity in ABO3 crystals was expanded to show that, depending on the number of electrons in the dn shell of the transition metal ion B4+ and their low spin or high spin arrangement (which controls the symmetry and spin multiplicity of the ground and PJTE active excited states of the [BO6] center), the ferroelectricity may coexist with a magnetic moment (multiferroicity). Moreover, in combination with the temperature dependent spin crossover phenomenon (which changes the spin multiplicity), this kind of multiferroicity may lead to a novel effect known as a magnetic-ferroelectric crossover.Solid state magnetic-dielectric bistability. Similar to the above-mentioned molecular bistability induced by the hidden PJTE, a magnetic-dielectric bistability due to two coexisting equilibrium configurations with corresponding properties may take place also in crystals with transition metal centers, subject to the electronic configuration with half-filled e2 or t3 shells. As in molecular systems, the latter produce a hidden PJTE and local bistability which, distinguished from the molecular case, are enhanced by the cooperative interactions, thus acquiring larger lifetimes. This crystal bistability was proved by calculations for LiCuO2 and NaCuO2 crystals, in which the Cu3+ ion has the electronic e2(d8) configuration (similar to the CuF3 molecule).Giant enhancement of observable properties in interaction with external perturbations. In a recent development it was shown that in inorganic crystals with PJTE centers, in which the local distortions are not ordered (before the phase transition to the cooperative phase), the effect of interaction with external perturbations contains an orientational contribution which enhances the observable properties by several orders of magnitude. This was demonstrated on the properties of crystals like paraelectric BaTiO3 in interaction with electric fields (in permittivity and electrostriction), or under a strain gradient (flexoelectricity). These giant enhancement effects occur due to the dynamic nature of the PJTE local dipolar distortions (their tunneling between the equivalent minima); the independently rotating dipole moments on each center become oriented (frozen) along the external perturbation resulting in an orientational polarization which is not there in the absence of the PJTE \n",
      "-Layered memristor In 2014, Bessonov et al. reported a flexible memristive device comprising a MoOx/MoS2 heterostructure sandwiched between silver electrodes on a plastic foil. The fabrication method is entirely based on printing and solution-processing technologies using two-dimensional layered transition metal dichalcogenides (TMDs). The memristors are mechanically flexible, optically transparent and produced at low cost. The memristive behaviour of switches was found to be accompanied by a prominent memcapacitive effect. High switching performance, demonstrated synaptic plasticity and sustainability to mechanical deformations promise to emulate the appealing characteristics of biological neural systems in novel computing technologies.\n",
      "\n",
      "\n",
      "\n",
      "What is the term used to describe the conduction that occurs in non-crystalline semiconductors by charges quantum tunnelling from one localised site to another?\n",
      "-Extrinsic (doped) semiconductors have a far more complicated temperature profile. As temperature increases starting from absolute zero they first decrease steeply in resistance as the carriers leave the donors or acceptors. After most of the donors or acceptors have lost their carriers, the resistance starts to increase again slightly due to the reducing mobility of carriers (much as in a metal). At higher temperatures, they behave like intrinsic semiconductors as the carriers from the donors/acceptors become insignificant compared to the thermally generated carriers.In non-crystalline semiconductors, conduction can occur by charges quantum tunnelling from one localised site to another. This is known as variable range hopping and has the characteristic form of where n = 2, 3, 4, depending on the dimensionality of the system.\n",
      "-Unlike in metals, the atoms that make up the bulk semiconductor crystal do not provide the electrons which are responsible for conduction. In semiconductors, electrical conduction is due to the mobile charge carriers, electrons or holes which are provided by impurities or dopant atoms in the crystal. In an extrinsic semiconductor, the concentration of doping atoms in the crystal largely determines the density of charge carriers, which determines its electrical conductivity, as well as a great many other electrical properties. This is the key to semiconductors' versatility; their conductivity can be manipulated over many orders of magnitude by doping.\n",
      "-The electrical conductivity of chemically pure semiconductors can still be affected by crystallographic defects of technological origin (like vacancies), some of which can behave similar to dopants. Their effect can often be neglected, though, and the number of electrons in the conduction band is then exactly equal to the number of holes in the valence band. The conduction of current of intrinsic semiconductor is enabled purely by electron excitation across the band-gap, which is usually small at room temperature except for narrow-bandgap semiconductors, like Hg0.8Cd0.2Te.\n",
      "-In semiconductors, which are the materials used to make electronic components like transistors and integrated circuits, two types of charge carrier are possible. In p-type semiconductors, \"effective particles\" known as electron holes with positive charge move through the crystal lattice, producing an electric current. The \"holes\" are, in effect, electron vacancies in the valence-band electron population of the semiconductor and are treated as charge carriers because they are mobile, moving from atom site to atom site. In n-type semiconductors, electrons in the conduction band move through the crystal, resulting in an electric current.In some conductors, such as ionic solutions and plasmas, positive and negative charge carriers coexist, so in these cases an electric current consists of the two types of carrier moving in opposite directions. In other conductors, such as metals, there are only charge carriers of one polarity, so an electric current in them simply consists of charge carriers moving in one direction.\n",
      "-Semiconductor materials are useful because their behavior can be easily manipulated by the deliberate addition of impurities, known as doping. Semiconductor conductivity can be controlled by the introduction of an electric or magnetic field, by exposure to light or heat, or by the mechanical deformation of a doped monocrystalline silicon grid; thus, semiconductors can make excellent sensors. Current conduction in a semiconductor occurs due to mobile or \"free\" electrons and electron holes, collectively known as charge carriers. Doping a semiconductor with a small proportion of an atomic impurity, such as phosphorus or boron, greatly increases the number of free electrons or holes within the semiconductor. When a doped semiconductor contains excess holes, it is called a p-type semiconductor (p for positive electric charge); when it contains excess free electrons, it is called an n-type semiconductor (n for a negative electric charge). A majority of mobile charge carriers have negative charges. The manufacture of semiconductors controls precisely the location and concentration of p- and n-type dopants. The connection of n-type and p-type semiconductors form p–n junctions.\n",
      "\n",
      "\n",
      "\n",
      "What is resistivity?\n",
      "-ResistivityElectrical resistivity (also called specific electrical resistance or volume resistivity) and its inverse, electrical conductivity, is a fundamental property of a material that quantifies how strongly it resists or conducts electric current. A low resistivity indicates a material that readily allows electric current. Resistivity is commonly represented by the Greek letter ρ (rho). The SI unit of electrical resistivity is the ohm-meter (Ω⋅m). For example, if a 1 m × 1 m × 1 m solid cube of material has sheet contacts on two opposite faces, and the resistance between these contacts is 1 Ω, then the resistivity of the material is 1 Ω⋅m.\n",
      "-Electrical resistivity (also called volume resistivity or specific electrical resistance) is a fundamental specific property of a material that measures its electrical resistance or how strongly it resists electric current. A low resistivity indicates a material that readily allows electric current. Resistivity is commonly represented by the Greek letter ρ (rho). The SI unit of electrical resistivity is the ohm-metre (Ω⋅m). For example, if a 1 m3 solid cube of material has sheet contacts on two opposite faces, and the resistance between these contacts is 1 Ω, then the resistivity of the material is 1 Ω⋅m.\n",
      "-Both resistance and resistivity describe how difficult it is to make electrical current flow through a material, but unlike resistance, resistivity is an intrinsic property and doesn't depend on geometric properties of a material. This means that all pure copper (Cu) wires (which have not been subjected to distortion of their crystalline structure etc.), irrespective of their shape and size, have the same resistivity, but a long, thin copper wire has a much larger resistance than a thick, short copper wire. Every material has its own characteristic resistivity. For example, rubber has a far larger resistivity than copper.\n",
      "-Resistivity, which is a characteristic of particles in an electric field, is a measure of a particle's resistance to transferring charge (both accepting and giving up charges). Resistivity is a function of a particle's chemical composition as well as flue gas operating conditions such as temperature and moisture. Particles can have high, moderate (normal), or low resistivity.\n",
      "-Bulk resistivity is defined using a more general version of Ohm’s Law, as given in Equation (1) below: Where: E is the Electric field strength.Unit:-(V/cm); j is the Current density.Unit:-(A/cm2); and ρ is the Resistivity.Unit:-(Ohm-cm) A better way of displaying this would be to solve for resistivity as a function of applied voltage and current, as given in Equation (2) below: Where: ρ = Resistivity.Unit:-(Ohm-cm) V = The applied DC potential.Unit:-(Volts); I = The measured current.Unit:-(Amperes); l = The ash layer thickness.Unit:-(cm); and A = The current measuring electrode face area.Unit:-(cm2).\n",
      "\n",
      "\n",
      "\n",
      "What did Newton adopt after his correspondence with Hooke in 1679-1680?\n",
      "-In regard to evidence that still survives of the earlier history, manuscripts written by Newton in the 1660s show that Newton himself had, by 1669, arrived at proofs that in a circular case of planetary motion, \"endeavour to recede\" (what was later called centrifugal force) had an inverse-square relation with distance from the center. After his 1679–1680 correspondence with Hooke, Newton adopted the language of inward or centripetal force. According to Newton scholar J. Bruce Brackenridge, although much has been made of the change in language and difference of point of view, as between centrifugal or centripetal forces, the actual computations and proofs remained the same either way. They also involved the combination of tangential and radial displacements, which Newton was making in the 1660s. The lesson offered by Hooke to Newton here, although significant, was one of perspective and did not change the analysis. This background shows there was basis for Newton to deny deriving the inverse square law from Hooke.\n",
      "-Newton's early work on motion In the 1660s Newton studied the motion of colliding bodies and deduced that the centre of mass of two colliding bodies remains in uniform motion. Surviving manuscripts of the 1660s also show Newton's interest in planetary motion and that by 1669 he had shown, for a circular case of planetary motion, that the force he called \"endeavour to recede\" (now called centrifugal force) had an inverse-square relation with distance from the center. After his 1679–1680 correspondence with Hooke, described below, Newton adopted the language of inward or centripetal force. According to Newton scholar J. Bruce Brackenridge, although much has been made of the change in language and difference of point of view, as between centrifugal or centripetal forces, the actual computations and proofs remained the same either way. They also involved the combination of tangential and radial displacements, which Newton was making in the 1660s. The difference between the centrifugal and centripetal points of view, though a significant change of perspective, did not change the analysis. Newton also clearly expressed the concept of linear inertia in the 1660s: for this Newton was indebted to Descartes' work published 1644.\n",
      "-In physics, the history of centrifugal and centripetal forces illustrates a long and complex evolution of thought about the nature of forces, relativity, and the nature of physical laws.\n",
      "-The principle of equivalence: There is no experiment observers can perform to distinguish whether an acceleration arises because of a gravitational force or because their reference frame is accelerating In short, centrifugal force played a key early role in establishing the set of inertial frames of reference and the significance of fictitious forces, even aiding in the development of general relativity.\n",
      "-Around 1914, the analogy between centrifugal force (sometimes used to create artificial gravity) and gravitational forces led to the equivalence principle of general relativity.\n",
      "\n",
      "\n",
      "\n",
      "What is the metallicity of Kapteyn's star estimated to be?\n",
      "-In 2014, the first planets around a halo star were announced around Kapteyn's star, the nearest halo star to Earth, around 13 light years away. However, later research suggests that Kapteyn b is just an artefact of stellar activity and that Kapteyn c needs more study to be confirmed. The metallicity of Kapteyn's star is estimated to be about 8 times less than the Sun.Different types of galaxies have different histories of star formation and hence planet formation. Planet formation is affected by the ages, metallicities, and orbits of stellar populations within a galaxy. Distribution of stellar populations within a galaxy varies between the different types of galaxies.\n",
      "-Metallicity of 89–112% (± 0.05 dex) of that of the Sun, meaning the star's proplyd would have had almost exactly the same amount of dust for planetary formation No stellar companion, because the Sun itself is a solitary star An age within 1 billion years from that of the Sun (3.6 to 5.6 Ga)The following are the known stars that come closest to satisfying the criteria for a solar twin. The Sun is listed for comparison. Highlighted boxes are out of range for a solar twin. The star may have been noted as solar twin in the past, but are more of a solar analog.\n",
      "-The remainder of the elements are collectively referred to as \"metals\", and the metallicity – the mass fraction of elements heavier than helium – is calculated as Z=∑e>HemeM=1−X−Y.\n",
      "For the surface of the Sun (symbol  ⊙ ), these parameters are measured to have the following values: Due to the effects of stellar evolution, neither the initial composition nor the present day bulk composition of the Sun is the same as its present-day surface composition.\n",
      "Chemical abundance ratios The overall stellar metallicity is conventionally defined using the total hydrogen content, since its abundance is considered to be relatively constant in the Universe, or the iron content of the star, which has an abundance that is generally linearly increasing in time in the Universe.\n",
      "-The star has a mass of 0.539 solar masses, a radius of 0.547 solar radii, and a temperature of about 3,652 K (3,379 °C; 6,114 °F). It is about 0.3-3 billion years old, with a metallicity of 0.2 Fe/H and a rotation period of 21.54 days. The star exhibits strong stellar activity, with three ultraviolet flares detected by 2021.\n",
      "-The distance is uncertain, with estimates between 3.5 kiloparsecs (11,410 light-years) and 6.9 kiloparsecs (22,500 light-years). Assuming a distance of 4.8 kiloparsecs (15,600 light-years), this star is calculated to be 229,000 times brighter than the Sun, 13 times more massive, and 1.26 times larger with a surface temperature of 112,200 K. This makes it currently the second smallest known WN star in the galaxy, after WR 2.\n",
      "\n",
      "\n",
      "\n",
      "What is the SI base unit of time and how is it defined?\n",
      "-A unit of time is any particular time interval, used as a standard way of measuring or expressing duration. The base unit of time in the International System of Units (SI), and by extension most of the Western world, is the second, defined as about 9 billion oscillations of the caesium atom. The exact modern SI definition is \"[The second] is defined by taking the fixed numerical value of the cesium frequency, ΔνCs, the unperturbed ground-state hyperfine transition frequency of the cesium 133 atom, to be 9 192 631 770 when expressed in the unit Hz, which is equal to s−1.\"Historically, many units of time were defined by the movements of astronomical objects.\n",
      "-In the International System of Units (SI), the unit of time is the second (symbol:  s ). It is a SI base unit, and has been defined since 1967 as \"the duration of 9,192,631,770 [cycles] of the radiation corresponding to the transition between the two hyperfine levels of the ground state of the caesium 133 atom\". This definition is based on the operation of a caesium atomic clock. These clocks became practical for use as primary reference standards after about 1955, and have been in use ever since.\n",
      "-International System of Units definition Since 1968, the SI has defined the second as the duration of 9192631770 cycles of radiation corresponding to the transition between two energy levels of the ground state of the caesium-133 atom. In 1997, the International Committee for Weights and Measures (CIPM) added that the preceding definition refers to a caesium atom at rest at a temperature of absolute zero.: 113 This definition makes the caesium oscillator the primary standard for time and frequency measurements, called the caesium standard. Following the 2019 redefinition of the SI base units, the definition of every base unit except the mole and almost every derived unit relies on the definition of the second.\n",
      "-The SI base units are the standard units of measurement defined by the International System of Units (SI) for the seven base quantities of what is now known as the International System of Quantities: they are notably a basic set from which all other SI units can be derived. The units and their physical quantities are the second for time, the metre (sometimes spelled meter) for length or distance, the kilogram for mass, the ampere for electric current, the kelvin for thermodynamic temperature, the mole for amount of substance, and the candela for luminous intensity. The SI base units are a fundamental part of modern metrology, and thus part of the foundation of modern science and technology.\n",
      "-Caesium-133 is the only stable isotope of caesium. The SI base unit of time, the second, is defined by a specific caesium-133 transition. Since 1967, the official definition of a second is: The second, symbol s, is defined by taking the fixed numerical value of the caesium frequency, ΔνCs, the unperturbed ground-state hyperfine transition frequency of the caesium-133 atom, to be 9192631770 when expressed in the unit Hz, which is equal to s−1.\n",
      "\n",
      "\n",
      "\n",
      "What is a planetary system?\n",
      "-A planetary system is a set of gravitationally bound non-stellar objects in or out of orbit around a star or star system. Generally speaking, systems with one or more planets constitute a planetary system, although such systems may also consist of bodies such as dwarf planets, asteroids, natural satellites, meteoroids, comets, planetesimals and circumstellar disks. The Sun together with the planetary system revolving around it, including Earth, forms the Solar System. The term exoplanetary system is sometimes used in reference to other planetary systems.\n",
      "-A planet is a large, rounded astronomical body that is neither a star nor its remnant. The best available theory of planet formation is the nebular hypothesis, which posits that an interstellar cloud collapses out of a nebula to create a young protostar orbited by a protoplanetary disk. Planets grow in this disk by the gradual accumulation of material driven by gravity, a process called accretion. The Solar System has at least eight planets: the terrestrial planets Mercury, Venus, Earth and Mars, and the giant planets Jupiter, Saturn, Uranus and Neptune. These planets each rotate around an axis tilted with respect to its orbital pole. All planets of the Solar System other than Mercury possess a considerable atmosphere, and some share such features as ice caps, seasons, volcanism, hurricanes, tectonics, and even hydrology. Apart from Venus and Mars, the Solar System planets generate magnetic fields, and all except Venus and Mercury have natural satellites. The giant planets bear planetary rings, the most prominent being those of Saturn.\n",
      "-photometric system photosphere plane of reference Also reference plane.\n",
      "An arbitrarily chosen, imaginary plane from which to measure and define orbital elements such as inclination and longitude of the ascending node. The ecliptic plane, invariable plane, and equatorial plane are all commonly used as reference planes in various contexts.\n",
      "planet A type of astronomical body orbiting a star or stellar remnant which is massive enough to be rounded by its own gravity (but not massive enough to achieve thermonuclear fusion) and has cleared its neighbouring region of all planetesimals.\n",
      "planetary Of or relating to a planet or planets.\n",
      "planetary body Also planetary object.\n",
      "Any secondary body that is geologically differentiated or in hydrostatic equilibrium and therefore has a planet-like geology, such as a planet, dwarf planet, or other planetary-mass object, but excluding smaller objects such as planetesimals.\n",
      "planetary differentiation The process of separating out different constituents of a planetary body, causing it to develop compositionally distinct layers (such as a metallic core).\n",
      "planetary nebula A type of emission nebula formed from a glowing shell of expanding plasma that has been ejected from a red giant star late in its life. The name derives from their resemblance to a planet. An example is the Ring Nebula.\n",
      "planetary science Also sometimes called planetology.\n",
      "The scientific study of planets, moons, and planetary systems, with the aim of understanding their formation, composition, topography, dynamics, and interactions with other bodies.\n",
      "planetary system Any set of gravitationally bound non-stellar objects in or out of orbit around a star or star system. In general, planetary systems include one or more planets, though such systems may also consist of dwarf planets, moons, asteroids, meteoroids, planetesimals, and debris discs, among other objects.\n",
      "planetary-mass object (PMO) Also planemo or planetary body.\n",
      "-Planetary science Planetary science is the study of the assemblage of planets, moons, dwarf planets, comets, asteroids, and other bodies orbiting the Sun, as well as extrasolar planets. The Solar System has been relatively well-studied, initially through telescopes and then later by spacecraft. This has provided a good overall understanding of the formation and evolution of the Sun's planetary system, although many new discoveries are still being made.The Solar System is divided into the inner Solar System (subdivided into the inner planets and the asteroid belt), the outer Solar System (subdivided into the outer planets and centaurs), comets, the trans-Neptunian region (subdivided into the Kuiper belt, and the scattered disc) and the farthest regions (e.g., boundaries of the heliosphere, and the Oort Cloud, which may extend as far as a light-year). The inner terrestrial planets consist of Mercury, Venus, Earth, and Mars. The outer giant planets are the gas giants (Jupiter and Saturn) and the ice giants (Uranus and Neptune).The planets were formed 4.6 billion years ago in the protoplanetary disk that surrounded the early Sun. Through a process that included gravitational attraction, collision, and accretion, the disk formed clumps of matter that, with time, became protoplanets. The radiation pressure of the solar wind then expelled most of the unaccreted matter, and only those planets with sufficient mass retained their gaseous atmosphere. The planets continued to sweep up, or eject, the remaining matter during a period of intense bombardment, evidenced by the many impact craters on the Moon. During this period, some of the protoplanets may have collided and one such collision may have formed the Moon.Once a planet reaches sufficient mass, the materials of different densities segregate within, during planetary differentiation. This process can form a stony or metallic core, surrounded by a mantle and an outer crust. The core may include solid and liquid regions, and some planetary cores generate their own magnetic field, which can protect their atmospheres from solar wind stripping.A planet or moon's interior heat is produced from the collisions that created the body, by the decay of radioactive materials (e.g. uranium, thorium, and 26Al), or tidal heating caused by interactions with other bodies. Some planets and moons accumulate enough heat to drive geologic processes such as volcanism and tectonics. Those that accumulate or retain an atmosphere can also undergo surface erosion from wind or water. Smaller bodies, without tidal heating, cool more quickly; and their geological activity ceases with the exception of impact cratering.\n",
      "-The Solar System is the gravitationally bound system of the Sun and the objects that orbit it. The largest of such objects are the eight planets, in order from the Sun: four terrestrial planets named Mercury, Venus, Earth and Mars, two gas giants named Jupiter and Saturn, and two ice giants named Uranus and Neptune. The terrestrial planets have a definite surface and are mostly made of rock and metal. The gas giants are mostly made of hydrogen and helium, while the ice giants are mostly made of 'volatile' substances such as water, ammonia, and methane. In some texts, these terrestrial and giant planets are called the inner Solar System and outer Solar System planets respectively.\n",
      "\n",
      "\n",
      "\n",
      "What is the result of the collapse of a cavitation bubble?\n",
      "-Inertial cavitation Inertial cavitation was first observed in the late 19th century, considering the collapse of a spherical void within a liquid. When a volume of liquid is subjected to a sufficiently low pressure, it may rupture and form a cavity. This phenomenon is coined cavitation inception and may occur behind the blade of a rapidly rotating propeller or on any surface vibrating in the liquid with sufficient amplitude and acceleration. A fast-flowing river can cause cavitation on rock surfaces, particularly when there is a drop-off, such as on a waterfall.Other ways of generating cavitation voids involve the local deposition of energy, such as an intense focused laser pulse (optic cavitation) or with an electrical discharge through a spark. Vapor gases evaporate into the cavity from the surrounding medium; thus, the cavity is not a vacuum at all, but rather a low-pressure vapor (gas) bubble. Once the conditions which caused the bubble to form are no longer present, such as when the bubble moves downstream, the surrounding liquid begins to implode due its higher pressure, building up inertia as it moves inward. As the bubble finally collapses, the inward inertia of the surrounding liquid causes a sharp increase of pressure and temperature of the vapor within. The bubble eventually collapses to a minute fraction of its original size, at which point the gas within dissipates into the surrounding liquid via a rather violent mechanism which releases a significant amount of energy in the form of an acoustic shock wave and as visible light. At the point of total collapse, the temperature of the vapor within the bubble may be several thousand kelvin, and the pressure several hundred atmospheres.Inertial cavitation can also occur in the presence of an acoustic field. Microscopic gas bubbles that are generally present in a liquid will be forced to oscillate due to an applied acoustic field. If the acoustic intensity is sufficiently high, the bubbles will first grow in size and then rapidly collapse. Hence, inertial cavitation can occur even if the rarefaction in the liquid is insufficient for a Rayleigh-like void to occur. High-power ultrasonics usually utilize the inertial cavitation of microscopic vacuum bubbles for treatment of surfaces, liquids, and slurries.\n",
      "-Cavitation is the formation of vapour bubbles in liquid caused by flow around an object. Bubbles form when water accelerates around sharp corners and the pressure drops below the vapour pressure. Pressure increases upon deceleration, and the water generally reabsorbs the vapour; however, vapour bubbles can implode and apply small concentrated impulses that may damage surfaces like ship propellers and pump impellers.\n",
      "-Upon irradiation with high intensity sound or ultrasound, acoustic cavitation usually occurs. Cavitation – the formation, growth, and implosive collapse of bubbles irradiated with sound — is the impetus for sonochemistry and sonoluminescence. Bubble collapse in liquids produces enormous amounts of energy from the conversion of kinetic energy of the liquid motion into heating the contents of the bubble. The compression of the bubbles during cavitation is more rapid than thermal transport, which generates a short-lived localized hot-spot. Experimental results have shown that these bubbles have temperatures around 5000 K, pressures of roughly 1000 atm, and heating and cooling rates above 1010 K/s. These cavitations can create extreme physical and chemical conditions in otherwise cold liquids.\n",
      "-Cavitation is a phenomenon in which the static pressure of a liquid reduces to below the liquid's vapour pressure, leading to the formation of small vapor-filled cavities in the liquid. When subjected to higher pressure, these cavities, called \"bubbles\" or \"voids\", collapse and can generate shock waves that may damage machinery. These shock waves are strong when they are very close to the imploded bubble, but rapidly weaken as they propagate away from the implosion. Cavitation is a significant cause of wear in some engineering contexts. Collapsing voids that implode near to a metal surface cause cyclic stress through repeated implosion. This results in surface fatigue of the metal, causing a type of wear also called \"cavitation\". The most common examples of this kind of wear are to pump impellers, and bends where a sudden change in the direction of liquid occurs. Cavitation is usually divided into two classes of behavior: inertial (or transient) cavitation and non-inertial cavitation.\n",
      "-Sound waves propagating through a liquid at ultrasonic frequencies have wavelengths many times longer than the molecular dimensions or the bond length between atoms in the molecule. Therefore, the sound wave cannot directly affect the vibrational energy of the bond, and can therefore not directly increase the internal energy of a molecule. Instead, sonochemistry arises from acoustic cavitation: the formation, growth, and implosive collapse of bubbles in a liquid. The collapse of these bubbles is an almost adiabatic process, thereby resulting in the massive build-up of energy inside the bubble, resulting in extremely high temperatures and pressures in a microscopic region of the sonicated liquid. The high temperatures and pressures result in the chemical excitation of any matter within or very near the bubble as it rapidly implodes. A broad variety of outcomes can result from acoustic cavitation including sonoluminescence, increased chemical activity in the solution due to the formation of primary and secondary radical reactions, and increased chemical activity through the formation of new, relatively stable chemical species that can diffuse further into the solution to create chemical effects (for example, the formation of hydrogen peroxide from the combination of two hydroxyl radicals following the dissociation of water vapor within collapsing bubbles when water is exposed to ultrasound).\n",
      "\n",
      "\n",
      "\n",
      "Who was Giordano Bruno?\n",
      "-Early speculations This space we declare to be infinite... In it are an infinity of worlds of the same kind as our own.\n",
      "In the sixteenth century, the Italian philosopher Giordano Bruno, an early supporter of the Copernican theory that Earth and other planets orbit the Sun (heliocentrism), put forward the view that the fixed stars are similar to the Sun and are likewise accompanied by planets.\n",
      "-Speculation on extrasolar planetary systems In the 16th century the Italian philosopher Giordano Bruno, an early supporter of the Copernican theory that Earth and other planets orbit the Sun, put forward the view that the fixed stars are similar to the Sun and are likewise accompanied by planets. He was burned at the stake for his ideas by the Roman Inquisition.In the 18th century the same possibility was mentioned by Sir Isaac Newton in the \"General Scholium\" that concludes his Principia. Making a comparison to the Sun's planets, he wrote \"And if the fixed stars are the centres of similar systems, they will all be constructed according to a similar design and subject to the dominion of One.\"His theories gained traction through the 19th and 20th centuries despite a lack of supporting evidence. Long before their confirmation by astronomers, conjecture on the nature of planetary systems had been a focus of the search for extraterrestrial intelligence and has been a prevalent theme in fiction, particularly science fiction.\n",
      "-1584 – Giordano Bruno published two important philosophical dialogues (La Cena de le Ceneri and De l'infinito universo et mondi) in which he argued against the planetary spheres and affirmed the Copernican principle. Bruno's infinite universe was filled with a substance—a \"pure air\", aether, or spiritus—that offered no resistance to the heavenly bodies which, in Bruno's view, rather than being fixed, moved under their own impetus (momentum). Most dramatically, he completely abandoned the idea of a hierarchical universe. Bruno's cosmology distinguishes between \"suns\" which produce their own light and heat, and have other bodies moving around them; and \"earths\" which move around suns and receive light and heat from them. Bruno suggested that some, if not all, of the objects classically known as fixed stars are in fact suns, so he was arguably the first person to grasp that \"stars are other suns with their own planets.\" Bruno wrote that other worlds \"have no less virtue nor a nature different from that of our Earth\" and, like Earth, \"contain animals and inhabitants\".\n",
      "-Plato, Aristotle and other like Greek thinkers of antiquity, and later the Ptolemaic model of the cosmos showed an Earth-centered universe. Ptolemy was influential with his heavily mathematical work, the Almagest, which attempts to explain the peculiarity of stars that moved. These \"wandering stars\", planets, moved across the background of fixed stars which were spread along a sphere surrounding encompassing the universe. This geocentric view was held through the Middle Ages, and was later countered by subsequent astronomers and mathematicians alike, such as Nicolaus Copernicus and Johannes Kepler, who challenged the long-standing view of geocentrism and constructed a Sun-centered universe, this being known as the heliocentric system. The tradition of thought which appears in all of these systems of the universe, even with their divergent mechanisms, is the presence of the sphere of fixed stars.\n",
      "-Early beliefs Prior to the Copernican Revolution, the Ptolemaic system, also known as the geocentric model, was widely accepted. This put the Earth at the center of the universe, with the Sun and other planets revolving around the Earth in an epicyclic orbit. Aristotle's geocentric model was also broadly acknowledged, along with his claim that the planets rotated but did not orbit. The reasoning behind this was due to the belief that all objects outside of the lunar sphere were celestial bodies, and therefore could not change, as they were made of quintessence.There were notable critiques of this model prior to Copernicus. In the Islamic world, Ibn al-Haytham doubted Ptolemy's notion of the planetary orbits, and Muhammad al-Battani recalculated the parameters. However, both still agreed with the geocentric model.One of the first known astronomers that supported the Heliocentric theory was Aristarchus of Samos. After observing a lunar eclipse, he came to the conclusion that the Sun was farther away from Earth than the Moon and that the Sun was much larger than Earth. He also claimed the Sun was a star. While Aristarchus was later an influence on Copernicus and his groundbreaking work, prior to the 17th century Aristarchus' findings were obstructed by the more established theories of Ptolemy and Aristotle.\n",
      "\n",
      "\n",
      "\n",
      "What are the Navier-Stokes equations?\n",
      "-The Navier–Stokes equations are a set of partial differential equations that describe the motion of fluids. They are given by: ∂v∂t+(v⋅∇)v=−1ρ∇p+ν∇2v+f ∇⋅v=0 where  v(x,t) is the velocity field of the fluid,  p(x,t) is the pressure,  ρ is the density,  ν is the kinematic viscosity, and  f(x,t) is an external force. The first equation is known as the momentum equation, and the second equation is known as the continuity equation.\n",
      "-The Navier–Stokes equations (named after Claude-Louis Navier and George Gabriel Stokes) are differential equations that describe the force balance at a given point within a fluid. For an incompressible fluid with vector velocity field  u , the Navier–Stokes equations are ∂u∂t+(u⋅∇)u=−1ρ∇p+ν∇2u .These differential equations are the analogues for deformable materials to Newton's equations of motion for particles – the Navier–Stokes equations describe changes in momentum (force) in response to pressure  p and viscosity, parameterized by the kinematic viscosity  ν . Occasionally, body forces, such as the gravitational force or Lorentz force are added to the equations.\n",
      "-In mathematics, the Navier–Stokes equations are a system of nonlinear partial differential equations for abstract vector fields of any size. In physics and engineering, they are a system of equations that model the motion of liquids or non-rarefied gases (in which the mean free path is short enough so that it can be thought of as a continuum mean instead of a collection of particles) using continuum mechanics. The equations are a statement of Newton's second law, with the forces modeled according to those in a viscous Newtonian fluid—as the sum of contributions by pressure, viscous stress and an external body force. Since the setting of the problem proposed by the Clay Mathematics Institute is in three dimensions, for an incompressible and homogeneous fluid, only that case is considered below.\n",
      "-The Navier–Stokes equations are strictly a statement of the balance of momentum. To fully describe fluid flow, more information is needed, how much depending on the assumptions made. This additional information may include boundary data (no-slip, capillary surface, etc.), conservation of mass, balance of energy, and/or an equation of state.\n",
      "-The Navier–Stokes equations mathematically express momentum balance and conservation of mass for Newtonian fluids. They are sometimes accompanied by an equation of state relating pressure, temperature and density. They arise from applying Isaac Newton's second law to fluid motion, together with the assumption that the stress in the fluid is the sum of a diffusing viscous term (proportional to the gradient of velocity) and a pressure term—hence describing viscous flow. The difference between them and the closely related Euler equations is that Navier–Stokes equations take viscosity into account while the Euler equations model only inviscid flow. As a result, the Navier–Stokes are a parabolic equation and therefore have better analytic properties, at the expense of having less mathematical structure (e.g. they are never completely integrable).\n",
      "\n",
      "\n",
      "\n",
      "What is the revised view of the atmosphere's nature based on the time-varying multistability that is associated with the modulation of large-scale processes and aggregated feedback of small-scale processes?\n",
      "-Model for the nature of chaos and order in the atmosphere The scientific community accepts that the chaotic features found in low-dimensional Lorenz models could represent features of the Earth's atmosphere (), yielding the statement of “weather is chaotic.” By comparison, based on the concept of attractor coexistence within the generalized Lorenz model and the original Lorenz model (), Shen and his co-authors proposed a revised view that “weather possesses both chaos and order with distinct predictability”. The revised view, which is a build-up of the conventional view, is used to suggest that “the chaotic and regular features found in theoretical Lorenz models could better represent features of the Earth's atmosphere”.\n",
      "-Atmospheric circulation is the large-scale movement of air and together with ocean circulation is the means by which thermal energy is redistributed on the surface of the Earth. The Earth's atmospheric circulation varies from year to year, but the large-scale structure of its circulation remains fairly constant. The smaller-scale weather systems – mid-latitude depressions, or tropical convective cells – occur chaotically, and long-range weather predictions of those cannot be made beyond ten days in practice, or a month in theory (see chaos theory and the butterfly effect).\n",
      "-The dual nature with distinct predictability Over 50 years since Lorenz’s 1963 study and a follow-up presentation in 1972, the statement “weather is chaotic” has been well accepted. Such a view turns our attention from regularity associated with Laplace’s view of determinism to irregularity associated with chaos. In contrast to single-type chaotic solutions, recent studies using a generalized Lorenz model have focused on the coexistence of chaotic and regular solutions that appear within the same model using the same modeling configurations but different initial conditions. The results, with attractor coexistence, suggest that the entirety of weather possesses a dual nature of chaos and order with distinct predictability.Using a slowly varying, periodic heating parameter within a generalized Lorenz model, Shen and his co-authors suggested a revised view: “The atmosphere possesses chaos and order; it includes, as examples, emerging organized systems (such as tornadoes) and time varying forcing from recurrent seasons”.\n",
      "-Climate networks enable insights into the dynamics of climate system over many spatial scales. The local degree centrality and related measures have been used to identify super-nodes and to associate them to known dynamical interrelations in the atmosphere, called teleconnection patterns. It was observed that climate networks possess “small world” properties owing to the long-range spatial connections.Steinhaeuser et al. applied complex networks to explore the multivariate and multi-scale dependence in climate data. Findings of the group suggested a close similarity of observed dependence patterns in multiple variables over multiple time and spatial scales.\n",
      "-Atmosphere The atmospheric component of the CM2.X models employs a 24-level atmosphere with horizontal resolution of 2° in east–west and 2.5° in north–south directions. This resolution is sufficient to resolve the large mid-latitude cyclones responsible for weather variability. It is too coarse, however, to resolve processes such as hurricanes or intense thunderstorm outbreaks. The atmosphere includes a representation of radiative fluxes, mixing in the atmospheric boundary layer, representations of the impacts of stratus and cumulus clouds, a scheme for representing drag on upper level winds caused by gravity waves, changes in the spatial distribution of ozone and the ability to represent the impact of multiple greenhouse gases.\n",
      "\n",
      "\n",
      "\n",
      "What is the reason that it is nearly impossible to see light emitted at the Lyman-alpha transition wavelength from a star farther than a few hundred light years from Earth?\n",
      "-Extinction provides one of the best ways of mapping the three-dimensional structure of the ISM, especially since the advent of accurate distances to millions of stars from the Gaia mission. The total amount of dust in front of each star is determined from its reddening, and the dust is then located along the line of sight by comparing the dust column density in front of stars projected close together on the sky, but at different distances. By 2022 it was possible to generate a map of ISM structures within 3 kpc (10,000 light years) of the Sun.Far ultraviolet light is absorbed effectively by the neutral hydrogen gas the ISM. Specifically, atomic hydrogen absorbs very strongly at about 121.5 nanometers, the Lyman-alpha transition, and also at the other Lyman series lines. Therefore, it is nearly impossible to see light emitted at those wavelengths from a star farther than a few hundred light years from Earth, because most of it is absorbed during the trip to Earth by intervening neutral hydrogen. All photons with wavelength < 91.6 nm, the Lyman limit, can ionize hydrogen and are also very strongly absorbed. The absorption gradually decreases with increasing photon energy, and the ISM begins to become transparent again in soft X-rays, with wavelengths shorter than about 1 nm.\n",
      "-The ISM is generally very transparent to radio waves, allowing unimpeded observations right through the disk of the Galaxy. There are a few exceptions to this rule. The most intense spectral lines in the radio spectrum can become opaque, so that only the surface of the line-emitting cloud is visible. This mainly affects the carbon monoxide lines at millimetre wavelengths that are used to trace molecular clouds, but the 21-cm line from neutral hydrogen can become opaque in the cold neutral medium. Such absorption only affects photons at the line frequencies: the clouds are otherwise transparent. The other significant absorption process occurs in dense ionized regions. These emit photons, including radio waves, via thermal bremsstrahlung. At short wavelengths, typically microwaves, these are quite transparent, but their brightness approaches the black body limit as  2.1 , and at wavelengths long enough that this limit is reached, they become opaque. Thus metre-wavelength observations show H II regions as cool spots blocking the bright background emission from Galactic synchrotron radiation, while at decametres the entire galactic plane is absorbed, and the longest radio waves observed, 1 km, can only propagate 10-50 parsecs through the Local Bubble. The frequency at which a particular nebula becomes optically thick depends on its emission measure EM=∫ne2dl ,the column density of squared electron number density. Exceptionally dense nebulae can become optically thick at centimetre wavelengths: these are just-formed and so both rare and small ('Ultra-compact H II regions') The general transparency of the ISM to radio waves, especially microwaves, may seem surprising since radio waves at frequencies > 10 GHz are significantly attenuated by Earth's atmosphere (as seen in the figure). But the column density through the atmosphere is vastly larger than the column through the entire Galaxy, due to the extremely low density of the ISM.\n",
      "-Since the hydrogen Lyman-alpha radiation is strongly absorbed by the air, its observation in laboratory requires use of vacuumed spectroscopic systems. For the same reason, Lyman-alpha astronomy is ordinarily carried out by satellite-borne instruments, except for observing extremely distant sources whose redshifts allow the line to penetrate the Earth atmosphere.\n",
      "The line was also observed in antihydrogen. Within the experimental uncertainties, the measured frequency is equal to that of hydrogen, in agreement with predictions of quantum electrodynamics.\n",
      "-The Hα line splitting is 44.5 nm. In similar white dwarfs an absorption line is expected to be seen instead, so that means the emission has sufficient energy to overpower any absorption. The emission was originally discovered by Jesse L. Greenstein. The original Hα line has a wavelength at 655.2 nm and is called the π component. The blue shifted component σ− has wavelength 633.4 nm and red shifted component line σ+ is at 678.2 nm.\n",
      "-For a neutral hydrogen atom, spectral lines are formed when an electron transitions between energy levels. The Lyman series of spectral lines are produced by electrons transitioning between the ground state and higher energy levels (excited states). The Lyman-alpha transition corresponds to an electron transitioning between the ground state (n = 1) and the first excited state (n = 2). The Lyman-alpha spectral line has a laboratory wavelength (or rest wavelength) of 1216 Å, which is in the ultraviolet portion of the electromagnetic spectrum.The Lyman-alpha absorption lines in the quasar spectra result from intergalactic gas through which the galaxy or quasar's light has traveled. Since neutral hydrogen clouds in the intergalactic medium are at different degrees of redshift (due to their varying distance from Earth), their absorption lines are observed at a range of wavelengths. Each individual cloud leaves its fingerprint as an absorption line at a different position in the observed spectrum.\n",
      "\n",
      "\n",
      "\n",
      "What is a Schwarzschild black hole?\n",
      "-According to Birkhoff's theorem, the Schwarzschild metric is the most general spherically symmetric, vacuum solution of the Einstein field equations. A Schwarzschild black hole or static black hole is a black hole that has no charge or angular momentum. A Schwarzschild black hole is described by the Schwarzschild metric, and cannot be distinguished from any other Schwarzschild black hole except by its mass.\n",
      "-Physical properties The simplest static black holes have mass but neither electric charge nor angular momentum. These black holes are often referred to as Schwarzschild black holes after Karl Schwarzschild who discovered this solution in 1916. According to Birkhoff's theorem, it is the only vacuum solution that is spherically symmetric. This means there is no observable difference at a distance between the gravitational field of such a black hole and that of any other spherical object of the same mass. The popular notion of a black hole \"sucking in everything\" in its surroundings is therefore correct only near a black hole's horizon; far away, the external gravitational field is identical to that of any other body of the same mass.Solutions describing more general black holes also exist. Non-rotating charged black holes are described by the Reissner–Nordström metric, while the Kerr metric describes a non-charged rotating black hole. The most general stationary black hole solution known is the Kerr–Newman metric, which describes a black hole with both charge and angular momentum.While the mass of a black hole can take any positive value, the charge and angular momentum are constrained by the mass. The total electric charge Q and the total angular momentum J are expected to satisfy the inequality Q24πϵ0+c2J2GM2≤GM2 for a black hole of mass M. Black holes with the minimum possible mass satisfying this inequality are called extremal. Solutions of Einstein's equations that violate this inequality exist, but they do not possess an event horizon. These solutions have so-called naked singularities that can be observed from the outside, and hence are deemed unphysical. The cosmic censorship hypothesis rules out the formation of such singularities, when they are created through the gravitational collapse of realistic matter. This is supported by numerical simulations.Due to the relatively large strength of the electromagnetic force, black holes forming from the collapse of stars are expected to retain the nearly neutral charge of the star. Rotation, however, is expected to be a universal feature of compact astrophysical objects. The black-hole candidate binary X-ray source GRS 1915+105 appears to have an angular momentum near the maximum allowed value. That uncharged limit is J≤GM2c, allowing definition of a dimensionless spin parameter such that 1.\n",
      "-Rotating and charged black holes The Schwarzschild solution supposes an object that is not rotating in space and is not charged. To account for charge, the metric must satisfy the Einstein Field equations like before, as well as Maxwell's equations in a curved spacetime. A charged, non-rotating mass is described by the Reissner–Nordström metric.  Rotating black holes are described by the Kerr metric and the Kerr–Newman metric.\n",
      "-In Einstein's theory of general relativity, the Schwarzschild metric (also known as the Schwarzschild solution) is an  exact solution to the Einstein field equations that describes the gravitational field outside a spherical mass, on the assumption that the electric charge of the mass, angular momentum of the mass, and universal cosmological constant are all zero. The solution is a useful approximation for describing slowly rotating astronomical objects such as many stars and planets, including Earth and the Sun. It was found by Karl Schwarzschild in 1916, and around the same time independently by Johannes Droste, who published his more complete and modern-looking discussion four months after Schwarzschild.According to Birkhoff's theorem, the Schwarzschild metric is the most general spherically symmetric vacuum solution of the Einstein field equations. A Schwarzschild black hole or static black hole is a black hole that has neither electric charge nor angular momentum. A Schwarzschild black hole is described by the Schwarzschild metric, and cannot be distinguished from any other Schwarzschild black hole except by its mass.\n",
      "-In terms of these properties, the four types of black holes can be defined as follows: Note that astrophysical black holes are expected to have non-zero angular momentum, due to their formation via collapse of rotating stellar objects, but effectively zero charge, since any net charge will quickly attract the opposite charge and neutralize. For this reason the term \"astrophysical\" black hole is usually reserved for the Kerr black hole.\n",
      "\n",
      "\n",
      "\n",
      "What is the definition of Atomristor?\n",
      "-Atomristor Atomristor is defined as the electrical devices showing memristive behavior in atomically thin nanomaterials or atomic sheets. In 2018, Ge and Wu et al. in the Akinwande group at the University of Texas, first reported a universal memristive effect in single-layer TMD (MX2, M = Mo, W; and X = S, Se) atomic sheets based on vertical metal-insulator-metal (MIM) device structure. The work was later extended to monolayer hexagonal boron nitride, which is the thinnest memory material of around 0.33 nm. These atomristors offer forming-free switching and both unipolar and bipolar operation. The switching behavior is found in single-crystalline and poly-crystalline films, with various conducting electrodes (gold, silver and graphene). Atomically thin TMD sheets are prepared via CVD/MOCVD, enabling low-cost fabrication. Afterwards, taking advantage of the low \"on\" resistance and large on/off ratio, a high-performance zero-power RF switch is proved based on MoS2 or h-BN atomristors, indicating a new application of memristors for 5G, 6G and THz communication and connectivity systems. In 2020, atomistic understanding of the conductive virtual point mechanism was elucidated in an article in nature nanotechnology.\n",
      "-Layered memristor In 2014, Bessonov et al. reported a flexible memristive device comprising a MoOx/MoS2 heterostructure sandwiched between silver electrodes on a plastic foil. The fabrication method is entirely based on printing and solution-processing technologies using two-dimensional layered transition metal dichalcogenides (TMDs). The memristors are mechanically flexible, optically transparent and produced at low cost. The memristive behaviour of switches was found to be accompanied by a prominent memcapacitive effect. High switching performance, demonstrated synaptic plasticity and sustainability to mechanical deformations promise to emulate the appealing characteristics of biological neural systems in novel computing technologies.\n",
      "-A memristor (; a portmanteau of memory resistor) is a non-linear two-terminal electrical component relating electric charge and magnetic flux linkage. It was described and named in 1971 by Leon Chua, completing a theoretical quartet of fundamental electrical components which comprises also the resistor, capacitor and inductor.Chua and Kang later generalized the concept to memristive systems. Such a system comprises a circuit, of multiple conventional components, which mimics key properties of the ideal memristor component and is also commonly referred to as a memristor. Several such memristor system technologies have been developed, notably ReRAM.\n",
      "-Carbon nanotube memristor In 2013, Ageev, Blinov et al. reported observing memristor effect in structure based on vertically aligned carbon nanotubes studying bundles of CNT by scanning tunneling microscope.\n",
      "Later it was found that CNT memristive switching is observed when a nanotube has a non-uniform elastic strain ΔL0. It was shown that the memristive switching mechanism of strained СNT is based on the formation and subsequent redistribution of non-uniform elastic strain and piezoelectric field Edef in the nanotube under the influence of an external electric field E(x,t).\n",
      "-According to the original 1971 definition, the memristor is the fourth fundamental circuit element, forming a non-linear relationship between electric charge and magnetic flux linkage. In 2011, Chua argued for a broader definition that includes all two-terminal non-volatile memory devices based on resistance switching. Williams argued that MRAM, phase-change memory and ReRAM are memristor technologies. Some researchers argued that biological structures such as blood and skin fit the definition. Others argued that the memory device under development by HP Labs and other forms of ReRAM are not memristors, but rather part of a broader class of variable-resistance systems, and that a broader definition of memristor is a scientifically unjustifiable land grab that favored HP's memristor patents.In 2011, Meuffels and Schroeder noted that one of the early memristor papers included a mistaken assumption regarding ionic conduction. In 2012, Meuffels and Soni discussed some fundamental issues and problems in the realization of memristors. They indicated inadequacies in the electrochemical modeling presented in the Nature article \"The missing memristor found\" because the impact of concentration polarization effects on the behavior of metal−TiO2−x−metal structures under voltage or current stress was not considered. This critique was referred to by Valov et al. in 2013.\n",
      "\n",
      "\n",
      "\n",
      "Who published the first theory that was able to encompass previously separate field theories to provide a unifying theory of electromagnetism?\n",
      "-Classic theory The first successful classical unified field theory was developed by James Clerk Maxwell. In 1820, Hans Christian Ørsted discovered that electric currents exerted forces on magnets, while in 1831, Michael Faraday made the observation that time-varying magnetic fields could induce electric currents. Until then, electricity and magnetism had been thought of as unrelated phenomena. In 1864, Maxwell published his famous paper on a dynamical theory of the electromagnetic field. This was the first example of a theory that was able to encompass previously separate field theories (namely electricity and magnetism) to provide a unifying theory of electromagnetism. By 1905, Albert Einstein had used the constancy of the speed of light in Maxwell's theory to unify our notions of space and time into an entity we now call spacetime and in 1915 he expanded this theory of special relativity to a description of gravity, general relativity, using a field to describe the curving geometry of four-dimensional spacetime.\n",
      "-Field theory had its origins in the 18th century in a mathematical formulation of Newtonian mechanics, but it was seen as deficient as it implied action at a distance. In 1852, Michael Faraday treated the magnetic field as a physical object, reasoning about lines of force. James Clerk Maxwell used Faraday's conceptualisation to help formulate his unification of electricity and magnetism in his electromagnetic theory.\n",
      "-Maxwell and the theoretical prediction of electromagnetic waves Between 1861 and 1865, based on the earlier experimental work of Faraday and other scientists and on his own modification to Ampere's law, James Clerk Maxwell developed his theory of electromagnetism, which predicted the existence of electromagnetic waves. In 1873 Maxwell described the theoretical basis of the propagation of electromagnetic waves in his paper to the Royal Society, \"A Dynamical Theory of the Electromagnetic Field.\" This theory united all previously unrelated observations, experiments and equations of electricity, magnetism, and optics into a consistent theory. His set of equations—Maxwell's equations—demonstrated that electricity, magnetism, and light are all manifestations of the same phenomenon, the electromagnetic field. Subsequently, all other classic laws or equations of these disciplines were special cases of Maxwell's equations. Maxwell's work in electromagnetism has been called the \"second great unification in physics\", after Newton's unification of gravity in the 17th century.Oliver Heaviside, later reformulated Maxwell's original equations into the set of four vector equations that are generally known today as Maxwell's equations. Neither Maxwell nor Heaviside transmitted or received radio waves; however, their equations for electromagnetic fields established principles for radio design, and remain the standard expression of classical electromagnetism.\n",
      "-Electromagnetism is one of the fundamental forces of nature. Early on, electricity and magnetism were studied separately and regarded as separate phenomena. Hans Christian Ørsted discovered that the two were related – electric currents give rise to magnetism. Michael Faraday discovered the converse, that magnetism could induce electric currents, and James Clerk Maxwell put the whole thing together in a unified theory of electromagnetism. Maxwell's equations further indicated that electromagnetic waves existed, and the experiments of Heinrich Hertz confirmed this, making radio possible. Maxwell also postulated, correctly, that light was a form of electromagnetic wave, thus making all of optics a branch of electromagnetism. Radio waves differ from light only in that the wavelength of the former is much longer than the latter. Albert Einstein showed that the magnetic field arises through the relativistic motion of the electric field and thus magnetism is merely a side effect of electricity. The modern theoretical treatment of electromagnetism is as a quantum field in quantum electrodynamics.\n",
      "-It was Hans Christian Ørsted who first proposed the connection between electricity and magnetism after observing the deflection of a compass needle by a nearby electric current. By the early 1830s Michael Faraday had demonstrated that magnetic fields and electricity could generate each other. In 1864 James Clerk Maxwell presented to the Royal Society a set of equations that described this relationship between electricity and magnetism. Maxwell's equations also predicted correctly that light is an electromagnetic wave. Starting with astronomy, the principles of natural philosophy crystallized into fundamental laws of physics which were enunciated and improved in the succeeding centuries. By the 19th century, the sciences had segmented into multiple fields with specialized researchers and the field of physics, although logically pre-eminent, no longer could claim sole ownership of the entire field of scientific research.\n",
      "\n",
      "\n",
      "\n",
      "What is the relevant type of coherence for the Young's double-slit interferometer?\n",
      "-In some systems, such as water waves or optics, wave-like states can extend over one or two dimensions. Spatial coherence describes the ability for two spatial points x1 and x2 in the extent of a wave to interfere when averaged over time. More precisely, the spatial coherence is the cross-correlation between two points in a wave for all times. If a wave has only 1 value of amplitude over an infinite length, it is perfectly spatially coherent. The range of separation between the two points over which there is significant interference defines the diameter of the coherence area, Ac (Coherence length, often a feature of a source, is usually an industrial term related to the coherence time of the source, not the coherence area in the medium.) Ac is the relevant type of coherence for the Young's double-slit interferometer. It is also used in optical imaging systems and particularly in various types of astronomy telescopes. Sometimes people also use \"spatial coherence\" to refer to the visibility when a wave-like state is combined with a spatially shifted copy of itself.\n",
      "-Young double slit experiment In Young's double slit experiment, light from a light source is allowed to pass through two pinholes separated by some distance, and a screen is placed some distance away from the pinholes where the interference between the light waves is observed (Figure. 1). Young's double slit experiment demonstrates the dependence of interference on coherence, specifically on the first-order correlation. This experiment is equivalent to the Mach–Zehnder interferometer with the caveat that Young's double slit experiment is concerned with spatial coherence, while the Mach–Zehnder interferometer relies on temporal coherence.The intensity measured at the position  r at time  t is ⟨I⟩=⟨|E+(r,t)|2⟩=⟨I⟩=I1+I2+2I1I2|γ(1)(x1,x2)|cosϕ(x1,x2) .Light field has highest degree of coherence when the corresponding interference pattern has the maximum contrast on the screen. The fringe contrast is defined as  V=Imax−IminImax+Imin Classically,  Iminmax=I1+I2±2I1I2|γ(1)(x1,x2)| and hence  V=2I1I2|γ(1)(x1,x2)|I1+I2 . As coherence is the ability to interfere visibility and coherence are linked: |γ(1)(x1,x2)|=1 means highest contrast, complete coherence 0<|γ(1)(x1,x2)|<1 means partial fringe visibility, partial coherence |γ(1)(x1,x2)|=0 means no contrast, complete incoherence.\n",
      "-Coherence controls the visibility or contrast of interference patterns. For example visibility of the double slit experiment pattern requires that both slits be illuminated by a coherent wave as illustrated in the figure. Large sources without collimation or sources that mix many different frequencies will have lower visibility.: 264 Coherence contains several distinct concepts. Spatial coherence describes the correlation (or predictable relationship) between waves at different points in space, either lateral or longitudinal. Temporal coherence describes the correlation between waves observed at different moments in time. Both are observed in the Michelson–Morley experiment and Young's interference experiment. Once the fringes are obtained in the Michelson interferometer, when one of the mirrors is moved away gradually from the beam-splitter, the time for the beam to travel increases and the fringes become dull and finally disappear, showing temporal coherence. Similarly, in a double-slit experiment, if the space between the two slits is increased, the coherence dies gradually and finally the fringes disappear, showing spatial coherence. In both cases, the fringe amplitude slowly disappears, as the path difference increases past the coherence length.\n",
      "-Interferometric visibility – which quantifies interference contrast in opticsPages displaying wikidata descriptions as a fallback Mutual coherence function Degree of coherence – Measurement in quantum optics Van Cittert–Zernike theorem Michelson stellar interferometer – astronomical instrument for measuring angular diameter of stars by means of interferometryPages displaying wikidata descriptions as a fallback Correlation interferometry – Astronomy devicePages displaying short descriptions of redirect targets Hanbury–Brown and Twiss effect – Quantum correlations related to wave-particle dualityPages displaying short descriptions of redirect targets Phase-contrast microscope – Optical microscopy techniquePages displaying short descriptions of redirect targets Englert–Greenberger duality relation – A relation of quantum opticsPages displaying short descriptions of redirect targets \n",
      "-The coherence length can also be measured using a Michelson interferometer and is the optical path length difference of a self-interfering laser beam which corresponds to  37 % fringe visibility, where the fringe visibility is defined as max min max min , where  I is the fringe intensity.\n",
      "In long-distance transmission systems, the coherence length may be reduced by propagation factors such as dispersion, scattering, and diffraction.\n",
      "\n",
      "\n",
      "\n",
      "What is the Peierls bracket in canonical quantization?\n",
      "-In theoretical physics, the Peierls bracket is an equivalent description of the Poisson bracket. It can be defined directly from the action and does not require the canonical coordinates and their canonical momenta to be defined in advance.The bracket [A,B] is defined as DA(B)−DB(A) ,as the difference between some kind of action of one quantity on the other, minus the flipped term.\n",
      "-The method does not apply to all possible actions (for instance, actions with a noncausal structure or actions with gauge \"flows\"). It starts with the classical algebra of all (smooth) functionals over the configuration space. This algebra is quotiented over by the ideal generated by the Euler–Lagrange equations. Then, this quotient algebra is converted into a Poisson algebra by introducing a Poisson bracket derivable from the action, called the Peierls bracket. This Poisson algebra is then ℏ -deformed in the same way as in canonical quantization.\n",
      "-In the Hamiltonian formulation of ordinary classical mechanics the Poisson bracket is an important concept. A \"canonical coordinate system\" consists of canonical position and momentum variables that satisfy canonical Poisson-bracket relations, where the Poisson bracket is given by for arbitrary phase space functions  f(qi,pj) and  g(qi,pj) . With the use of Poisson brackets, the Hamilton's equations can be rewritten as, These equations describe a \"flow\" or orbit in phase space generated by the Hamiltonian  H . Given any phase space function  F(q,p) , we have In canonical quantization the phase space variables are promoted to quantum operators on a Hilbert space and the Poisson bracket between phase space variables is replaced by the canonical commutation relation: In the so-called position representation this commutation relation is realized by the choice: and  The dynamics are described by Schrödinger equation: where  H^ is the operator formed from the Hamiltonian  H(q,p) with the replacement  q↦q and  p↦−iℏddq \n",
      "-In mathematics and classical mechanics, the Poisson bracket is an important binary operation in Hamiltonian mechanics, playing a central role in Hamilton's equations of motion, which govern the time evolution of a Hamiltonian dynamical system. The Poisson bracket also distinguishes a certain class of coordinate transformations, called canonical transformations, which map canonical coordinate systems into canonical coordinate systems. A \"canonical coordinate system\" consists of canonical position and momentum variables (below symbolized by  qi and  pi , respectively) that satisfy canonical Poisson bracket relations. The set of possible canonical transformations is always very rich. For instance, it is often possible to choose the Hamiltonian itself  H=H(q,p,t) as one of the new canonical momentum coordinates.\n",
      "-In quantum mechanics, the Peierls bracket becomes a commutator i.e. a Lie bracket.\n",
      "\n",
      "\n",
      "\n",
      "What is the isophotal diameter used for in measuring a galaxy's size?\n",
      "-Isophotal diameter The isophotal diameter is introduced as a conventional way of measuring a galaxy's size based on its apparent surface brightness. Isophotes are curves in a diagram - such as a picture of a galaxy - that adjoins points of equal brightnesses, and are useful in defining the extent of the galaxy. The apparent brightness flux of a galaxy is measured in units of magnitudes per square arcsecond (mag/arcsec2; sometimes expressed as mag arcsec−2), which defines the brightness depth of the isophote. To illustrate how this unit works, a typical galaxy has a brightness flux of 18 mag/arcsec2 at its central region. This brightness is equivalent to the light of an 18th magnitude hypothetical point object (like a star) being spread out evenly in a one square arcsecond area of the sky. For the purposes of objectivity, the spectrum of light being used is sometimes also given in figures. As an example, the Milky Way has an average surface brightness of 22.1 B-mag/arcsec−2, where B-mag refers to the brightness at the B-band (445 nm wavelength of light, in the blue part of the visible spectrum).\n",
      "-Examples of isophotal diameter measurements: Large Magellanic Cloud - 9.86 kiloparsecs (32,200 light-years) at the 25.0 B-mag/arcsec2 isophote.\n",
      "Milky Way - has a diameter at the 25.0 B-mag/arcsec2 isophote of 26.8 ± 1.1 kiloparsecs (87,400 ± 3,590 light-years).\n",
      "Messier 87 - has a has a diameter at the 25.0 B-mag/arcsec2 isophote of 40.55 kiloparsecs (132,000 light-years).\n",
      "Andromeda Galaxy - has a has a diameter at the 25.0 B-mag/arcsec2 isophote of 46.56 kiloparsecs (152,000 light-years).\n",
      "-This conventional standard, however, is not universally agreed upon. Erik Holmberg in 1958 measured the diameters of at least 300 galaxies at the isophote of about 26.5 mag/arcsec2 (originally defined as where the photographic brightness density with respect to plate background is 0.5%). Various other surveys such that of the ESO in 1989 use isophotes as faint as 27.0 mag/arcsec2. Nevertheless, corrections of these diameters were introduced by both the Second and Third Reference Catalogue of Galaxies (RC2 and RC3), at least to those galaxies being covered by the two catalogues.\n",
      "-The extragalactic distance scale is a series of techniques used today by astronomers to determine the distance of cosmological bodies beyond our own galaxy, which are not easily obtained with traditional methods. Some procedures utilize properties of these objects, such as stars, globular clusters, nebulae, and galaxies as a whole. Other methods are based more on the statistics and probabilities of things such as entire galaxy clusters.\n",
      "-In defining Re, it is necessary that the overall brightness flux galaxy should be captured, with a method employed by Bershady in 2000 suggesting to measure twice the size where the brightness flux of an arbitrarily chosen radius, defined as the local flux, divided by the overall average flux equals to 0.2. Using half-light radius allows a rough estimate of a galaxy's size, but is not particularly helpful in determining its morphology.Variations of this method exist. In particular, in the ESO-Uppsala Catalogue of Galaxies values of 50%, 70%, and 90% of the total blue light (the light detected through a B-band specific filter) had been used to calculate a galaxy's diameter.\n",
      "\n",
      "\n",
      "\n",
      "What is the Maxwell's Demon thought experiment?\n",
      "-Maxwell's demon James Clerk Maxwell imagined one container divided into two parts, A and B. Both parts are filled with the same gas at equal temperatures and placed next to each other, separated by a wall. Observing the molecules on both sides, an imaginary demon guards a microscopic trapdoor in the wall. When a faster-than-average molecule from A flies towards the trapdoor, the demon opens it, and the molecule will fly from A to B. The average speed of the molecules in B will have increased while in A they will have slowed down on average. Since average molecular speed corresponds to temperature, the temperature decreases in A and increases in B, contrary to the second law of thermodynamics.One response to this question was suggested in 1929 by Leó Szilárd and later by Léon Brillouin. Szilárd pointed out that a real-life Maxwell's demon would need to have some means of measuring molecular speed, and that the act of acquiring information would require an expenditure of energy. Likewise, Brillouin demonstrated that the decrease in entropy caused by the demon would be less than the entropy produced by choosing molecules based on their speed.Maxwell's 'demon' repeatedly alters the permeability of the wall between A and B. It is therefore performing thermodynamic operations on a microscopic scale, not just observing ordinary spontaneous or natural macroscopic thermodynamic processes.\n",
      "-Maxwell's demon is a thought experiment that would hypothetically violate the second law of thermodynamics. It was proposed by the physicist James Clerk Maxwell in 1867. In his first letter, Maxwell referred to the entity as a \"finite being\" or a \"being who can play a game of skill with the molecules\". Lord Kelvin would later call it a \"demon\".In the thought experiment, a demon controls a small massless door between two chambers of gas. As individual gas molecules (or atoms) approach the door, the demon quickly opens and closes the door to allow only fast-moving molecules to pass through in one direction, and only slow-moving molecules to pass through in the other. Because the kinetic temperature of a gas depends on the velocities of its constituent molecules, the demon's actions cause one chamber to warm up and the other to cool down. This would decrease the total entropy of the system, without applying any work, thereby violating the second law of thermodynamics.\n",
      "-Thought experiments In some cases a thought (or gedanken) experiment appears to suggest that perpetual motion may be possible through accepted and understood physical processes. However, in all cases, a flaw has been found when all of the relevant physics is considered. Examples include: Maxwell's demon: This was originally proposed to show that the Second Law of Thermodynamics applied in the statistical sense only, by postulating a \"demon\" that could select energetic molecules and extract their energy. Subsequent analysis (and experiment) have shown there is no way to physically implement such a system that does not result in an overall increase in entropy.\n",
      "-This technique is widely described as a \"Maxwell's demon\" because it realizes Maxwell's process of creating a temperature difference by sorting high and low energy atoms into different containers. However, scientists have pointed out that it is not a true Maxwell's demon in the sense that it does not violate the second law of thermodynamics; it does not result in a net decrease in entropy and cannot be used to produce useful energy. This is because the process requires more energy from the laser beams than could be produced by the temperature difference generated. The atoms absorb low entropy photons from the laser beam and emit them in a random direction, thus increasing the entropy of the environment.In 2014, Pekola et al. demonstrated an experimental realization of a Szilárd engine. Only a year later and based on an earlier theoretical proposal, the same group presented the first experimental realization of an autonomous Maxwell's demon, which extracts microscopic information from a system and reduces its entropy by applying feedback. The demon is based on two capacitively coupled single-electron devices, both integrated on the same electronic circuit. The operation of the demon is directly observed as a temperature drop in the system, with a simultaneous temperature rise in the demon arising from the thermodynamic cost of generating the mutual information. In 2016, Pekola et al. demonstrated a proof-of-principle of an autonomous demon in coupled single-electron circuits, showing a way to cool critical elements in a circuit with information as a fuel. Pekola et al. have also proposed that a simple qubit circuit, e.g., made of a superconducting circuit, could provide a basis to study a quantum Szilard's engine.\n",
      "-The thought experiment first appeared in a letter Maxwell wrote to Peter Guthrie Tait on 11 December 1867. It appeared again in a letter to John William Strutt in 1871, before it was presented to the public in Maxwell's 1872 book on thermodynamics titled Theory of Heat.In his letters and books, Maxwell described the agent opening the door between the chambers as a \"finite being\". William Thomson (Lord Kelvin) was the first to use the word \"demon\" for Maxwell's concept, in the journal Nature in 1874, and implied that he intended the Greek mythology interpretation of a daemon, a supernatural being working in the background, rather than a malevolent being.\n",
      "\n",
      "\n",
      "\n",
      "What is the application of Memristor?\n",
      "-Memristors remain a laboratory curiosity, as yet made in insufficient numbers to gain any commercial applications. Despite this lack of mass availability, according to Allied Market Research the memristor market was worth $3.2 million in 2015 and will be worth $79.0 million by 2022.A potential application of memristors is in analog memories for superconducting quantum computers.Memristors can potentially be fashioned into non-volatile solid-state memory, which could allow greater data density than hard drives with access times similar to DRAM, replacing both components. HP prototyped a crossbar latch memory that can fit 100 gigabits in a square centimeter, and proposed a scalable 3D design (consisting of up to 1000 layers or 1 petabit per cm3). In May 2008 HP reported that its device reaches currently about one-tenth the speed of DRAM. The devices' resistance would be read with alternating current so that the stored value would not be affected. In May 2012, it was reported that the access time had been improved to 90 nanoseconds, which is nearly one hundred times faster than the contemporaneous Flash memory. At the same time, the energy consumption was just one percent of that consumed by Flash memory.Memristor have applications in programmable logic signal processing, Super-resolution imaging physical neural networks, control systems, reconfigurable computing, in-memory computing, brain–computer interfaces and RFID. Memristive devices are potentially used for stateful logic implication, allowing a replacement for CMOS-based logic computation Several early works have been reported in this direction.In 2009, a simple electronic circuit consisting of an LC network and a memristor was used to model experiments on adaptive behavior of unicellular organisms. It was shown that subjected to a train of periodic pulses, the circuit learns and anticipates the next pulse similar to the behavior of slime molds Physarum polycephalum where the viscosity of channels in the cytoplasm responds to periodic environment changes. Applications of such circuits may include, e.g., pattern recognition. The DARPA SyNAPSE project funded HP Labs, in collaboration with the Boston University Neuromorphics Lab, has been developing neuromorphic architectures which may be based on memristive systems. In 2010, Versace and Chandler described the MoNETA (Modular Neural Exploring Traveling Agent) model. MoNETA is the first large-scale neural network model to implement whole-brain circuits to power a virtual and robotic agent using memristive hardware. Application of the memristor crossbar structure in the construction of an analog soft computing system was demonstrated by Merrikh-Bayat and Shouraki. In 2011, they showed how memristor crossbars can be combined with fuzzy logic to create an analog memristive neuro-fuzzy computing system with fuzzy input and output terminals. Learning is based on the creation of fuzzy relations inspired from Hebbian learning rule.\n",
      "-Neuromemristive systems Neuromemristive systems is a subclass of neuromorphic computing systems that focuses on the use of memristors to implement neuroplasticity. While neuromorphic engineering focuses on mimicking biological behavior, neuromemristive systems focus on abstraction. For example, a neuromemristive system may replace the details of a cortical microcircuit's behavior with an abstract neural network model.There exist several neuron inspired threshold logic functions implemented with memristors that have applications in high level pattern recognition applications. Some of the applications reported recently include speech recognition, face recognition and object recognition. They also find applications in replacing conventional digital logic gates.For (quasi)ideal passive memristive circuits, the evolution of the memristive memories can be written in a closed form (Caravelli-Traversa-Di Ventra equation): ddtX→=−αX→+1β(I−χΩX)−1ΩS→ as a function of the properties of the physical memristive network and the external sources. The equation is valid for the case of the Williams-Strukov original toy model, as in the case of ideal memristors,  α=0 . However, the hypothesis of the existence of an ideal memristor is debatable. In the equation above,  α is the \"forgetting\" time scale constant, typically associated to memory volatility, while  off on off is the ratio of off and on values of the limit resistances of the memristors,  S→ is the vector of the sources of the circuit and  Ω is a projector on the fundamental loops of the circuit. The constant  β has the dimension of a voltage and is associated to the properties of the memristor; its physical origin is the charge mobility in the conductor. The diagonal matrix and vector  diag ⁡(X→) and  X→ respectively, are instead the internal value of the memristors, with values between 0 and 1. This equation thus requires adding extra constraints on the memory values in order to be reliable.\n",
      "-Memory storage Electronic memory designs in the past have largely relied on the formation of transistors. However, research into crossbar switch based electronics have offered an alternative using reconfigurable interconnections between vertical and horizontal wiring arrays to create ultra high density memories. Two leaders in this area are Nantero which has developed a carbon nanotube based crossbar memory called Nano-RAM and Hewlett-Packard which has proposed the use of memristor material as a future replacement of Flash memory.An example of such novel devices is based on spintronics. The dependence of the resistance of a material (due to the spin of the electrons) on an external field is called magnetoresistance. This effect can be significantly amplified (GMR - Giant Magneto-Resistance) for nanosized objects, for example when two ferromagnetic layers are separated by a nonmagnetic layer, which is several nanometers thick (e.g. Co-Cu-Co). The GMR effect has led to a strong increase in the data storage density of hard disks and made the gigabyte range possible. The so-called tunneling magnetoresistance (TMR) is very similar to GMR and based on the spin dependent tunneling of electrons through adjacent ferromagnetic layers. Both GMR and TMR effects can be used to create a non-volatile main memory for computers, such as the so-called magnetic random access memory or MRAM.\n",
      "-Memristive neural network Greg Snider of HP Labs describes a system of cortical computing with memristive nanodevices. The memristors (memory resistors) are implemented by thin film materials in which the resistance is electrically tuned via the transport of ions or oxygen vacancies within the film. DARPA's SyNAPSE project has funded IBM Research and HP Labs, in collaboration with the Boston University Department of Cognitive and Neural Systems (CNS), to develop neuromorphic architectures which may be based on memristive systems.\n",
      "-Following this claim, Leon Chua has argued that the memristor definition could be generalized to cover all forms of two-terminal non-volatile memory devices based on resistance switching effects. Chua also argued that the memristor is the oldest known circuit element, with its effects predating the resistor, capacitor, and inductor. There are, however, some serious doubts as to whether a genuine memristor can actually exist in physical reality. Additionally, some experimental evidence contradicts Chua's generalization since a non-passive nanobattery effect is observable in resistance switching memory. A simple test has been proposed by Pershin and Di Ventra to analyze whether such an ideal or generic memristor does actually exist or is a purely mathematical concept. Up to now, there seems to be no experimental resistance switching device (ReRAM) which can pass the test.These devices are intended for applications in nanoelectronic memory devices, computer logic, and neuromorphic/neuromemristive computer architectures. In 2013, Hewlett-Packard CTO Martin Fink suggested that memristor memory may become commercially available as early as 2018. In March 2012, a team of researchers from HRL Laboratories and the University of Michigan announced the first functioning memristor array built on a CMOS chip.\n",
      "\n",
      "\n",
      "\n",
      "What is the effect generated by a spinning superconductor?\n",
      "-London moment Conversely, a spinning superconductor generates a magnetic field, precisely aligned with the spin axis. The effect, the London moment, was put to good use in Gravity Probe B. This experiment measured the magnetic fields of four superconducting gyroscopes to determine their spin axes. This was critical to the experiment since it is one of the few ways to accurately determine the spin axis of an otherwise featureless sphere.\n",
      "-Magnetization Although this magnetic flux distribution seems somewhat counter-intuitive to those familiar with simple bar magnets or solenoids, the reason for this flux distribution can be intuitively visualised using Mallinson's original diagram (note that it uses the negative y component, unlike the diagram in Mallinson's article). The diagram shows the field from a strip of ferromagnetic material with alternating magnetization in the y direction (top left) and in the x direction (top right). Note that the field above the plane is in the same direction for both structures, but the field below the plane is in opposite directions. The effect of superimposing both of these structures is shown in the figure.\n",
      "-Superconductors can enhance central effects in spintronics such as magnetoresistance effects, spin lifetimes and dissipationless spin-currents.The simplest method of generating a spin-polarised current in a metal is to pass the current through a ferromagnetic material. The most common applications of this effect involve giant magnetoresistance (GMR) devices. A typical GMR device consists of at least two layers of ferromagnetic materials separated by a spacer layer. When the two magnetization vectors of the ferromagnetic layers are aligned, the electrical resistance will be lower (so a higher current flows at constant voltage) than if the ferromagnetic layers are anti-aligned. This constitutes a magnetic field sensor.\n",
      "-The magnetic field strength associated with a rotating superconductor is given by: B=−2MQω, where M and Q are the mass and the charge of the superconducting charge carriers respectively. For the case of Cooper pairs of electrons, M = 2me and Q = 2e. Despite the electrons existing in a strongly interacting environment, me denotes here the mass of the bare electrons (as in vacuum), and not e.g. the effective mass of conducting electrons of the normal phase.\n",
      "-In 1992 Evgeny Podkletnov published a heavily debated journal article claiming a specific type of rotating superconductor could shield gravitational force. Independently of this, from 1991 to 1993 Ning Li and Douglas Torr published a number of articles about gravitational effects in superconductors. One finding they derived is the source of gravitomagnetic flux in a type II superconductor material is due to spin alignment of the lattice ions. Quoting from their third paper: \"It is shown that the coherent alignment of lattice ion spins will generate a detectable gravitomagnetic field, and in the presence of a time-dependent applied magnetic vector potential field, a detectable gravitoelectric field.\" The claimed size of the generated force has been disputed by some but defended by others. In 1997 Li published a paper attempting to replicate Podkletnov's results and showed the effect was very small, if it existed at all. Li is reported to have left the University of Alabama in 1999 to found the company AC Gravity LLC. AC Gravity was awarded a U.S. DOD grant for $448,970 in 2001 to continue anti-gravity research. The grant period ended in 2002 but no results from this research were ever made public.In 2002 Phantom Works, Boeing's advanced research and development facility in Seattle, approached Evgeny Podkletnov directly. Phantom Works was blocked by Russian technology transfer controls. At this time Lieutenant General George Muellner, the outgoing head of the Boeing Phantom Works, confirmed that attempts by Boeing to work with Podkletnov had been blocked by Moscow, also commenting that \"The physical principles – and Podkletnov's device is not the only one – appear to be valid... There is basic science there. They're not breaking the laws of physics. The issue is whether the science can be engineered into something workable\"Froning and Roach (2002) put forward a paper that builds on the work of Puthoff, Haisch and Alcubierre. They used fluid dynamic simulations to model the interaction of a vehicle (like that proposed by Alcubierre) with the zero-point field. Vacuum field perturbations are simulated by fluid field perturbations and the aerodynamic resistance of viscous drag exerted on the interior of the vehicle is compared to the Lorentz force exerted by the zero-point field (a Casimir-like force is exerted on the exterior by unbalanced zero-point radiation pressures). They find that the optimized negative energy required for an Alcubierre drive is where it is a saucer-shaped vehicle with toroidal electromagnetic fields. The EM fields distort the vacuum field perturbations surrounding the craft sufficiently to affect the permeability and permittivity of space.\n",
      "\n",
      "\n",
      "\n",
      "What is the main focus of cryogenic and noble liquid detectors in dark matter experiments?\n",
      "-Noble gas scintillators Noble gas scintillators use the property of certain materials to scintillate, which is when a material absorbs energy from a particle and remits the same amount of energy as light. Of particular interest for dark matter detection is the use of noble gases, even more specifically liquid xenon.  The XENON series of experiments, also located at the Gran Sasso National Lab, is a forefront user of liquid xenon scintillators. Common across all generations of the experiment, the detector consists of a tank of liquid xenon with a gaseous layer on top. At the top and bottom of the detector is a layer of photomultiplier tubes (PMTs). When a dark matter particle collides with the liquid xenon, it rapidly releases a photon which is detected by the PMTs. To cross reference this data point an electric field is applied which is sufficiently large to prevent complete recombination of the electrons knocked loose by the interaction. These drift to the top of the detector and are also detected, creating two separate detections for each event. Measuring the time delay between these allows for a complete 3-D reconstruction of the interaction. The detector is also able to discriminate between electronic recoils and nuclear recoils, as both types of events would produce differing ratios of the photon energy and the released electron energy.  The most recently completed version of the XENON experiment is XENON1T, which used 3.2 tons of liquid xenon. This experiment produced a then record limit for the cross section of WIMP dark matter of 4.1×10−47 cm2 at a mass of 30 GeV/c2. The most recent iteration of the XENON succession is XENONnT, which is currently running with 8 tones of liquid xenon. This experiment is projected to be able to probe WIMP-nucleon cross sections of 1.4×10−48 cm2 for a 50 GeV/c2 WIMP mass. At this ultra-low cross section, interference from the background neutrino flux is predicted to be problematic.\n",
      "-These experiments mostly use either cryogenic or noble liquid detector technologies. Cryogenic detectors operating at temperatures below 100 mK, detect the heat produced when a particle hits an atom in a crystal absorber such as germanium. Noble liquid detectors detect scintillation produced by a particle collision in liquid xenon or argon. Cryogenic detector experiments include: CDMS, CRESST, EDELWEISS, EURECA. Noble liquid experiments include LZ, XENON, DEAP, ArDM, WARP, DarkSide, PandaX, and LUX, the Large Underground Xenon experiment. Both of these techniques focus strongly on their ability to distinguish background particles (which predominantly scatter off electrons) from dark matter particles (that scatter off nuclei). Other experiments include SIMPLE and PICASSO.\n",
      "-Cryogenic dark matter experiments use particle detectors operating at millikelvin temperatures to search for the elastic scattering of WIMPs of an atomic nuclei. A particle interaction inside an absorber crystal will create a large number of phonons, these thermalise inside a thermometer on the crystal surface, which records the rise in temperature. Such cryogenic detectors are used as they combine a high sensitivity with a low energy threshold and excellent resolution.\n",
      "-Since liquid argon is a scintillating material a particle interacting with it produces light in proportion to the energy deposited from the incident particle, this is a linear effect for low energies before quenching becomes a major contributing factor. The interaction of a particle with the argon causes ionization and recoiling along the path of interaction. The recoiling argon nuclei undergo recombination or self-trapping, ultimately resulting in the emission of 128nm vacuum ultra-violet (VUV) photons. Additionally liquid argon has the unique property of being transparent to its own scintillation light, this allows for light yields of tens of thousands of photons produced for every MeV of energy deposited.  The elastic scattering of a WIMP dark matter particle with an argon nucleus is expected to cause the nucleus to recoil. This is expected to be a very low energy interaction (keV) and requires a low detection threshold in order to be sensitive. Due to the necessarily low detection threshold, the number of background events detected is very high. The faint signature of a dark matter particle such as a WIMP will be masked by the many different types of possible background events. A technique for identifying these non-dark matter events is pulse shape discrimination (PSD), which characterizes an event based on the timing signature of the scintillation light from liquid argon.  PSD is possible in a liquid argon detector because interactions due to different incident particles such as electrons, high energy photons, alphas, and neutrons create different proportions of excited states of the recoiling argon nuclei, these are known as singlet and triplet states and they decay with characteristic lifetimes of 6 ns and 1300 ns respectively. Interactions from gammas and electrons produce primarily triplet excited states through electronic recoils, while neutron and alpha interactions produce primarily singlet excited states through nuclear recoils. It is expected that WIMP-nucleon interactions also produce a nuclear recoil type signal due to the elastic scattering of the dark matter particle with the argon nucleus.  By using the arrival time distribution of light for an event, it is possible to identify its likely source. This is done quantitatively by measuring the ratio of the light measured by the photo-detectors in a \"prompt\" window (<60 ns) over the light measured in a \"late\" window (<10,000 ns). In DEAP this parameter is called Fprompt. Nuclear recoil type events have high Fprompt (~0.7) values while electronic recoil events have a low Fprompt value (~0.3). Due to this separation in Fprompt for WIMP-like (Nuclear Recoil) and background-like (Electronic Recoil) events, it is possible to uniquely identify the most dominant sources of background in the detector.The most abundant background in DEAP comes from the beta decay of Argon-39 which has an activity of approximately 1 Bq/kg in atmospheric argon. Discrimination of beta and gamma background events from nuclear recoils in the energy region of interest (near 20 keV of electron energy) is required to be better than 1 in 108 to sufficiently suppress these backgrounds for a dark matter search in liquid atmospheric argon.\n",
      "-Condensed noble gases, most notably liquid xenon and liquid argon, are excellent radiation detection media. They can produce two signatures for each particle interaction: a fast flash of light (scintillation) and the local release of charge (ionisation). In two-phase xenon – so called since it involves liquid and gas phases in equilibrium – the scintillation light produced by an interaction in the liquid is detected directly with photomultiplier tubes; the ionisation electrons released at the interaction site are drifted up to the liquid surface under an external electric field, and subsequently emitted into a thin layer of xenon vapour. Once in the gas, they generate a second, larger pulse of light (electroluminescence or proportional scintillation), which is detected by the same array of photomultipliers. These systems are also known as xenon 'emission detectors'.This configuration is that of a time projection chamber (TPC); it allows three-dimensional reconstruction of the interaction site, since the depth coordinate (z) can be measured very accurately from the time separation between the two light pulses. The horizontal coordinates can be reconstructed from the hit pattern in the photomultiplier array(s). Critically for WIMP searches, the ratio between the two response channels (scintillation and ionisation) allows the rejection of the predominant backgrounds for WIMP searches: gamma and beta radiation from trace radioactivity in detector materials and the immediate surroundings. WIMP candidate events produce lower ionisation/scintillation ratios than the more prevalent background interactions.\n",
      "\n",
      "\n",
      "\n",
      "What is a pycnometer?\n",
      "-A gas pycnometer is a laboratory device used for measuring the density—or, more accurately, the volume—of solids, be they regularly shaped, porous or non-porous, monolithic, powdered, granular or in some way comminuted, employing some method of gas displacement and the volume:pressure relationship known as Boyle's Law. A gas pycnometer is also sometimes referred to as a helium pycnometer.\n",
      "-Volume vs density While pycnometers (of any type) are recognized as density measuring devices they are in fact devices for measuring volume only. Density is merely calculated as the ratio of mass to volume; mass being invariably measured on a discrete device, usually by weighing. The volume measured in a gas pycnometer is that amount of three-dimensional space which is inaccessible to the gas used, i.e. that volume within the sample chamber from which the gas is excluded. Therefore, the volume measured considering the finest scale of surface roughness will depend on the atomic or molecular size of the gas. Helium therefore is most often prescribed as the measurement gas, not only is it of small size, it is also inert and the most ideal gas.\n",
      "-Pycnometer A pycnometer (from Ancient Greek: πυκνός, romanized: puknos, lit. 'dense'), also called pyknometer or specific gravity bottle, is a device used to determine the density of a liquid. A pycnometer is usually made of glass, with a close-fitting ground glass stopper with a capillary tube through it, so that air bubbles may escape from the apparatus. This device enables a liquid's density to be measured accurately by reference to an appropriate working fluid, such as water or mercury, using an analytical balance.If the flask is weighed empty, full of water, and full of a liquid whose relative density is desired, the relative density of the liquid can easily be calculated. The particle density of a powder, to which the usual method of weighing cannot be applied, can also be determined with a pycnometer. The powder is added to the pycnometer, which is then weighed, giving the weight of the powder sample. The pycnometer is then filled with a liquid of known density, in which the powder is completely insoluble. The weight of the displaced liquid can then be determined, and hence the relative density of the powder.\n",
      "-Volumetric measurement A gas pycnometer can be used to measure the volume of a powder sample. A sample of known mass is loaded into a chamber of known volume that is connected by a closed valve to a gas reservoir, also of known volume, at a higher pressure than the chamber. After the valve is opened, the final pressure in the system allows the total gas volume to be determined by application of Boyle's law.\n",
      "-Pycnometer is the preferred spelling in modern American English usage. Pyknometer is to be found in older texts, and is used interchangeably with pycnometer in British English. The term has its origins in the Greek word πυκνός, meaning \"dense\".\n",
      "The density calculated from a volume measured using a gas pycnometer is often referred to as skeletal density, true density or helium density.\n",
      "For non-porous solids a pycnometer can be used to measure particle density.\n",
      "An extreme example of the gas displacement principle for volume measurement is described in U.S. Patent 5,231,873 (Lindberg, 1993) wherein a chamber large enough to hold a flatbed truck is used to measure the volume of a load of timber.\n",
      "\n",
      "\n",
      "\n",
      "What is the estimated redshift of CEERS-93316, a candidate high-redshift galaxy observed by the James Webb Space Telescope?\n",
      "-MIRI low-resolution spectroscopy (LRS): a hot super-Earth planet L 168-9 b (TOI-134) around a bright M-dwarf starWithin two weeks of the first Webb images, several preprint papers described a wide range of high redshift and very luminous (presumably large) galaxies believed to date from 235 million years (z=16.7) to 280 million years after the Big Bang, far earlier than previously known. On 17 August 2022, NASA released a large mosaic image of 690 individual frames taken by the Near Infrared Camera (NIRCam) on Webb of numerous very early galaxies. Some early galaxies observed by Webb like CEERS-93316, which has an estimated redshift of approximately z=16.7 corresponding to 235.8 million years after the Big Bang, are high redshift galaxy candidates. In September 2022, primordial black holes were proposed as explaining these unexpectedly large and early galaxies.In June 2023 detection of organic molecules 12 billion light-years away in a galaxy called SPT0418-47 using the Webb telescope was announced.On 12 July 2023, NASA celebrated the first year of operations with the release of Webb’s image of a small star-forming region in the Rho Ophiuchi cloud complex, 390 light years away.\n",
      "-Redshift (z) can be expressed by the following equations: In these equations, frequency is denoted by  f and wavelength by  λ . The larger the value of z, the more redshifted the light and the farther away the object is from the Earth. As of January 2013, the largest galaxy redshift of z~12 was found using the Hubble Ultra-Deep Field, corresponding to an age of over 13 billion years (the universe is approximately 13.82 billion years old).The Doppler effect and Hubble's law can be combined to form the equation Hubble c where c is the speed of light.\n",
      "-The decoupling, or the last scattering, is thought to have occurred about 300,000 years after the Big Bang, or at a redshift of about  1100 . We can determine both the approximate angular diameter of the universe and the physical size of the particle horizon that had existed at this time.  The angular diameter distance, in terms of redshift  z , is described by  dA(z)=r(z)/(1+z) . If we assume a flat cosmology then,  r(z)=∫temt0dta(t)=∫aem1daa2H(a)=∫0zdzH(z).\n",
      "-While some scientists have claimed other objects (such as Abell 1835 IR1916) have higher redshifts (and therefore are seen in an earlier stage of the universe's evolution), IOK-1's age and composition have been more reliably established. In December 2012, astronomers reported that UDFj-39546284 is the most distant object known and has a redshift value of 11.9. The object, estimated to have existed around 380 million years after the Big Bang (which was about 13.8 billion years ago), is about 13.42 billion light travel distance years away. The existence of galaxies so soon after the Big Bang suggests that protogalaxies must have grown in the so-called \"dark ages\". As of May 5, 2015, the galaxy EGS-zs8-1 is the most distant and earliest galaxy measured, forming 670 million years after the Big Bang. The light from EGS-zs8-1 has taken 13 billion years to reach Earth, and is now 30 billion light-years away, because of the expansion of the universe during 13 billion years. On 17 August 2022, NASA released a large mosaic image of 690 individual frames taken by the Near Infrared Camera (NIRCam) on the James Webb Space Telescope (JWST) of numerous very early galaxies.In May 2023, a study in the journal Nature identified an ultra-faint galaxy named JD1. Galaxy JD1 was observed by the JWST using the near-infrared spectrograph instrument NIRSpec and was found to have a distance value of redshift z=9.79. This means that JD1 was observed at 480 million years after the Big Bang when the universe was only about 4% of its present age. Observations of this ultra-faint galaxy were aided by the effect of a gravitational lens in the galaxy cluster Abell 2744 which helped make the image of JD1 larger and 13 times brighter than it otherwise would be. This effect and the use of the JWST's NIRCam showed JD1's structure to be three starforming clumps of dust and gas. One of the authors of the study Tommaso Treu said: \"The combination of JWST and the magnifying power of gravitational lensing is a revolution. We are rewriting the book on how galaxies formed and evolved in the immediate aftermath of the Big Bang.\" The detailed process by which the earliest galaxies formed is an open question in astrophysics. Theories can be divided into two categories: top-down and bottom-up. In top-down correlations (such as the Eggen–Lynden-Bell–Sandage [ELS] model), protogalaxies form in a large-scale simultaneous collapse lasting about one hundred million years. In bottom-up theories (such as the Searle-Zinn [SZ] model), small structures such as globular clusters form first, and then a number of such bodies accrete to form a larger galaxy.\n",
      "-Other large numbers, as regards length and time, are found in astronomy and cosmology. For example, the current Big Bang model suggests that the universe is 13.8 billion years (4.355 × 1017 seconds) old, and that the observable universe is 93 billion light years across (8.8 × 1026 metres), and contains about 5 × 1022 stars, organized into around 125 billion (1.25 × 1011) galaxies, according to Hubble Space Telescope observations. There are about 1080 atoms in the observable universe, by rough estimation.According to Don Page, physicist at the University of Alberta, Canada, the longest finite time that has so far been explicitly calculated by any physicist is 10 10 10 10 10 1.1 years which corresponds to the scale of an estimated Poincaré recurrence time for the quantum state of a hypothetical box containing a black hole with the estimated mass of the entire universe, observable or not, assuming a certain inflationary model with an inflaton whose mass is 10−6 Planck masses. This time assumes a statistical model subject to Poincaré recurrence. A much simplified way of thinking about this time is in a model where the universe's history repeats itself arbitrarily many times due to properties of statistical mechanics; this is the time scale when it will first be somewhat similar (for a reasonable choice of \"similar\") to its current state again.\n",
      "\n",
      "\n",
      "\n",
      "What is bollard pull primarily used for measuring?\n",
      "-Bollard pull is a conventional measure of the pulling (or towing) power of a watercraft. It is defined as the force (usually in tonnes-force or kilonewtons (kN)) exerted by a vessel under full power, on a shore-mounted bollard through a tow-line, commonly measured in a practical test (but sometimes simulated) under test conditions that include calm water, no tide, level trim, and sufficient depth and side clearance for a free propeller stream. Like the horsepower or mileage rating of a car, it is a convenient but idealized number that must be adjusted for operating conditions that differ from the test. The bollard pull of a vessel may be reported as two numbers, the static or maximum bollard pull – the highest force measured – and the steady or continuous bollard pull, the average of measurements over an interval of, for example, 10 minutes. An equivalent measurement on land is known as drawbar pull, or tractive force, which is used to measure the total horizontal force generated by a locomotive, a piece of heavy machinery such as a tractor, or a truck, (specifically a ballast tractor), which is utilized to move a load.\n",
      "-Practical bollard pull tests under simplified conditions are conducted for human powered vehicles. There, bollard pull is often a category in competitions and gives an indication of the power train efficiency. Although conditions for such measurements are inaccurate in absolute terms, they are the same for all competitors. Hence, they can still be valid for comparing several craft.\n",
      "-A truck scale (US), weighbridge (non-US) or railroad scale is a large set of scales, usually mounted permanently on a concrete foundation, that is used to weigh entire rail or road vehicles and their contents. By weighing the vehicle both empty and when loaded, the load carried by the vehicle can be calculated.\n",
      "The key component that uses a weighbridge in order to make the weigh measurement is load cells.\n",
      "-A ballast tractor is a specially weighted tractor unit of a heavy hauler combination. It is designed to utilize a drawbar to pull or push heavy or exceptionally large trailer loads which are loaded in a hydraulic modular trailer. When feasible, lowboy-style semi-trailers are used to minimize a load's center of gravity. Typical drivetrains are 6×4 and 6×6 but also available in 8×6 and 8×8. Typical ballast tractor loads include oil rig modules, bridge sections, buildings, ship sections, and industrial machinery such as generators and turbines.\n",
      "-The same type of \"weigh bar\" can be used to measure horizontal loads and \"drawbar pull\" of wheeled/tracked or vehicles or \"bollard pull\" of boats or the \"thrust\" of jet engines when a proper \"test rig\" is designed and constructed to provide \"frictionless\" fore-aft movement of the load relative to the weigh bars.\n",
      "\n",
      "\n",
      "\n",
      "What is the piezoelectric strain coefficient for AT-cut quartz crystals?\n",
      "-Amplitude of motion The amplitude of lateral displacement rarely exceeds a nanometer. More specifically one has u0=4(nπ)2dQUel with u0 the amplitude of lateral displacement, n the overtone order, d the piezoelectric strain coefficient, Q the quality factor, and Uel the amplitude of electrical driving. The piezoelectric strain coefficient is given as d = 3.1·10‑12 m/V for AT-cut quartz crystals. Due to the small amplitude, stress and strain usually are proportional to each other. The QCM operates in the range of linear acoustics.\n",
      "-AT-cut crystals are singularly rotated Y-axis cuts in which the top and bottom half of the crystal move in opposite directions (thickness shear vibration) during oscillation.  The AT-cut crystal is easily manufactured. However, it has limitations at high and low temperature, as it is easily disrupted by internal stresses caused by temperature gradients in these temperature extremes (relative to room temperature, ~25 °C). These internal stress points produce undesirable frequency shifts in the crystal, decreasing its accuracy. The relationship between temperature and frequency is cubic. The cubic relationship has an inflection point near room temperature. As a consequence the AT-cut quartz crystal is most effective when operating at or near room temperature. For applications which are above room temperature, water cooling is often helpful.\n",
      "-The strain-charge for a material of the 4mm (C4v) crystal class (such as a poled piezoelectric ceramic such as tetragonal PZT or BaTiO3) as well as the 6mm crystal class may also be written as (ANSI IEEE 176): 11 12 13 21 22 23 31 32 33 44 55 66 11 12 31 32 33 24 15 15 24 31 32 33 11 22 33 ][E1E2E3] where the first equation represents the relationship for the converse piezoelectric effect and the latter for the direct piezoelectric effect.Although the above equations are the most used form in literature, some comments about the notation are necessary. Generally, D and E are vectors, that is, Cartesian tensors of rank 1; and permittivity ε is a Cartesian tensor of rank 2. Strain and stress are, in principle, also rank-2 tensors. But conventionally, because strain and stress are all symmetric tensors, the subscript of strain and stress can be relabeled in the following fashion: 11 → 1; 22 → 2; 33 → 3; 23 → 4; 13 → 5; 12 → 6. (Different conventions may be used by different authors in literature. For example, some use 12 → 4; 23 → 5; 31 → 6 instead.) That is why S and T appear to have the \"vector form\" of six components. Consequently, s appears to be a 6-by-6 matrix instead of a rank-3 tensor. Such a relabeled notation is often called Voigt notation. Whether the shear strain components S4, S5, S6 are tensor components or engineering strains is another question. In the equation above, they must be engineering strains for the 6,6 coefficient of the compliance matrix to be written as shown, i.e., 2(sE11 − sE12). Engineering shear strains are double the value of the corresponding tensor shear, such as S6 = 2S12 and so on. This also means that s66 = 1/G12, where G12 is the shear modulus.\n",
      "-Non linear piezoelectric effects in polar semiconductors are the manifestation that the strain induced piezoelectric polarization depends not just on the product of the first order piezoelectric coefficients times the strain tensor components but also on the product of the second order (or higher) piezoelectric coefficients times products of the strain tensor components. The idea was put forward for zincblende GaAs and InAs semiconductors since 2006, and then extended to all commonly used wurtzite and zincblende semiconductors. Given the difficulty of finding direct experimental evidence for the existence of these effects, there are different schools of thought on how one can calculate reliably all the piezoelectric coefficients.\n",
      "-q=βr−(βr.r^)r^ β=(∂u1x1∂u1x2∂u1x3∂u2x1∂u2x2∂u2x3∂u3x1∂u3x2∂u3x3),r=(r1r2r3) The shifts are measured in the phosphor (detector) plane ( β3r3=0 ), and the relationship is simplified; thus, eight out of the nine displacement gradient tensor components can be calculated by measuring the shift at four distinct, widely spaced regions on the EBSP. This shift is then corrected to the sample frame (flipped around Y-axis) because EBSP is recorded on the phosphor screen and is inverted as in a mirror. They are then corrected around the X-axis by 24° (i.e., 20° sample tilt plus ≈4° camera tilt and assuming no angular effect from the beam movement). Using infinitesimal strain theory, the deformation gradient is then split into elastic strain (symmetric part, where  ij=ji ),  eij and lattice rotations (asymmetric part, where  ii=jj=0 ),  ωij .eij=12(βij+βijr),ωij=12(βij−βijr) These measurements do not provide information about the volumetric/hydrostatic strain tensors. By imposing a boundary condition that the stress normal to the surface ( 33 ) is zero (i.e., traction-free surface), and using Hooke's law with anisotropic elastic stiffness constants, the missing ninth degree of freedom can be estimated in this constrained minimisation problem by using a nonlinear solver.\n",
      "\n",
      "\n",
      "\n",
      "What is the difference between probability mass function (PMF) and probability density function (PDF)?\n",
      "-The terms probability distribution function and probability function have also sometimes been used to denote the probability density function. However, this use is not standard among probabilists and statisticians. In other sources, \"probability distribution function\" may be used when the probability distribution is defined as a function over general sets of values or it may refer to the cumulative distribution function, or it may be a probability mass function (PMF) rather than the density. \"Density function\" itself is also used for the probability mass function, leading to further confusion. In general though, the PMF is used in the context of discrete random variables (random variables that take values on a countable set), while the PDF is used in the context of continuous random variables.\n",
      "-A probability mass function differs from a probability density function (PDF) in that the latter is associated with continuous rather than discrete random variables. A PDF must be integrated over an interval to yield a probability.The value of the random variable having the largest probability mass is called the mode.\n",
      "-In statistics, especially in Bayesian statistics, the kernel of a probability density function (pdf) or probability mass function (pmf) is the form of the pdf or pmf in which any factors that are not functions of any of the variables in the domain are omitted. Note that such factors may well be functions of the parameters of the pdf or pmf. These factors form part of the normalization factor of the probability distribution, and are unnecessary in many situations. For example, in pseudo-random number sampling, most sampling algorithms ignore the normalization factor. In addition, in Bayesian analysis of conjugate prior distributions, the normalization factors are generally ignored during the calculations, and only the kernel considered. At the end, the form of the kernel is examined, and if it matches a known distribution, the normalization factor can be reinstated. Otherwise, it may be unnecessary (for example, if the distribution only needs to be sampled from).\n",
      "-In probability and statistics, a probability mass function is a function that gives the probability that a discrete random variable is exactly equal to some value. Sometimes it is also known as the discrete probability density function. The probability mass function is often the primary means of defining a discrete probability distribution, and such functions exist for either scalar or multivariate random variables whose domain is discrete.\n",
      "-The first illustration involves a continuous probability distribution, for which the random variables have a probability density function. The second illustration, for which most of the computation can be done by hand, involves a discrete probability distribution, which is characterized by a probability mass function.\n",
      "\n",
      "\n",
      "\n",
      "How do the Lunar Laser Ranging Experiment, radar astronomy, and the Deep Space Network determine distances to the Moon, planets, and spacecraft?\n",
      "-Distance measurement Radar systems measure the distance to a target by the time it takes a radio-wave pulse to return to the radar antenna after being reflected by the target: the distance to the target is half the round-trip transit time multiplied by the speed of light. A Global Positioning System (GPS) receiver measures its distance to GPS satellites based on how long it takes for a radio signal to arrive from each satellite, and from these distances calculates the receiver's position. Because light travels about 300000 kilometres (186000 mi) in one second, these measurements of small fractions of a second must be very precise. The Lunar Laser Ranging experiment, radar astronomy and the Deep Space Network determine distances to the Moon, planets and spacecraft, respectively, by measuring round-trip transit times.\n",
      "-Millimeter-precision measurements of the lunar distance are made by measuring the time taken for laser beam light to travel between stations on Earth and retroreflectors placed on the Moon. The Moon is spiraling away from Earth at an average rate of 3.8 cm (1.5 in) per year, as detected by the Lunar Laser Ranging experiment.\n",
      "-Solar System Besides terrestrial tests also astrometric tests using Lunar Laser Ranging (LLR), i.e. sending laser signals from Earth to Moon and back, have been conducted. They are ordinarily used to test general relativity and are evaluated using the Parameterized post-Newtonian formalism. However, since these measurements are based on the assumption that the speed of light is constant, they can also be used as tests of special relativity by analyzing potential distance and orbit oscillations. For instance, Zoltán Lajos Bay and White (1981) demonstrated the empirical foundations of the Lorentz group and thus special relativity by analyzing the planetary radar and LLR data.In addition to the terrestrial Kennedy–Thorndike experiments mentioned above, Müller & Soffel (1995) and Müller et al. (1999) tested the RMS velocity dependence parameter by searching for anomalous distance oscillations using LLR. Since time dilation is already confirmed to high precision, a positive result would prove that light speed depends on the observer's velocity and length contraction is direction dependent (like in the other Kennedy–Thorndike experiments). However, no anomalous distance oscillations have been observed, with a RMS velocity dependence limit of  12 10 −5 , comparable to that of Hils and Hall (1990, see table above on the right).\n",
      "-Interferometry Interferometry is another method to find the wavelength of electromagnetic radiation for determining the speed of light. A coherent beam of light (e.g. from a laser), with a known frequency (f), is split to follow two paths and then recombined. By adjusting the path length while observing the interference pattern and carefully measuring the change in path length, the wavelength of the light (λ) can be determined. The speed of light is then calculated using the equation c = λf.\n",
      "-There are different ways to determine the value of c. One way is to measure the actual speed at which light waves propagate, which can be done in various astronomical and Earth-based setups. However, it is also possible to determine c from other physical laws where it appears, for example, by determining the values of the electromagnetic constants ε0 and μ0 and using their relation to c. Historically, the most accurate results have been obtained by separately determining the frequency and wavelength of a light beam, with their product equalling c. This is described in more detail in the \"Interferometry\" section below.\n",
      "\n",
      "\n",
      "\n",
      "What is the Ozma Problem?\n",
      "-The last several chapters deal with a conundrum called the Ozma Problem, which examines whether there is any fundamental asymmetry to the universe. This discussion concerns various aspects of atomic and subatomic physics and how they relate to mirror asymmetry and the related concepts of chirality, antimatter, magnetic and electrical polarity, parity, charge and spin. Time invariance (and reversal) is discussed. Implications for particle physics, theoretical physics and cosmology are covered and brought up to date (in later editions of the book) with regard to Grand Unified Theories, theories of everything, superstring theory and M-theory.\n",
      "-In physical cosmology, the baryon asymmetry problem, also known as the matter asymmetry problem or the matter–antimatter asymmetry problem, is the observed imbalance in baryonic matter (the type of matter experienced in everyday life) and antibaryonic matter in the observable universe. Neither the standard model of particle physics nor the theory of general relativity provides a known explanation for why this should be so, and it is a natural assumption that the universe is neutral with all conserved charges. The Big Bang should have produced equal amounts of matter and antimatter. Since this does not seem to have been the case, it is likely some physical laws must have acted differently or did not exist for matter and antimatter. Several competing hypotheses exist to explain the imbalance of matter and antimatter that resulted in baryogenesis. However, there is as of yet no consensus theory to explain the phenomenon, which has been described as \"one of the great mysteries in physics\".\n",
      "-The Ozma Problem The 18th chapter, \"The Ozma Problem\", poses a problem that Gardner claims would arise if Earth should ever enter into communication with life on another planet through Project Ozma. This is the problem of how to communicate the meaning of left and right, where the two communicants are conditionally not allowed to view any one object in common.\n",
      "-The problem was first implied in Immanuel Kant's discussion of a hand isolated in space, which would have no meaning as left or right by itself; Gardner posits that Kant would today explain his problem using the reversibility of objects through a higher dimension. A three-dimensional hand can be reversed in a mirror or a hypothetical fourth dimension. In more easily visualizable terms, an outline of a hand in Flatland could be flipped over; the meaning of left or right would not apply until a being missing a corresponding hand came along. Charles Howard Hinton expressed the essential problem in 1888, as did William James in his The Principles of Psychology (1890). Gardner follows the thread of several false leads on the road to the solution of the problem, such as the magnetic poles of astronomical bodies and the chirality of life molecules, which could be arbitrary based on how life locally originated.The solution to the Ozma Problem was finally realized in the famous Wu experiment, conducted in 1956 by Chinese-American physicist Chien-Shiung Wu (1912–1997), involving the beta decay of cobalt-60. At a conference earlier that year, Richard Feynman had asked (on behalf of Martin M. Block) whether parity was sometimes violated, leading Tsung-Dao Lee and Chen-Ning Yang to propose Wu's experiment, for which Lee and Yang were awarded the 1957 Nobel Prize in Physics. It was the first experiment to disprove the conservation of parity, and according to Gardner, one could use it to convey the meaning of left and right to remote extraterrestrials. An earlier example of asymmetry had actually been detected as early as 1928 in the decay of a radionuclide of radium, but its significance was not then realized.\n",
      "-The Ozsváth–Schücking metric, or the Ozsváth–Schücking solution, is a vacuum solution of the Einstein field equations. The metric was published by István Ozsváth and Engelbert Schücking in 1962. It is noteworthy among vacuum solutions for being the first known solution that is stationary, globally defined, and singularity-free but nevertheless not isometric to the Minkowski metric. This stands in contradiction to a claimed strong Mach principle, which would forbid a vacuum solution from being anything but Minkowski without singularities, where the singularities are to be construed as mass as in the Schwarzschild metric.With coordinates  {x0,x1,x2,x3} , define the following tetrad: e(0)=12+(x3)2(x3∂0−∂1+∂2) e(1)=14+2(x3)2[(x3−2+(x3)2)∂0+(1+(x3)2−x32+(x3)2)∂1+∂2] e(2)=14+2(x3)2[(x3+2+(x3)2)∂0+(1+(x3)2+x32+(x3)2)∂1+∂2] e(3)=∂3 It is straightforward to verify that e(0) is timelike, e(1), e(2), e(3) are spacelike, that they are all orthogonal, and that there are no singularities. The corresponding proper time is dτ2=−(dx0)2+4(x3)(dx0)(dx2)−2(dx1)(dx2)−2(x3)2(dx2)2−(dx3)2.\n",
      "\n",
      "\n",
      "\n",
      "What is a Hilbert space in quantum mechanics?\n",
      "-In the mathematically rigorous formulation of quantum mechanics, the state of a quantum mechanical system is a vector  ψ belonging to a (separable) complex Hilbert space  H . This vector is postulated to be normalized under the Hilbert space inner product, that is, it obeys  ⟨ψ,ψ⟩=1 , and it is well-defined up to a complex number of modulus 1 (the global phase), that is,  ψ and  eiαψ represent the same physical system. In other words, the possible states are points in the projective space of a Hilbert space, usually called the complex projective space. The exact nature of this Hilbert space is dependent on the system – for example, for describing position and momentum the Hilbert space is the space of complex square-integrable functions  L2(C) , while the Hilbert space for the spin of a single proton is simply the space of two-dimensional complex vectors  C2 with the usual inner product.\n",
      "-Mathematical In a formal setup, any system in quantum mechanics is described by a state, which is a vector |Ψ⟩, residing in an abstract complex vector space, called a Hilbert space. It may be either infinite- or finite-dimensional. A usual presentation of that Hilbert space is a special function space, called L2(X), on certain set X, that is either some configuration space or a discrete set.\n",
      "-Preliminaries Introductory courses on physics or chemistry typically introduce the Schrödinger equation in a way that can be appreciated knowing only the concepts and notations of basic calculus, particularly derivatives with respect to space and time. A special case of the Schrödinger equation that admits a statement in those terms is the position-space Schrödinger equation for a single nonrelativistic particle in one dimension: Here,  Ψ(x,t) is a wave function, a function that assigns a complex number to each point  x at each time  t . The parameter  m is the mass of the particle, and  V(x,t) is the potential that represents the environment in which the particle exists.: 74  The constant  i is the imaginary unit, and  ℏ is the reduced Planck constant, which has units of action (energy multiplied by time).: 10 Broadening beyond this simple case, the mathematical formulation of quantum mechanics developed by Paul Dirac, David Hilbert, John von Neumann, and Hermann Weyl defines the state of a quantum mechanical system to be a vector  |ψ⟩ belonging to a (separable) Hilbert space  H . This vector is postulated to be normalized under the Hilbert space's inner product, that is, in Dirac notation it obeys  ⟨ψ|ψ⟩=1 . The exact nature of this Hilbert space is dependent on the system – for example, for describing position and momentum the Hilbert space is the space of complex square-integrable functions  L2(C) , while the Hilbert space for the spin of a single proton is simply the space of two-dimensional complex vectors  C2 with the usual inner product.: 322 Physical quantities of interest – position, momentum, energy, spin – are represented by \"observables\", which are Hermitian (more precisely, self-adjoint) linear operators acting on the Hilbert space. A wave function can be an eigenvector of an observable, in which case it is called an eigenstate, and the associated eigenvalue corresponds to the value of the observable in that eigenstate. More generally, a quantum state will be a linear combination of the eigenstates, known as a quantum superposition. When an observable is measured, the result will be one of its eigenvalues with probability given by the Born rule: in the simplest case the eigenvalue  λ is non-degenerate and the probability is given by  |⟨λ|ψ⟩|2 , where  |λ⟩ is its associated eigenvector. More generally, the eigenvalue is degenerate and the probability is given by  ⟨ψ|Pλ|ψ⟩ , where  Pλ is the projector onto its associated eigenspace.A momentum eigenstate would be a perfectly monochromatic wave of infinite extent, which is not square-integrable. Likewise a position eigenstate would be a Dirac delta distribution, not square-integrable and technically not a function at all. Consequently, neither can belong to the particle's Hilbert space. Physicists sometimes introduce fictitious \"bases\" for a Hilbert space comprising elements outside that space. These are invented for calculational convenience and do not represent physical states.: 100–105  Thus, a position-space wave function  Ψ(x,t) as used above can be written as the inner product of a time-dependent state vector  |Ψ(t)⟩ with unphysical but convenient \"position eigenstates\"  |x⟩ Time-dependent equation The form of the Schrödinger equation depends on the physical situation. The most general form is the time-dependent Schrödinger equation, which gives a description of a system evolving with time:: 143  where  t is time,  |Ψ(t)⟩ is the state vector of the quantum system ( Ψ being the Greek letter psi), and  H^ is an observable, the Hamiltonian operator.\n",
      "-Specifically, in quantum mechanics a state space is a complex Hilbert space in which each unit vector represents a different state that could come out of a measurement. Each unit vector specifies a different dimension, so the numbers of dimensions in this Hilbert space depends on the system we choose to describe. Any state vector in this space can be written as a linear combination of unit vectors. Having an nonzero component along multiple dimensions is called a superposition. These state vectors, using Dirac's bra–ket notation, can often be treated like coordinate vectors and operated on using the rules of linear algebra. This Dirac formalism of quantum mechanics can replace calculation of complicated integrals with simpler vector operations.\n",
      "-Background In elementary quantum mechanics, the state of a quantum-mechanical system is represented by a complex-valued wavefunction ψ(x, t). More abstractly, the state may be represented as a state vector, or ket, |ψ⟩. This ket is an element of a Hilbert space, a vector space containing all possible states of the system. A quantum-mechanical operator is a function which takes a ket |ψ⟩ and returns some other ket |ψ′⟩.\n",
      "\n",
      "\n",
      "\n",
      "What is the significance of the speed of light in vacuum?\n",
      "-The speed at which light waves propagate in vacuum is independent both of the motion of the wave source and of the inertial frame of reference of the observer. This invariance of the speed of light was postulated by Einstein in 1905, after being motivated by Maxwell's theory of electromagnetism and the lack of evidence for motion against the luminiferous aether; it has since been consistently confirmed by many experiments. It is only possible to verify experimentally that the two-way speed of light (for example, from a source to a mirror and back again) is frame-independent, because it is impossible to measure the one-way speed of light (for example, from a source to a distant detector) without some convention as to how clocks at the source and at the detector should be synchronized. However, by adopting Einstein synchronization for the clocks, the one-way speed of light becomes equal to the two-way speed of light by definition. The special theory of relativity explores the consequences of this invariance of c with the assumption that the laws of physics are the same in all inertial frames of reference. One consequence is that c is the speed at which all massless particles and waves, including light, must travel in vacuum.\n",
      "-The speed of light in vacuum is defined to be exactly 299 792 458 m/s (approx. 186,282 miles per second). The fixed value of the speed of light in SI units results from the fact that the metre is now defined in terms of the speed of light. All forms of electromagnetic radiation move at exactly this same speed in vacuum.\n",
      "-Faster-than-light communication is, according to relativity, equivalent to time travel. What we measure as the speed of light in vacuum (or near vacuum) is actually the fundamental physical constant c. This means that all inertial and, for the coordinate speed of light, non-inertial observers, regardless of their relative velocity, will always measure zero-mass particles such as photons traveling at c in vacuum. This result means that measurements of time and velocity in different frames are no longer related simply by constant shifts, but are instead related by Poincaré transformations. These transformations have important implications: The relativistic momentum of a massive particle would increase with speed in such a way that at the speed of light an object would have infinite momentum.\n",
      "-Basics While the speed of light in vacuum is a universal constant (c = 299,792,458 m/s), the speed in a material may be significantly less, as it is perceived to be slowed by the medium. For example, in water it is only 0.75c. Matter can accelerate to a velocity higher than this (although still less than c, the speed of light in vacuum) during nuclear reactions and in particle accelerators. Cherenkov radiation results when a charged particle, most commonly an electron, travels through a dielectric (can be polarized electrically) medium with a speed greater than light's speed in that medium.\n",
      "-The simplest picture of light given by classical physics is of a wave or disturbance in the electromagnetic field. In a vacuum, Maxwell's equations predict that these disturbances will travel at a specific speed, denoted by the symbol c. This well-known physical constant is commonly referred to as the speed of light. The postulate of the constancy of the speed of light in all inertial reference frames lies at the heart of special relativity and has given rise to a popular notion that the \"speed of light is always the same\". However, in many situations light is more than a disturbance in the electromagnetic field.\n",
      "\n",
      "\n",
      "\n",
      "What is the term used to describe the proportionality factor to the Stefan-Boltzmann law that is utilized in subsequent evaluations of the radiative behavior of grey bodies?\n",
      "-Stefan–Boltzmann lawThe Stefan–Boltzmann law describes the power radiated from a black body in terms of its temperature. Specifically, the Stefan–Boltzmann law states that the total energy radiated per unit surface area of a black body across all wavelengths per unit time  j⋆ (also known as the black-body radiant emittance) is directly proportional to the fourth power of the black body's thermodynamic temperature T: j⋆=σT4.\n",
      "-The Stefan–Boltzmann law, also known as Stefan's law, describes the intensity of the thermal radiation emitted by matter in terms of that matter's temperature. It is named for Josef Stefan, who empirically derived the relationship, and Ludwig Boltzmann who derived the law theoretically.\n",
      "For an ideal absorber/emitter or black body, the Stefan–Boltzmann law states that the total energy radiated per unit surface area per unit time (also known as the radiant exitance) is directly proportional to the fourth power of the black body's temperature, T: M∘=σT4.\n",
      "-The Stefan–Boltzmann law, also known as Stefan's law, describes the intensity of the thermal radiation emitted by matter in terms of that matter's temperature. It is named for Josef Stefan, who empirically derived the relationship, and Ludwig Boltzmann who derived the law theoretically.\n",
      "For an ideal absorber/emitter or black body, the Stefan–Boltzmann law states that the total energy radiated per unit surface area per unit time (also known as the radiant exitance) is directly proportional to the fourth power of the black body's temperature, T: M∘=σT4.\n",
      "-The Stefan–Boltzmann law may be expressed as a formula for radiance as a function of temperature. Radiance is measured in watts per square metre per steradian (W m-2 sr-1). The Stefan–Boltzmann law for the radiance of a black body is:: 26  LΩ∘=M∘π=σπT4.\n",
      "The Stefan–Boltzmann law expressed as a formula for radiation energy density is: we∘=4cM∘=4cσT4 where  c is the speed of light.\n",
      "-The Stefan–Boltzmann law may be expressed as a formula for radiance as a function of temperature. Radiance is measured in watts per square metre per steradian (W m-2 sr-1). The Stefan–Boltzmann law for the radiance of a black body is:: 26  LΩ∘=M∘π=σπT4.\n",
      "The Stefan–Boltzmann law expressed as a formula for radiation energy density is: we∘=4cM∘=4cσT4 where  c is the speed of light.\n",
      "\n",
      "\n",
      "\n",
      "What is the reason for the formation of stars exclusively within molecular clouds?\n",
      "-Star formation The formation of stars occurs exclusively within molecular clouds. This is a natural consequence of their low temperatures and high densities, because the gravitational force acting to collapse the cloud must exceed the internal pressures that are acting \"outward\" to prevent a collapse. There is observed evidence that the large, star-forming clouds are confined to a large degree by their own gravity (like stars, planets, and galaxies) rather than by external pressure. The evidence comes from the fact that the \"turbulent\" velocities inferred from CO linewidth scale in the same manner as the orbital velocity (a virial relation).\n",
      "-Star formation An example problem is that of star formation. Stars form out of the interstellar medium, with this formation mostly occurring in giant molecular clouds such as the Rosette Nebula. An interstellar cloud can collapse due to its self-gravity if it is large enough; however, in the ordinary interstellar medium this can only happen if the cloud has a mass of several thousands of solar masses—much larger than that of any star. Stars may still form, however, from processes that occur if the magnetic pressure is much larger than the thermal pressure, which is the case in giant molecular clouds. These processes rely on the interaction of magnetohydrodynamic waves with a thermal instability. A magnetohydrodynamic wave in a medium in which the magnetic pressure is much larger than the thermal pressure can produce dense regions, but they cannot by themselves make the density high enough for self-gravity to act. However, the gas in star forming regions is heated by cosmic rays and is cooled by radiative processes. The net result is that a gas in a thermal equilibrium state in which heating balances cooling can exist in three different phases at the same pressure: a warm phase with a low density, an unstable phase with intermediate density and a cold phase at low temperature. An increase in pressure due to a supernova or a spiral density wave can shift the gas from the warm phase to the unstable phase, with a magnetohydrodynamic wave then being able to produce dense fragments in the cold phase whose self-gravity is strong enough for them to collapse into stars.\n",
      "-Star formation begins in relatively small molecular clouds called dense cores. Each dense core is initially in balance between self-gravity, which tends to compress the object, and both gas pressure and magnetic pressure, which tend to inflate it. As the dense core accrues mass from its larger, surrounding cloud, self-gravity begins to overwhelm pressure, and collapse begins. Theoretical modeling of an idealized spherical cloud initially supported only by gas pressure indicates that the collapse process spreads from the inside toward the outside. Spectroscopic observations of dense cores that do not yet contain stars indicate that contraction indeed occurs. So far, however, the predicted outward spread of the collapse region has not been observed.\n",
      "-Star formation The formation of a star begins with gravitational instability within a molecular cloud, caused by regions of higher density—often triggered by compression of clouds by radiation from massive stars, expanding bubbles in the interstellar medium, the collision of different molecular clouds, or the collision of galaxies (as in a starburst galaxy). When a region reaches a sufficient density of matter to satisfy the criteria for Jeans instability, it begins to collapse under its own gravitational force.As the cloud collapses, individual conglomerations of dense dust and gas form \"Bok globules\". As a globule collapses and the density increases, the gravitational energy converts into heat and the temperature rises. When the protostellar cloud has approximately reached the stable condition of hydrostatic equilibrium, a protostar forms at the core. These pre-main-sequence stars are often surrounded by a protoplanetary disk and powered mainly by the conversion of gravitational energy. The period of gravitational contraction lasts about 10 million years for a star like the sun, up to 100 million years for a red dwarf.Early stars of less than 2 M☉ are called T Tauri stars, while those with greater mass are Herbig Ae/Be stars. These newly formed stars emit jets of gas along their axis of rotation, which may reduce the angular momentum of the collapsing star and result in small patches of nebulosity known as Herbig–Haro objects.\n",
      "-The densest molecular clouds have significantly higher pressure than the interstellar average, since they are bound together by their own gravity. When stars form in such clouds, especially OB stars, they convert the surrounding gas into the warm ionized phase, a temperature increase of several hundred. Initially the gas is still at molecular cloud densities, and so at vastly higher pressure than the ISM average: this is a classical H II region. The large overpressure causes the ionized gas to expand away from the remaining molecular gas (a Champagne flow), and the flow will continue until either the molecular cloud is fully evaporated or the OB stars reach the end of their lives, after a few millions years. At this point the OB stars explode as supernovas, creating blast waves in the warm gas that increase temperatures to the coronal phase (supernova remnants, SNR). These too expand and cool over several million years until they return to average ISM pressure.\n",
      "\n",
      "\n",
      "\n",
      "What is the identity operation in symmetry groups?\n",
      "-The symmetry group operations (symmetry operations) are the isometries of three-dimensional space R3 that leave the origin fixed, forming the group O(3). These operations can be categorized as: The direct (orientation-preserving) symmetry operations, which form the group SO(3): The identity operation, denoted by E or the identity matrix I.\n",
      "Rotation about an axis through the origin by an angle θ. Rotation by θ = 360°/n for any positive integer n is denoted Cn (from the Schoenflies notation for the group Cn that it generates). The identity operation, also written C1, is a special case of the rotation operator.\n",
      "The indirect (orientation-reversing) operations: Inversion, denoted i or Ci. The matrix notation is −I.\n",
      "Reflection in a plane through the origin, denoted σ.\n",
      "-Basic point group symmetry operations The five basic symmetry operations mentioned above are: Identity Operation E (from the German 'Einheit' meaning unity): The identity operation leaves the molecule unchanged. It forms the identity element in the symmetry group. Though its inclusion seems to be trivial, it is important also because even for the most asymmetric molecule, this symmetry is present. The corresponding symmetry element is the entire molecule itself.\n",
      "-In chemistry, there are five important symmetry operations. They are identity operation (E), rotation operation or proper rotation (Cn), reflection operation (σ), inversion (i) and rotation reflection operation or improper rotation (Sn). The identity operation (E) consists of leaving the molecule as it is. This is equivalent to any number of full rotations around any axis. This is a symmetry of all molecules, whereas the symmetry group of a chiral molecule consists of only the identity operation. An identity operation is a characteristic of every molecule even if it has no symmetry. Rotation around an axis (Cn) consists of rotating the molecule around a specific axis by a specific angle. It is rotation through the angle 360°/n, where n is an integer, about a rotation axis. For example, if a water molecule rotates 180° around the axis that passes through the oxygen atom and between the hydrogen atoms, it is in the same configuration as it started. In this case, n = 2, since applying it twice produces the identity operation. In molecules with more than one rotation axis, the Cn axis having the largest value of n is the highest order rotation axis or principal axis. For example in boron trifluoride (BF3), the highest order of rotation axis is C3, so the principal axis of rotation is C3.\n",
      "-Molecular symmetry is responsible for many physical and spectroscopic properties of compounds and provides relevant information about how chemical reactions occur. In order to assign a point group for any given molecule, it is necessary to find the set of symmetry operations present on it. The symmetry operation is an action, such as a rotation around an axis or a reflection through a mirror plane. In other words, it is an operation that moves the molecule such that it is indistinguishable from the original configuration. In group theory, the rotation axes and mirror planes are called \"symmetry elements\". These elements can be a point, line or plane with respect to which the symmetry operation is carried out. The symmetry operations of a molecule determine the specific point group for this molecule.\n",
      "-In group theory, geometry, representation theory and molecular geometry, a symmetry operation is a geometric transformation of an object that leaves the object looking the same after it has been carried out. For example, as transformations of an object in space, rotations, reflections and inversions are all symmetry operations. Such symmetry operations are performed with respect to symmetry elements (for example, a point, line or plane). In the context of molecular symmetry, a symmetry operation is a permutation of atoms such that the molecule or crystal is transformed into a state indistinguishable from the starting state.\n",
      "\n",
      "\n",
      "\n",
      "What is a regular polytope?\n",
      "-In mathematics, a regular polytope is a polytope whose symmetry group acts transitively on its flags, thus giving it the highest degree of symmetry. All its elements or j-faces (for all 0 ≤ j ≤ n, where n is the dimension of the polytope) — cells, faces and so on — are also transitive on the symmetries of the polytope, and are regular polytopes of dimension ≤ n.\n",
      "-A regular polyhedron is a polyhedron whose symmetry group acts transitively on its flags. A regular polyhedron is highly symmetrical, being all of edge-transitive, vertex-transitive and face-transitive. In classical contexts, many different equivalent definitions are used; a common one is that the faces are congruent regular polygons which are assembled in the same way around each vertex.\n",
      "-For example, a flag of a polyhedron comprises one vertex, one edge incident to that vertex, and one polygonal face incident to both, plus the two improper faces.\n",
      "A polytope may be regarded as regular if, and only if, its symmetry group is transitive on its flags. This definition excludes chiral polytopes.\n",
      "-Regular polytopes Regular polytopes have the highest degree of symmetry of all polytopes. The symmetry group of a regular polytope acts transitively on its flags; hence, the dual polytope of a regular polytope is also regular.\n",
      "There are three main classes of regular polytope which occur in any number of dimensions: Simplices, including the equilateral triangle and the regular tetrahedron.\n",
      "Hypercubes or measure polytopes, including the square and the cube.\n",
      "-And so on, a regular n-polytope is an n-dimensional polytope whose (n − 1)-dimensional faces are all regular and congruent, and whose vertex figures are all regular and congruent.This is a \"recursive\" definition. It defines regularity of higher dimensional figures in terms of regular figures of a lower dimension. There is an equivalent (non-recursive) definition, which states that a polytope is regular if it has a sufficient degree of symmetry.\n",
      "\n",
      "\n",
      "\n",
      "What is the reason behind the largest externally observed electrical effects when two conductors are separated by the smallest distance without touching?\n",
      "-Contact electrification If two conducting surfaces are moved relative to each other, and there is potential difference in the space between them, then an electric current will be driven. This is because the surface charge on a conductor depends on the magnitude of the electric field, which in turn depends on the distance between the surfaces. The externally observed electrical effects are largest when the conductors are separated by the smallest distance without touching (once brought into contact, the charge will instead flow internally through the junction between the conductors). Since two conductors in equilibrium can have a built-in potential difference due to work function differences, this means that bringing dissimilar conductors into contact, or pulling them apart, will drive electric currents. These contact currents can damage sensitive microelectronic circuitry and occur even when the conductors would be grounded in the absence of motion.\n",
      "-Conductors, typically in the form of wires, may be used to transmit electrical energy or signals using an alternating current flowing through that conductor. The charge carriers constituting that current, usually electrons, are driven by an electric field due to the source of electrical energy. A current in a conductor produces a magnetic field in and around the conductor. When the intensity of current in a conductor changes, the magnetic field also changes. The change in the magnetic field, in turn, creates an electric field which opposes the change in current intensity. This opposing electric field is called “counter-electromotive force” (back EMF). The back EMF is strongest at the center of the conductor, and forces the conducting electrons to the outside of the conductor, as shown in the diagram on the right.Regardless of the driving force, the current density is found to be greatest at the conductor's surface, with a reduced magnitude deeper in the conductor. That decline in current density is known as the skin effect and the skin depth is a measure of the depth at which the current density falls to 1/e of its value near the surface.\n",
      "-Electrostatic pressure On a conductor, a surface charge will experience a force in the presence of an electric field. This force is the average of the discontinuous electric field at the surface charge. This average in terms of the field just outside the surface amounts to: P=ε02E2, This pressure tends to draw the conductor into the field, regardless of the sign of the surface charge.\n",
      "-A surface charge is an electric charge present on a two-dimensional surface. These electric charges are constrained on this 2-D surface, and surface charge density, measured in coulombs per square meter (C•m−2), is used to describe the charge distribution on the surface. The electric potential is continuous across a surface charge and the electric field is discontinuous, but not infinite; this is unless the surface charge consists of a dipole layer. In comparison, the potential and electric field both diverge at any point charge or linear charge.\n",
      "-The electric potential is the same everywhere inside the conductor and is constant across the surface of the conductor. This follows from the first statement because the field is zero everywhere inside the conductor and therefore the potential is constant within the conductor too.\n",
      "The electric field is perpendicular to the surface of a conductor. If this were not the case, the field would have a nonzero component on the surface of the conductor, which would cause the charges in the conductor to move around until that component of the field is zero.\n",
      "\n",
      "\n",
      "\n",
      "What is the formalism that angular momentum is associated with in rotational invariance?\n",
      "-In modern (20th century) theoretical physics, angular momentum (not including any intrinsic angular momentum – see below) is described using a different formalism, instead of a classical pseudovector. In this formalism, angular momentum is the 2-form Noether charge associated with rotational invariance. As a result, angular momentum is not conserved for general curved spacetimes, unless it happens to be asymptotically rotationally invariant.In classical mechanics, the angular momentum of a particle can be reinterpreted as a plane element: in which the exterior product (∧) replaces the cross product (×) (these products have similar characteristics but are nonequivalent). This has the advantage of a clearer geometric interpretation as a plane element, defined using the vectors x and p, and the expression is true in any number of dimensions. In Cartesian coordinates: or more compactly in index notation: The angular velocity can also be defined as an anti-symmetric second order tensor, with components ωij. The relation between the two anti-symmetric tensors is given by the moment of inertia which must now be a fourth order tensor: Again, this equation in L and ω as tensors is true in any number of dimensions. This equation also appears in the geometric algebra formalism, in which L and ω are bivectors, and the moment of inertia is a mapping between them.\n",
      "-Noether's theorem states that every conservation law is associated with a symmetry (invariant) of the underlying physics. The symmetry associated with conservation of angular momentum is rotational invariance. The fact that the physics of a system is unchanged if it is rotated by any angle about an axis implies that angular momentum is conserved.\n",
      "-Angular momentum is an important dynamical quantity derived from position and momentum. It is a measure of an object's rotational motion and resistance to changes in its rotation. Also, in the same way momentum conservation corresponds to translational symmetry, angular momentum conservation corresponds to rotational symmetry – the connection between symmetries and conservation laws is made by Noether's theorem. While these concepts were originally discovered in classical mechanics, they are also true and significant in special and general relativity. In terms of abstract algebra, the invariance of angular momentum, four-momentum, and other symmetries in spacetime, are described by the Lorentz group, or more generally the Poincaré group.\n",
      "-In physics, if a system behaves the same regardless of how it is oriented in space, then its Lagrangian is rotationally invariant. According to Noether's theorem, if the action (the integral over time of its Lagrangian) of a physical system is invariant under rotation, then angular momentum is conserved.\n",
      "Application to quantum mechanics In quantum mechanics, rotational invariance is the property that after a rotation the new system still obeys Schrödinger's equation. That is [R,E−H]=0 for any rotation R. Since the rotation does not depend explicitly on time, it commutes with the energy operator. Thus for rotational invariance we must have [R, H] = 0.\n",
      "For infinitesimal rotations (in the xy-plane for this example; it may be done likewise for any plane) by an angle dθ the (infinitesimal) rotation operator is R=1+Jzdθ, then [1+Jzdθ,ddt]=0, thus ddtJz=0, in other words angular momentum is conserved.\n",
      "-Like linear momentum, angular momentum is vector quantity, and its conservation implies that the direction of the spin axis tends to remain unchanged. For this reason, the spinning top remains upright whereas a stationary one falls over immediately.\n",
      "The angular momentum equation can be used to relate the moment of the resultant force on a body about an axis (sometimes called torque), and the rate of rotation about that axis.\n",
      "\n",
      "\n",
      "\n",
      "Which hand should be used to apply the right-hand rule when tightening or loosening nuts, screws, bolts, bottle caps, and jar lids?\n",
      "-To apply the right-hand rule, place one's loosely clenched right hand above the object with the thumb pointing in the direction one wants the screw, nut, bolt, or cap ultimately to move, and the curl of the fingers, from the palm to the tips, will indicate in which way one needs to turn the screw, nut, bolt or cap to achieve the desired result. Almost all threaded objects obey this rule except for a few left-handed exceptions described below.\n",
      "-Shop-work Typical nuts, screws, bolts, bottle caps, and jar lids are tightened (moved away from the observer) clockwise and loosened (moved towards the observer) counterclockwise in accordance with the right-hand rule.\n",
      "-By common convention, right-handedness is the default handedness for screw threads. Therefore, most threaded parts and fasteners have right-handed threads. Left-handed thread applications include: Where the rotation of a shaft would cause a conventional right-handed nut to loosen rather than to tighten due to applied torque or to fretting induced precession. Examples include: The left hand pedal on a bicycle.\n",
      "-The reason for the clockwise standard for most screws and bolts is that supination of the arm, which is used by a right-handed person to tighten a screw clockwise, is generally stronger than pronation used to loosen.  Sometimes the opposite (left-handed, counterclockwise, reverse) sense of threading is used for a special reason. A thread might need to be left-handed to prevent operational stresses from loosening it. For example, some older cars and trucks had right-handed lug nuts on the right wheels and left-handed lug nuts on the left wheels, so that, as the vehicle moved forward, the lug nuts tended to tighten rather than loosen. For bicycle pedals, the one on the left must be reverse-threaded to prevent it unscrewing during use. Similarly, the flyer whorl of a spinning wheel uses a left-hand thread to keep it from loosening. A turnbuckle has right-handed threads on one end and left-handed threads on the other. Some gas fittings are left-handed to prevent disastrous misconnections: oxygen fittings are right-handed, but acetylene, propane, and other flammable gases are unmistakably distinguished by left-handed fittings.\n",
      "-By common convention, right-handedness is the default handedness for screw threads. Therefore, most threaded parts and fasteners have right-handed threads. One explanation for why right-handed threads became standard is that for a right-handed person, tightening a right-handed screw with a screwdriver is easier than tightening a left-handed screw, because it uses the stronger supinator muscle of the arm rather than the weaker pronator muscle. Since most people are right-handed, right-handed threads became standard on threaded fasteners.\n",
      "\n",
      "\n",
      "\n",
      "What is the Minkowski diagram used for?\n",
      "-Overview The term Minkowski diagram refers to a specific form of spacetime diagram frequently used in special relativity. A Minkowski diagram is a two-dimensional graphical depiction of a portion of Minkowski space, usually where space has been curtailed to a single dimension. The units of measurement in these diagrams are taken such that the light cone at an event consists of the lines of slope plus or minus one through that event. The horizontal lines correspond to the usual notion of simultaneous events for a stationary observer at the origin.\n",
      "-Graphical representation of the Lorentz transformation Spacetime diagrams (Minkowski diagrams) are an extremely useful aid to visualizing how coordinates transform between different reference frames. Although it is not as easy to perform exact computations using them as directly invoking the Lorentz transformations, their main power is their ability to provide an intuitive grasp of the results of a relativistic scenario.To draw a spacetime diagram, begin by considering two Galilean reference frames, S and S', in standard configuration, as shown in Fig. 2-1.: 155–199 Fig. 3-1a. Draw the  x and  t axes of frame S. The  x axis is horizontal and the  t (actually  ct ) axis is vertical, which is the opposite of the usual convention in kinematics. The  ct axis is scaled by a factor of  c so that both axes have common units of length. In the diagram shown, the gridlines are spaced one unit distance apart. The 45° diagonal lines represent the worldlines of two photons passing through the origin at time  0.\n",
      "-Minkowski's principal tool is the Minkowski diagram, and he uses it to define concepts and demonstrate properties of Lorentz transformations (e.g. proper time and length contraction) and to provide geometrical interpretation to the generalization of Newtonian mechanics to relativistic mechanics. For these special topics, see the referenced articles, as the presentation below will be principally confined to the mathematical structure (Minkowski metric and from it derived quantities and the Poincaré group as symmetry group of spacetime) following from the invariance of the spacetime interval on the spacetime manifold as consequences of the postulates of special relativity, not to specific application or derivation of the invariance of the spacetime interval. This structure provides the background setting of all present relativistic theories, barring general relativity for which flat Minkowski spacetime still provides a springboard as curved spacetime is locally Lorentzian.\n",
      "-The most well-known class of spacetime diagrams are known as Minkowski diagrams, developed by Hermann Minkowski in 1908. Minkowski diagrams are two-dimensional graphs that depict events as happening in a universe consisting of one space dimension and one time dimension. Unlike a regular distance-time graph, the distance is displayed on the horizontal axis and time on the vertical axis. Additionally, the time and space units of measurement are chosen in such a way that an object moving at the speed of light is depicted as following a 45° angle to the diagram's axes.\n",
      "-A particular Minkowski diagram illustrates the result of a Lorentz transformation. The Lorentz transformation relates two inertial frames of reference, where an observer stationary at the event (0, 0) makes a change of velocity along the x-axis. As shown in Fig 2-1, the new time axis of the observer forms an angle α with the previous time axis, with α < π/4. In the new frame of reference the simultaneous events lie parallel to a line inclined by α to the previous lines of simultaneity. This is the new x-axis. Both the original set of axes and the primed set of axes have the property that they are orthogonal with respect to the Minkowski inner product or relativistic dot product.\n",
      "\n",
      "\n",
      "\n",
      "What are the two main interpretations for the disparity between the presence of matter and antimatter in the observable universe?\n",
      "-There are two main interpretations for this disparity: either the universe began with a small preference for matter (total baryonic number of the universe different from zero), or the universe was originally perfectly symmetric, but somehow a set of phenomena contributed to a small imbalance in favour of matter over time. The second point of view is preferred, although there is no clear experimental evidence indicating either of them to be the correct one.\n",
      "-Matter–antimatter asymmetry. The universe is made out of mostly matter. However, the standard model predicts that matter and antimatter should have been created in (almost) equal amounts if the initial conditions of the universe did not involve disproportionate matter relative to antimatter. Yet, there is no mechanism in the Standard Model to sufficiently explain this asymmetry.\n",
      "-There is strong evidence that the observable universe is composed almost entirely of ordinary matter, as opposed to an equal mixture of matter and antimatter. This asymmetry of matter and antimatter in the visible universe is one of the great unsolved problems in physics. The process by which this inequality between matter and antimatter particles developed is called baryogenesis.\n",
      "-There is considerable speculation both in science and science fiction as to why the observable universe is apparently almost entirely matter (in the sense of quarks and leptons but not antiquarks or antileptons), and whether other places are almost entirely antimatter (antiquarks and antileptons) instead. In the early universe, it is thought that matter and antimatter were equally represented, and the disappearance of antimatter requires an asymmetry in physical laws called CP (charge-parity) symmetry violation, which can be obtained from the Standard Model, but at this time the apparent asymmetry of matter and antimatter in the visible universe is one of the great unsolved problems in physics. Possible processes by which it came about are explored in more detail under baryogenesis.\n",
      "-According to the theory of the Big Bang, matter and antimatter would have existed in the same amount at the beginning of the Universe. If this was true, particles and antiparticles would have annihilated each other, creating photons, and thus the Universe would have been only compounded by light (one particle of matter for 1018 photons). However, only matter has remained and at a rate of one billion times more particles than expected. What happened then, for the antimatter to disappear in favor of matter? A possible answer to this question is baryogenesis, the hypothetical physical process that took place during the early universe that produced baryonic asymmetry, i.e. the imbalance of matter (baryons) and antimatter (antibaryons) in the observed universe. However, baryogenesis is only possible under the following conditions proposed by Andrei Sakharov in 1967: Baryon number  B violation.\n",
      "\n",
      "\n",
      "\n",
      "What is the Ramsauer-Townsend effect?\n",
      "-The Ramsauer–Townsend effect, also sometimes called the Ramsauer effect or the Townsend effect, is a physical phenomenon involving the scattering of low-energy electrons by atoms of a noble gas. The effect can not be explained by classical mechanics, but requires the wave theory of quantum mechanics.\n",
      "-If one tries to predict the probability of collision with a classical model that treats the electron and atom as hard spheres, one finds that the probability of collision should be independent of the incident electron energy (see Kukolich ). However, Ramsauer and Townsend observed that for slow-moving electrons in argon, krypton, or xenon, the probability of collision between the electrons and gas atoms obtains a minimum value for electrons with a certain amount of kinetic energy (about 1 electron volts for xenon gas). This is the Ramsauer–Townsend effect.\n",
      "-Predicting from theory the kinetic energy that will produce a Ramsauer–Townsend minimum is quite complicated since the problem involves understanding the wave nature of particles. However, the problem has been extensively investigated both experimentally and theoretically and is well understood (see Johnson and Guet).\n",
      "In 1970 Gryzinski has proposed classical explanation of Ramsauer effect using effective picture of atom as oscillating multipole of electric field (dipole, quadrupole, octupole), which was a consequence of his free-fall atomic model.\n",
      "-No good explanation for the phenomenon existed until the introduction of quantum mechanics, which explains that the effect results from the wave-like properties of the electron. A simple model of the collision that makes use of wave theory can predict the existence of the Ramsauer–Townsend minimum. Bohr presents one such model that considers the atom as a finite square potential well.\n",
      "-The effect is named for Carl Ramsauer (1879-1955) and John Sealy Townsend (1868-1957), who each independently studied the collisions between atoms and low-energy electrons in the early 1920s.\n",
      "\n",
      "\n",
      "\n",
      "What is Minkowski space?\n",
      "-For an overview, Minkowski space is a 4-dimensional real vector space equipped with a non-degenerate, symmetric bilinear form on the tangent space at each point in spacetime, here simply called the Minkowski inner product, with metric signature either (+ − − −) or (− + + +). The tangent space at each event is a vector space of the same dimension as spacetime, 4.\n",
      "-In mathematical physics, Minkowski space (or Minkowski spacetime) () combines inertial space and time manifolds (x,y) with a non-inertial reference frame of space and time (x',t') into a four-dimensional model relating a position (inertial frame of reference) to the field (physics). A four-vector (x,y,z,t) consists of a coordinate axes such as a Euclidean space plus time. This may be used with the non-inertial frame to illustrate specifics of motion, but should not be confused with the spacetime model generally.  The model helps show how a spacetime interval between any two events is independent of the inertial frame of reference in which they are recorded. Mathematician Hermann Minkowski developed it from the work of Hendrik Lorentz, Henri Poincaré, and others, and said it \"was grown on experimental physical grounds.\"  Minkowski space is closely associated with Einstein's theories of special relativity and general relativity and is the most common mathematical structure by which special relativity is formalized. While the individual components in Euclidean space and time might differ due to length contraction and time dilation, in Minkowski spacetime, all frames of reference will agree on the total interval in spacetime between events. Minkowski space differs from four-dimensional Euclidean space insofar as it treats time differently than the three spatial dimensions.\n",
      "-Minkowski space (or Minkowski spacetime) is a mathematical setting in which special relativity is conveniently formulated. Minkowski space is named for the German mathematician Hermann Minkowski, who around 1907 realized that the theory of special relativity (previously developed by Poincaré and Einstein) could be elegantly described using a four-dimensional spacetime, which combines the dimension of time with the three dimensions of space.\n",
      "-Introducing more terminology (but not more structure), Minkowski space is thus a pseudo-Euclidean space with total dimension n = 4 and signature (3, 1) or (1, 3). Elements of Minkowski space are called events. Minkowski space is often denoted R3,1 or R1,3 to emphasize the chosen signature, or just M. It is perhaps the simplest example of a pseudo-Riemannian manifold.\n",
      "-In mathematics and physics, super Minkowski space or Minkowski superspace is a supersymmetric extension of Minkowski space, sometimes used as the base manifold (or rather, supermanifold) for superfields. It is acted on by the super Poincaré algebra.\n",
      "\n",
      "\n",
      "\n",
      "What is the Optical Signal-to-Noise Ratio (OSNR)?\n",
      "-Optical signals have a carrier frequency (about 200 THz and more) that is much higher than the modulation frequency. This way the noise covers a bandwidth that is much wider than the signal itself. The resulting signal influence relies mainly on the filtering of the noise. To describe the signal quality without taking the receiver into account, the optical SNR (OSNR) is used. The OSNR is the ratio between the signal power and the noise power in a given bandwidth. Most commonly a reference bandwidth of 0.1 nm is used. This bandwidth is independent of the modulation format, the frequency and the receiver. For instance an OSNR of 20 dB/0.1 nm could be given, even the signal of 40 GBit DPSK would not fit in this bandwidth. OSNR is measured with an optical spectrum analyzer.\n",
      "-Signal-to-noise ratio is defined as the ratio of the power of a signal (meaningful input) to the power of background noise (meaningless or unwanted input): SNR=PsignalPnoise, where P is average power. Both signal and noise power must be measured at the same or equivalent points in a system, and within the same system bandwidth.\n",
      "-In scientific imaging, the two-dimensional spectral signal-to-noise ratio (SSNR) is a signal-to-noise ratio measure which measures the normalised cross-correlation coefficient between several two-dimensional images over corresponding rings in Fourier space as a function of spatial frequency (Unser, Trus & Steven 1987). It is a multi-particle extension of the Fourier ring correlation (FRC), which is related to the Fourier shell correlation. The SSNR is a popular method for finding the resolution of a class average in cryo-electron microscopy.\n",
      "-Peak signal-to-noise ratio (PSNR) is an engineering term for the ratio between the maximum possible power of a signal and the power of corrupting noise that affects the fidelity of its representation. Because many signals have a very wide dynamic range, PSNR is usually expressed as a logarithmic quantity using the decibel scale.\n",
      "PSNR is commonly used to quantify reconstruction quality for images and video subject to lossy compression.\n",
      "-Signal-to-noise ratio (SNR or S/N) is a measure used in science and engineering that compares the level of a desired signal to the level of background noise. SNR is defined as the ratio of signal power to noise power, often expressed in decibels. A ratio higher than 1:1 (greater than 0 dB) indicates more signal than noise.\n",
      "\n",
      "\n",
      "\n",
      "What is the interpretation of supersymmetry in stochastic supersymmetric theory?\n",
      "-Spontaneous breakdown of a topological supersymmetry Kinematic dynamo can be also viewed as the phenomenon of the spontaneous breakdown of the topological supersymmetry of the associated stochastic differential equation related to the flow of the background matter. Within stochastic supersymmetric theory, this supersymmetry is an intrinsic property of all stochastic differential equations, its interpretation is that the model’s phase space preserves continuity via continuous time flows. When the continuity of that flow spontaneously breaks down, the system is in the stochastic state of deterministic chaos. In other words, kinematic dynamo arises because of chaotic flow in the underlying background matter.\n",
      "-Supersymmetric theory of stochastic dynamics or stochastics (STS) is an exact theory of stochastic (partial) differential equations (SDEs), the class of mathematical models with the widest applicability covering, in particular, all continuous time dynamical systems, with and without noise. The main utility of the theory from the physical point of view is a rigorous theoretical explanation of the ubiquitous spontaneous long-range dynamical behavior that manifests itself across disciplines via such phenomena as 1/f, flicker, and crackling noises and the power-law statistics, or Zipf's law, of instantonic processes like earthquakes and neuroavalanches. From the mathematical point of view, STS is interesting because it bridges the two major parts of mathematical physics – the dynamical systems theory and topological field theories. Besides these and related disciplines such as algebraic topology and supersymmetric field theories, STS is also connected with the traditional theory of stochastic differential equations and the theory of pseudo-Hermitian operators.\n",
      "-In a supersymmetric theory the equations for force and the equations for matter are identical. In theoretical and mathematical physics, any theory with this property has the principle of supersymmetry (SUSY). Dozens of supersymmetric theories exist. Supersymmetry is a spacetime symmetry between two basic classes of particles: bosons, which have an integer-valued spin and follow Bose–Einstein statistics, and fermions, which have a half-integer-valued spin and follow Fermi–Dirac statistics.In supersymmetry, each particle from one class would have an associated particle in the other, known as its superpartner, the spin of which differs by a half-integer. For example, if the electron exists in a supersymmetric theory, then there would be a particle called a selectron (superpartner electron), a bosonic partner of the electron. In the simplest supersymmetry theories, with perfectly \"unbroken\" supersymmetry, each pair of superpartners would share the same mass and internal quantum numbers besides spin. More complex supersymmetry theories have a spontaneously broken symmetry, allowing superpartners to differ in mass.Supersymmetry has various applications to different areas of physics, such as quantum mechanics, statistical mechanics, quantum field theory, condensed matter physics, nuclear physics, optics, stochastic dynamics, astrophysics, quantum gravity, and cosmology. Supersymmetry has also been applied to high energy physics, where a supersymmetric extension of the Standard Model is a possible candidate for physics beyond the Standard Model. However, no supersymmetric extensions of the Standard Model have been experimentally verified.\n",
      "-Supersymmetric theory of stochastic dynamics can be interesting in different ways. For example, STS offers a promising realization of the concept of supersymmetry. In general, there are two major problems in the context of supersymmetry. The first is establishing connections between this mathematical entity and the real world. Within STS, supersymmetry is the most common symmetry in nature because it is pertinent to all continuous time dynamical systems. The second is the spontaneous breakdown of supersymmetry. This problem is particularly important for particle physics because supersymmetry of elementary particles, if exists at extremely short scale, must be broken spontaneously at large scale. This problem is nontrivial because supersymmetries are hard to break spontaneously, the very reason behind the introduction of soft or explicit supersymmetry breaking. Within STS, spontaneous breakdown of supersymmetry is indeed a nontrivial dynamical phenomenon that has been variously known across disciplines as chaos, turbulence, self-organized criticality etc.\n",
      "-The main idea of the theory is to study, instead of trajectories, the SDE-defined temporal evolution of differential forms. This evolution has an intrinsic BRST or topological supersymmetry representing the preservation of topology and/or the concept of proximity in the phase space by continuous time dynamics. The theory identifies a model as chaotic, in the generalized, stochastic sense, if its ground state is not supersymmetric, i.e., if the supersymmetry is broken spontaneously. Accordingly, the emergent long-range behavior that always accompanies dynamical chaos and its derivatives such as turbulence and self-organized criticality can be understood as a consequence of the Goldstone theorem.\n",
      "\n",
      "\n",
      "\n",
      "What is the purpose of expressing a map's scale as a ratio, such as 1:10,000?\n",
      "-South-up map orientation is the orientation of a map with south up, at the top of the map, amounting to a 180-degree rotation of the map from the standard convention of north-up. Maps in this orientation are sometimes called upside down maps or reversed maps.Other maps with non-standard orientation include T and O maps, polar maps, and Dymaxion maps.\n",
      "-Many maps are drawn to a scale expressed as a ratio, such as 1:10,000, which means that 1 unit of measurement on the map corresponds to 10,000 of that same unit on the ground. The scale statement can be accurate when the region mapped is small enough for the curvature of the Earth to be neglected, such as a city map. Mapping larger regions, where the curvature cannot be ignored, requires projections to map from the curved surface of the Earth to the plane. The impossibility of flattening the sphere to the plane without distortion means that the map cannot have a constant scale. Rather, on most projections, the best that can be attained is an accurate scale along one or two paths on the projection. Because scale differs everywhere, it can only be measured meaningfully as point scale per location. Most maps strive to keep point scale variation within narrow bounds. Although the scale statement is nominal it is usually accurate enough for most purposes unless the map covers a large fraction of the earth. At the scope of a world map, scale as a single number is practically meaningless throughout most of the map. Instead, it usually refers to the scale along the equator.\n",
      "-An origin must be assigned to a specific spatial location or landmark, and the orientation of the axes must be defined using available directional cues for all but one axis.Consider as an example superimposing 3D Cartesian coordinates over all points on the Earth (that is, geospatial 3D). Kilometers are a good choice of units, since the original definition of the kilometer was geospatial, with 10,000 km equaling the surface distance from the equator to the North Pole. Based on symmetry, the gravitational center of the Earth suggests a natural placement of the origin (which can be sensed via satellite orbits). The axis of Earth's rotation provides a natural orientation for the X, Y, and Z axes, strongly associated with \"up vs. down\", so positive Z can adopt the direction from the geocenter to the North Pole. A location on the equator is needed to define the X-axis, and the prime meridian stands out as a reference orientation, so the X-axis takes the orientation from the geocenter out to 0 degrees longitude, 0 degrees latitude. With three dimensions, and two perpendicular axes orientations pinned down for X and Z, the Y-axis is determined by the first two choices. In order to obey the right-hand rule, the Y-axis must point out from the geocenter to 90 degrees longitude, 0 degrees latitude. From a longitude of −73.985656 degrees, a latitude 40.748433 degrees, and Earth radius of 40,000/2π km, and transforming from spherical to Cartesian coordinates, one can estimate the geocentric coordinates of the Empire State Building, (x, y, z) = (1,330.53 km, 4,635.75 km, 4,155.46 km). GPS navigation relies on such geocentric coordinates.\n",
      "-Map scales require careful discussion. A town plan may be constructed as an exact scale drawing, but for larger areas a map projection is necessary and no projection can represent the Earth's surface at a uniform scale. In general the scale of a projection depends on position and direction. The variation of scale may be considerable in small scale maps which may cover the globe. In large scale maps of small areas the variation of scale may be insignificant for most purposes but it is always present. The scale of a map projection must be interpreted as a nominal scale. (The usage large and small in relation to map scales relates to their expressions as fractions. The fraction 1/10,000 used for a local map is much larger than the1/100,000,000 used for a global map. There is no fixed dividing line between small and large scales.) A scale model is a representation or copy of an object that is larger or smaller than the actual size of the object being represented. Very often the scale model is smaller than the original and used as a guide to making the object in full size.\n",
      "-Throughout history, maps have been made with varied orientations, and reversing the orientation of maps is technically very easy to do. As such, some cartographers maintain that the issue of south-up map orientation is itself trivial. More noteworthy than the technical matter of orientation, per se, is the history of explicitly using south-up map orientation as a political statement, that is, creating south-up oriented maps with the express rationale of reacting to the north-up oriented world maps that have dominated map publication during the modern age.\n",
      "\n",
      "\n",
      "\n",
      "What is the main sequence in astronomy?\n",
      "-In astronomy, the main sequence is a continuous and distinctive band of stars that appears on plots of stellar color versus brightness. These color-magnitude plots are known as Hertzsprung–Russell diagrams after their co-developers, Ejnar Hertzsprung and Henry Norris Russell. Stars on this band are known as main-sequence stars or dwarf stars. These are the most numerous true stars in the universe and include the Sun.\n",
      "-main sequence A category of stars which form a continuous and distinctive band on plots of stellar temperature versus luminosity, in particular the Hertzsprung–Russell diagram. These stars are characterized by being in hydrostatic equilibrium and undergoing nuclear fusion of hydrogen-1 in their core region. The Sun is a main-sequence star.\n",
      "major axis See semi-major axis.\n",
      "March equinox Also the Northward equinox.\n",
      "-The main sequence luminosity function maps the distribution of main sequence stars according to their luminosity. It is used to compare star formation and death rates, and evolutionary models, with observations. Main sequence luminosity functions vary depending on their host galaxy and on selection criteria for the stars, for example in the Solar neighbourhood or the Small Magellanic Cloud.\n",
      "-This is a single-lined spectroscopic binary star system in a circular orbit with an orbital period of 3.7005 days. It is an ellipsoidal variable, which means the orbit is sufficiently close that the shapes of the components are being distorted by their mutual gravitation. This is causing the visual magnitude of the system to vary regularly by 0m.05 over the course of each orbit, as the orientation of the stars change with respect to the Earth. Detailed analysis of the light curve suggests that the primary star is also pulsating and is probably a Slowly pulsating B-type star.The primary component is a B-type giant star with a stellar classification of B2 III. It is only about 16 million years old and spins with a projected rotational velocity of 90 km/s. Despite the spectral class, the primary star is thought to be at or near the end of its main sequence evolution. It has about 12.5 times the mass of the Sun and radiates 11,262 times the solar luminosity from its outer atmosphere at an effective temperature of 14,496 K.The secondary star is not detectable clearly, but modelling of the brightness variations and orbit suggest that it is a main sequence star with a spectral class of about B6. It is smaller, cooler, and much less luminous than the primary, and orbits at about 26 astronomical units.\n",
      "-A new star will sit at a specific point on the main sequence of the Hertzsprung–Russell diagram, with the main-sequence spectral type depending upon the mass of the star. Small, relatively cold, low-mass red dwarfs fuse hydrogen slowly and will remain on the main sequence for hundreds of billions of years or longer, whereas massive, hot O-type stars will leave the main sequence after just a few million years. A mid-sized yellow dwarf star, like the Sun, will remain on the main sequence for about 10 billion years. The Sun is thought to be in the middle of its main sequence lifespan.\n",
      "\n",
      "\n",
      "\n",
      "Who proposed the concept of \"maximal acceleration\"?\n",
      "-The gradual acceptance of Einstein's theories of relativity and the quantized nature of light transmission, and of Niels Bohr's model of the atom created as many problems as they solved, leading to a full-scale effort to reestablish physics on new fundamental principles. Expanding relativity to cases of accelerating reference frames (the \"general theory of relativity\") in the 1910s, Einstein posited an equivalence between the inertial force of acceleration and the force of gravity, leading to the conclusion that space is curved and finite in size, and the prediction of such phenomena as gravitational lensing and the distortion of time in gravitational fields.\n",
      "-Early researchers (before the 1950s): Max Born Albert Einstein Niels Bohr J. S. Bell Hugh Everett III David Bohm1950s–2010s: Roland Omnès W. H. Zurek Erich Joos Max Tegmark Maximilian Schlosshauer H. D. Zeh David Deutsch Robert B. Griffiths Bernard d'Espagnat Carl von Weizsäcker2000s or later: Bob Coecke Robert Spekkens \n",
      "-Albert Einstein presented the theories of special relativity and general relativity in publications that either contained no formal references to previous literature, or referred only to a small number of his predecessors for fundamental results on which he based his theories, most notably to the work of Henri Poincaré and Hendrik Lorentz for special relativity, and to the work of David Hilbert, Carl F. Gauss, Bernhard Riemann, and Ernst Mach for general relativity. Subsequently, claims have been put forward about both theories, asserting that they were formulated, either wholly or in part, by others before Einstein. At issue is the extent to which Einstein and various other individuals should be credited for the formulation of these theories, based on priority considerations.\n",
      "-Albert Einstein presented the theories of special relativity and general relativity in publications that either contained no formal references to previous literature, or referred only to a small number of his predecessors for fundamental results on which he based his theories, most notably to the work of Henri Poincaré and Hendrik Lorentz for special relativity, and to the work of David Hilbert, Carl F. Gauss, Bernhard Riemann, and Ernst Mach for general relativity. Subsequently, claims have been put forward about both theories, asserting that they were formulated, either wholly or in part, by others before Einstein. At issue is the extent to which Einstein and various other individuals should be credited for the formulation of these theories, based on priority considerations.\n",
      "-The idea to use centrifugal acceleration to simulate increased gravitational acceleration was first proposed by Phillips (1869). Pokrovsky and Fedorov (1936) in the Soviet Union and Bucky (1931) in the United States were the first to implement the idea. Andrew N. Schofield (e.g. Schofield 1980) played a key role in modern development of centrifuge modeling.\n",
      "\n",
      "\n",
      "\n",
      "What is indirect photophoresis?\n",
      "-Direct photophoresis is caused by the transfer of photon momentum to a particle by refraction and reflection. Movement of particles in the forward direction occurs when the particle is transparent and has an index of refraction larger compared to its surrounding medium. Indirect photophoresis occurs as a result of an increase in the kinetic energy of molecules when particles absorb incident light only on the irradiated side, thus creating a temperature gradient within the particle. In this situation the surrounding gas layer reaches temperature equilibrium with the surface of the particle. Molecules with higher kinetic energy in the region of higher gas temperature impinge on the particle with greater momenta than molecules in the cold region; this causes a migration of particles in a direction opposite to the surface temperature gradient. The component of the photophoretic force responsible for this phenomenon is called the radiometric force. This comes as a result of uneven distribution of radiant energy (source function within a particle).\n",
      "-Photophoresis denotes the phenomenon that small particles suspended in gas (aerosols) or liquids (hydrocolloids) start to migrate when illuminated by a sufficiently intense beam of light. The existence of this phenomenon is owed to a non-uniform distribution of temperature of an illuminated particle in a fluid medium. Separately from photophoresis, in a fluid mixture of different kinds of particles, the migration of some kinds of particles may be due to differences in their absorptions of thermal radiation and other thermal effects collectively known as thermophoresis. In laser photophoresis, particles migrate once they have a refractive index different from their surrounding medium. The migration of particles is usually possible when the laser is slightly or not focused. A particle with a higher refractive index compared to its surrounding molecule moves away from the light source due to momentum transfer from absorbed and scattered light photons. This is referred to as a radiation pressure force. This force depends on light intensity and particle size but has nothing to do with the surrounding medium. Just like in Crookes radiometer, light can heat up one side and gas molecules bounce from that surface with greater velocity, hence push the particle to the other side. Under certain conditions, with particles of diameter comparable to the wavelength of light, the phenomenon of a negative indirect photophoresis occurs, due to the unequal heat generation on the laser irradiation between the back and front sides of particles, this produces a temperature gradient in the medium around the particle such that molecules at the far side of the particle from the light source may get to heat up more, causing the particle to move towards the light source.If the suspended particle is rotating, it will also experience the Yarkovsky effect.\n",
      "-Indirect photophoretic force depends on the physical properties of the particle and the surrounding medium.\n",
      "-Electrophoresis involves the migration of macromolecules under the influence of an electric field. Electrophoretic light scattering involves passing an electric field through a liquid which makes particles move. The bigger the charge is on the particles, the faster they are able to move.\n",
      "-Electrophoresis involves the migration of macromolecules under the influence of an electric field. Electrophoretic light scattering involves passing an electric field through a liquid which makes particles move. The bigger the charge is on the particles, the faster they are able to move.\n",
      "\n",
      "\n",
      "\n",
      "What does Earnshaw's theorem state?\n",
      "-Earnshaw's theorem states that a collection of point charges cannot be maintained in a stable stationary equilibrium configuration solely by the electrostatic interaction of the charges. This was first proven by British mathematician Samuel Earnshaw in 1842.\n",
      "It is usually cited in reference to magnetic fields, but was first applied to electrostatic fields.\n",
      "Earnshaw's theorem applies to classical inverse-square law forces (electric and gravitational) and also to the magnetic forces of permanent magnets, if the magnets are hard (the magnets do not vary in strength with external fields). Earnshaw's theorem forbids magnetic levitation in many common situations.\n",
      "If the materials are not hard, Braunbeck's extension shows that materials with relative magnetic permeability greater than one (paramagnetism) are further destabilising, but materials with a permeability less than one (diamagnetic materials) permit stable configurations.\n",
      "-Similar to point masses, in electromagnetism physicists discuss a point charge, a point particle with a nonzero electric charge. The fundamental equation of electrostatics is Coulomb's law, which describes the electric force between two point charges. Another result, Earnshaw's theorem, states that a collection of point charges cannot be maintained in a static equilibrium configuration solely by the electrostatic interaction of the charges. The electric field associated with a classical point charge increases to infinity as the distance from the point charge decreases towards zero, which suggests that the model is no longer accurate in this limit.\n",
      "-For quite some time, Earnshaw's theorem posed a startling question of why matter is stable and holds together, since much evidence was found that matter was held together electromagnetically despite the proven instability of static charge configurations. Since Earnshaw's theorem only applies to stationary charges, there were attempts to explain stability of atoms using planetary models, such as Nagaoka's Saturnian model (1904) and Rutherford's planetary model (1911), where the point electrons are circling a positive point charge in the center. Yet, the stability of such planetary models was immediately questioned: electrons have nonzero acceleration when moving along a circle, and hence they would radiate the energy via a non-stationary electromagnetic field. Bohr's model of 1913 formally prohibited this radiation without giving an explanation for its absence.\n",
      "-Detailed proofs Earnshaw's theorem was originally formulated for electrostatics (point charges) to show that there is no stable configuration of a collection of point charges. The proofs presented here for individual dipoles should be generalizable to collections of magnetic dipoles because they are formulated in terms of energy, which is additive. A rigorous treatment of this topic is, however, currently beyond the scope of this article.\n",
      "-Earnshaw's theorem proved conclusively that it is not possible to levitate stably using only static, macroscopic, paramagnetic fields. The forces acting on any paramagnetic object in any combinations of gravitational, electrostatic, and magnetostatic fields will make the object's position, at best, unstable along at least one axis, and it can be in unstable equilibrium along all axes. However, several possibilities exist to make levitation viable, for example, the use of electronic stabilization or diamagnetic materials (since relative magnetic permeability is less than one); it can be shown that diamagnetic materials are stable along at least one axis, and can be stable along all axes. Conductors can have a relative permeability to alternating magnetic fields of below one, so some configurations using simple AC-driven electromagnets are self stable.\n",
      "\n",
      "\n",
      "\n",
      "What is radiosity in radiometry?\n",
      "-In radiometry, radiosity is the radiant flux leaving (emitted, reflected and transmitted by) a surface per unit area, and spectral radiosity is the radiosity of a surface per unit frequency or wavelength, depending on whether the spectrum is taken as a function of frequency or of wavelength. The SI unit of radiosity is the watt per square metre (W/m2), while that of spectral radiosity in frequency is the watt per square metre per hertz (W·m−2·Hz−1) and that of spectral radiosity in wavelength is the watt per square metre per metre (W·m−3)—commonly the watt per square metre per nanometre (W·m−2·nm−1). The CGS unit erg per square centimeter per second (erg·cm−2·s−1) is often used in astronomy. Radiosity is often called intensity in branches of physics other than radiometry, but in radiometry this usage leads to confusion with radiant intensity.\n",
      "-The radiosity method, in the context of computer graphics, derives from (and is fundamentally the same as) the radiosity method in heat transfer. In this context, radiosity is the total radiative flux (both reflected and re-radiated) leaving a surface; this is also sometimes known as radiant exitance. Calculation of radiosity, rather than surface temperatures, is a key aspect of the radiosity method that permits linear matrix methods to be applied to the problem.\n",
      "-More correctly, radiosity B is the energy per unit area leaving the patch surface per discrete time interval and is the combination of emitted and reflected energy: cos cos ⁡θx′⋅Vis(x,x′)dA′ where: B(x)i dAi is the total energy leaving a small area dAi around a point x.\n",
      "E(x)i dAi is the emitted energy.\n",
      "ρ(x) is the reflectivity of the point, giving reflected energy per unit area by multiplying by the incident energy per unit area (the total energy which arrives from other patches).\n",
      "S denotes that the integration variable x' runs over all the surfaces in the scene r is the distance between x and x'  θx and θx' are the angles between the line joining x and x' and vectors normal to the surface at x and x' respectively.\n",
      "-In 3D computer graphics, radiosity is an application of the finite element method to solving the rendering equation for scenes with surfaces that reflect light diffusely. Unlike rendering methods that use Monte Carlo algorithms (such as path tracing), which handle all types of light paths, typical radiosity only account for paths (represented by the code \"LD*E\") which leave a light source and are reflected diffusely some number of times (possibly zero) before hitting the eye. Radiosity is a global illumination algorithm in the sense that the illumination arriving on a surface comes not just directly from the light sources, but also from other surfaces reflecting light. Radiosity is viewpoint independent, which increases the calculations involved, but makes them useful for all viewpoints.\n",
      "-Radiosity is a method which attempts to simulate the way in which directly illuminated surfaces act as indirect light sources that illuminate other surfaces. This produces more realistic shading and seems to better capture the 'ambience' of an indoor scene. A classic example is a way that shadows 'hug' the corners of rooms.\n",
      "The optical basis of the simulation is that some diffused light from a given point on a given surface is reflected in a large spectrum of directions and illuminates the area around it.\n",
      "The simulation technique may vary in complexity. Many renderings have a very rough estimate of radiosity, simply illuminating an entire scene very slightly with a factor known as ambiance. However, when advanced radiosity estimation is coupled with a high quality ray tracing algorithm, images may exhibit convincing realism, particularly for indoor scenes.\n",
      "\n",
      "\n",
      "\n",
      "What is a virtual particle?\n",
      "-A virtual particle is a theoretical transient particle that exhibits some of the characteristics of an ordinary particle, while having its existence limited by the uncertainty principle. The concept of virtual particles arises in the perturbation theory of quantum field theory where interactions between ordinary particles are described in terms of exchanges of virtual particles. A process involving virtual particles can be described by a schematic representation known as a Feynman diagram, in which virtual particles are represented by internal lines.Virtual particles do not necessarily carry the same mass as the corresponding real particle, although they always conserve energy and momentum. The closer its characteristics come to those of ordinary particles, the longer the virtual particle exists. They are important in the physics of many processes, including particle scattering and Casimir forces. In quantum field theory, forces—such as the electromagnetic repulsion or attraction between two charges—can be thought of as due to the exchange of virtual photons between the charges. Virtual photons are the exchange particle for the electromagnetic interaction.\n",
      "-In formal terms, a particle is considered to be an eigenstate of the particle number operator a†a, where a is the particle annihilation operator and a† the particle creation operator (sometimes collectively called ladder operators). In many cases, the particle number operator does not commute with the Hamiltonian for the system. This implies the number of particles in an area of space is not a well-defined quantity but, like other quantum observables, is represented by a probability distribution. Since these particles are not certain to exist, they are called virtual particles or vacuum fluctuations of vacuum energy. In a certain sense, they can be understood to be a manifestation of the time-energy uncertainty principle in a vacuum.An important example of the \"presence\" of virtual particles in a vacuum is the Casimir effect. Here, the explanation of the effect requires that the total energy of all of the virtual particles in a vacuum can be added together. Thus, although the virtual particles themselves are not directly observable in the laboratory, they do leave an observable effect: Their zero-point energy results in forces acting on suitably arranged metal plates or dielectrics. On the other hand, the Casimir effect can be interpreted as the relativistic van der Waals force.\n",
      "-The concept of virtual particles arises in the perturbation theory of quantum field theory, an approximation scheme in which interactions (in essence, forces) between actual particles are calculated in terms of exchanges of virtual particles. Such calculations are often performed using schematic representations known as Feynman diagrams, in which virtual particles appear as internal lines. By expressing the interaction in terms of the exchange of a virtual particle with four-momentum q, where q is given by the difference between the four-momenta of the particles entering and leaving the interaction vertex, both momentum and energy are conserved at the interaction vertices of the Feynman diagram.: 119 A virtual particle does not precisely obey the energy–momentum relation m2c4 = E2 − p2c2. Its kinetic energy may not have the usual relationship to velocity. It can be negative.: 110  This is expressed by the phrase off mass shell.: 119  The probability amplitude for a virtual particle to exist tends to be canceled out by destructive interference over longer distances and times. As a consequence, a real photon is massless and thus has only two polarization states, whereas a virtual one, being effectively massive, has three polarization states.\n",
      "-In particle physics, this inequality permits a qualitative understanding of virtual particles, which carry momentum. The exchange of virtual particles with real particles is responsible for the creation of all known fundamental forces (more accurately known as fundamental interactions). Virtual photons are also responsible for the electrostatic interaction between electric charges (which results in Coulomb's law), for spontaneous radiative decay of excited atomic and nuclear states, for the Casimir force, for the Van der Waals force and some other observable phenomena.\n",
      "-Quantum tunnelling may be considered a manifestation of virtual particle exchanges.: 235  The range of forces carried by virtual particles is limited by the uncertainty principle, which regards energy and time as conjugate variables; thus, virtual particles of larger mass have more limited range.Written in the usual mathematical notations, in the equations of physics, there is no mark of the distinction between virtual and actual particles. The amplitudes of processes with a virtual particle interfere with the amplitudes of processes without it, whereas for an actual particle the cases of existence and non-existence cease to be coherent with each other and do not interfere any more. In the quantum field theory view, actual particles are viewed as being detectable excitations of underlying quantum fields. Virtual particles are also viewed as excitations of the underlying fields, but appear only as forces, not as detectable particles. They are \"temporary\" in the sense that they appear in some calculations, but are not detected as single particles. Thus, in mathematical terms, they never appear as indices to the scattering matrix, which is to say, they never appear as the observable inputs and outputs of the physical process being modelled.\n",
      "\n",
      "\n",
      "\n",
      "Who proposed the principle of \"complexity from noise\" and when was it first introduced?\n",
      "-An early example of algorithm complexity analysis is the running time analysis of the Euclidean algorithm done by Gabriel Lamé in 1844.\n",
      "Before the actual research explicitly devoted to the complexity of algorithmic problems started off, numerous foundations were laid out by various researchers. Most influential among these was the definition of Turing machines by Alan Turing in 1936, which turned out to be a very robust and flexible simplification of a computer.\n",
      "-Complexity and chaos theory Complex systems theory is rooted in chaos theory, which in turn has its origins more than a century ago in the work of the French mathematician Henri Poincaré. Chaos is sometimes viewed as extremely complicated information, rather than as an absence of order. Chaotic systems remain deterministic, though their long-term behavior can be difficult to predict with any accuracy. With perfect knowledge of the initial conditions and the relevant equations describing the chaotic system's behavior, one can theoretically make perfectly accurate predictions of the system, though in practice this is impossible to do with arbitrary accuracy. Ilya Prigogine argued that complexity is non-deterministic and gives no way whatsoever to precisely predict the future.The emergence of complex systems theory shows a domain between deterministic order and randomness which is complex. This is referred to as the \"edge of chaos\".\n",
      "-The cybernetician William Ross Ashby formulated the original principle of self-organization in 1947. It states that any deterministic dynamic system automatically evolves towards a state of equilibrium that can be described in terms of an attractor in a basin of surrounding states. Once there, the further evolution of the system is constrained to remain in the attractor. This constraint implies a form of mutual dependency or coordination between its constituent components or subsystems. In Ashby's terms, each subsystem has adapted to the environment formed by all other subsystems.The cybernetician Heinz von Foerster formulated the principle of \"order from noise\" in 1960. It notes that self-organization is facilitated by random perturbations (\"noise\") that let the system explore a variety of states in its state space. This increases the chance that the system will arrive into the basin of a \"strong\" or \"deep\" attractor, from which it then quickly enters the attractor itself. The biophysicist Henri Atlan developed this concept by proposing the principle of \"complexity from noise\" (French: le principe de complexité par le bruit) first in the 1972 book L'organisation biologique et la théorie de l'information and then in the 1979 book Entre le cristal et la fumée. The physicist and chemist Ilya Prigogine formulated a similar principle as \"order through fluctuations\" or \"order out of chaos\". It is applied in the method of simulated annealing for problem solving and machine learning.\n",
      "-The composer Iannis Xenakis (1960) was the first to explicate a compositional theory for grains of sound. He began by adopting the following lemma: \"All sound, even continuous musical variation, is conceived as an assemblage of a large number of elementary sounds adequately disposed in time. In the attack, body, and decline of a complex sound, thousands of pure sounds appear in a more or less short interval of time  Δt .\" Xenakis created granular sounds using analog tone generators and tape splicing. These appear in the composition Analogique A-B for string orchestra and tape (1959).\n",
      "-Since the ground-breaking 1965 paper by Juris Hartmanis and Richard E. Stearns and the 1979 book by Michael Garey and David S. Johnson on NP-completeness, the term \"computational complexity\" (of algorithms) has become commonly referred to as asymptotic computational complexity.  Further, unless specified otherwise, the term \"computational complexity\" usually refers to the upper bound for the asymptotic computational complexity of an algorithm or a problem, which is usually written in terms of the big O notation, e.g..  O(n3).\n",
      "\n",
      "\n",
      "\n",
      "What is the order parameter that breaks the electromagnetic gauge symmetry in superconductors?\n",
      "-In superconductors, there is a condensed-matter collective field ψ, which acts as the order parameter breaking the electromagnetic gauge symmetry.\n",
      "-Higgs mechanism The strong, weak, and electromagnetic forces can all be understood as arising from gauge symmetries, which is a redundancy in the description of the symmetry. The Higgs mechanism, the spontaneous symmetry breaking of gauge symmetries, is an important component in understanding the superconductivity of metals and the origin of particle masses in the standard model of particle physics.  The term \"spontaneous symmetry breaking\" is a misnomer here as Elitzur's theorem states that local gauge symmetries can never be spontaneously broken. Rather, after gauge fixing, the global symmetry (or redundancy) can be broken in a manner formally resembling spontaneous symmetry breaking.  One important consequence of the distinction between true symmetries and gauge symmetries, is that the massless Nambu–Goldstone resulting from spontaneous breaking of a gauge symmetry are absorbed in the description of the gauge vector field, providing massive vector field modes, like the plasma mode in a superconductor, or the Higgs mode observed in particle physics.\n",
      "-The scanning SQUID microscope was originally developed for an experiment to test the pairing symmetry of the high-temperature cuprate superconductor YBCO. Standard superconductors are isotropic with respect to their superconducting properties, that is, for any direction of electron momentum  k in the superconductor, the magnitude of the order parameter and consequently the superconducting energy gap will be the same. However, in the high-temperature cuprate superconductors, the order parameter instead follows the equation  Δ(k)=Δ0(cos(kxa)−(kya)) , meaning that when crossing over any of the [110] directions in momentum space one will observe a sign change in the order parameter. The form of this function is equal to that of the l = 2 spherical harmonic function, giving it the name d-wave superconductivity. As the superconducting electrons are described by a single coherent wavefunction, proportional to exp(-iφ), where φ is known as the phase of the wavefunction, this property can be also interpreted as a phase shift of π under a 90 degree rotation.\n",
      "-Based on Landau's previously established theory of second-order phase transitions, Ginzburg and Landau argued that the free energy density  fs of a superconductor near the superconducting transition can be expressed in terms of a complex order parameter field  ψ(r)=|ψ(r)|eiϕ(r) , where the quantity  |ψ(r)|2 is a measure of the local density of superconducting electrons  ns(r) analogous to a quantum mechanical wave function. While  ψ(r) is nonzero below a phase transition into a superconducting state, no direct interpretation of this parameter was given in the original paper. Assuming smallness of  |ψ| and smallness of its gradients, the free energy density has the form of a field theory and exhibits U(1) gauge symmetry: where fn is the free energy density of the normal phase, α and  β are phenomenological parameters, m∗ is an effective mass, e∗ is an effective charge (usually  2e , where e is the charge of an electron), A is the magnetic vector potential, and B=∇×A is the magnetic field.The total free energy is given by  F=∫fsd3r . By minimizing  F with respect to variations in the order parameter  ψ and the vector potential  A , one arrives at the Ginzburg–Landau equations where  J denotes the dissipation-less electric current density and Re the real part. The first equation — which bears some similarities to the time-independent Schrödinger equation, but is principally different due to a nonlinear term — determines the order parameter,  ψ . The second equation then provides the superconducting current.\n",
      "-The original idea on the parameter κ belongs to Landau. The ratio κ = λ/ξ is presently known as the Ginzburg–Landau parameter. It has been proposed by Landau that Type I superconductors are those with 0 < κ < 1/√2, and Type II superconductors those with κ > 1/√2.\n",
      "\n",
      "\n",
      "\n",
      "What is the reason for the sun appearing slightly yellowish when viewed from Earth?\n",
      "-As a tropospheric cloud matures, the dense water droplets may combine to produce larger droplets, which may combine to form droplets large enough to fall as rain. By this process of accumulation, the space between droplets becomes increasingly larger, permitting light to penetrate farther into the cloud. If the cloud is sufficiently large and the droplets within are spaced far enough apart, it may be that a percentage of the light which enters the cloud is not reflected back out before it is absorbed. A simple example of this is being able to see farther in heavy rain than in heavy fog. This process of reflection/absorption is what causes the range of cloud color from white to black.Other colors occur naturally in clouds. Bluish-grey is the result of light scattering within the cloud. In the visible spectrum, blue and green are at the short end of light's visible wavelengths, while red and yellow are at the long end. The short rays are more easily scattered by water droplets, and the long rays are more likely to be absorbed. The bluish color is evidence that such scattering is being produced by rain-sized droplets in the cloud. A cumulonimbus cloud emitting green is a sign that it is a severe thunderstorm, capable of heavy rain, hail, strong winds and possible tornadoes. The exact cause of green thunderstorms is still unknown, but it could be due to the combination of reddened sunlight passing through very optically thick clouds. Yellowish clouds may occur in the late spring through early fall months during forest fire season. The yellow color is due to the presence of pollutants in the smoke. Yellowish clouds caused by the presence of nitrogen dioxide are sometimes seen in urban areas with high air pollution levels.Red, orange and pink clouds occur almost entirely at sunrise and sunset and are the result of the scattering of sunlight by the atmosphere. When the angle between the Sun and the horizon is less than 10 percent, as it is just after sunrise or just prior to sunset, sunlight becomes too red due to refraction for any colors other than those with a reddish hue to be seen. The clouds do not become that color; they are reflecting long and unscattered rays of sunlight, which are predominant at those hours. The effect is much like if a person were to shine a red spotlight on a white sheet. In combination with large, mature thunderheads this can produce blood-red clouds. Clouds look darker in the near-infrared because water absorbs solar radiation at those wavelengths.\n",
      "-Except for direct sunlight, most of the light in the daytime sky is caused by scattering, which is dominated by a small-particle limit called Rayleigh scattering. The scattering due to molecule-sized particles (as in air) is greater in the directions both toward and away from the source of light than it is in directions perpendicular to the incident path. Scattering is significant for light at all visible wavelengths, but is stronger at the shorter (bluer) end of the visible spectrum, meaning that the scattered light is bluer than its source: the Sun. The remaining direct sunlight, having lost some of its shorter-wavelength components, appears slightly less blue.Scattering also occurs even more strongly in clouds. Individual water droplets refract white light into a set of colored rings. If a cloud is thick enough, scattering from multiple water droplets will wash out the set of colored rings and create a washed-out white color.The sky can turn a multitude of colors such as red, orange, purple, and yellow (especially near sunset or sunrise) when the light must travel a much longer path (or optical depth) through the atmosphere. Scattering effects also partially polarize light from the sky and are most pronounced at an angle 90° from the Sun. Scattered light from the horizon travels through as much as 38 times the air mass as does light from the zenith, causing a blue gradient looking vivid at the zenith and pale near the horizon. Red light is also scattered if there is enough air between the source and the observer, causing parts of the sky to change color as the Sun rises or sets. As the air mass nears infinity, scattered daylight appears whiter and whiter.Apart from the Sun, distant clouds or snowy mountaintops may appear yellow. The effect is not very obvious on clear days, but is very pronounced when clouds cover the line of sight, reducing the blue hue from scattered sunlight. At higher altitudes, the sky tends toward darker colors since scattering is reduced due to lower air density. An extreme example is the Moon, where no atmospheric scattering occurs, making the lunar sky black even when the Sun is visible.Sky luminance distribution models have been recommended by the International Commission on Illumination (CIE) for the design of daylighting schemes. Recent developments relate to \"all sky models\" for modelling sky luminance under weather conditions ranging from clear to overcast.\n",
      "-Sunlight and neutrinos The Sun emits light across the visible spectrum, so its color is white, with a CIE color-space index near (0.3, 0.3), when viewed from space or when the Sun is high in the sky. The Solar radiance per wavelength peaks in the green portion of the spectrum when viewed from space. When the Sun is very low in the sky, atmospheric scattering renders the Sun yellow, red, orange, or magenta, and in rare occasions even green or blue. Despite its typical whiteness (white sunrays, white ambient light, white illumination of the Moon, etc.), some cultures mentally picture the Sun as yellow and some even red; the reasons for this are cultural and exact ones are the subject of debate.\n",
      "-The blue colour of the sky results from Rayleigh scattering, as the size of the gas particles in the atmosphere is much smaller than the wavelength of visible light. Rayleigh scattering is much greater for blue light than for other colours due to its shorter wavelength. As sunlight passes through the atmosphere, its blue component is Rayleigh scattered strongly by atmospheric gases but the longer wavelength (e.g. red/yellow) components are not. The sunlight arriving directly from the Sun therefore appears to be slightly yellow, while the light scattered through rest of the sky appears blue. During sunrises and sunsets, the effect of Rayleigh scattering on the spectrum of the transmitted light is much greater due to the greater distance the light rays have to travel through the high-density air near the Earth's surface.\n",
      "-The color of light from the sky is a result of Rayleigh scattering of sunlight, which results in a perceived blue color. On a sunny day, Rayleigh scattering gives the sky a blue gradient, darkest around the zenith and brightest near the horizon. Light rays coming from the zenith take the shortest-possible path (1⁄38) through the air mass, yielding less scattering. Light rays coming from the horizon take the longest-possible path through the air, yielding more scattering.The blueness is at the horizon because the blue light coming from great distances is also preferentially scattered. This results in a red shift of the distant light sources that is compensated by the blue hue of the scattered light in the line of sight. In other words, the red light scatters also; if it does so at a point a great distance from the observer it has a much higher chance of reaching the observer than blue light. At distances nearing infinity, the scattered light is therefore white. Distant clouds or snowy mountaintops will seem yellow for that reason; that effect is not obvious on clear days, but very pronounced when clouds are covering the line of sight reducing the blue hue from scattered sunlight.\n",
      "\n",
      "\n",
      "\n",
      "What is the Landau-Lifshitz-Gilbert equation used for in physics?\n",
      "-In physics, the Landau–Lifshitz–Gilbert equation, named for Lev Landau, Evgeny Lifshitz, and T. L. Gilbert, is a name used for a differential equation describing the precessional motion of magnetization M in a solid. It is a modification by Gilbert of the original equation of Landau and Lifshitz.\n",
      "-In 1955 Gilbert replaced the damping term in the Landau–Lifshitz (LL) equation by one that depends on the time derivative of the magnetization: This is the Landau–Lifshitz–Gilbert (LLG) equation, where η is the damping parameter, which is characteristic of the material. It can be transformed into the Landau–Lifshitz equation: where and λ=γ2η1+γ2η2Ms2.\n",
      "In this form of the LL equation, the precessional term γ' depends on the damping term. This better represents the behavior of real ferromagnets when the damping is large.\n",
      "-In a ferromagnet, the magnitude of the magnetization M at each point equals the saturation magnetization Ms (although it can be smaller when averaged over a chunk of volume). The Landau–Lifshitz–Gilbert equation predicts the rotation of the magnetization in response to torques. An earlier, but equivalent, equation (the Landau–Lifshitz equation) was introduced by Landau & Lifshitz (1935): where γ is the electron gyromagnetic ratio and λ is a phenomenological damping parameter, often replaced by λ=αγMs, where α is a dimensionless constant called the damping factor. The effective field Heff is a combination of the external magnetic field, the demagnetizing field (magnetic field due to the magnetization), and some quantum mechanical effects. To solve this equation, additional equations for the demagnetizing field must be included.\n",
      "-The purpose of dynamic micromagnetics is to predict the time evolution of the magnetic configuration of a sample subject to some non-steady conditions such as the application of a field pulse or an AC field. This is done by solving the Landau-Lifshitz-Gilbert equation, which is a partial differential equation describing the evolution of the magnetization in terms of the local effective field acting on it.\n",
      "-With these considerations, the differential equation governing the behavior of a magnetic moment in the presence of an applied magnetic field with damping can be written in the most familiar form of the Landau-Lifshitz-Gilbert equation, d m d t = − γ μ 0 m × H e f f + α m ( m × d m d t ) {\\frac {{\\mathrm {d} }{\\mathbf {m} }}{{\\mathrm {d} }t}}=-\\gamma \\mu _{0}{\\mathbf {m} }\\times {\\mathbf {H_{eff}} }+{\\frac {\\alpha }{m}}\\left({\\mathbf {m} }\\times {\\frac {{\\mathrm {d} }{\\mathbf {m} }}{{\\mathrm {d} }t}}\\right) .Since without damping  d m d t {\\tfrac {{\\mathrm {d} }{\\mathbf {m} }}{{\\mathrm {d} }t}} is directed perpendicular to both the moment and the field, the damping term of the Landau-Lifshitz-Gilbert equation provides for a change in the moment towards the applied field. The Landau-Lifshitz-Gilbert equation can also be written in terms of torques, d m d t = − γ ( τ + τ d ) {\\frac {{\\mathrm {d} }{\\mathbf {m} }}{{\\mathrm {d} }t}}=-\\gamma \\left({\\boldsymbol {\\tau }}+{\\boldsymbol {\\tau _{d}}}\\right) ,where the damping torque is given by τ d = − α γ m ( m × d m d t ) {\\boldsymbol {\\tau _{d}}}=-{\\frac {\\alpha }{\\gamma m}}\\left({\\mathbf {m} }\\times {\\frac {{\\mathrm {d} }{\\mathbf {m} }}{{\\mathrm {d} }t}}\\right) .By way of the micromagnetic theory, the Landau-Lifshitz-Gilbert equation also applies to the mesoscopic- and macroscopic-scale magnetization  M M of a sample by simple substitution, d M d t = − γ μ 0 M × H e f f + α M ( M × d M d t ) {\\frac {{\\mathrm {d} }{\\mathbf {M} }}{{\\mathrm {d} }t}}=-\\gamma \\mu _{0}{\\mathbf {M} }\\times {\\mathbf {H_{eff}} }+{\\frac {\\alpha }{M}}\\left({\\mathbf {M} }\\times {\\frac {{\\mathrm {d} }{\\mathbf {M} }}{{\\mathrm {d} }t}}\\right) .\n",
      "\n",
      "\n",
      "\n",
      "What is spatial dispersion?\n",
      "-In the physics of continuous media, spatial dispersion is a phenomenon where material parameters such as permittivity or conductivity have dependence on wavevector. Normally, such a dependence is assumed to be absent for simplicity, however spatial dispersion exists to varying degrees in all materials.\n",
      "-In electromagnetics and optics, the term dispersion generally refers to aforementioned temporal or frequency dispersion. Spatial dispersion refers to the non-local response of the medium to the space; this can be reworded as the wavevector dependence of the permittivity. For an exemplary anisotropic medium, the spatial relation between electric and electric displacement field can be expressed as a convolution: Di(t,r)=Ei(t,r)+∫0∞∫fik(τ;r,r′)Ek(t−τ,r′)dV′dτ, where the kernel  fik is dielectric response (susceptibility); its indices make it in general a tensor to account for the anisotropy of the medium. Spatial dispersion is negligible in most macroscopic cases, where the scale of variation of  Ek(t−τ,r′) is much larger than atomic dimensions, because the dielectric kernel dies out at macroscopic distances. Nevertheless, it can result in non-negligible macroscopic effects, particularly in conducting media such as metals, electrolytes and plasmas. Spatial dispersion also plays role in optical activity and Doppler broadening, as well as in the theory of metamaterials.\n",
      "-Spatial dispersion can be compared to temporal dispersion, the latter often just called dispersion. Temporal dispersion represents memory effects in systems, commonly seen in optics and electronics. Spatial dispersion on the other hand represents spreading effects and is usually significant only at microscopic length scales. Spatial dispersion contributes relatively small perturbations to optics, giving weak effects such as optical activity. Spatial dispersion and temporal dispersion may occur in the same system.\n",
      "-In electromagnetism, spatial dispersion plays a role in a few material effects such as optical activity and doppler broadening. Spatial dispersion also plays an important role in the understanding of electromagnetic metamaterials. Most commonly, the spatial dispersion in permittivity ε is of interest.\n",
      "Crystal optics Inside crystals there may be a combination of spatial dispersion, temporal dispersion, and anisotropy. The constitutive relation for the polarization vector can be written as: Pi(k→,ω)=∑j(ϵij(k→,ω)−ϵ0δij)Ej(k→,ω), i.e., the permittivity is a wavevector- and frequency-dependent tensor.\n",
      "Considering Maxwell's equations, one can find the plane wave normal modes inside such crystals. These occur when the following relationship is satisfied for a nonzero electric field vector  E→ 0.\n",
      "Spatial dispersion in  ϵ(k→,ω) can lead to strange phenomena, such as the existence of multiple modes at the same frequency and wavevector direction, but with different wavevector magnitudes.\n",
      "Nearby crystal surfaces and boundaries, it is no longer valid to describe system response in terms of wavevectors. For a full description it is necessary to return to a full nonlocal response function (without translational symmetry), however the end effect can sometimes be described by \"additional boundary conditions\" (ABC's).\n",
      "In isotropic media In materials that have no relevant crystalline structure, spatial dispersion can be important.\n",
      "-The origin of spatial dispersion is nonlocal response, where response to a force field appears at many locations, and can appear even in locations where the force is zero. This usually arises due to a spreading of effects by the hidden microscopic degrees of freedom.As an example, consider the current  J(x,t) that is driven in response to an electric field  E(x,t) , which is varying in space (x) and time (t). Simplified laws such as Ohm's law would say that these are directly proportional to each other,  J=σE , but this breaks down if the system has memory (temporal dispersion) or spreading (spatial dispersion). The most general linear response is given by: J(x,t)=∫−∞−∞dx′∫−∞−∞dt′σ(x,x′,t,t′)E(x′,t′), where  σ(x,x′,t,t′)dx′dt′ is the nonlocal conductivity function.\n",
      "\n",
      "\n",
      "\n",
      "What are the constituents of cold dark matter?\n",
      "-The constituents of cold dark matter are unknown. Possibilities range from large objects like MACHOs (such as black holes and Preon stars) or RAMBOs (such as clusters of brown dwarfs), to new particles such as [WIMPs and axions The 1997 DAMA/NaI experiment and its successor DAMA/LIBRA in 2013, claimed to directly detect dark matter particles passing through the Earth, but many researchers remain skeptical, as negative results from similar experiments seem incompatible with the DAMA results.\n",
      "-Dark matter is detected through its gravitational interactions with ordinary matter and radiation. As such, it is very difficult to determine what the constituents of cold dark matter are. The candidates fall roughly into three categories: Axions, very light particles with a specific type of self-interaction that makes them a suitable CDM candidate. In recent years, axions have become one of the most promising candidates for dark matter. Axions have the theoretical advantage that their existence solves the strong CP problem in quantum chromodynamics, but axion particles have only been theorized and never detected. Axions are an example of a more general category of particle called a WISP (weakly interacting \"slender\" or \"slim\" particle), which are the low-mass counterparts of WIMPs.Massive compact halo objects (MACHOs), large, condensed objects such as black holes, neutron stars, white dwarfs, very faint stars, or non-luminous objects like planets. The search for these objects consists of using gravitational lensing to detect the effects of these objects on background galaxies. Most experts believe that the constraints from those searches rule out MACHOs as a viable dark matter candidate.Weakly interacting massive particles (WIMPs). There is no currently known particle with the required properties, but many extensions of the standard model of particle physics predict such particles. The search for WIMPs involves attempts at direct detection by highly sensitive detectors, as well as attempts at production of WIMPs by particle accelerators. Historically, WIMPs were regarded as one of the most promising candidates for the composition of dark matter, but in recent years WIMPs have since been supplanted by axions with the non-detection of WIMPs in experiments. The DAMA/NaI experiment and its successor DAMA/LIBRA have claimed to have directly detected dark matter particles passing through the Earth, but many scientists remain skeptical because no results from similar experiments seem compatible with the DAMA results.\n",
      "-The main theoretical characteristics of a WIMP are: Interactions only through the weak nuclear force and gravity, or possibly other interactions with cross-sections no higher than the weak scale; Large mass compared to standard particles (WIMPs with sub-GeV masses may be considered to be light dark matter).Because of their lack of electromagnetic interaction with normal matter, WIMPs would be invisible through normal electromagnetic observations. Because of their large mass, they would be relatively slow moving and therefore \"cold\". Their relatively low velocities would be insufficient to overcome the mutual gravitational attraction, and as a result, WIMPs would tend to clump together. WIMPs are considered one of the main candidates for cold dark matter, the others being massive compact halo objects (MACHOs) and axions. These names were deliberately chosen for contrast, with MACHOs named later than WIMPs. In contrast to MACHOs, there are no known stable particles within the Standard Model of particle physics that have all the properties of WIMPs. The particles that have little interaction with normal matter, such as neutrinos, are all very light, and hence would be fast moving, or \"hot\".\n",
      "-Baryonic matter Most of the ordinary matter familiar to astronomers, including planets, brown dwarfs, red dwarfs, visible stars, white dwarfs, neutron stars, and black holes, is called baryonic matter (referring to the baryons that dominate the mass of most ordinary matter). Solitary black holes, neutron stars, burnt-out dwarfs, and other massive objects that that are hard to detect are collectively known as MACHOs; some scientists initially hoped that baryonic MACHOs could account for and explain all the dark matter.However, multiple lines of evidence suggest the majority of dark matter is not baryonic: Sufficient diffuse, baryonic gas or dust would be visible when backlit by stars.\n",
      "-Exotic dark matter In Lambda-CDM, dark matter is an extremely inert form of matter that does not interact with both ordinary matter (baryons) and light, but still exerts gravitational effects. To produce the large-scale structure we see today, dark matter is \"cold\" (the 'C' in Lambda-CDM), i.e. non-relativistic. Dark matter has not been conclusively identified, and its exact nature is the subject of intense study. The leading dark matter candidates are weakly interacting massive particles (WIMPs) and axions. Both of these are new elementary particles not included in the Standard Model of Particle Physics. A major difference between the two is their mass: WIMPs generally have masses in the GeV range, while axions are much lighter, with masses in the meV range or lower.\n",
      "\n",
      "\n",
      "\n",
      "What is the mechanism of FTIR?\n",
      "-Frustrated TIR can be observed by looking into the top of a glass of water held in one's hand (Fig. 10). If the glass is held loosely, contact may not be sufficiently close and widespread to produce a noticeable effect. But if it is held more tightly, the ridges of one's fingerprints interact strongly with the evanescent waves, allowing the ridges to be seen through the otherwise totally reflecting glass-air surface.The same effect can be demonstrated with microwaves, using paraffin wax as the \"internal\" medium (where the incident and reflected waves exist). In this case the permitted gap width might be (e.g.) 1 cm or several cm, which is easily observable and adjustable.The term frustrated TIR also applies to the case in which the evanescent wave is scattered by an object sufficiently close to the reflecting interface. This effect, together with the strong dependence of the amount of scattered light on the distance from the interface, is exploited in total internal reflection microscopy.The mechanism of FTIR is called evanescent-wave coupling, and is a good analog to visualize quantum tunneling. Due to the wave nature of matter, an electron has a non-zero probability of \"tunneling\" through a barrier, even if classical mechanics would say that its energy is insufficient. Similarly, due to the wave nature of light, a photon has a non-zero probability of crossing a gap, even if ray optics would say that its approach is too oblique.\n",
      "-Nanoscale and spectroscopy below the diffraction limit The spatial resolution of FTIR can be further improved below the micrometer scale by integrating it into scanning near-field optical microscopy platform. The corresponding technique is called nano-FTIR and allows for performing broadband spectroscopy on materials in ultra-small quantities (single viruses and protein complexes) and with 10 to 20 nm spatial resolution.\n",
      "-Nano-FTIR detects the tip-scattered light interferometrically. The sample stage is placed into one arm of a conventional Michelson interferometer, while a mirror on a piezo stage is placed into another, reference arm. Recording the backscattered signal while translating the reference mirror yields an interferogram. The subsequent Fourier transform of this interferogram returns the near-field spectra of the sample.  Placement of the sample stage into one of the interferometer's arms (instead of outside of the interferometer as typically implemented in conventional FTIR) is a key element of nano-FTIR. It boosts the weak near-field signal due to interference with the strong reference field, helps to eliminate the background caused by parasitic scattering off everything that falls into large diffraction-limited beam focus, and most importantly, allows for recording of both amplitude s and phase φ spectra of the tip-scattered radiation. With the detection of phase, nano-FTIR provides complete information about near fields, which is essential for quantitative studies and many other applications. For example, for soft matter samples (organics, polymers, biomaterials, etc.), φ directly relates to the absorption in the sample material. This permits a direct comparison of nano-FTIR spectra with conventional absorption spectra of the sample material, thus allowing for simple spectroscopic identification according to standard FTIR databases.\n",
      "-FTIR is a method of measuring infrared absorption and emission spectra. For a discussion of why people measure infrared absorption and emission spectra, i.e. why and how substances absorb and emit infrared light, see the article: Infrared spectroscopy.\n",
      "-As a direct consequence of being quantitative technique (i.e. capable of highly reproducible detection of both near-field amplitude & phase and well understood near-field interaction models), nano-FTIR also provides means for the quantitative studies of the sample interior (within the probing range of the tip near field, of course). This is often achieved by a simple method of utilizing signals recorded at multiple demodulation orders naturally returned by nano-FTIR in the process of background suppression. It has been shown that higher harmonics probe smaller volumes below the tip, thus encoding the volumetric structure of a sample. This way, nano-FTIR has a demonstrated capability for the recovery of thickness and permittivity of layered films and nanostructures, which has been utilized for the nanoscale depth profiling of multiphase materials and high-Tc cuprate nanoconstriction devices patterned by focused ion beams. In other words, nano-FTIR has a unique capability of recovering the same information about thin-film samples that is typically returned by ellipsometry or impedance spectroscopy, yet with nanoscale spatial resolution. This capability proved crucial for disentangling different surface states in topological insulators.\n",
      "\n",
      "\n",
      "\n",
      "What is the origin of the permanent moment in paramagnetism?\n",
      "-Constituent atoms or molecules of paramagnetic materials have permanent magnetic moments (dipoles), even in the absence of an applied field. The permanent moment generally is due to the spin of unpaired electrons in atomic or molecular electron orbitals (see Magnetic moment). In pure paramagnetism, the dipoles do not interact with one another and are randomly oriented in the absence of an external field due to thermal agitation, resulting in zero net magnetic moment. When a magnetic field is applied, the dipoles will tend to align with the applied field, resulting in a net magnetic moment in the direction of the applied field. In the classical description, this alignment can be understood to occur due to a torque being provided on the magnetic moments by an applied field, which tries to align the dipoles parallel to the applied field. However, the true origins of the alignment can only be understood via the quantum-mechanical properties of spin and angular momentum.\n",
      "-Magnetic moments are permanent dipole moments within an atom that comprise electron angular momentum and spin by the relation μl = el/2me, where me is the mass of an electron, μl is the magnetic moment, and l is the angular momentum; this ratio is called the gyromagnetic ratio.\n",
      "The electrons in an atom contribute magnetic moments from their own angular momentum and from their orbital momentum around the nucleus. Magnetic moments from the nucleus are insignificant in contrast to the magnetic moments from the electrons. Thermal contributions result in higher energy electrons disrupting the order and the destruction of the alignment between dipoles.\n",
      "Ferromagnetic, paramagnetic, ferrimagnetic and antiferromagnetic materials have different intrinsic magnetic moment structures. At a material's specific Curie temperature (TC), these properties change. The transition from antiferromagnetic to paramagnetic (or vice versa) occurs at the Néel temperature (TN), which is analogous to Curie temperature.\n",
      "Orientations of magnetic moments in materials \n",
      "-The preferred classical explanation of a magnetic moment has changed over time. Before the 1930s, textbooks explained the moment using hypothetical magnetic point charges. Since then, most have defined it in terms of Ampèrian currents. In magnetic materials, the cause of the magnetic moment are the spin and orbital angular momentum states of the electrons, and varies depending on whether atoms in one region are aligned with atoms in another.\n",
      "-In the 19th century, it was thought that magnetic fields are due to currents in matter, and Ampère postulated that permanent magnets are caused by permanent atomic currents. The motion of classical charged particles could not explain permanent currents though, as shown by Larmor. In order to have ferromagnetism, the atoms must have permanent magnetic moments which are not due to the motion of classical charges.\n",
      "-Paramagnetism In a paramagnetic material there are unpaired electrons; i.e., atomic or molecular orbitals with exactly one electron in them. While paired electrons are required by the Pauli exclusion principle to have their intrinsic ('spin') magnetic moments pointing in opposite directions, causing their magnetic fields to cancel out, an unpaired electron is free to align its magnetic moment in any direction. When an external magnetic field is applied, these magnetic moments will tend to align themselves in the same direction as the applied field, thus reinforcing it.\n",
      "\n",
      "\n",
      "\n",
      "What is the reason that Newton's second law cannot be used to calculate the development of a physical system in quantum mechanics?\n",
      "-Spin is an intrinsic form of angular momentum carried by elementary particles, and thus by composite particles such as hadrons, atomic nuclei, and atoms.: 183–184  Spin should not be understood as in the \"rotating internal mass\" sense: spin is a quantized wave property.The existence of electron spin angular momentum is inferred from experiments, such as the Stern–Gerlach experiment, in which silver atoms were observed to possess two possible discrete angular momenta despite having no orbital angular momentum. The existence of the electron spin can also be inferred theoretically from the spin–statistics theorem and from the Pauli exclusion principle—and vice versa, given the particular spin of the electron, one may derive the Pauli exclusion principle.\n",
      "-Relation to Newton's second law of motion While angular momentum total conservation can be understood separately from Newton's laws of motion as stemming from Noether's theorem in systems symmetric under rotations, it can also be understood simply as an efficient method of calculation of results that can also be otherwise arrived at directly from Newton's second law, together with laws governing the forces of nature (such as Newton's third law, Maxwell's equations and Lorentz force). Indeed, given initial conditions of position and velocity for every point, and the forces at such a condition, one may use Newton's second law to calculate the second derivative of position, and solving for this gives full information on the development of the physical system with time. Note, however, that this is no longer true in quantum mechanics, due to the existence of particle spin, which is angular momentum that cannot be described by the cumulative effect of point-like motions in space.\n",
      "-In a conservative field, the total mechanical energy (kinetic and potential) is conserved: (where 'ṙ' denotes the derivative of 'r' with respect to time, that is the velocity,'I' denotes moment of inertia of that body and 'ω' denotes angular velocity), and in a central force field, so is the angular momentum: because the torque exerted by the force is zero. As a consequence, the body moves on the plane perpendicular to the angular momentum vector and containing the origin, and obeys Kepler's second law. (If the angular momentum is zero, the body moves along the line joining it with the origin.) It can also be shown that an object that moves under the influence of any central force obeys Kepler's second law. However, the first and third laws depend on the inverse-square nature of Newton's law of universal gravitation and do not hold in general for other central forces.\n",
      "-Newton's derivation begins with a particle moving under an arbitrary central force F1(r); the motion of this particle under this force is described by its radius r(t) from the center as a function of time, and also its angle θ1(t). In an infinitesimal time dt, the particle sweeps out an approximate right triangle whose area is dA1=12r2dθ1 Since the force acting on the particle is assumed to be a central force, the particle sweeps out equal angles in equal times, by Newton's Proposition 2. Expressed another way, the rate of sweeping out area is constant dA1dt=12r2dθ1dt=constant This constant areal velocity can be calculated as follows. At the apapsis and periapsis, the positions of closest and furthest distance from the attracting center, the velocity and radius vectors are perpendicular; therefore, the angular momentum L1 per mass m of the particle (written as h1) can be related to the rate of sweeping out areas h1=L1m=rv1=r2dθ1dt=2dA1dt Now consider a second particle whose orbit is identical in its radius, but whose angular variation is multiplied by a constant factor k θ2(t)=kθ1(t) The areal velocity of the second particle equals that of the first particle multiplied by the same factor k h2=2dA2dt=r2dθ2dt=kr2dθ1dt=2kdA1dt=kh1 Since k is a constant, the second particle also sweeps out equal areas in equal times. Therefore, by Proposition 2, the second particle is also acted upon by a central force F2(r). This is the conclusion of Proposition 43.\n",
      "-General considerations A rotational analog of Newton's third law of motion might be written, \"In a closed system, no torque can be exerted on any matter without the exertion on some other matter of an equal and opposite torque about the same axis.\" Hence, angular momentum can be exchanged between objects in a closed system, but total angular momentum before and after an exchange remains constant (is conserved).Seen another way, a rotational analogue of Newton's first law of motion might be written, \"A rigid body continues in a state of uniform rotation unless acted by an external influence.\" Thus with no external influence to act upon it, the original angular momentum of the system remains constant.The conservation of angular momentum is used in analyzing central force motion. If the net force on some body is directed always toward some point, the center, then there is no torque on the body with respect to the center, as all of the force is directed along the radius vector, and none is perpendicular to the radius. Mathematically, torque  τ=r×F=0, because in this case  r and  F are parallel vectors. Therefore, the angular momentum of the body about the center is constant. This is the case with gravitational attraction in the orbits of planets and satellites, where the gravitational force is always directed toward the primary body and orbiting bodies conserve angular momentum by exchanging distance and velocity as they move about the primary. Central force motion is also used in the analysis of the Bohr model of the atom.\n",
      "\n",
      "\n",
      "\n",
      "What is the butterfly effect, as defined by Lorenz in his book \"The Essence of Chaos\"?\n",
      "-Butterfly effect The butterfly effect is the notion that small events can have large, widespread consequences. The term describes events observed in chaos theory where a very small change in initial conditions results in vastly different outcomes. The term was coined by mathematician Edward Lorenz years after the phenomenon was first described.The butterfly effect has found its way into popular imagination. For example, in Ray Bradbury's 1952 short story A Sound of Thunder, the killing of a single insect millions of years in the past drastically changes the world, and in the 2004 film The Butterfly Effect, the protagonist's small changes to his past results in extreme changes.\n",
      "-In chaos theory, the butterfly effect is the sensitive dependence on initial conditions in which a small change in one state of a deterministic nonlinear system can result in large differences in a later state.\n",
      "-The butterfly effect describes a phenomenon in chaos theory whereby a minor change in circumstances can cause a large change in outcome. The scientific concept is attributed to Edward Lorenz, a mathematician and meteorologist who used the metaphor to describe his research findings related to chaos theory and weather prediction, initially in a 1972 paper titled \"Predictability: Does the Flap of a Butterfly's Wings in Brazil Set Off a Tornado in Texas?\" The butterfly metaphor is attributed to the 1952 Ray Bradbury short story \"A Sound of Thunder\".The concept has been widely adopted by popular culture, and interpreted to mean that small events have a rippling effect that cause much larger events to occur, and has become a common reference.\n",
      "-The branch of mathematics known as Chaos Theory focuses on the behavior of systems that are highly sensitive to initial conditions. It suggests that a small change in an initial condition can completely alter the progression of a system. This phenomenon is known as the butterfly effect, which claims that a butterfly flapping its wings in Brazil can cause a tornado in Texas. The nature of chaos theory suggests that the predictability of any system is limited because it is impossible to know all of the minutiae of a system at the present time. In principal, the deterministic systems that chaos theory attempts to analyze can be predicted, but uncertainty in a forecast increases exponentially with elapsed time.As documented in, three major kinds of butterfly effects within Lorenz studies include: the sensitive dependence on initial conditions, the ability of a tiny perturbation to create an organized circulation at large distances, and the hypothetical role of small-scale processes in contributing to finite predictability. The three kinds of butterfly effects are not exactly the same.\n",
      "-The term is closely associated with the work of mathematician and meteorologist Edward Norton Lorenz. He noted that the butterfly effect is derived from the metaphorical example of the details of a tornado (the exact time of formation, the exact path taken) being influenced by minor perturbations such as a distant butterfly flapping its wings several weeks earlier. Lorenz originally used a seagull causing a storm but was persuaded to make it more poetic with the use of a butterfly and tornado by 1972. He discovered the effect when he observed runs of his weather model with initial condition data that were rounded in a seemingly inconsequential manner. He noted that the weather model would fail to reproduce the results of runs with the unrounded initial condition data. A very small change in initial conditions had created a significantly different outcome.The idea that small causes may have large effects in weather was earlier acknowledged by French mathematician and engineer Henri Poincaré. American mathematician and philosopher Norbert Wiener also contributed to this theory. Lorenz's work placed the concept of instability of the Earth's atmosphere onto a quantitative base and linked the concept of instability to the properties of large classes of dynamic systems which are undergoing nonlinear dynamics and deterministic chaos.The butterfly effect concept has since been used outside the context of weather science as a broad term for any situation where a small change is supposed to be the cause of larger consequences.\n",
      "\n",
      "\n",
      "\n",
      "What is the role of CYCLOIDEA genes in the evolution of bilateral symmetry?\n",
      "-Evolution of symmetry in plants Early flowering plants had radially symmetric flowers but since then many plants have evolved bilaterally symmetrical flowers. The evolution of bilateral symmetry is due to the expression of CYCLOIDEA genes. Evidence for the role of the CYCLOIDEA gene family comes from mutations in these genes which cause a reversion to radial symmetry. The CYCLOIDEA genes encode transcription factors, proteins which control the expression of other genes. This allows their expression to influence developmental pathways relating to symmetry. For example, in Antirrhinum majus, CYCLOIDEA is expressed during early development in the dorsal domain of the flower meristem and continues to be expressed later on in the dorsal petals to control their size and shape. It is believed that the evolution of specialized pollinators may play a part in the transition of radially symmetrical flowers to bilaterally symmetrical flowers.\n",
      "-Arabidopsis thaliana has a gene called AGAMOUS that plays an important role in defining how many petals and sepals and other organs are generated. Mutations in this gene give rise to the floral meristem obtaining an indeterminate fate, and proliferation of floral organs in double-flowered forms of roses, carnations and morning glory. These phenotypes have been selected by horticulturists for their increased number of petals. Several studies on diverse plants like petunia, tomato, Impatiens, maize, etc. have suggested that the enormous diversity of flowers is a result of small changes in genes controlling their development.The Floral Genome Project confirmed that the ABC Model of flower development is not conserved across all angiosperms. Sometimes expression domains change, as in the case of many monocots, and also in some basal angiosperms like Amborella. Different models of flower development like the Fading boundaries model, or the Overlapping-boundaries model which propose non-rigid domains of expression, may explain these architectures. There is a possibility that from the basal to the modern angiosperms, the domains of floral architecture have become more and more fixed through evolution.\n",
      "-Arabidopsis thaliana has a gene called AGAMOUS that plays an important role in defining how many petals and sepals and other organs are generated. Mutations in this gene give rise to the floral meristem obtaining an indeterminate fate, and many floral organs keep on getting produced. We have flowers like roses, carnations and morning glory, for example, that have very dense floral organs. These flowers have been selected by horticulturists since long for increased number of petals. Researchers have found that the morphology of these flowers is because of strong mutations in the AGAMOUS homolog in these plants, which leads to them making a large number of petals and sepals. Several studies on diverse plants like petunia, tomato, impatiens, maize etc. have suggested that the enormous diversity of flowers is a result of small changes in genes controlling their development.Some of these changes also cause changes in expression patterns of the developmental genes, resulting in different phenotypes. The Floral Genome Project looked at the EST data from various tissues of many flowering plants. The researchers confirmed that the ABC Model of flower development is not conserved across all angiosperms. Sometimes expression domains change, as in the case of many monocots, and also in some basal angiosperms like Amborella. Different models of flower development like the fading boundaries model, or the overlapping-boundaries model which propose non-rigid domains of expression, may explain these architectures. There is a possibility that from the basal to the modern angiosperms, the domains of floral architecture have gotten more and more fixed through evolution.\n",
      "-Function The genes identified in the tissues of A. thaliana were able to be separated out and categorized based on location and function for both male and female reproductive organs. The male reproductive organs of the plant are much easier to be utilized for experimental procedures due to their capability to be easily isolated from the plant compared to the female organs. The genes that have been linked to pollen and pollen production show strong co-expression due to their classification as duplicated genes across various tissues. It has been identified that genes associated with pollen and the pollen tubes have a relatively high number of expressed polymorphisms through purifying selection. Identifiable features of adaptive evolution expressed in pollen associated genes are comparable to the increased levels of adaptive evolution in other comparable species. With adaptation regulated by mutation rates, the sex-biased genes associated with the male organs of the plant could show higher adaptation rates due to their presence being in a haploid state. In this haploid state, mutations are directly exposed to the opportunity of rapid selection. Pollen interactions associated with sporophytic tissue are not expressed in genes linked to female reproductive organs. The mechanisms involved in pollen formation and development of the pollen tube are important for pollen selection as well as protein composition of the pollen. Pollen surface proteins are produced in the sporophytic tissue of the anther and have expressed higher levels of purified selection with an increase in adaptive evolution from the oleopollenins of the anther.\n",
      "-Although asymmetry is typically associated with being unfit, some species have evolved to be asymmetrical as an important adaptation. Many members of the phylum Porifera (sponges) have no symmetry, though some are radially symmetric.\n",
      "Symmetry breaking The presence of these asymmetrical features requires a process of symmetry breaking during development, both in plants and animals. Symmetry breaking occurs at several different levels in order to generate the anatomical asymmetry which we observe. These levels include asymmetric gene expression, protein expression, and activity of cells.\n",
      "\n",
      "\n",
      "\n",
      "What is the required excess quark per billion quark-antiquark pairs in the early universe in order to provide all the observed matter in the universe?\n",
      "-The Standard Model can incorporate baryogenesis, though the amount of net baryons (and leptons) thus created may not be sufficient to account for the present baryon asymmetry. There is a required one excess quark per billion quark-antiquark pairs in the early universe in order to provide all the observed matter in the universe. This insufficiency has not yet been explained, theoretically or otherwise.\n",
      "-The majority of ordinary matter in the universe is found in atomic nuclei, which are made of neutrons and protons. These nucleons are made up of smaller particles called quarks, and antimatter equivalents for each are predicted to exist by the Dirac equation in 1928. Since then, each kind of antiquark has been experimentally verified. Hypotheses investigating the first few instants of the universe predict a composition with an almost equal number of quarks and antiquarks. Once the universe expanded and cooled to a critical temperature of approximately 2×1012 K, quarks combined into normal matter and antimatter and proceeded to annihilate up to the small initial asymmetry of about one part in five billion, leaving the matter around us. Free and separate individual quarks and antiquarks have never been observed in experiments—quarks and antiquarks are always found in groups of three (baryons), or bound in quark–antiquark pairs (mesons). Likewise, there is no experimental evidence that there are any significant concentrations of antimatter in the observable universe.\n",
      "-One of the outstanding problems in modern physics is the predominance of matter over antimatter in the universe. The universe, as a whole, seems to have a nonzero positive baryon number density – that is, there is more matter than antimatter. Since it is assumed in cosmology that the particles we see were created using the same physics we measure today, it would normally be expected that the overall baryon number should be zero, as matter and antimatter should have been created in equal amounts. This has led to a number of proposed mechanisms for symmetry breaking that favour the creation of normal matter (as opposed to antimatter) under certain conditions. This imbalance would have been exceptionally small, on the order of 1 in every 1010 particles a small fraction of a second after the Big Bang, but after most of the matter and antimatter annihilated, what was left over was all the baryonic matter in the current universe, along with a much greater number of bosons.\n",
      "-Because of the extremely high energies involved, quark-antiquark pairs are produced by pair production and thus QGP is a roughly equal mixture of quarks and antiquarks of various flavors, with only a slight excess of quarks. This property is not a general feature of conventional plasmas, which may be too cool for pair production (see however pair instability supernova).\n",
      "-According to the current models of big bang nucleosynthesis, the primordial composition of visible matter of the universe should be about 75% hydrogen and 25% helium-4 (in mass). Neutrons are made up of one up and two down quarks, while protons are made of two up and one down quark. Since the other common elementary particles (such as electrons, neutrinos, or weak bosons) are so light or so rare when compared to atomic nuclei, we can neglect their mass contribution to the observable universe's total mass. Therefore, one can conclude that most of the visible mass of the universe consists of protons and neutrons, which, like all baryons, in turn consist of up quarks and down quarks.\n",
      "\n",
      "\n",
      "\n",
      "What is the meaning of the term \"horror vacui\"?\n",
      "-In visual art, horror vacui (Latin for 'fear of empty space'; UK: ; US: ), or kenophobia (Greek for 'fear of the empty'), is a phenomenon in which the entire surface of a space or an artwork is filled with detail and content, leaving as little perceived emptiness as possible. It relates to the antiquated physical idea, horror vacui, proposed by Aristotle who held that \"nature abhors an empty space\".\n",
      "-In physics, horror vacui, or plenism (), commonly stated as \"nature abhors a vacuum\", is a postulate attributed to Aristotle, who articulated a belief, later criticized by the atomism of Epicurus and Lucretius, that nature contains no vacuums because the denser surrounding material continuum would immediately fill the rarity of an incipient void. He also argued against the void in a more abstract sense (as \"separable\"), for example, that by definition a void (equivocally?) itself, is nothing, and following Plato, nothing cannot rightly be said to exist. Furthermore, insofar as it would be featureless, it could neither be encountered by the senses, nor could its supposition lend additional explanatory power. Hero of Alexandria challenged the theory in the first century AD, but his attempts to create an artificial vacuum failed. The theory was debated in the context of 17th-century fluid mechanics, by Thomas Hobbes and Robert Boyle, among others, and through the early 18th century by Sir Isaac Newton and Gottfried Leibniz.\n",
      "-Medieval thought experiments into the idea of a vacuum considered whether a vacuum was present, if only for an instant, between two flat plates when they were rapidly separated. There was much discussion of whether the air moved in quickly enough as the plates were separated, or, as Walter Burley postulated, whether a 'celestial agent' prevented the vacuum arising. The commonly held view that nature abhorred a vacuum was called horror vacui. There was even speculation that even God could not create a vacuum if he wanted and the 1277 Paris condemnations of Bishop Étienne Tempier, which required there to be no restrictions on the powers of God, led to the conclusion that God could create a vacuum if he so wished. Jean Buridan reported in the 14th century that teams of ten horses could not pull open bellows when the port was sealed.\n",
      "-A vacuum (PL: vacuums or vacua) is a space devoid of matter. The word is derived from the Latin adjective vacuus for \"vacant\" or \"void\". An approximation to such vacuum is a region with a gaseous pressure much less than atmospheric pressure. Physicists often discuss ideal test results that would occur in a perfect vacuum, which they sometimes simply call \"vacuum\" or free space, and use the term partial vacuum to refer to an actual imperfect vacuum as one might have in a laboratory or in space. In engineering and applied physics on the other hand, vacuum refers to any space in which the pressure is considerably lower than atmospheric pressure. The Latin term in vacuo is used to describe an object that is surrounded by a vacuum.\n",
      "-In 350 BCE, Greek philosopher Aristotle suggested that nature abhors a vacuum, a principle that became known as the horror vacui. This concept built upon a 5th-century BCE ontological argument by the Greek philosopher Parmenides, who denied the possible existence of a void in space. Based on this idea that a vacuum could not exist, in the West it was widely held for many centuries that space could not be empty. As late as the 17th century, the French philosopher René Descartes argued that the entirety of space must be filled.In ancient China, the 2nd-century astronomer Zhang Heng became convinced that space must be infinite, extending well beyond the mechanism that supported the Sun and the stars. The surviving books of the Hsüan Yeh school said that the heavens were boundless, \"empty and void of substance\". Likewise, the \"sun, moon, and the company of stars float in the empty space, moving or standing still\".The Italian scientist Galileo Galilei knew that air had mass and so was subject to gravity. In 1640, he demonstrated that an established force resisted the formation of a vacuum. It would remain for his pupil Evangelista Torricelli to create an apparatus that would produce a partial vacuum in 1643. This experiment resulted in the first mercury barometer and created a scientific sensation in Europe. The French mathematician Blaise Pascal reasoned that if the column of mercury was supported by air, then the column ought to be shorter at higher altitude where the air pressure is lower. In 1648, his brother-in-law, Florin Périer, repeated the experiment on the Puy de Dôme mountain in central France and found that the column was shorter by three inches. This decrease in pressure was further demonstrated by carrying a half-full balloon up a mountain and watching it gradually expand, then contract upon descent.\n",
      "\n",
      "\n",
      "\n",
      "What is the Droste effect?\n",
      "-The effect is named after a Dutch brand of cocoa, with an image designed by Jan Misset in 1904. It has since been used in the packaging of a variety of products. The effect is seen in the Dutch artist M. C. Escher's 1956 lithograph Print Gallery, which portrays a gallery that depicts itself. Apart from advertising, the Droste effect is displayed in the model village at Bourton-on-the-Water: this contains a model of itself, with two further iterations. The effect has been a motif, too, for the cover of many comic books, where it was especially popular in the 1940s.\n",
      "-The Droste effect (Dutch pronunciation: [ˈdrɔstə]), known in art as an example of mise en abyme, is the effect of a picture recursively appearing within itself, in a place where a similar picture would realistically be expected to appear. This produces a loop which in theory could go on forever, but in practice only continues as far as the image's resolution allows.\n",
      "-M. C. Escher The Dutch artist M. C. Escher made use of the Droste effect in his 1956 lithograph Print Gallery, which portrays a gallery containing a print which depicts the gallery, each time both reduced and rotated, but with a void at the centre of the image. The work has attracted the attention of mathematicians including Hendrik Lenstra. They devised a method of filling in the artwork's central void in an additional application of the Droste effect by successively rotating and shrinking an image of the artwork.\n",
      "-Origins The Droste effect is named after the image on the tins and boxes of Droste cocoa powder which displayed a nurse carrying a serving tray with a cup of hot chocolate and a box with the same image, designed by Jan Misset. This familiar image was introduced in 1904 and maintained for decades with slight variations from 1912 by artists including Adolphe Mouron. The poet and columnist Nico Scheepmaker introduced wider usage of the term in the late 1970s.\n",
      "-Advertising In the 20th century, the Droste effect was used to market a variety of products. The packaging of Land O'Lakes butter featured a Native American woman holding a package of butter with a picture of herself. Morton Salt similarly made use of the effect. The cover of the 1969 vinyl album Ummagumma by Pink Floyd shows the band members sitting in various places, with a picture on the wall showing the same scene, but the order of the band members rotated. The logo of The Laughing Cow cheese spread brand pictures a cow with earrings. On closer inspection, these are seen to be images of the circular cheese spread package, each bearing the image of the laughing cow.  The Droste effect is a theme in Russell Hoban's children's novel, The Mouse and His Child, appearing in the form of a label on a can of \"Bonzo Dog Food\" which depicts itself.\n",
      "\n",
      "\n",
      "\n",
      "What is water hammer?\n",
      "-Water hammer: Water hammer (or more generally, fluid hammer) is a pressure surge or wave caused when a fluid (usually a liquid but sometimes also a gas) in motion is forced to stop or change direction suddenly (momentum change). Water hammer commonly occurs when a valve closes suddenly at an end of a pipeline system, and a pressure wave propagates in the pipe. It's also called hydraulic shock.\n",
      "-Hydraulic hammer A hydraulic hammer is a modern type of piling hammer used instead of diesel and air hammers for driving steel pipe, precast concrete, and timber piles. Hydraulic hammers are more environmentally acceptable than older, less efficient hammers as they generate less noise and pollutants. In many cases the dominant noise is caused by the impact of the hammer on the pile, or the impacts between components of the hammer, so that the resulting noise level can be similar to diesel hammers.\n",
      "-Hydraulic shock (colloquial: water hammer; fluid hammer) is a pressure surge or wave caused when a fluid in motion, usually a liquid but sometimes also a gas is forced to stop or change direction suddenly; a momentum change. This phenomenon commonly occurs when a valve closes suddenly at an end of a pipeline system, and a pressure wave propagates in the pipe.\n",
      "-The water hammer principle can be used to create a simple water pump called a hydraulic ram.\n",
      "Leaks can sometimes be detected using water hammer.\n",
      "Enclosed air pockets can be detected in pipelines.\n",
      "The water hammer from a liquid jet created by a collapsing microcavity is studied for potential applications noninvasive transdermal drug delivery.\n",
      "-A water hammer was a Victorian toy in which a tube was half filled with fluid, the remainder being a vacuum. Each time the tube was inverted or shaken, the impact of the fluid at each end would sound like a hammer blow.This is associated with increased stroke volume of the left ventricle and decrease in the peripheral resistance leading to the widened pulse pressure of aortic regurgitation.\n",
      "\n",
      "\n",
      "\n",
      "What is the reason for the stochastic nature of all observed resistance-switching processes?\n",
      "-A \"resistance switching\" event can simply be enforced by setting the external bias to a value above a certain threshold value. This is the trivial case, i.e., the free-energy barrier for the transition {i} → {j} is reduced to zero. In case one applies biases below the threshold value, there is still a finite probability that the device will switch in course of time (triggered by a random thermal fluctuation), but – as one is dealing with probabilistic processes – it is impossible to predict when the switching event will occur. That is the basic reason for the stochastic nature of all observed resistance-switching (ReRAM) processes. If the free-energy barriers are not high enough, the memory device can even switch without having to do anything.\n",
      "-The above-mentioned thermodynamic principle furthermore implies that the operation of two-terminal non-volatile memory devices (e.g. \"resistance-switching\" memory devices (ReRAM)) cannot be associated with the memristor concept, i.e., such devices cannot by itself remember their current or voltage history. Transitions between distinct internal memory or resistance states are of probabilistic nature. The probability for a transition from state {i} to state {j} depends on the height of the free-energy barrier between both states. The transition probability can thus be influenced by suitably driving the memory device, i.e., by \"lowering\" the free-energy barrier for the transition {i} → {j} by means of, for example, an externally applied bias.\n",
      "-Within this context, Meuffels and Soni pointed to a fundamental thermodynamic principle: Non-volatile information storage requires the existence of free-energy barriers that separate the distinct internal memory states of a system from each other; otherwise, one would be faced with an \"indifferent\" situation, and the system would arbitrarily fluctuate from one memory state to another just under the influence of thermal fluctuations. When unprotected against thermal fluctuations, the internal memory states exhibit some diffusive dynamics, which causes state degradation. The free-energy barriers must therefore be high enough to ensure a low bit-error probability of bit operation. Consequently, there is always a lower limit of energy requirement – depending on the required bit-error probability – for intentionally changing a bit value in any memory device.In the general concept of memristive system the defining equations are (see Theory): y(t)=g(x,u,t)u(t),x˙=f(x,u,t), where u(t) is an input signal, and y(t) is an output signal. The vector x represents a set of n state variables describing the different internal memory states of the device. ẋ is the time-dependent rate of change of the state vector x with time.\n",
      "-When one wants to go beyond mere curve fitting and aims at a real physical modeling of non-volatile memory elements, e.g., resistive random-access memory devices, one has to keep an eye on the aforementioned physical correlations. To check the adequacy of the proposed model and its resulting state equations, the input signal u(t) can be superposed with a stochastic term ξ(t), which takes into account the existence of inevitable thermal fluctuations. The dynamic state equation in its general form then finally reads: x˙=f(x,u(t)+ξ(t),t), where ξ(t) is, e.g., white Gaussian current or voltage noise. On base of an analytical or numerical analysis of the time-dependent response of the system towards noise, a decision on the physical validity of the modeling approach can be made, e.g., would the system be able to retain its memory states in power-off mode? Such an analysis was performed by Di Ventra and Pershin with regard to the genuine current-controlled memristor. As the proposed dynamic state equation provides no physical mechanism enabling such a memristor to cope with inevitable thermal fluctuations, a current-controlled memristor would erratically change its state in course of time just under the influence of current noise. Di Ventra and Pershin thus concluded that memristors whose resistance (memory) states depend solely on the current or voltage history would be unable to protect their memory states against unavoidable Johnson–Nyquist noise and permanently suffer from information loss, a so-called \"stochastic catastrophe\". A current-controlled memristor can thus not exist as a solid-state device in physical reality.\n",
      "-When a two-terminal non-volatile memory device is found to be in a distinct resistance state {j}, there exists therefore no physical one-to-one relationship between its present state and its foregoing voltage history. The switching behavior of individual non-volatile memory devices thus cannot be described within the mathematical framework proposed for memristor/memristive systems.\n",
      "\n",
      "\n",
      "\n",
      "What is the Einstein@Home project?\n",
      "-The Einstein@Home project is a distributed computing project similar to SETI@home intended to detect this type of simple gravitational wave. By taking data from LIGO and GEO, and sending it out in little pieces to thousands of volunteers for parallel analysis on their home computers, Einstein@Home can sift through the data far more quickly than would be possible otherwise.\n",
      "-The Einstein@Home project is a distributed computing project similar to SETI@home intended to detect this type of gravitational wave. By taking data from LIGO and GEO, and sending it out in little pieces to thousands of volunteers for parallel analysis on their home computers, Einstein@Home can sift through the data far more quickly than would be possible otherwise.\n",
      "-Einstein@Home is a volunteer computing project that searches for signals from spinning neutron stars in data from gravitational-wave detectors, from large radio telescopes, and from a gamma-ray telescope. Neutron stars are detected by their pulsed radio and gamma-ray emission as radio and/or gamma-ray pulsars. They also might be observable as continuous gravitational wave sources if they are rapidly spinning and non-axisymmetrically deformed. The project was officially launched on 19 February 2005 as part of the American Physical Society's contribution to the World Year of Physics 2005 event.Einstein@Home searches data from the LIGO gravitational-wave detectors. The project conducts the most sensitive all-sky searches for continuous gravitational waves. While no such signal has yet been detected, the upper limits set by Einstein@Home analyses provide astrophysical constraints on the Galactic population of spinning neutron stars.\n",
      "-The Einstein@Home project was originally created to perform all-sky searches for previously unknown continuous gravitational-wave (CW) sources using data from the Laser Interferometer Gravitational-Wave Observatory (LIGO) detector instruments in Washington and Louisiana, USA. The best understood potential CW sources are rapidly spinning neutron stars (including pulsars) which are expected to emit gravitational waves due to a deviation from Rotational symmetry. Besides validating Einstein's theory of General Relativity, direct detection of gravitational waves would also constitute an important new astronomical tool. As most neutron stars are electromagnetically invisible, gravitational-wave observations might also reveal completely new populations of neutron stars. A CW detection could potentially be extremely helpful in neutron-star astrophysics and would eventually provide unique insights into the nature of matter at high densities, because it provides a way of examining the bulk motion of the matter.Since March 2009, part of the Einstein@Home computing power has also been used to analyze data taken by the PALFA Consortium at the Arecibo Observatory in Puerto Rico. This search effort is designed to find radio pulsars in tight binary systems. It is expected that there is one radio pulsar detectable from Earth in an orbital system with a period of less than one hour. A similar search has also been performed on two archival data sets from the Parkes Multi-beam Pulsar Survey. The Einstein@Home radio pulsar search employs mathematical methods developed for the search for gravitational waves.Since July 2011, Einstein@Home is also analyzing data from the Large Area Telescope (LAT), the primary instrument on Fermi Gamma-ray Space Telescope to search for pulsed gamma-ray emission from spinning neutron stars (gamma-ray pulsars). Some neutron stars are only detectable by their pulsed gamma-ray emission, which originates in a different area of the neutron star magnetosphere than the radio emission. Identifying the neutron star's rotation rate is computationally difficult, because for a typical gamma-ray pulsar only thousands of gamma-ray photons will be detected by the LAT over the course of billions of rotations. The Einstein@Home analysis of the LAT data makes use of methods initially developed for the detection of continuous gravitational waves.\n",
      "-Einstein@Home uses the power of volunteer computing in solving the computationally intensive problem of analyzing a large volume of data. Such an approach was pioneered by the SETI@home project, which is designed to look for signs of extraterrestrial life by analyzing radio wave data. Einstein@Home runs through the same software platform as SETI@home, the Berkeley Open Infrastructure for Network Computing (BOINC). As of July 2022, more than 487,000 volunteers in 226 countries had participated in the project, making it the third-most-popular active BOINC application. Users regularly contribute about 12.7 petaFLOPS of computational power, which would rank Einstein@Home among the top 45 on the TOP500 list of supercomputers.\n",
      "\n",
      "\n",
      "\n",
      "What happens to an initially inhomogeneous physical system that is isolated by a thermodynamic operation?\n",
      "-An isolated physical system may be inhomogeneous, or may be composed of several subsystems separated from each other by walls. If an initially inhomogeneous physical system, without internal walls, is isolated by a thermodynamic operation, it will in general over time change its internal state. Or if it is composed of several subsystems separated from each other by walls, it may change its state after a thermodynamic operation that changes its walls. Such changes may include change of temperature or spatial distribution of temperature, by changing the state of constituent materials. A rod of iron, initially prepared to be hot at one end and cold at the other, when isolated, will change so that its temperature becomes uniform all along its length; during the process, the rod is not in thermal equilibrium until its temperature is uniform. In a system prepared as a block of ice floating in a bath of hot water, and then isolated, the ice can melt; during the melting, the system is not in thermal equilibrium; but eventually, its temperature will become uniform; the block of ice will not re-form. A system prepared as a mixture of petrol vapour and air can be ignited by a spark and produce carbon dioxide and water; if this happens in an isolated system, it will increase the temperature of the system, and during the increase, the system is not in thermal equilibrium; but eventually, the system will settle to a uniform temperature.\n",
      "-Two initial thermodynamic systems, each isolated in their separate states of internal thermodynamic equilibrium, can, by a thermodynamic operation, be coalesced into a single new final isolated thermodynamic system. If the initial systems differ in chemical constitution, then the eventual thermodynamic equilibrium of the final system can be the result of chemical reaction. Alternatively, an isolated thermodynamic system, in the absence of some catalyst, can be in a metastable equilibrium; introduction of a catalyst, or some other thermodynamic operation, such as release of a spark, can trigger a chemical reaction. The chemical reaction will, in general, transform some chemical potential energy into thermal energy. If the joint system is kept isolated, then its internal energy remains unchanged. Such thermal energy manifests itself, however, in changes in the non-chemical state variables (such as temperature, pressure, volume) of the joint systems, as well as the changes in the mole numbers of the chemical constituents that describe the chemical reaction.\n",
      "-A collection of matter may be entirely isolated from its surroundings. If it has been left undisturbed for an indefinitely long time, classical thermodynamics postulates that it is in a state in which no changes occur within it, and there are no flows within it. This is a thermodynamic state of internal equilibrium. (This postulate is sometimes, but not often, called the \"minus first\" law of thermodynamics. One textbook calls it the \"zeroth law\", remarking that the authors think this more befitting that title than its more customary definition, which apparently was suggested by Fowler.) Such states are a principal concern in what is known as classical or equilibrium thermodynamics, for they are the only states of the system that are regarded as well defined in that subject. A system in contact equilibrium with another system can by a thermodynamic operation be isolated, and upon the event of isolation, no change occurs in it. A system in a relation of contact equilibrium with another system may thus also be regarded as being in its own state of internal thermodynamic equilibrium.\n",
      "-Homogeneity in the absence of external forces A thermodynamic system consisting of a single phase in the absence of external forces, in its own internal thermodynamic equilibrium, is homogeneous. This means that the material in any small volume element of the system can be interchanged with the material of any other geometrically congruent volume element of the system, and the effect is to leave the system thermodynamically unchanged. In general, a strong external force field makes a system of a single phase in its own internal thermodynamic equilibrium inhomogeneous with respect to some intensive variables. For example, a relatively dense component of a mixture can be concentrated by centrifugation.\n",
      "-An isolated system obeys the conservation law that its total energy–mass stays constant. Most often, in thermodynamics, mass and energy are treated as separately conserved.\n",
      "\n",
      "\n",
      "\n",
      "What is the concept of simultaneity in Einstein's book, Relativity?\n",
      "-Absolute simultaneity refers to the concurrence of events in time at different locations in space in a manner agreed upon in all frames of reference. The theory of relativity does not have a concept of absolute time because there is a relativity of simultaneity. An event that is simultaneous with another event in one frame of reference may be in the past or future of that event in a different frame of reference,: 59  which negates absolute simultaneity.\n",
      "-In physics, the relativity of simultaneity is the concept that distant simultaneity – whether two spatially separated events occur at the same time – is not absolute, but depends on the observer's reference frame. This possibility was raised by mathematician Henri Poincaré in 1900, and thereafter became a central idea in the special theory of relativity.\n",
      "-Einstein (The Meaning of Relativity): \"Two events taking place at the points A and B of a system K are simultaneous if they appear at the same instant when observed from the middle point, M, of the interval AB. Time is then defined as the ensemble of the indications of similar clocks, at rest relative to K, which register the same simultaneously.\" Einstein wrote in his book, Relativity, that simultaneity is also relative, i.e., two events that appear simultaneous to an observer in a particular inertial reference frame need not be judged as simultaneous by a second observer in a different inertial frame of reference.\n",
      "-The simultaneity of both systems, whatever meaning is attributed to it, could only be observed by comparing two distant measurements, within the constraints of the speed of light. The simultaneity's influence cannot be causal, nor can it transmit information (which amounts to the same thing). This property is therefore compatible with the theory of relativity, according to which no information can travel faster than the speed of light.\n",
      "-A fuller explanation of the concept of coordinate time arises from its relations with proper time and with clock synchronization. Synchronization, along with the related concept of simultaneity, has to receive careful definition in the framework of general relativity theory, because many of the assumptions inherent in classical mechanics and classical accounts of space and time had to be removed. Specific clock synchronization procedures were defined by Einstein and give rise to a limited concept of simultaneity.Two events are called simultaneous in a chosen reference frame if and only if the chosen coordinate time has the same value for both of them; and this condition allows for the physical possibility and likelihood that they will not be simultaneous from the standpoint of another reference frame.But outside special relativity, the coordinate time is not a time that could be measured by a clock located at the place that nominally defines the reference frame, e.g. a clock located at the solar system barycenter would not measure the coordinate time of the barycentric reference frame, and a clock located at the geocenter would not measure the coordinate time of a geocentric reference frame.\n",
      "\n",
      "\n",
      "\n",
      "What is the Josephson effect?\n",
      "-In physics, the Josephson effect is a phenomenon that occurs when two superconductors are placed in proximity, with some barrier or restriction between them. It is an example of a macroscopic quantum phenomenon, where the effects of quantum mechanics are observable at ordinary, rather than atomic, scale. The Josephson effect has many practical applications because it exhibits a precise relationship between different physical measures, such as voltage and frequency, facilitating highly accurate measurements.\n",
      "-In 1962, Brian Josephson made the important theoretical prediction that a supercurrent can flow between two pieces of superconductor separated by a thin layer of insulator. This phenomenon, now called the Josephson effect, is exploited by superconducting devices such as SQUIDs. It is used in the most accurate available measurements of the magnetic flux quantum h/2e, and thus (coupled with the quantum Hall resistivity) for Planck's constant h. Josephson was awarded the Nobel Prize in Physics for this work in 1973.\n",
      "-In 1962, Josephson made the important theoretical prediction that a supercurrent can flow between two pieces of superconductor separated by a thin layer of insulator. This phenomenon, now called the Josephson effect, is exploited by superconducting devices such as SQUIDs. It is used in the most accurate available measurements of the magnetic flux quantum Φ0 = h/(2e), where h is the Planck constant. Coupled with the quantum Hall resistivity, this leads to a precise measurement of the Planck constant. Josephson was awarded the Nobel Prize for this work in 1973.In 2008, it was proposed that the same mechanism that produces superconductivity could produce a superinsulator state in some materials, with almost infinite electrical resistance. The first development and study of superconducting Bose–Einstein condensate (BEC) in 2020 suggests that there is a \"smooth transition between\" BEC and Bardeen-Cooper-Shrieffer regimes.\n",
      "-In 1962, the first commercial superconducting wire, a niobium-titanium alloy, was developed by researchers at Westinghouse, allowing the construction of the first practical superconducting magnets. In the same year, Josephson made the important theoretical prediction that a supercurrent can flow between two pieces of superconductor separated by a thin layer of insulator. This phenomenon, now called the Josephson effect, is exploited by superconducting devices such as SQUIDs. It is used in the most accurate available measurements of the magnetic flux quantum  Φ0=h2e , and thus (coupled with the quantum Hall resistivity) for the Planck constant h. Josephson was awarded the Nobel Prize for this work in 1973.\n",
      "-SQUIDs, or superconducting quantum interference devices, are very sensitive magnetometers that operate via the Josephson effect. They are widely used in science and engineering.\n",
      "In precision metrology, the Josephson effect provides an exactly reproducible conversion between frequency and voltage. Since the frequency is already defined precisely and practically by the caesium standard, the Josephson effect is used, for most practical purposes, to give the standard representation of a volt, the Josephson voltage standard.\n",
      "\n",
      "\n",
      "\n",
      "What is the SI unit of the physical quantity m/Q?\n",
      "-Units The SI unit for pressure is the pascal (Pa), equal to one newton per square metre (N/m2, or kg·m−1·s−2). This name for the unit was added in 1971; before that, pressure in SI was expressed simply in newtons per square metre.\n",
      "-Pascal (unit) – the SI unit of pressure, which uses the symbol Pa and is defined as one newton per square metre. It is also used to quantify internal pressure, stress, Young's modulus, and ultimate tensile strength.\n",
      "Physics – Pinion – Piston – Pitch drop experiment – Plain bearing – Plasma processing – Plasticity – Pneumatics – Poisson's ratio – Position vector – Potential difference – Power – the amount of energy transferred or converted per unit time. Power is a scalar quantity.\n",
      "-The metre per second is the unit of both speed (a scalar quantity) and velocity (a vector quantity, which has direction and magnitude) in the International System of Units (SI), equal to the speed of a body covering a distance of one metre in a time of one second.\n",
      "The SI unit symbols are m/s, m·s−1, m s−1, or m/s. Sometimes it is abbreviated as \"mps\".\n",
      "-The pascal can be expressed using SI derived units, or alternatively solely SI base units, as: 1Pa=1Nm2=1kgm⋅s2=1Jm3 where N is the newton, m is the metre, kg is the kilogram, s is the second, and J is the joule.One pascal is the pressure exerted by a force of magnitude one newton perpendicularly upon an area of one square metre.\n",
      "-This use of the unit of pressure provides an intuitive understanding for how a body's mass can apply force to a scale's surface area i.e.kilogram-force per square (centi-)metre.  In SI units, the unit is converted to the SI derived unit pascal (Pa), which is defined as one newton per square metre (N/m2). A newton is equal to 1 kg⋅m/s2, and a kilogram-force is 9.80665 N, meaning that 1 kgf/cm2 equals 98.0665 kilopascals (kPa).\n",
      "\n",
      "\n",
      "\n",
      "How many crystallographic point groups are there in three-dimensional space?\n",
      "-Up to conjugacy the set of three-dimensional point groups consists of 7 infinite series, and 7 other individual groups. In crystallography, only those point groups are considered which preserve some crystal lattice (so their rotations may only have order 1, 2, 3, 4, or 6). This crystallographic restriction of the infinite families of general point groups results in 32 crystallographic point groups (27 individual groups from the 7 series, and 5 of the 7 other individuals).\n",
      "-In the classification of crystals, each point group defines a so-called (geometric) crystal class. There are infinitely many three-dimensional point groups. However, the crystallographic restriction on the general point groups results in there being only 32 crystallographic point groups. These 32 point groups are one-and-the-same as the 32 types of morphological (external) crystalline symmetries derived in 1830 by Johann Friedrich Christian Hessel from a consideration of observed crystal forms.  The point group of a crystal determines, among other things, the directional variation of physical properties that arise from its structure, including optical properties such as birefringency, or electro-optical features such as the Pockels effect. For a periodic crystal (as opposed to a quasicrystal), the group must maintain the three-dimensional translational symmetry that defines crystallinity.\n",
      "-The trigonal crystal system consists of the 5 point groups that have a single three-fold rotation axis, which includes space groups 143 to 167. These 5 point groups have 7 corresponding space groups (denoted by R) assigned to the rhombohedral lattice system and 18 corresponding space groups (denoted by P) assigned to the hexagonal lattice system. Hence, the trigonal crystal system is the only crystal system whose point groups have more than one lattice system associated with their space groups.\n",
      "-The geometric symmetries of crystals are described by space groups, which allow translations and contain point groups as subgroups. Discrete point groups in more than one dimension come in infinite families, but from the crystallographic restriction theorem and one of Bieberbach's theorems, each number of dimensions has only a finite number of point groups that are symmetric over some lattice or grid with that number of dimensions. These are the crystallographic point groups.\n",
      "-§ The seven remaining point groups, which have multiple 3-or-more-fold rotation axes; these groups can also be characterized as point groups having multiple 3-fold rotation axes. The possible combinations are: Four 3-fold axes (the three tetrahedral symmetries T, Th, and Td) Four 3-fold axes and three 4-fold axes (octahedral symmetries O and Oh) Ten 3-fold axes and six 5-fold axes (icosahedral symmetries I and Ih)According to the crystallographic restriction theorem, only a limited number of point groups are compatible with discrete translational symmetry: 27 from the 7 infinite series, and 5 of the 7 others. Together, these make up the 32 so-called crystallographic point groups.\n",
      "\n",
      "\n",
      "\n",
      "What is the Liouville density?\n",
      "-A classical particle has a definite position and momentum, and hence it is represented by a point in phase space. Given a collection (ensemble) of particles, the probability of finding a particle at a certain position in phase space is specified by a probability distribution, the Liouville density. This strict interpretation fails for a quantum particle, due to the uncertainty principle. Instead, the above quasiprobability Wigner distribution plays an analogous role, but does not satisfy all the properties of a conventional probability distribution; and, conversely, satisfies boundedness properties unavailable to classical distributions.\n",
      "-In physics, Liouville's theorem, named after the French mathematician Joseph Liouville, is a key theorem in classical statistical and Hamiltonian mechanics. It asserts that the phase-space distribution function is constant along the trajectories of the system—that is that the density of system points in the vicinity of a given system point traveling through phase-space is constant with time. This time-independent density is in statistical mechanics known as the classical a priori probability.There are related mathematical results in symplectic topology and ergodic theory; systems obeying Liouville's theorem are examples of incompressible dynamical systems.\n",
      "-The evolution of an N-particle system in absence of quantum fluctuations is given by the Liouville equation for the probability density function  fN=fN(q1…qN,p1…pN,t) in 6N-dimensional phase space (3 space and 3 momentum coordinates per particle) ∂fN∂t+∑i=1Npim∂fN∂qi+∑i=1NFi∂fN∂pi=0, where  qi,pi are the coordinates and momentum for  i -th particle with mass  m , and the net force acting on the  i -th particle is ext ∂qi, where  Φij(qi,qj) is the pair potential for interaction between particles, and  ext (qi) is the external-field potential. By integration over part of the variables, the Liouville equation can be transformed into a chain of equations where the first equation connects the evolution of one-particle probability density function with the two-particle probability density function, second equation connects the two-particle probability density function with the three-particle probability density function, and generally the s-th equation connects the s-particle probability density function fs(q1…qs,p1…ps,t)=∫fN(q1…qN,p1…pN,t)dqs+1…dqNdps+1…dpN with the (s + 1)-particle probability density function: ∂fs∂t+∑i=1spim∂fs∂qi−∑i=1s(∑j=1≠is∂Φij∂qi+∂Φiext∂qi)∂fs∂pi=(N−s)∑i=1s∫∂Φis+1∂qi∂fs+1∂pidqs+1dps+1.\n",
      "-However, the meaning of Liouville’s theorem in mechanics is rather different from the theorem of conservation of étendue. Liouville’s theorem is essentially statistical in nature, and it refers to the evolution in time of an ensemble of mechanical systems of identical properties but with different initial conditions. Each system is represented by a single point in phase space, and the theorem states that the average density of points in phase space is constant in time. An example would be the molecules of a perfect classical gas in equilibrium in a container. Each point in phase space, which in this example has 2N dimensions, where N is the number of molecules, represents one of an ensemble of identical containers, an ensemble large enough to permit taking a statistical average of the density of representative points. Liouville’s theorem states that if all the containers remain in equilibrium, the average density of points remains constant.\n",
      "-The starting point is the quantum mechanical version of the von Neumann equation, also known as the Liouville equation: ∂tρ=iℏ[ρ,H]=Lρ, where the Liouville operator  L is defined as  LA=iℏ[A,H] The density operator (density matrix)  ρ is split by means of a projection operator P into two parts  ρ=(P+Q)ρ ,  where  Q≡1−P . The projection operator  P selects the aforementioned relevant part from the density operator, for which an equation of motion is to be derived.\n",
      "\n",
      "\n",
      "\n",
      "What are the four qualitative levels of crystallinity described by geologists?\n",
      "-Geologists describe four qualitative levels of crystallinity: holocrystalline rocks are completely crystalline; hypocrystalline rocks are partially crystalline, with crystals embedded in an amorphous or glassy matrix; hypohyaline rocks are partially glassy; holohyaline rocks (such as obsidian) are completely glassy.\n",
      "-Examples include: Serpentine subgroup Antigorite – Mg3Si2O5(OH)4 Chrysotile – Mg3Si2O5(OH)4 Lizardite – Mg3Si2O5(OH)4 Clay minerals group 1:1 clay minerals (TO) Halloysite – Al2Si2O5(OH)4 Kaolinite – Al2Si2O5(OH)4 2:1 clay minerals (TOT) Pyrophyllite – Al2Si4O10(OH)2 Talc – Mg3Si4O10(OH)2 Illite – (K,H3O)(Al,Mg,Fe)2(Si,Al)4O10[(OH)2,(H2O)] Montmorillonite (smectite) – (Na,Ca)0.33(Al,Mg)2Si4O10(OH)2·nH2O Chlorite – (Mg,Fe)3(Si,Al)4O10(OH)2·(Mg,Fe)3(OH)6 Vermiculite – (Mg,Fe,Al)3(Al,Si)4O10(OH)2·4H2O Other clay minerals Sepiolite – Mg4Si6O15(OH)2·6H2O Palygorskite (or attapulgite) – (Mg,Al)2Si4O10(OH)·4(H2O) Mica group Biotite – K(Mg,Fe)3(AlSi3)O10(OH)2 Fuchsite – K(Al,Cr)2(AlSi3)O10(OH)2 Muscovite – KAl2(AlSi3)O10(OH)2 Phlogopite – KMg3(AlSi3)O10(OH)2 Lepidolite – K(Li,Al)2–3(AlSi3)O10(OH)2 Margarite – CaAl2(Al2Si2)O10(OH)2 Glauconite – (K,Na)(Al,Mg,Fe)2(Si,Al)4O10(OH)2 \n",
      "-The crystal structure is the arrangement of atoms in a crystal. It is represented by a lattice of points which repeats a basic pattern, called a unit cell, in three dimensions. The lattice can be characterized by its symmetries and by the dimensions of the unit cell. These dimensions are represented by three Miller indices.: 91–92  The lattice remains unchanged by certain symmetry operations about any given point in the lattice: reflection, rotation, inversion, and rotary inversion, a combination of rotation and reflection. Together, they make up a mathematical object called a crystallographic point group or crystal class. There are 32 possible crystal classes. In addition, there are operations that displace all the points: translation, screw axis, and glide plane. In combination with the point symmetries, they form 230 possible space groups.: 125–126 Most geology departments have X-ray powder diffraction equipment to analyze the crystal structures of minerals.: 54–55  X-rays have wavelengths that are the same order of magnitude as the distances between atoms. Diffraction, the constructive and destructive interference between waves scattered at different atoms, leads to distinctive patterns of high and low intensity that depend on the geometry of the crystal. In a sample that is ground to a powder, the X-rays sample a random distribution of all crystal orientations. Powder diffraction can distinguish between minerals that may appear the same in a hand sample, for example quartz and its polymorphs tridymite and cristobalite.: 54 Isomorphous minerals of different compositions have similar powder diffraction patterns, the main difference being in spacing and intensity of lines. For example, the NaCl (halite) crystal structure is space group Fm3m; this structure is shared by sylvite (KCl), periclase (MgO), bunsenite (NiO), galena (PbS), alabandite (MnS), chlorargyrite (AgCl), and osbornite (TiN).: 150–151 \n",
      "-Crystal structure and habit Crystal structure results from the orderly geometric spatial arrangement of atoms in the internal structure of a mineral. This crystal structure is based on regular internal atomic or ionic arrangement that is often expressed in the geometric form that the crystal takes. Even when the mineral grains are too small to see or are irregularly shaped, the underlying crystal structure is always periodic and can be determined by X-ray diffraction. Minerals are typically described by their symmetry content. Crystals are restricted to 32 point groups, which differ by their symmetry. These groups are classified in turn into more broad categories, the most encompassing of these being the six crystal families.These families can be described by the relative lengths of the three crystallographic axes, and the angles between them; these relationships correspond to the symmetry operations that define the narrower point groups. They are summarized below; a, b, and c represent the axes, and α, β, γ represent the angle opposite the respective crystallographic axis (e.g. α is the angle opposite the a-axis, viz. the angle between the b and c axes): The hexagonal crystal family is also split into two crystal systems – the trigonal, which has a three-fold axis of symmetry, and the hexagonal, which has a six-fold axis of symmetry.\n",
      "-When an element has allotropes with different densities, one representative allotrope is typically selected in summary presentations, while densities for each allotrope can be stated where more detail is provided. For example, the three familiar allotropes of carbon (amorphous carbon, graphite, and diamond) have densities of 1.8–2.1, 2.267, and 3.515 g/cm3, respectively.\n",
      "Crystal structures The elements studied to date as solid samples have eight kinds of crystal structures: cubic, body-centered cubic, face-centered cubic, hexagonal, monoclinic, orthorhombic, rhombohedral, and tetragonal. For some of the synthetically produced transuranic elements, available samples have been too small to determine crystal structures.\n",
      "Occurrence and origin on Earth Chemical elements may also be categorized by their origin on Earth, with the first 94 considered naturally occurring, while those with atomic numbers beyond 94 have only been produced artificially as the synthetic products of human-made nuclear reactions.\n",
      "\n",
      "\n",
      "\n",
      "What is an order parameter?\n",
      "-Order parameters An order parameter is a measure of the degree of order across the boundaries in a phase transition system; it normally ranges between zero in one phase (usually above the critical point) and nonzero in the other. At the critical point, the order parameter susceptibility will usually diverge.\n",
      "An example of an order parameter is the net magnetization in a ferromagnetic system undergoing a phase transition. For liquid/gas transitions, the order parameter is the difference of the densities.\n",
      "-From a theoretical perspective, order parameters arise from symmetry breaking. When this happens, one needs to introduce one or more extra variables to describe the state of the system. For example, in the ferromagnetic phase, one must provide the net magnetization, whose direction was spontaneously chosen when the system cooled below the Curie point. However, note that order parameters can also be defined for non-symmetry-breaking transitions.\n",
      "-A parameter (from Ancient Greek παρά (pará) 'beside, subsidiary', and μέτρον (métron) 'measure'), generally, is any characteristic that can help in defining or classifying a particular system (meaning an event, project, object, situation, etc.). That is, a parameter is an element of a system that is useful, or critical, when identifying the system, or when evaluating its performance, status, condition, etc.\n",
      "-Pair distribution function – This parameter is usually used in physics to characterize the degree of spatial order in a system of particles. It also describes the density, but this measure describes the density at a distance away from a given point. Cavagna et al. found that flocks of starlings exhibited more structure than a gas but less than a liquid.\n",
      "-Essential in synergetics is the order-parameter concept which was originally introduced in the Ginzburg–Landau theory in order to describe phase transitions in thermodynamics. The order parameter concept is generalized by Haken to the \"enslaving-principle\" saying that the dynamics of fast-relaxing (stable) modes is completely determined by the 'slow' dynamics of, as a rule, only a few 'order-parameters' (unstable modes). The order parameters can be interpreted as the amplitudes of the unstable modes determining the macroscopic pattern.\n",
      "\n",
      "\n",
      "\n",
      "What is the significance of the discovery of the Crab pulsar?\n",
      "-The discovery of the Crab pulsar provided confirmation of the rotating neutron star model of pulsars. The Crab pulsar 33-millisecond pulse period was too short to be consistent with other proposed models for pulsar emission. Moreover, the Crab pulsar is so named because it is located at the center of the Crab Nebula, consistent with the 1933 prediction of Baade and Zwicky.\n",
      "-In 1968, Richard V. E. Lovelace and collaborators discovered period  33 ms of the Crab pulsar using Arecibo Observatory. After this discovery, scientists concluded that pulsars were rotating neutron stars. Before that, many scientists believed that pulsars were pulsating white dwarfs.\n",
      "-Neutron stars Pulsars Supernovae sometimes leave behind dense spinning neutron stars called pulsars. They emit jets of charged particles which emit synchrotron radiation in the radio spectrum. Examples include the Crab Pulsar, the first pulsar to be discovered. Pulsars and quasars (dense central cores of extremely distant galaxies) were both discovered by radio astronomers. In 2003 astronomers using the Parkes radio telescope discovered two pulsars orbiting each other, the first such system known.\n",
      "-On 11 January 2017, the first results from a survey of 118 unidentified pulsar-like sources from the Fermi-LAT Catalog were published. A total of 13 new pulsars were found. Most of them are young and were formed in supernovae several tens to hundreds of thousands of years ago. The discoveries and the methods used in the survey were published in the first of two associated papers. The second paper reports faint radio pulsations from two of the 13 gamma-ray pulsars, and presents modeling of the gamma-ray and radio pulse profiles with different geometric emission models.The discovery of two millisecond pulsars discovered by Einstein@Home through their pulsed gamma radiation was published on 28 February 2018. PSR J1035−6720, spinning at 348 Hertz, has detectable radio pulsations which were found in follow-up searches. The other discovery PSR J1744−7619 is the first radio-quiet millisecond pulsar ever discovered. The project also announced that it was searching for gamma-ray pulsars in binary systems, which are more difficult to find due to the additional orbital parameters.The first Einstein@Home discovery of a gamma-ray pulsar in a binary system was published on 22 October 2020. PSR J1653-0158, a neutron star with about two solar masses and one of the highest known rotation frequencies of 508 Hertz, orbits the common center of mass with a companion of only 1% of the Sun’s mass. The orbital period is 75 minutes, shorter than that of any comparable binary systems. The discovery was made using a GPU-accelerated version of a modified gamma-ray pulsar search code, which included binary orbital parameters. No radio waves were found in follow-up searches. A search for gravitational waves from the pulsar discovered no such emission. The pulsar is from a class known as black widow pulsars. The pulsar evaporates its companion with its energetic radiation and a particle wind. The ablated material fills the binary system with a cloud of plasma absorbing radio waves, but not gamma radiation.\n",
      "-The word \"pulsar\" first appeared in print in 1968: An entirely novel kind of star came to light on Aug. 6 last year and was referred to, by astronomers, as LGM (Little Green Men). Now it is thought to be a novel type between a white dwarf and a neutron [star]. The name Pulsar is likely to be given to it. Dr. A. Hewish told me yesterday: '... I am sure that today every radio telescope is looking at the Pulsars.' The existence of neutron stars was first proposed by Walter Baade and Fritz Zwicky in 1934, when they argued that a small, dense star consisting primarily of neutrons would result from a supernova. Based on the idea of magnetic flux conservation from magnetic main sequence stars, Lodewijk Woltjer proposed in 1964 that such neutron stars might contain magnetic fields as large as 1014 to 1016 gauss (=1010 to 1012 tesla). In 1967, shortly before the discovery of pulsars, Franco Pacini suggested that a rotating neutron star with a magnetic field would emit radiation, and even noted that such energy could be pumped into a supernova remnant around a neutron star, such as the Crab Nebula. After the discovery of the first pulsar, Thomas Gold independently suggested a rotating neutron star model similar to that of Pacini, and explicitly argued that this model could explain the pulsed radiation observed by Bell Burnell and Hewish.  In 1968, Richard V. E. Lovelace with collaborators discovered period  33 ms of the Crab Nebula pulsar using Arecibo Observatory.\n",
      "\n",
      "\n",
      "\n",
      "What is the De Haas-Van Alphen effect?\n",
      "-Several experimental techniques allow for the measurement of the electronic properties of a material. An important effect in metals under strong magnetic fields, is the oscillation of the differential susceptibility as function of 1/H. This behaviour is known as the De Haas–Van Alphen effect and relates the period of the susceptibility with the Fermi surface of the material.\n",
      "An analogue non-linear relation between magnetization and magnetic field happens for antiferromagnetic materials.\n",
      "-When the magnetic susceptibility is measured in response to an AC magnetic field (i.e. a magnetic field that varies sinusoidally), this is called AC susceptibility. AC susceptibility (and the closely related \"AC permeability\") are complex number quantities, and various phenomena, such as resonance, can be seen in AC susceptibility that cannot occur in constant-field (DC) susceptibility. In particular, when an AC field is applied perpendicular to the detection direction (called the \"transverse susceptibility\" regardless of the frequency), the effect has a peak at the ferromagnetic resonance frequency of the material with a given static applied field. Currently, this effect is called the microwave permeability or network ferromagnetic resonance in the literature. These results are sensitive to the domain wall configuration of the material and eddy currents.\n",
      "-The Fermi gas (an ensemble of non-interacting fermions) is part of the basis for understanding of the thermodynamic properties of metals. In 1930 Landau derived an estimate for the magnetic susceptibility of a Fermi gas, known as Landau susceptibility, which is constant for small magnetic fields. Landau also noticed that the susceptibility oscillates with high frequency for large magnetic fields, this physical phenomenon is known as the De Haas–Van Alphen effect.\n",
      "-The differential magnetic susceptibility of a material is defined as χ=∂M∂H where  H is the applied external magnetic field and  M the magnetization of the material. Such that  B=μ0(H+M) , where  μ0 is the vacuum permeability. For practical purposes, the applied and the measured field are approximately the same  B≈μ0H (if the material is not ferromagnetic).\n",
      "-The primary measurement in magnetochemistry is magnetic susceptibility. This measures the strength of interaction on placing the substance in a magnetic field. The volume magnetic susceptibility, represented by the symbol  χv is defined by the relationship M→=χvH→ where,  M→ is the magnetization of the material (the magnetic dipole moment per unit volume), measured in amperes per meter (SI units), and  H→ is the magnetic field strength, also measured in amperes per meter. Susceptibility is a dimensionless quantity. For chemical applications the molar magnetic susceptibility (χmol) is the preferred quantity. It is measured in m3·mol−1 (SI) or cm3·mol−1 (CGS) and is defined as mol =Mχv/ρ where ρ is the density in kg·m−3 (SI) or g·cm−3 (CGS) and M is molar mass in kg·mol−1 (SI) or g·mol−1 (CGS).\n",
      "\n",
      "\n",
      "\n",
      "What is a \"coffee ring\" in physics?\n",
      "-In physics, a \"coffee ring\" is a pattern left by a puddle of particle-laden liquid after it evaporates. The phenomenon is named for the characteristic ring-like deposit along the perimeter of a spill of coffee. It is also commonly seen after spilling red wine. The mechanism behind the formation of these and similar rings is known as the coffee ring effect or in some instances, the coffee stain effect, or simply ring stain.\n",
      "-The coffee-ring pattern originates from the capillary flow induced by the evaporation of the drop: liquid evaporating from the edge is replenished by liquid from the interior. The resulting current can carry nearly all the dispersed material to the edge. As a function of time, this process exhibits a \"rush-hour\" effect, that is, a rapid acceleration of the flow towards the edge at the final stage of the drying process.Evaporation induces a Marangoni flow inside a droplet. The flow, if strong, redistributes particles back to the center of the droplet. Thus, for particles to accumulate at the edges, the liquid must have a weak Marangoni flow, or something must occur to disrupt the flow. For example, surfactants can be added to reduce the liquid's surface tension gradient, disrupting the induced flow. Water has a weak Marangoni flow to begin with, which is then reduced significantly by natural surfactants.Interaction of the particles suspended in a droplet with the free surface of the droplet is important in creating a coffee ring. \"When the drop evaporates, the free surface collapses and traps the suspended particles ... eventually all the particles are captured by the free surface and stay there for the rest of their trip towards the edge of the drop.\" This result means that surfactants can be used to manipulate the motion of the solute particles by changing the surface tension of the drop, rather than trying to control the bulk flow inside the drop. A number of interesting morphologies of the deposited particles can result. For example, an enantiopure poly (isocyanate) derivative has been shown to form ordered arrays of squashed donut structures.\n",
      "-Coffee preparation is the process of turning coffee beans into liquid coffee. While the particular steps vary with the type of coffee and with the raw materials, the process includes four basic steps: raw coffee beans must be roasted, the roasted coffee beans must then be ground, and the ground coffee must then be mixed with hot or cold water (depending on the method of brewing) for a specific time (brewed), the liquid coffee extraction must be separated from the used grounds, and finally, if desired, the extracted coffee is combined with other elements of the desired beverage, such as sweeteners, dairy products, dairy alternatives, or toppings (such as shaved chocolate).  Coffee is usually brewed hot, at close to the boiling point of water, immediately before drinking, yielding a hot beverage capable of scalding if splashed or spilled; if not consumed promptly, coffee is often sealed into a vacuum flask or insulated bottle to maintain its temperature. In most areas, coffee may be purchased unprocessed, or already roasted, or already roasted and ground. Whole roast coffee or ground coffee is often vacuum-packed to prevent oxidation and lengthen its shelf life. Especially in hot climates, some find cold or iced coffee more refreshing. This can be prepared well in advance as it maintains its character when stored cold better than as a hot beverage.  Even with the same roast, the character of the extraction is highly dependent on distribution of particle sizes produced by the grinding process, temperature of the grounds after grinding, freshness of the roast and grind, brewing process and equipment, temperature of the water, character of the water itself, contact time with hot water (less sensitive with cold water), and the brew ratio employed. Preferred brew ratios of water to coffee often fall into the range of 15–18:1 by mass; even within this fairly small range, differences are easily perceived by an experienced coffee drinker. Processes can range from extremely manual (e.g. hand grinding with manual pour-over in steady increments) to totally automated by a single appliance with a reservoir of roast beans which it automatically measures and grinds, and water, which it automatically heats and doses. Another common style of automated coffee maker is fed a single-serving \"pod\" of pre-measured coffee grounds for each beverage.  Characteristics which may be emphasized or deemphasized by different preparation methods include: acidity (brightness), aroma (especially more delicate floral and citrus notes), mouthfeel (body), astringency, bitterness (both positive and negative), and the duration and intensity of flavour perception in the mouth (finish). The addition of sweeteners, dairy products (e.g. milk or cream), or dairy alternatives (e.g. almond milk) also changes the perceived character of the brewed coffee. Principally, dairy products mute delicate aromas and thicken mouthfeel (particularly when frothed), while sweeteners mask astringency and bitterness.\n",
      "-Coffee preparation is the process of turning coffee beans into liquid coffee. While the particular steps vary with the type of coffee and with the raw materials, the process includes four basic steps: raw coffee beans must be roasted, the roasted coffee beans must then be ground, and the ground coffee must then be mixed with hot or cold water (depending on the method of brewing) for a specific time (brewed), the liquid coffee extraction must be separated from the used grounds, and finally, if desired, the extracted coffee is combined with other elements of the desired beverage, such as sweeteners, dairy products, dairy alternatives, or toppings (such as shaved chocolate).  Coffee is usually brewed hot, at close to the boiling point of water, immediately before drinking, yielding a hot beverage capable of scalding if splashed or spilled; if not consumed promptly, coffee is often sealed into a vacuum flask or insulated bottle to maintain its temperature. In most areas, coffee may be purchased unprocessed, or already roasted, or already roasted and ground. Whole roast coffee or ground coffee is often vacuum-packed to prevent oxidation and lengthen its shelf life. Especially in hot climates, some find cold or iced coffee more refreshing. This can be prepared well in advance as it maintains its character when stored cold better than as a hot beverage.  Even with the same roast, the character of the extraction is highly dependent on distribution of particle sizes produced by the grinding process, temperature of the grounds after grinding, freshness of the roast and grind, brewing process and equipment, temperature of the water, character of the water itself, contact time with hot water (less sensitive with cold water), and the brew ratio employed. Preferred brew ratios of water to coffee often fall into the range of 15–18:1 by mass; even within this fairly small range, differences are easily perceived by an experienced coffee drinker. Processes can range from extremely manual (e.g. hand grinding with manual pour-over in steady increments) to totally automated by a single appliance with a reservoir of roast beans which it automatically measures and grinds, and water, which it automatically heats and doses. Another common style of automated coffee maker is fed a single-serving \"pod\" of pre-measured coffee grounds for each beverage.  Characteristics which may be emphasized or deemphasized by different preparation methods include: acidity (brightness), aroma (especially more delicate floral and citrus notes), mouthfeel (body), astringency, bitterness (both positive and negative), and the duration and intensity of flavour perception in the mouth (finish). The addition of sweeteners, dairy products (e.g. milk or cream), or dairy alternatives (e.g. almond milk) also changes the perceived character of the brewed coffee. Principally, dairy products mute delicate aromas and thicken mouthfeel (particularly when frothed), while sweeteners mask astringency and bitterness.\n",
      "-A coffeemaker, coffee maker or coffee machine is a cooking appliance used to brew coffee. While there are many different types of coffeemakers, the two most common brewing principles use gravity or pressure to move hot water through coffee grounds. In the most common devices, coffee grounds are placed into a paper or metal filter inside a funnel, which is set over a glass or ceramic coffee pot, a cooking pot in the kettle family. Cold water is poured into a separate chamber, which is then boiled and directed into the funnel and allowed to drip through the grounds under gravity. This is also called automatic drip-brew. Coffee makers that use pressure to force water through the coffee grounds are called espresso makers, and they produce espresso coffee.\n",
      "\n",
      "\n",
      "\n",
      "What is the significance of probability amplitudes in quantum mechanics?\n",
      "-In quantum mechanics, a probability amplitude is a complex number used for describing the behaviour of systems. The modulus squared of this quantity represents a probability density.\n",
      "-Probability amplitudes have special significance because they act in quantum mechanics as the equivalent of conventional probabilities, with many analogous laws, as described above. For example, in the classic double-slit experiment, electrons are fired randomly at two slits, and the probability distribution of detecting electrons at all parts on a large screen placed behind the slits, is questioned. An intuitive answer is that P(through either slit) = P(through first slit) + P(through second slit), where P(event) is the probability of that event. This is obvious if one assumes that an electron passes through either slit. When no measurement apparatus that determines through which slit the electrons travel is installed, the observed probability distribution on the screen reflects the interference pattern that is common with light waves. If one assumes the above law to be true, then this pattern cannot be explained. The particles cannot be said to go through either slit and the simple explanation does not work. The correct explanation is, however, by the association of probability amplitudes to each event. The complex amplitudes which represent the electron passing each slit (ψfirst and ψsecond) follow the law of precisely the form expected: ψtotal = ψfirst + ψsecond. This is the principle of quantum superposition. The probability, which is the modulus squared of the probability amplitude, then, follows the interference pattern under the requirement that amplitudes are complex: Here,  φ1 and  φ2 are the arguments of ψfirst and ψsecond respectively. A purely real formulation has too few dimensions to describe the system's state when superposition is taken into account. That is, without the arguments of the amplitudes, we cannot describe the phase-dependent interference. The crucial term  first second cos {\\textstyle 2\\left|\\psi _{\\text{first}}\\right|\\left|\\psi _{\\text{second}}\\right|\\cos(\\varphi _{1}-\\varphi _{2})} is called the \"interference term\", and this would be missing if we had added the probabilities.\n",
      "-That basic scaffolding remains when one moves to a quantum description, but some conceptual changes are needed. One is that whereas we might expect in our everyday life that there would be some constraints on the points to which a particle can move, that is not true in full quantum electrodynamics. There is a nonzero probability amplitude of an electron at A, or a photon at B, moving as a basic action to any other place and time in the universe. That includes places that could only be reached at speeds greater than that of light and also earlier times. (An electron moving backwards in time can be viewed as a positron moving forward in time.): 89, 98–99 Probability amplitudes Quantum mechanics introduces an important change in the way probabilities are computed. Probabilities are still represented by the usual real numbers we use for probabilities in our everyday world, but probabilities are computed as the square modulus of probability amplitudes, which are complex numbers.\n",
      "-Probability amplitudes The probability for a photon to be in a particular polarization state depends on the probability distribution over the fields as calculated by the classical Maxwell's equations (in the Glauber-Sudarshan P-representation of a one-photon Fock state.) The expectation value of the photon number in a coherent state in a limited region of space is quadratic in the fields. In quantum mechanics, by analogy, the state or probability amplitude of a single particle contains the basic probability information. In general, the rules for combining probability amplitudes look very much like the classical rules for composition of probabilities: The probability amplitude for two successive probabilities is the product of amplitudes for the individual possibilities. ...\n",
      "-The nature of probability in quantum mechanics Probability for a single photon There are two ways in which probability can be applied to the behavior of photons; probability can be used to calculate the probable number of photons in a particular state, or probability can be used to calculate the likelihood of a single photon to be in a particular state. The former interpretation violates energy conservation. The latter interpretation is the viable, if nonintuitive, option. Dirac explains this in the context of the double-slit experiment: Some time before the discovery of quantum mechanics people realized that the connection between light waves and photons must be of a statistical character. What they did not clearly realize, however, was that the wave function gives information about the probability of one photon being in a particular place and not the probable number of photons in that place. The importance of the distinction can be made clear in the following way. Suppose we have a beam of light consisting of a large number of photons split up into two components of equal intensity. On the assumption that the beam is connected with the probable number of photons in it, we should have half the total number going into each component. If the two components are now made to interfere, we should require a photon in one component to be able to interfere with one in the other. Sometimes these two photons would have to annihilate one another and other times they would have to produce four photons. This would contradict the conservation of energy. The new theory, which connects the wave function with probabilities for one photon gets over the difficulty by making each photon go partly into each of the two components. Each photon then interferes only with itself. Interference between two different photons never occurs.—Paul Dirac, The Principles of Quantum Mechanics, 1930, Chapter 1 Probability amplitudes The probability for a photon to be in a particular polarization state depends on the fields as calculated by the classical Maxwell's equations. The polarization state of the photon is proportional to the field. The probability itself is quadratic in the fields and consequently is also quadratic in the quantum state of polarization. In quantum mechanics, therefore, the state or probability amplitude contains the basic probability information. In general, the rules for combining probability amplitudes look very much like the classical rules for composition of probabilities: [The following quote is from Baym, Chapter 1] The probability amplitude for two successive probabilities is the product of amplitudes for the individual possibilities. For example, the amplitude for the x polarized photon to be right circularly polarized and for the right circularly polarized photon to pass through the y-polaroid is  ⟨R|x⟩⟨y|R⟩, the product of the individual amplitudes.\n",
      "\n",
      "\n",
      "\n",
      "What is the relationship between the amplitude of a sound wave and its loudness?\n",
      "-The amplitude of sound waves and audio signals (which relates to the volume) conventionally refers to the amplitude of the air pressure in the wave, but sometimes the amplitude of the displacement (movements of the air or the diaphragm of a speaker) is described. The logarithm of the amplitude squared is usually quoted in dB, so a null amplitude corresponds to −∞ dB. Loudness is related to amplitude and intensity and is one of the most salient qualities of a sound, although in general sounds it can be recognized independently of amplitude. The square of the amplitude is proportional to the intensity of the wave.\n",
      "-Important basic characteristics of waves are wavelength, amplitude, period, and frequency. Wavelength is the length of the repeating wave shape. Amplitude is the maximum displacement of the particles of the medium, which is determined by the energy of the wave. A period (measured in seconds) is the time for one wave to pass a given point. Frequency of the wave is the number of waves passing a given point in a unit of time. Frequency is measured in hertz (hz); (Hz cycles per second) and is perceived as pitch. Each complete vibration of a sound wave is called a cycle. Two other physical properties of sound are intensity and duration. Intensity is measured in decibels (dB) and is perceived as loudness.\n",
      "-The ordinary frequency (f) of the wave is given by f=ω2π.\n",
      "The wavelength can be calculated as the relation between a wave's speed and ordinary frequency.\n",
      "λ=cf.\n",
      "For sound waves, the amplitude of the wave is the difference between the pressure of the undisturbed air and the maximum pressure caused by the wave.\n",
      "Sound's propagation speed depends on the type, temperature, and composition of the medium through which it propagates.\n",
      "-The perception of loudness is related to sound pressure level (SPL), frequency content and duration of a sound. The relationship between SPL and loudness of a single tone can be approximated by Stevens's power law in which SPL has an exponent of 0.67. A more precise model known as the Inflected Exponential function, indicates that loudness increases with a higher exponent at low and high levels and with a lower exponent at moderate levels.The sensitivity of the human ear changes as a function of frequency, as shown in the equal-loudness graph. Each line on this graph shows the SPL required for frequencies to be perceived as equally loud, and different curves pertain to different sound pressure levels. It also shows that humans with normal hearing are most sensitive to sounds around 2–4 kHz, with sensitivity declining to either side of this region. A complete model of the perception of loudness will include the integration of SPL by frequency.Historically, loudness was measured using an \"ear-balance\" audiometer in which the amplitude of a sine wave was adjusted by the user to equal the perceived loudness of the sound being evaluated. Contemporary standards for measurement of loudness are based on the summation of energy in critical bands.\n",
      "-Sound power is related to sound intensity: P=AI, where A stands for the area; I stands for the sound intensity.Sound power is related sound energy density: P=Acw, where c stands for the speed of sound; w stands for the sound energy density.\n",
      "\n",
      "\n",
      "\n",
      "What are coherent turbulent structures?\n",
      "-Turbulent flows are complex multi-scale and chaotic motions that need to be classified into more elementary components, referred to coherent turbulent structures. Such a structure must have temporal coherence, i.e. it must persist in its form for long enough periods that the methods of time-averaged statistics can be applied. Coherent structures are typically studied on very large scales, but can be broken down into more elementary structures with coherent properties of their own, such examples include hairpin vortices. Hairpins and coherent structures have been studied and noticed in data since the 1930s, and have been since cited in thousands of scientific papers and reviews.\n",
      "-A turbulent flow is a flow regime in fluid dynamics where fluid velocity varies significantly and irregularly in both position and time. Furthermore, a coherent structure is defined as a turbulent flow whose vorticity expression, which is usually stochastic, contains orderly components that can be described as being instantaneously coherent over the spatial extent of the flow structure. In other words, underlying the three-dimensional chaotic vorticity expressions typical of turbulent flows, there is an organized component of that vorticity which is phase-correlated over the entire space of the structure. The instantaneously space and phase correlated vorticity found within the coherent structure expressions can be defined as coherent vorticity, hence making coherent vorticity the main characteristic identifier for coherent structures. Another characteristic inherent in turbulent flows is their intermittency, but intermittency is a very poor identifier of the boundaries of a coherent structure, hence it is generally accepted that the best way to characterize the boundary of a structure is by identifying and defining the boundary of the coherent vorticity.By defining and identifying coherent structure in this manner, turbulent flows can be decomposed into coherent structures and incoherent structures depending on their coherence, particularly their correlations with their vorticity. Hence, similarly organized events in an ensemble average of organized events can be defined as a coherent structure, and whatever events not identified as similar or phase and space aligned in the ensemble average is an incoherent turbulent structure.  Other attempts at defining a coherent structure can be done through examining the correlation between their momenta or pressure and their turbulent flows. However, it often leads to false indications of turbulence, since pressure and velocity fluctuations over a fluid could be well correlated in the absence of any turbulence or vorticity. Some coherent structures, such as vortex rings, etc. can be large-scale motions comparable to the extent of the shear flow. There are also coherent motions at much smaller scales such as hairpin vortices and typical eddies, which are typically known as coherent substructures, as in coherent structures which can be broken up into smaller more elementary substructures.\n",
      "-Although a coherent structure is by definition characterized by high levels of coherent vorticity, Reynolds stress, production, and heat and mass transportation, it does not necessarily require a high level of kinetic energy. In fact, one of the main roles of coherent structures is the large-scale transport of mass, heat, and momentum without requiring the high amounts of energy normally needed. Consequently, this implies that coherent structures are not the main production and cause of Reynolds stress, and incoherent turbulence can be similarly significant.Coherent structures cannot superimpose, i.e. they cannot overlap and each coherent structure has its own independent domain and boundary. Since eddies coexist as spatial superpositions, a coherent structure is not an eddy. For example, eddies dissipate energy by obtaining energy from the mean flow at large scales, and eventually dissipating it at the smallest scales. There is no such analogous exchange of energy between coherent structures, and any interaction such as tearing between coherent structures simply results in a new structure. However, two coherent structures can interact and influence each other. The mass of a structure change with time, with the typical case being that structures increase in volume via the diffusion of vorticity.  One of the most fundamental quantities of coherent structures is characterized by coherent vorticity,  Ωc . Perhaps the next most critical measures of coherent structures are the coherent vs. incoherent Reynold's stresses,  −ucνc and  −⟨urνr⟩ . These represent the transports of momentum, and their relative strength indicates how much momentum is being transported by coherent structures as compared to incoherent structures. The next most significant measures include contoured depictions of coherent strain rate and shear production. A useful property of such contours is that they are invariant under Galilean transformations, hence the contours of coherent vorticity constitute an excellent identifier to the structure's boundaries. The contours of these properties not only locate where exactly coherent structure quantities have their peaks and saddles, but also identify where the incoherent turbulent structures are when overlaid on their directional gradients. In addition, spatial contours can be drawn describe the shape, size, and strength of the coherent structures, depicting not only the mechanics but also the dynamical evolution of coherent structures. For example, in order for a structure to be evolving, and hence dominant, its coherent vorticity, coherent Reynolds stress, and production terms should be larger than the time averaged values of the flow structures.\n",
      "-In many geophysical flows (rivers, atmospheric boundary layer), the flow turbulence is dominated by the coherent structures and turbulent events. A turbulent event is a series of turbulent fluctuations that contain more energy than the average flow turbulence. The turbulent events are associated with coherent flow structures such as eddies and turbulent bursting, and they play a critical role in terms of sediment scour, accretion and transport in rivers as well as contaminant mixing and dispersion in rivers and estuaries, and in the atmosphere.\n",
      "-Flow visualization experiments, using smoke and dye as tracers, have been historically used to simulate coherent structures and verify theories, but computer models are now the dominant tools widely used in the field to verify and understand the formation, evolution, and other properties of such structures. The kinematic properties of these motions include size, scale, shape, vorticity, energy, and the dynamic properties govern the way coherent structures grow, evolve, and decay. Most coherent structures are studied only within the confined forms of simple wall turbulence, which approximates the coherence to be steady, fully developed, incompressible, and with a zero pressure gradient in the boundary layer. Although such approximations depart from reality, they contain sufficient parameters needed to understand turbulent coherent structures in a highly conceptual degree.\n",
      "\n",
      "\n",
      "\n",
      "What is the main factor that determines the occurrence of each type of supernova?\n",
      "-Progenitor The supernova classification type is closely tied to the type of star at the time of the collapse. The occurrence of each type of supernova depends on the progenitor star's metallicity, since this affects the strength of the stellar wind and thereby the rate at which the star loses mass.Type Ia supernovae are produced from white dwarf stars in binary star systems and occur in all galaxy types. Core collapse supernovae are only found in galaxies undergoing current or very recent star formation, since they result from short-lived massive stars. They are most commonly found in type Sc spirals, but also in the arms of other spiral galaxies and in irregular galaxies, especially starburst galaxies.Type Ib and Ic supernovae are hypothesized to have been produced by core collapse of massive stars that have lost their outer layer of hydrogen and helium, either via strong stellar winds or mass transfer to a companion. They normally occur in regions of new star formation, and are extremely rare in elliptical galaxies. The progenitors of type IIn supernovae also have high rates of mass loss in the period just prior to their explosions. Type Ic supernovae have been observed to occur in regions that are more metal-rich and have higher star-formation rates than average for their host galaxies. The table shows the progenitor for the main types of core collapse supernova, and the approximate proportions that have been observed in the local neighbourhood.\n",
      "-All known life requires the complex chemistry of metallic elements. The absorption spectrum of a star reveals the presence of metals within, and studies of stellar spectra reveal that many, perhaps most, stars are poor in metals. Because heavy metals originate in supernova explosions, metallicity increases in the universe over time. Low metallicity characterizes the early universe: globular clusters and other stars that formed when the universe was young, stars in most galaxies other than large spirals, and stars in the outer regions of all galaxies. Metal-rich central stars capable of supporting complex life are therefore believed to be most common in the less dense regions of the larger spiral galaxies—where radiation also happens to be weak.\n",
      "-In the current system of stellar classification, stars are grouped according to temperature, with the massive, very young and energetic Class O stars boasting temperatures in excess of 30,000 K while the less massive, typically older Class M stars exhibit temperatures less than 3,500 K. Because luminosity is proportional to temperature to the fourth power, the large variation in stellar temperatures produces an even vaster variation in stellar luminosity. Because the luminosity depends on a high power of the stellar mass, high mass luminous stars have much shorter lifetimes. The most luminous stars are always young stars, no more than a few million years for the most extreme. In the Hertzsprung–Russell diagram, the x-axis represents temperature or spectral type while the y-axis represents luminosity or magnitude. The vast majority of stars are found along the main sequence with blue Class O stars found at the top left of the chart while red Class M stars fall to the bottom right. Certain stars like Deneb and Betelgeuse are found above and to the right of the main sequence, more luminous or cooler than their equivalents on the main sequence. Increased luminosity at the same temperature, or alternatively cooler temperature at the same luminosity, indicates that these stars are larger than those on the main sequence and they are called giants or supergiants.\n",
      "-Unlike the other types of supernovae, Type Ia supernovae generally occur in all types of galaxies, including ellipticals. They show no preference for regions of current stellar formation. As white dwarf stars form at the end of a star's main sequence evolutionary period, such a long-lived star system may have wandered far from the region where it originally formed. Thereafter a close binary system may spend another million years in the mass transfer stage (possibly forming persistent nova outbursts) before the conditions are ripe for a Type Ia supernova to occur.A long-standing problem in astronomy has been the identification of supernova progenitors. Direct observation of a progenitor would provide useful constraints on supernova models. As of 2006, the search for such a progenitor had been ongoing for longer than a century. Observation of the supernova SN 2011fe has provided useful constraints. Previous observations with the Hubble Space Telescope did not show a star at the position of the event, thereby excluding a red giant as the source. The expanding plasma from the explosion was found to contain carbon and oxygen, making it likely the progenitor was a white dwarf primarily composed of these elements.\n",
      "-Supergiant luminosity classes are easy to determine and apply to large numbers of stars, but they group several very different types of stars into a single category. An evolutionary definition restricts the term supergiant to those massive stars which start core helium fusion without developing a degenerate helium core and without undergoing a helium flash. They will universally go on to burn heavier elements and undergo core-collapse resulting in a supernova.Less massive stars may develop a supergiant spectral luminosity class at relatively low luminosity, around 1,000 L☉ when they are on the asymptotic giant branch (AGB) undergoing helium shell burning. Researchers now prefer to categorize these as AGB stars distinct from supergiants because they are less massive, have different chemical compositions at the surface, undergo different types of pulsation and variability, and will evolve differently, usually producing a planetary nebula and white dwarf. Most AGB stars will not become supernovae although there is interest in a class of super-AGB stars, those almost massive enough to undergo full carbon fusion, which may produce peculiar supernovae although without ever developing an iron core. One notable group of low mass high luminosity stars are the RV Tauri variables, AGB or post-AGB stars lying on the instability strip and showing distinctive semi-regular variations.\n",
      "\n",
      "\n",
      "\n",
      "What is the Erlangen program?\n",
      "-In mathematics, the Erlangen program is a method of characterizing geometries based on group theory and projective geometry. It was published by Felix Klein in 1872 as Vergleichende Betrachtungen über neuere geometrische Forschungen. It is named after the University Erlangen-Nürnberg, where Klein worked.\n",
      "-When topology is routinely described in terms of properties invariant under homeomorphism, one can see the underlying idea in operation. The groups involved will be infinite-dimensional in almost all cases – and not Lie groups – but the philosophy is the same. Of course this mostly speaks to the pedagogical influence of Klein. Books such as those by H.S.M. Coxeter routinely used the Erlangen program approach to help 'place' geometries. In pedagogic terms, the program became transformation geometry, a mixed blessing in the sense that it builds on stronger intuitions than the style of Euclid, but is less easily converted into a logical system.\n",
      "-In 1872, Felix Klein noted that the many branches of geometry which had been developed during the 19th century (affine geometry, projective geometry, hyperbolic geometry, etc.) could all be treated in a uniform way. He did this by considering the groups under which the geometric objects were invariant. This unification of geometry goes by the name of the Erlangen programme.The general theory of angle can be unified with invariant measure of area. The hyperbolic angle is defined in terms of area, very nearly the area associated with natural logarithm. The circular angle also has area interpretation when referred to a circle with radius equal to the square root of two. These areas are invariant with respect to hyperbolic rotation and circular rotation respectively. These affine transformations are effected by elements of the special linear group SL(2,R). Inspection of that group reveals shear mappings which increase or decrease slopes but differences of slope do not change. A third type of angle, also interpreted as an area dependent on slope differences, is invariant because of area preservation of a shear mapping.\n",
      "-In the seminal paper which introduced categories, Saunders Mac Lane and Samuel Eilenberg stated: \"This may be regarded as a continuation of the Klein Erlanger Program, in the sense that a geometrical space with its group of transformations is generalized to a category with its algebra of mappings.\"Relations of the Erlangen program with work of Charles Ehresmann on groupoids in geometry is considered in the article below by Pradines.In mathematical logic, the Erlangen program also served as an inspiration for Alfred Tarski in his analysis of logical notions.\n",
      "-In mathematics, a Klein geometry is a type of geometry motivated by Felix Klein in his influential Erlangen program. More specifically, it is a homogeneous space X together with a transitive action on X by a Lie group G, which acts as the symmetry group of the geometry.\n",
      "For background and motivation see the article on the Erlangen program.\n",
      "\n",
      "\n",
      "\n",
      "What is emissivity?\n",
      "-Emissivity is a term that is often misunderstood and misused. It represents a material's ability to emit thermal radiation and is an optical property of matter.\n",
      "-Emissivity is the value given to materials based on the ratio of heat emitted compared to a perfect black body, on a scale from zero to one. A black body would have an emissivity of 1 and a perfect reflector would have a value of 0.\n",
      "-Thermal emittance or thermal emissivity ( ε ) is the ratio of the radiant emittance of heat of a specific object or surface to that of a standard black body. Emissivity and emittivity are both dimensionless quantities given in the range of 0 to 1, representing the comparative/relative emittance with respect to a blackbody operating in similar conditions, but emissivity refers to a material property (of a homogeneous material), while emittivity refers to specific samples or objects.For building products, thermal emittance measurements are taken for wavelengths in the infrared. Determining the thermal emittance and solar reflectance of building materials, especially roofing materials, can be very useful for reducing heating and cooling energy costs in buildings. Combined index Solar Reflectance Index (SRI) is often used to determine the overall ability to reflect solar heat and release thermal heat. A roofing surface with high solar reflectance and high thermal emittance will reflect solar heat and release absorbed heat readily. High thermal emittance material radiates thermal heat back into the atmosphere more readily than one with a low thermal emittance. In common construction applications, the thermal emittance of a surface is usually higher than 0.8–0.85.High thermal emittance materials are essential to passive daytime radiative cooling, which uses surfaces high in thermal emittance and solar reflectance to lower surface temperatures by dissipating heat to outer space. It has been proposed as a solution to energy crises and global warming.\n",
      "-The emissivity of a material (usually written ε or e) is the relative ability of its surface to emit energy by radiation. A black body has an emissivity of 1 and a perfect reflector has an emissivity of 0.In radiative heat transfer, a view factor quantifies the relative importance of the radiation that leaves an object (person or surface) and strikes another one, considering the other surrounding objects. In enclosures, radiation leaving a surface is conserved, therefore, the sum of all view factors associated with a given object is equal to 1.\n",
      "-All materials in existence give off, or emit, energy by thermal radiation as a result of their temperature. The amount of energy radiated depends on the surface temperature and a property called emissivity (also called \"emittance\"). Emissivity is expressed as a number between zero and one at a given wavelength. The higher the emissivity, the greater the emitted radiation at that wavelength. A related material property is reflectivity (also called \"reflectance\"). This is a measure of how much energy is reflected by a material at a given wavelength. Reflectivity is also expressed as a number between 0 and 1 (or a percentage between 0 and 100). At a given wavelength and angle of incidence the emissivity and reflectivity values sum to 1 by Kirchhoff's law.Radiant barrier materials must have low emissivity (usually 0.1 or less) at the wavelengths at which they are expected to function. For typical building materials, the wavelengths are in the mid- and long-infrared spectrum, in the range of 3-15 micrometres.Radiant barriers may or may not exhibit high visual reflectivity. While reflectivity and emissivity must sum to 1 at a given wavelength, reflectivity at one set of wavelengths (visible) and emissivity at a different set of wavelengths (thermal) do not necessarily sum to 1. Therefore, it is possible to create visibly dark colored surfaces with low thermal emissivity.To perform properly, radiant barriers need to face open space (e.g., air or vacuum) through which there would otherwise be radiation.\n",
      "\n",
      "\n",
      "\n",
      "Who was the first person to describe the pulmonary circulation system?\n",
      "-Pre-modern The earliest descriptions of the coronary and pulmonary circulation systems can be found in the Commentary on Anatomy in Avicenna's Canon, published in 1242 by Ibn al-Nafis. In his manuscript, al-Nafis wrote that blood passes through the pulmonary circulation instead of moving from the right to the left ventricle as previously believed by Galen. His work was later translated into Latin by Andrea Alpago.In Europe, the teachings of Galen continued to dominate the academic community and his doctrines were adopted as the official canon of the Church. Andreas Vesalius questioned some of Galen's beliefs of the heart in De humani corporis fabrica (1543), but his magnum opus was interpreted as a challenge to the authorities and he was subjected to a number of attacks. Michael Servetus wrote in Christianismi Restitutio (1553) that blood flows from one side of the heart to the other via the lungs.\n",
      "-The pulmonary circulation is archaically known as the \"lesser circulation\" which is still used in non-English literature.The discovery of the pulmonary circulation has been attributed to many scientists with credit distributed in varying ratios by varying sources. In much of modern medical literature, the discovery is credited to English physician William Harvey (1578 – 1657 CE) based on the comprehensive completeness and correctness of his model, despite its relative recency. Other sources credit Greek philosopher Hippocrates (460 – 370 BCE), Spanish physician Michael Servetus (c. 1509 – 1553 CE), Arab physician Ibn al-Nafis (1213 – 1288 CE), and Syrian physician Qusta ibn Luqa. Several figures such as Hippocrates and al-Nafis receive credit for accurately predicting or developing specific elements of the modern model of pulmonary circulation: Hippocrates for being the first to describe pulmonary circulation as a discrete system separable from systemic circulation as a whole and al-Nafis for making great strides over the understanding of those before him and towards a rigorous model. There is a great deal of subjectivity involved in deciding at which point a complex system is \"discovered\", as it is typically elucidated in piecemeal form so that the very first description, most complete or accurate description, and the most significant forward leaps in understanding are all considered acts of discovery of varying significance.Primitive descriptions of the cardiovascular system are found throughout several ancient cultures. The earliest known description of the role of air in circulation was produced in Egypt in 3500 BCE. At the time, the Egyptians believed that the heart was the origin of many channels that connected different parts of the body to each other and transported air – as well as urine, blood, and the soul – between them. The Edwin Smith Papyrus (1700 BCE), named for American Egyptologist Edwin Smith (1822 – 1906 CE) who purchased the scroll in 1862, provided evidence that Egyptians believed that the heartbeat created a pulse that transported the above substances throughout the body. A second scroll, the Ebers Papyrus (c. 1550 BCE), also emphasized the importance of the heart and its connection to vessels throughout the body and described methods to detect cardiac disease through pulse abnormalities. Although they had knowledge of the heartbeat, vessels, and pulse, the Egyptians attributed the movement of substances through the vessels to air that resided in these channels, rather than to the heart's exertion of pressure. The Egyptians knew that air played an important role in circulation but did not yet have a conception of the role of the lungs.\n",
      "-One of the first major discoveries relevant to the field of pulmonology was the discovery of pulmonary circulation. Originally, it was thought that blood reaching the right side of the heart passed through small 'pores' in the septum into the left side to be oxygenated, as theorized by Galen; however, the discovery of pulmonary circulation disproves this theory, which had previously been accepted since the 2nd century. Thirteenth-century anatomist and physiologist Ibn Al-Nafis accurately theorized that there was no 'direct' passage between the two sides (ventricles) of the heart. He believed that the blood must have passed through the pulmonary artery, through the lungs, and back into the heart to be pumped around the body. This is believed by many to be the first scientific description of pulmonary circulation.Although pulmonary medicine only began to evolve as a medical specialty in the 1950s, William Welch and William Osler founded the 'parent' organization of the American Thoracic Society, the National Association for the Study and Prevention of Tuberculosis. The care, treatment, and study of tuberculosis of the lung is recognised as a discipline in its own right, phthisiology. When the specialty did begin to evolve, several discoveries were being made linking the respiratory system and the measurement of arterial blood gases, attracting more and more physicians and researchers to the developing field.\n",
      "-The next addition to the historical understanding of pulmonary circulation arrived with the Ancient Greeks. Physician Alcmaeon (520 – 450 BCE) proposed that the brain, not the heart, was the connection point for all of the vessels in the body. He believed that the function of these vessels was to bring the \"spirit\" (\"pneuma\") and air to the brain. Empedocles (492 – 432 BCE), a philosopher, proposed a series of pipes, impermeable to blood but continuous with blood vessels, that carried the pneuma throughout the body. He proposed that this spirit was internalized by pulmonary respiration.Hippocrates was the first to describe pulmonary circulation as a discrete system, separable from systemic circulation, in his Corpus Hippocraticum, which is often regarded as the foundational text of modern medicine. Hippocrates developed the view that the liver and spleen produced blood, and that this traveled to the heart to be cooled by the lungs that surrounded it. He described the heart as having two ventricles connected by an interventricular septum, and depicted the heart as the nexus point of all of the vessels of the body. He proposed that some vessels carried only blood and that others carried only air. He hypothesized that these air-carrying vessels were divisible into the pulmonary veins, which carried in air to the left ventricle, and the pulmonary artery, which carried in air to the right ventricle and blood to the lungs. He also proposed the existence of two atria of the heart functioning to capture air. He was one of the first to begin to accurately describe the anatomy of the heart and to describe the involvement of the lungs in circulation. His descriptions built substantially on previous and contemporaneous efforts but, by modern standards, his conceptions of pulmonary circulation and of the functions of the parts of the heart were still largely inaccurate.Greek philosopher and scientist Aristotle (384 – 322 BCE) followed Hippocrates and proposed that the heart had three ventricles, rather than two, that all connected to the lungs. Greek physician Erasistratus (315 – 240 BCE) agreed with Hippocrates and Aristotle that the heart was the origin of all of the vessels in the body but proposed a system in which air was drawn into the lungs and traveled to the left ventricle via pulmonary veins. It was transformed there into the pneuma and distributed throughout the body by arteries, which contained only air. In this system, veins distributed blood throughout the body, and thus blood did not circulate, but rather was consumed by the organs.The Greek physician Galen (129 – c. 210 CE) provided the next insights into pulmonary circulation. Though many of his theories, like those of his predecessors, were marginally or completely incorrect, his theory of pulmonary circulation dominated the medical community's understanding for hundreds of years after his death. Galen contradicted Erasistratus before him by proposing that arteries carried both air and blood, rather than air alone (which was essentially correct, leaving aside that blood vessels carry constituents of air and not air itself). He proposed that the liver was the originating point of all blood vessels. He also theorized that the heart was not a pumping muscle but rather an organ through which blood passed. Galen's theory included a new description of pulmonary circulation: air was inhaled into the lungs where it became the pneuma. Pulmonary veins transmitted this pneuma to the left ventricle of the heart to cool the blood simultaneously arriving there. This mixture of pneuma, blood, and cooling produced the vital spirits that could then be transported throughout the body via arteries. Galen further proposed that the heat of the blood arriving in the heart produced noxious vapors that were expelled through the same pulmonary veins that first brought the pneuma. He wrote that the right ventricle played a different role to the left: it transported blood to the lungs where the impurities were vented out so that clean blood could be distributed throughout the body. Though Galen's description of the anatomy of the heart was more complete than those of his predecessors, it included several mistakes. Most notably, Galen believed that blood flowed between the two ventricles of the heart through small, invisible pores in the interventricular septum.The next significant developments in the understanding of pulmonary circulation did not arrive until centuries later. Persian polymath Avicenna (c. 980 – 1037 CE) wrote a medical encyclopedia entitled The Canon of Medicine. In it, he translated and compiled contemporary medical knowledge and added some new information of his own. However, Avicenna's description of pulmonary circulation reflected the incorrect views of Galen.The Arab physician, Ibn al-Nafis, wrote the Commentary on Anatomy in Avicenna's Canon in 1242 in which he provided possibly the first known description of the system that remains substantially congruent with modern understandings, in spite of its flaws. Ibn al-Nafis made two key improvements on Galen's ideas. First, he disproved the existence of the pores in the interventricular septum that Galen had believed allowed blood to flow between the left and right ventricles. Second, he surmised that the only way for blood to get from the right to the left ventricle in the absence of interventricular pores was a system like pulmonary circulation. He also described the anatomy of the lungs in clear and basically correct detail, which his predecessors had not. However, like Aristotle and Galen, al-Nafis still believed in the quasi-mythical concept of vital spirit and that it was formed in the left ventricle from a mixture of blood and air. Despite the enormity of Ibn al-Nafis's improvements on the theories that preceded him, his commentary on The Canon was not widely known to Western scholars until the manuscript was discovered in Berlin, Germany, in 1924. As a result, the ongoing debate among Western scholars as to how credit for the discovery should be apportioned failed to include Ibn al-Nafis until, at earliest, the mid-20th century (shortly after which he came to enjoy a share of this credit). In 2021, several researchers described a text predating the work of al-Nafis, fargh- beyn-roh va nafs, in which there is a comparable report on pulmonary circulation. The researchers argue that its author, Qusta ibn Luqa, is the best candidate for the discoverer of pulmonary circulation on a similar basis to arguments in favour of al-Nafis generally.It took centuries for other scientists and physicians to reach conclusions that were similar to and then more accurate than those of al-Nafis and ibn Luqa. This later progress, constituting the gap between medieval and modern understanding, occurred throughout Europe. Italian polymath Leonardo da Vinci (1452 – 1519 CE) was one of the first to propose that the heart was just a muscle, rather than a vessel of spirits and air, but he still subscribed to Galen's ideas of circulation and defended the existence of interventricular pores. The Flemish physician Andreas Vesalius (1514 – 1564 CE) published corrections to Galen's view of circulatory anatomy, questioning the existence of interventricular pores, in his book De humani corporis fabrica libri septem in 1543. Spanish Michael Servetus, after him, was the first European physician to accurately describe pulmonary circulation. His assertions largely matched those of al-Nafis. In subsequent centuries, he has frequently been credited with the discovery, but some historians have propounded the idea that he potentially had access to Ibn al-Nafis's work while writing his own texts. Servetus published his findings in Christianismi Restituto (1553): a theological work that was considered heretical by Catholics and Calvinists alike. As a result, both book and author were burned at the stake and only a few copies of his work survived. Italian physician Realdo Colombo (c. 1515 – 1559 CE) published a book, De re anatomica libri XV, in 1559 that accurately described pulmonary circulation. It is still a matter of debate among historians as to whether Colombo reached his conclusions alone or based them to an unknown degree on the works of al-Nafis and Servetus. Finally, in 1628, the influential British physician William Harvey (1578 – 1657 AD) provided at the time the most complete and accurate description of pulmonary circulation of any scholar worldwide in his treatise Exercitatio Anatomica de Motu Cordis et Sanguinis in Animalibus. At the macroscopic level, his model is still recognizable in and reconcilable with modern understandings of pulmonary circulation.\n",
      "-The earliest known writings on the circulatory system are found in the Ebers Papyrus (16th century BCE), an ancient Egyptian medical papyrus containing over 700 prescriptions and remedies, both physical and spiritual. In the papyrus, it acknowledges the connection of the heart to the arteries. The Egyptians thought air came in through the mouth and into the lungs and heart. From the heart, the air travelled to every member through the arteries. Although this concept of the circulatory system is only partially correct, it represents one of the earliest accounts of scientific thought.\n",
      "\n",
      "\n",
      "\n",
      "What is the fate of a carbocation formed in crystalline naphthalene?\n",
      "-Radical formation In a condensed phase, the carbocation can also gain an electron from surrounding molecules, thus becoming an electrically neutral radical. For example, in crystalline naphthalene, a molecule with tritium substituted for hydrogen in the 1 (or 2) position will be turned by decay into a cation with a positive charge at that position. That charge will however be quickly neutralized by an electron transported through the lattice, turning the molecule into the 1-naphthyl (or 2-naphthyl) radical; which are stable, trapped in the solid, below 170 K (−103 °C).\n",
      "-In the basic method, a molecule (R,R′,R″)C−T is prepared where the vacant bond of the desired radical or ion is satisfied by an atom of tritium 3H, the radioactive isotope of hydrogen with mass number 3. As the tritium undergoes beta decay (with a half-life of 12.32 years), it is transformed into an ion of helium-3, creating the cation (R,R′,R″)C−[3He]+.In the decay, an electron and an antineutrino are ejected at great speed from the tritium nucleus, changing one of the neutrons into a proton with the release of 18,600 electronvolts (eV) of energy. The neutrino escapes the system; the electron is generally captured within a short distance, but far enough away from the site of the decay that it can be considered lost from the molecule. Those two particles carry away most of the released energy, but their departure causes the nucleus to recoil, with about 1.6 eV of energy. This recoil energy is larger than the bond strength of the carbon–helium bond (about 1 eV), so this bond breaks. The helium atom almost always leaves as a neutral 3He, leaving behind the carbocation [(R,R′,R″)C]+.These events happen very quickly compared to typical molecular relaxation times, so the carbocation is usually created in the same conformation and electronic configuration as the original neutral molecule. For example, decay of tritiated methane, CH3T (R = R′ = R″ = H) produces the carbenium ion H3C+ in a tetrahedral conformation, with one of the orbitals having a single unpaired electron and the other three forming a trigonal pyramid. The ion then relaxes to its more favorable trigonal planar form, with release of about 30 kcal/mol of energy—that goes into vibrations and rotation of the ion.The carbocation then can interact with surrounding molecules in many reactions that cannot be achieved by other means. When formed within a rarefied gas, the carbocation and its reactions can be studied by mass spectrometry techniques. However the technique can be used also in condensed matter (liquids and solids). In liquid phase, the carbocation is initially formed in the same solvation state as the parent molecule, and some reactions may happen before the solvent shells around it have time to rearrange. In a crystalline solid, the cation is formed in the same crystalline site; and the nature, position, and orientation of the other reagent(s) are strictly constrained.\n",
      "-Whereas the carbon–helium-ion bond breaks spontaneously and immediately to yield a carbocation, bonds of other elements to helium are more stable. For example, molecular tritium T2 or tritium-hydrogen HT. On decay, these form a stable helium hydride ion [HeH]+ (respectively [3HeT]+ or [3HeH]+), which is stable enough to persist. This cation is claimed to be the strongest acid known, and will protonate any other molecule it comes in contact with. This is another route to creating cations that are not obtainable in other ways. In particular [HeH]+ (or [HeT]+) will protonate methane CH4 to the carbonium ion [CH5]+ (or [CH4T]+).Other structures that are expected to be stable when formed by beta-decay of tritium precursors include 3HeLi+, B2H53He+, and BeH3He+ according to theoretical calculations.\n",
      "-The effect of hyperconjugation is strongly stabilizing for carbocations: hyperconjugation with alkyl substituents is often as stabilizing or even more so than conjugation with a π system. Although conjugation to unsaturated groups results in significant stabilization by the mesomeric effect (resonance), the benefit is partially offset by the presence of a more electronegative sp2 or sp carbon next to the carbocationic center. Thus, as reflected by hydride ion affinities, a secondary carbocation is more stabilized than the allyl cation, while a tertiary carbocation is more stabilized than the benzyl cation — results that may seem counterintuitive on first glance.\n",
      "-A carbocation may be stabilized by resonance by a carbon–carbon double bond or by the lone pair of a heteroatom adjacent to the ionized carbon. In order for a carbocation to be resonance-stabilized, the molecular orbital of the donating group must have the proper symmetry, orientation, and energy level to interact with the empty 2p orbital of the carbocation. Such cations as allyl cation CH2=CH−CH+2 and benzyl cation C6H5−CH+2 are more stable than most other carbocations due to donation of electron density from π systems to the cationic center. Furthermore, carbocations present in aromatic molecules are especially stabilized, largely due to the delocalized π electrons characteristic of aromatic rings. Molecules that can form allyl or benzyl carbocations are especially reactive. These carbocations where the C+ is adjacent to another carbon atom that has a double or triple bond have extra stability because of the overlap of the empty p orbital of the carbocation with the p orbitals of the π bond. This overlap of the orbitals allows the positive charge to be dispersed and electron density from the π system to be shared with the electron-deficient center, resulting in stabilization. The doubly- and triply-benzylic carbocations, diphenylcarbenium and triphenylcarbenium (trityl) cation, are particularly stable. For the same reasons, the partial p character of strained C–C bonds in cyclopropyl groups also allows for donation of electron density and stabilizes the cyclopropylmethyl (cyclopropylcarbinyl) cation.\n",
      "\n",
      "\n",
      "\n",
      "What is the main focus of the Environmental Science Center at Qatar University?\n",
      "-Environmental science is an interdisciplinary academic field that integrates physics, biology, and geography (including ecology, chemistry, plant science, zoology, mineralogy, oceanography, limnology, soil science, geology and physical geography, and atmospheric science) to the study of the environment, and the solution of environmental problems. Environmental science emerged from the fields of natural history and medicine during the Enlightenment. Today it provides an integrated, quantitative, and interdisciplinary approach to the study of environmental systems.Environmental studies incorporates more of the social sciences for understanding human relationships, perceptions and policies towards the environment. Environmental engineering focuses on design and technology for improving environmental quality in every aspect.\n",
      "-This is a glossary of environmental science.\n",
      "Environmental science is the study of interactions among physical, chemical, and biological components of the environment. Environmental science provides an integrated, quantitative, and interdisciplinary approach to the study of environmental systems.\n",
      "-Atmospheric sciences Atmospheric sciences focus on the Earth's atmosphere, with an emphasis upon its interrelation to other systems. Atmospheric sciences can include studies of meteorology, greenhouse gas phenomena, atmospheric dispersion modeling of airborne contaminants, sound propagation phenomena related to noise pollution, and even light pollution.\n",
      "Taking the example of the global warming phenomena, physicists create computer models of atmospheric circulation and infrared radiation transmission, chemists examine the inventory of atmospheric chemicals and their reactions, biologists analyze the plant and animal contributions to carbon dioxide fluxes, and specialists such as meteorologists and oceanographers add additional breadth in understanding the atmospheric dynamics.\n",
      "-Environmental studies is a broader academic discipline that is the systematic study of the interaction of humans with their environment. It is a broad field of study that includes: The natural environment Built environments Social environmentsEnvironmentalism is a broad social and philosophical movement that, in a large part, seeks to minimize and compensate for the negative effect of human activity on the biophysical environment. The issues of concern for environmentalists usually relate to the natural environment with the more important ones being climate change, species extinction, pollution, and old growth forest loss.\n",
      "-Environmental science is the study of the interactions within the biophysical environment. Part of this scientific discipline is the investigation of the effect of human activity on the environment.\n",
      "Ecology, a sub-discipline of biology and a part of environmental sciences, is often mistaken as a study of human-induced effects on the environment.\n",
      "\n",
      "\n",
      "\n",
      "What is the purpose of obtaining surgical resection specimens?\n",
      "-Surgical resection specimens are obtained by the therapeutic surgical removal of an entire diseased area or organ (and occasionally multiple organs). These procedures are often intended as definitive surgical treatment of a disease in which the diagnosis is already known or strongly suspected. However, pathological analysis of these specimens is critically important in confirming the previous diagnosis, staging the extent of malignant disease, establishing whether or not the entire diseased area was removed (a process called \"determination of the surgical margin\", often using frozen section), identifying the presence of unsuspected concurrent diseases, and providing information for postoperative treatment, such as adjuvant chemotherapy in the case of cancer.\n",
      "-== Need == In medicine, a laboratory specimen is a biological specimen of a medical patient's tissue, fluids, or other material used for laboratory analysis to assist in differential diagnosis or staging of a disease process. For example, to detect breast cancer, the breast tissue is biopsied, and the extracted specimen is sent to a lab for analysis and testing. This method of testing often yields extremely high levels of accuracy, with a reported 1-2% of cases having incorrect biopsy resultsGeneral areas for cellular tissue extraction:  Bone marrow aspiration Cardiac Core Endometrial biopsy Endoscopic biopsy Excisional and incisional Fine-needle aspiration Lymph node \n",
      "-The practice of surgical pathology allows for definitive diagnosis of disease (or lack thereof) in any case where tissue is surgically removed from a patient. This is usually performed by a combination of gross (i.e., macroscopic) and histologic (i.e., microscopic) examination of the tissue, and may involve evaluations of molecular properties of the tissue by immunohistochemistry or other laboratory tests.\n",
      "-Surgical pathology Surgical pathology is one of the primary areas of practice for most anatomical pathologists. Surgical pathology involves the gross and microscopic examination of surgical specimens, as well as biopsies submitted by surgeons and non-surgeons such as general internists, medical subspecialists, dermatologists, and interventional radiologists. Often an excised tissue sample is the best and most definitive evidence of disease (or lack thereof) in cases where tissue is surgically removed from a patient. These determinations are usually accomplished by a combination of gross (i.e., macroscopic) and histologic (i.e., microscopic) examination of the tissue, and may involve evaluations of molecular properties of the tissue by immunohistochemistry or other laboratory tests.There are two major types of specimens submitted for surgical pathology analysis: biopsies and surgical resections. A biopsy is a small piece of tissue removed primarily for surgical pathology analysis, most often in order to render a definitive diagnosis. Types of biopsies include core biopsies, which are obtained through the use of large-bore needles, sometimes under the guidance of radiological techniques such as ultrasound, CT scan, or magnetic resonance imaging. Incisional biopsies are obtained through diagnostic surgical procedures that remove part of a suspicious lesion, whereas excisional biopsies remove the entire lesion, and are similar to therapeutic surgical resections. Excisional biopsies of skin lesions and gastrointestinal polyps are very common. The pathologist's interpretation of a biopsy is critical to establishing the diagnosis of a benign or malignant tumor, and can differentiate between different types and grades of cancer, as well as determining the activity of specific molecular pathways in the tumor. Surgical resection specimens are obtained by the therapeutic surgical removal of an entire diseased area or organ (and occasionally multiple organs). These procedures are often intended as definitive surgical treatment of a disease in which the diagnosis is already known or strongly suspected, but pathological analysis of these specimens remains important in confirming the previous diagnosis.\n",
      "-After the biopsy is performed, the sample of tissue that was removed from the patient is sent to the pathology laboratory. A pathologist specializes in diagnosing diseases (such as cancer) by examining tissue under a microscope. When the laboratory (see Histology) receives the biopsy sample, the tissue is processed and an extremely thin slice of tissue is removed from the sample and attached to a glass slide. Any remaining tissue is saved for use in later studies, if required.The slide with the tissue attached is treated with dyes that stain the tissue, which allows the individual cells in the tissue to be seen more clearly. The slide is then given to the pathologist, who examines the tissue under a microscope, looking for any abnormal findings. The pathologist then prepares a report that lists any abnormal or important findings from the biopsy. This report is sent to the surgeon who originally performed the biopsy on the patient.\n",
      "\n",
      "\n",
      "\n",
      "What is the function of mammary glands in mammals?\n",
      "-A mammary gland is an exocrine gland in humans and other mammals that produces milk to feed young offspring. Mammals get their name from the Latin word mamma, \"breast\". The mammary glands are arranged in organs such as the breasts in primates (for example, humans and chimpanzees), the udder in ruminants (for example, cows, goats, sheep, and deer), and the dugs of other animals (for example, dogs and cats). Lactorrhea, the occasional production of milk by the glands, can occur in any mammal, but in most mammals, lactation, the production of enough milk for nursing, occurs only in phenotypic females who have gestated in recent months or years. It is directed by hormonal guidance from sex steroids. In a few mammalian species, male lactation can occur. With humans, male lactation can occur only under specific circumstances.\n",
      "-The primary function of the breasts, as mammary glands, is the nourishing of an infant with breast milk. Milk is produced in milk-secreting cells in the alveoli. When the breasts are stimulated by the suckling of her baby, the mother's brain secretes oxytocin. High levels of oxytocin trigger the contraction of muscle cells surrounding the alveoli, causing milk to flow along the ducts that connect the alveoli to the nipple.Full-term newborns have an instinct and a need to suck on a nipple, and breastfed babies nurse for both nutrition and for comfort. Breast milk provides all necessary nutrients for the first six months of life, and then remains an important source of nutrition, alongside solid foods, until at least one or two years of age.\n",
      "-In females, it serves as the mammary gland, which produces and secretes milk to feed infants. Subcutaneous fat covers and envelops a network of ducts that converge on the nipple, and these tissues give the breast its size and shape. At the ends of the ducts are lobules, or clusters of alveoli, where milk is produced and stored in response to hormonal signals. During pregnancy, the breast responds to a complex interaction of hormones, including estrogens, progesterone, and prolactin, that mediate the completion of its development, namely lobuloalveolar maturation, in preparation of lactation and breastfeeding.\n",
      "-Mammals are divided into 3 groups: prototherians, metatherians, and eutherians. In the case of prototherians, both males and females have functional mammary glands, but their mammary glands are without nipples. These mammary glands are modified sebaceous glands. Concerning metatherians and eutherians, only females have functional mammary glands. Their mammary glands can be termed as breasts or udders. In the case of breasts, each mammary gland has its own nipple (e.g., human mammary glands). In the case of udders, pairs of mammary glands comprise a single mass, with more than one nipple (or teat) hanging from it. For instance, cows and buffalo each have one udder with four teats, whereas sheep and goats each have two teats protruding from the udder. These mammary glands are modified sweat glands.\n",
      "-The chief function of a lactation is to provide nutrition and immune protection to the young after birth. Due to lactation, the mother-young pair can survive even if food is scarce or too hard for the young to attain, expanding the environmental conditions the species can withstand. The costly investment of energy and resources into milk is outweighed by the benefit to offspring survival. In almost all mammals, lactation induces a period of infertility (in humans, lactational amenorrhea), which serves to provide the optimal birth spacing for survival of the offspring.\n",
      "\n",
      "\n",
      "\n",
      "What is the relationship between interstellar and cometary chemistry?\n",
      "-Such IR observations have determined that in dense clouds (where there are enough particles to attenuate the destructive UV radiation) thin ice layers coat the microscopic particles, permitting some low-temperature chemistry to occur. Since dihydrogen is by far the most abundant molecule in the universe, the initial chemistry of these ices is determined by the chemistry of the hydrogen. If the hydrogen is atomic, then the H atoms react with available O, C and N atoms, producing \"reduced\" species like H2O, CH4, and NH3. However, if the hydrogen is molecular and thus not reactive, this permits the heavier atoms to react or remain bonded together, producing CO, CO2, CN, etc. These mixed-molecular ices are exposed to ultraviolet radiation and cosmic rays, which results in complex radiation-driven chemistry. Lab experiments on the photochemistry of simple interstellar ices have produced amino acids. The similarity between interstellar and cometary ices (as well as comparisons of gas phase compounds) have been invoked as indicators of a connection between interstellar and cometary chemistry. This is somewhat supported by the results of the analysis of the organics from the comet samples returned by the Stardust mission but the minerals also indicated a surprising contribution from high-temperature chemistry in the solar nebula.\n",
      "-Chemistry in cometary comae The chemical composition of comets should reflect both the conditions in the outer solar nebula some 4.5 × 109 ayr, and the nature of the natal interstellar cloud from which the Solar System was formed. While comets retain a strong signature of their ultimate interstellar origins, significant processing must have occurred in the protosolar nebula. Early models of coma chemistry showed that reactions can occur rapidly in the inner coma, where the most important reactions are proton transfer reactions. Such reactions can potentially cycle deuterium between the different coma molecules, altering the initial D/H ratios released from the nuclear ice, and necessitating the construction of accurate models of cometary deuterium chemistry, so that gas-phase coma observations can be safely extrapolated to give nuclear D/H ratios.\n",
      "-Research is progressing on the way in which interstellar and circumstellar molecules form and interact, e.g. by including non-trivial quantum mechanical phenomena for synthesis pathways on interstellar particles. This research could have a profound impact on our understanding of the suite of molecules that were present in the molecular cloud when our solar system formed, which contributed to the rich carbon chemistry of comets and asteroids and hence the meteorites and interstellar dust particles which fall to the Earth by the ton every day.\n",
      "-Interstellar molecules are formed by chemical reactions within very sparse interstellar or circumstellar clouds of dust and gas. Usually this occurs when a molecule becomes ionised, often as the result of an interaction with cosmic rays. This positively charged molecule then draws in a nearby reactant by electrostatic attraction of the neutral molecule's electrons. Molecules can also be generated by reactions between neutral atoms and molecules, although this process is generally slower. The dust plays a critical role of shielding the molecules from the ionizing effect of ultraviolet radiation emitted by stars. The Murchison meteorite contains the organic molecules uracil and xanthine, which must therefore already have been present in the early Solar System, where they could have played a role in the origin of life.Nitriles, key molecular precursors of the RNA World scenario, are among the most abundant chemical families in the universe and have been found in molecular clouds in the center of the Milky Way, protostars of different masses, meteorites and comets, and also in the atmosphere of Titan, the largest moon of Saturn.Evidence for the extraterrestrial creation of organic molecules includes both their discovery in various contexts in space, and their laboratory synthesis under extraterrestrial conditions: \n",
      "-PAHs, subjected to interstellar medium (ISM) conditions, are transformed, through hydrogenation, oxygenation, and hydroxylation, to more complex organic compounds—\"a step along the path toward amino acids and nucleotides, the raw materials of proteins and DNA, respectively\". Further, as a result of these transformations, the PAHs lose their spectroscopic signature which could be one of the reasons \"for the lack of PAH detection in interstellar ice grains, particularly the outer regions of cold, dense clouds or the upper molecular layers of protoplanetary disks.\"Low-temperature chemical pathways from simple organic compounds to complex PAHs are of interest. Such chemical pathways may help explain the presence of PAHs in the low-temperature atmosphere of Saturn's moon Titan, and may be significant pathways, in terms of the PAH world hypothesis, in producing precursors to biochemicals related to life as we know it.\n",
      "\n",
      "\n",
      "\n",
      "What is the reason for recycling rare metals according to the United Nations?\n",
      "-Recycling Demand for metals is closely linked to economic growth given their use in infrastructure, construction, manufacturing, and consumer goods. During the 20th century, the variety of metals used in society grew rapidly. Today, the development of major nations, such as China and India, and technological advances, are fuelling ever more demand. The result is that mining activities are expanding, and more and more of the world's metal stocks are above ground in use, rather than below ground as unused reserves. An example is the in-use stock of copper. Between 1932 and 1999, copper in use in the U.S. rose from 73 g to 238 g per person.Metals are inherently recyclable, so in principle, can be used over and over again, minimizing these negative environmental impacts and saving energy. For example, 95% of the energy used to make aluminum from bauxite ore is saved by using recycled material.Globally, metal recycling is generally low. In 2010, the International Resource Panel, hosted by the United Nations Environment Programme published reports on metal stocks that exist within society and their recycling rates. The authors of the report observed that the metal stocks in society can serve as huge mines above ground. They warned that the recycling rates of some rare metals used in applications such as mobile phones, battery packs for hybrid cars and fuel cells are so low that unless future end-of-life recycling rates are dramatically stepped up these critical metals will become unavailable for use in modern technology.\n",
      "-During the 20th century, the variety of metals used in society grew rapidly. Today, the development of major nations such as China and India and advances in technologies are fueling an ever-greater demand. The result is that metal mining activities are expanding and more and more of the world's metal stocks are above ground in use rather than below ground as unused reserves. An example is the in-use stock of copper. Between 1932 and 1999, copper in use in the US rose from 73 kilograms (161 lb) to 238 kilograms (525 lb) per person.95% of the energy used to make aluminium from bauxite ore is saved by using recycled material. However, levels of metals recycling are generally low. In 2010, the International Resource Panel, hosted by the United Nations Environment Programme (UNEP), published reports on metal stocks that exist within society and their recycling rates.The report's authors observed that the metal stocks in society can serve as huge mines above ground. However, they warned that the recycling rates of some rare metals used in applications such as mobile phones, battery packs for hybrid cars, and fuel cells are so low that unless future end-of-life recycling rates are dramatically stepped up these critical metals will become unavailable for use in modern technology.As recycling rates are low and so much metal has already been extracted, some landfills now contain higher concentrations of metal than mines themselves. This is especially true of aluminum, used in cans, and precious metals, found in discarded electronics. Furthermore, waste after 15 years has still not broken down, so less processing would be required when compared to mining ores. A study undertaken by Cranfield University has found £360 million of metals could be mined from just four landfill sites. There is also up to 20 MJ/kg of energy in waste, potentially making the re-extraction more profitable. However, although the first landfill mine opened in Tel Aviv, Israel in 1953, little work has followed due to the abundance of accessible ores.\n",
      "-The USGS reported a current total reserve base of copper in potentially recoverable ores of 1.6 billion tonnes as of 2005, of which 950 million tonnes were considered economically recoverable. A 2013 global assessment identified \"455 known deposits (with well-defined identified resources) that contain about 1.8 billion metric tons of copper\", and predicted \"a mean of 812 undiscovered deposits within the uppermost kilometer of the earth's surface\" containing another 3.1 billion metric tons of copper \"which represents about 180 times 2012 global copper production from all types of copper deposits.\" Known resources Recycling In the US, more copper is recovered and put back into service from recycled material than is derived from newly mined ore. Copper's recycle value is so great that premium-grade scrap normally has at least 95% of the value of primary metal from newly mined ore. In Europe, about 50% of copper demand comes from recycling (as of 2016).As of 2011, recycled copper provided 35% of total worldwide copper usage.\n",
      "-In 2007, the OECD estimated 670 years of economically recoverable uranium in total conventional resources and phosphate ores assuming the then-current use rate.Light water reactors make relatively inefficient use of nuclear fuel, mostly using only the very rare uranium-235 isotope. Nuclear reprocessing can make this waste reusable, and newer reactors also achieve a more efficient use of the available resources than older ones. With a pure fast reactor fuel cycle with a burn up of all the uranium and actinides (which presently make up the most hazardous substances in nuclear waste), there is an estimated 160,000 years worth of uranium in total conventional resources and phosphate ore at the price of 60–100 US$/kg. However, reprocessing is expensive, possibly dangerous and can be used to manufacture nuclear weapons. One analysis found that for uranium prices could increase by two orders of magnitudes between 2035 and 2100 and that there could be a shortage near the end of the century. A 2017 study by researchers from MIT and WHOI found that \"at the current consumption rate, global conventional reserves of terrestrial uranium (approximately 7.6 million tonnes) could be depleted in a little over a century\". Limited uranium-235 supply may inhibit substantial expansion with the current nuclear technology. While various ways to reduce dependence on such resources are being explored, new nuclear technologies are considered to not be available in time for climate change mitigation purposes or competition with alternatives of renewables in addition to being more expensive and require costly research and development. A study found it to be uncertain whether identified resources will be developed quickly enough to provide uninterrupted fuel supply to expanded nuclear facilities and various forms of mining may be challenged by ecological barriers, costs, and land requirements. Researchers also report considerable import dependence of nuclear energy.Unconventional uranium resources also exist. Uranium is naturally present in seawater at a concentration of about 3 micrograms per liter, with 4.4 billion tons of uranium considered present in seawater at any time.\n",
      "-Recycling steel saves energy and natural resources. The steel industry saves enough energy to power about 18 million households for a year, on a yearly basis. Recycling metal also uses about 74 percent less energy than making metal. Thus, recyclers of end-of-life vehicles save an estimated 85 million barrels of oil annually that would have been used in the manufacturing of other parts. Likewise, car recycling keeps 11 million tons of steel and 800,000 non-ferrous metals out of landfills and back in consumer use.\n",
      "\n",
      "\n",
      "\n",
      "What is radiometric dating?\n",
      "-Radiometric dating measures the steady decay of radioactive elements in an object to determine its age. It is used to calculate dates for the older part of the planet's geological record. The theory is very complicated but, in essence, the radioactive elements within an object decay to form isotopes of each chemical element. Isotopes are atoms of the element that differ in mass but share the same general properties. Geologists are most interested in the decay of isotopes carbon-14 (into nitrogen-14) and potassium-40 (into argon-40). Carbon-14 aka radiocarbon dating works for organic materials that are less than about 50,000 years old. For older periods, the potassium-argon dating process is more accurate.\n",
      "-Radiometric dating, radioactive dating or radioisotope dating is a technique which is used to date materials such as rocks or carbon, in which trace radioactive impurities were selectively incorporated when they were formed. The method compares the abundance of a naturally occurring radioactive isotope within the material to the abundance of its decay products, which form at a known constant rate of decay. The use of radiometric dating was first published in 1907 by Bertram Boltwood and is now the principal source of information about the absolute age of rocks and other geological features, including the age of fossilized life forms or the age of Earth itself, and can also be used to date a wide range of natural and man-made materials.\n",
      "-Radiometric dating is how geologist determine the age of a rock. In a closed system, the amount of radiogenic isotopes present in a sample is a direct function of time and the decay rate of the mineral. Therefore, to find the age of a sample, geologists find the ratio of daughter isotopes to remaining parent isotopes present in the mineral through different methods, such as mass spectrometry. From the known parent isotopes and the decay constant, we can then determine the age. Different ions can be analyzed for this and are called different dating.\n",
      "-Together with stratigraphic principles, radiometric dating methods are used in geochronology to establish the geologic time scale. Among the best-known techniques are radiocarbon dating, potassium–argon dating and uranium–lead dating. By allowing the establishment of geological timescales, it provides a significant source of information about the ages of fossils and the deduced rates of evolutionary change. Radiometric dating is also used to date archaeological materials, including ancient artifacts.\n",
      "-Radiometric dating is based on the known and constant rate of decay of radioactive isotopes into their radiogenic daughter isotopes. Particular isotopes are suitable for different applications due to the types of atoms present in the mineral or other material and its approximate age. For example, techniques based on isotopes with half-lives in the thousands of years, such as carbon-14, cannot be used to date materials that have ages on the order of billions of years, as the detectable amounts of the radioactive atoms and their decayed daughter isotopes will be too small to measure within the uncertainty of the instruments.\n",
      "\n",
      "\n",
      "\n",
      "What is the role of methane in Fischer-Tropsch processes?\n",
      "-The Fischer–Tropsch process is a collection of chemical reactions that converts a mixture of carbon monoxide and hydrogen, known as syngas, into liquid hydrocarbons. These reactions occur in the presence of metal catalysts, typically at temperatures of 150–300 °C (302–572 °F) and pressures of one to several tens of atmospheres. The Fischer–Tropsch process is an important reaction in both coal liquefaction and gas to liquids technology for producing liquid hydrocarbons.In the usual implementation, carbon monoxide and hydrogen, the feedstocks for FT, are produced from coal, natural gas, or biomass in a process known as gasification. The process then converts these gases into synthetic lubrication oil and synthetic fuel. This process has received intermittent attention as a source of low-sulfur diesel fuel and to address the supply or cost of petroleum-derived hydrocarbons. Fischer-Tropsch process is discussed as a step of producing carbon-neutral liquid hydrocarbon fuels from CO2 and hydrogen.The process was first developed by Franz Fischer and Hans Tropsch at the Kaiser Wilhelm Institute for Coal Research in Mülheim an der Ruhr, Germany, in 1925.\n",
      "-Carbon monoxide and methanol are important chemical feedstocks. CO is utilized by myriad carbonylation reactions. Together with hydrogen, it is the feed for the Fischer–Tropsch process, which affords liquid fuels. Methanol is the precursor to acetic acid, dimethyl ether, formaldehyde, and many methyl compounds (esters, amines, halides). A larger scale application is methanol to olefins, which produces ethylene and propylene.In contrast to the situation for carbon monoxide and methanol, methane and carbon dioxide have limited uses as feedstocks to chemicals and fuels. This disparity contrasts with the relative abundance of methane and carbon dioxide. Methane is often partially converted to carbon monoxide for utilization in Fischer-Tropsch processes. Of interest for upgrading methane is its oxidative coupling: 2CH4 + O2 → C2H4 + 2H2OConversion of carbon dioxide to unsaturated hydrocarbons via electrochemical reduction is a hopeful avenue of research, but no stable and economic technology yet has been developed.\n",
      "-Fischer–Tropsch process The Fischer–Tropsch process is used to produce synfuels from gasified biomass. Carbonaceous material is gasified and the gas is processed to make purified syngas (a mixture of carbon monoxide and hydrogen). The Fischer–Tropsch polymerizes syngas into diesel-range hydrocarbons. While biodiesel and bio-ethanol production so far only use parts of a plant, i.e. oil, sugar, starch or cellulose, BtL production can gasify and utilize the entire plant.\n",
      "-The Fischer–Tropsch process involves a series of chemical reactions that produce a variety of hydrocarbons, ideally having the formula (CnH2n+2). The more useful reactions produce alkanes as follows: (2n + 1) H2 + n CO → CnH2n+2 + n H2Owhere n is typically 10–20. The formation of methane (n = 1) is unwanted. Most of the alkanes produced tend to be straight-chain, suitable as diesel fuel. In addition to alkane formation, competing reactions give small amounts of alkenes, as well as alcohols and other oxygenated hydrocarbons.The reaction is a highly exothermic reaction due to a standard reaction enthalpy (ΔH) of −165 kJ/mol CO combined.\n",
      "-In order to obtain the mixture of CO and H2 required for the Fischer–Tropsch process, methane (main component of natural gas) may be subjected to partial oxidation which yields a raw synthesis gas mixture of mostly carbon dioxide, carbon monoxide, hydrogen gas (and sometimes water and nitrogen). The ratio of carbon monoxide to hydrogen in the raw synthesis gas mixture can be adjusted e.g. using the water gas shift reaction. Removing impurities, particularly nitrogen, carbon dioxide and water, from the raw synthesis gas mixture yields pure synthesis gas (syngas).\n",
      "\n",
      "\n",
      "\n",
      "What is a phageome?\n",
      "-A phageome is a community of bacteriophages and their metagenomes localized in a particular environment, similar to a microbiome. The term was first used in an article by Modi et al in 2013 and has continued to be used in scientific articles that relate to bacteriophages and their metagenomes. A bacteriophage, or phage for short, is a virus that has the ability to infect bacteria and archaea, and can replicate inside of them. Phageome is a subcategory of virome, which is all of the viruses that are associated with a host or environment. Phages make up the majority of most viromes and are currently understood as being the most abundant organism. Oftentimes scientists will look only at a phageome instead of a virome while conducting research.\n",
      "-Bacteriophages, often just called phages, are viruses that parasite bacteria and archaea. Marine phages parasite marine bacteria and archaea, such as cyanobacteria. They are a common and diverse group of viruses and are the most abundant biological entity in marine environments, because their hosts, bacteria, are typically the numerically dominant cellular life in the sea. Generally there are about 1 million to 10 million viruses in each mL of seawater, or about ten times more double-stranded DNA viruses than there are cellular organisms, although estimates of viral abundance in seawater can vary over a wide range. Tailed bacteriophages appear to dominate marine ecosystems in number and diversity of organisms. Bacteriophages belonging to the families Corticoviridae, Inoviridae and Microviridae are also known to infect diverse marine bacteria.\n",
      "-Definitions Microbial communities have commonly been defined as the collection of microorganisms living together. More specifically, microbial communities are defined as multi-species assemblages, in which (micro) organisms interact with each other in a contiguous environment. In 1988, Whipps and colleagues working on the ecology of rhizosphere microorganisms provided the first definition of the term microbiome. They described the microbiome as a combination of the words micro and biome, naming a \"characteristic microbial community\" in a \"reasonably well-defined habitat which has distinct physio-chemical properties\" as their \"theatre of activity\". This definition represents a substantial advancement of the definition of a microbial community, as it defines a microbial community with distinct properties and functions and its interactions with its environment, resulting in the formation of specific ecological niches.However, many other microbiome definitions have been published in recent decades. By 2020 the most cited definition was by Lederberg, and described microbiomes within an ecological context as a community of commensal, symbiotic, and pathogenic microorganisms within a body space or other environment. Marchesi and Ravel focused in their definition on the genomes and microbial (and viral) gene expression patterns and proteomes in a given environment and its prevailing biotic and abiotic conditions. All these definitions imply that general concepts of macro-ecology could be easily applied to microbe-microbe as well as to microbe-host interactions. However, the extent to which these concepts, developed for macro-eukaryotes, can be applied to prokaryotes with their different lifestyles regarding dormancy, variation of phenotype, and horizontal gene transfer as well as to micro-eukaryotes that is not quite clear. This raises the challenge of considering an entirely novel body of conceptual ecology models and theory for microbiome ecology, particularly in relation to the diverse hierarchies of interactions of microbes with one another and with the host biotic and abiotic environments. Many current definitions fail to capture this complexity and describe the term microbiome as encompassing the genomes of microorganisms only.\n",
      "-Viruses Metagenomic sequencing is particularly useful in the study of viral communities. As viruses lack a shared universal phylogenetic marker (as 16S RNA for bacteria and archaea, and 18S RNA for eukarya), the only way to access the genetic diversity of the viral community from an environmental sample is through metagenomics. Viral metagenomes (also called viromes) should thus provide more and more information about viral diversity and evolution. For example, a metagenomic pipeline called Giant Virus Finder showed the first evidence of existence of giant viruses in a saline desert and in Antarctic dry valleys.\n",
      "-Bacteriophages are among the most common and diverse entities in the biosphere. Bacteriophages are ubiquitous viruses, found wherever bacteria exist. It is estimated there are more than 1031 bacteriophages on the planet, more than every other organism on Earth, including bacteria, combined. Viruses are the most abundant biological entity in the water column of the world's oceans, and the second largest component of biomass after prokaryotes, where up to 9x108 virions per millilitre have been found in microbial mats at the surface, and up to 70% of marine bacteria may be infected by phages.Phages have been used since the late 20th century as an alternative to antibiotics in the former Soviet Union and Central Europe, as well as in France. They are seen as a possible therapy against multi-drug-resistant strains of many bacteria (see phage therapy).Phages are known to interact with the immune system both indirectly via bacterial expression of phage-encoded proteins and directly by influencing innate immunity and bacterial clearance. Phage–host interactions are becoming increasingly important areas of research.\n",
      "\n",
      "\n",
      "\n",
      "What is organography?\n",
      "-Organography (from Greek όργανο, organo, \"organ\"; and -γραφή, -graphy) is the scientific description of the structure and function of the organs of living things.\n",
      "-Organography as a scientific study starts with Aristotle, who considered the parts of plants as \"organs\" and began to consider the relationship between different organs and different functions. In the 17th century Joachim Jung, clearly articulated that plants are composed of different organ types such as root, stem and leaf, and he went on to define these organ types on the basis of form and position.\n",
      "-Organ patterns Growth and division of plant cells together result in the growth of tissue, and specific tissue growth contributes to the development of plant organs.\n",
      "-Plant morphology treats both the vegetative structures of plants, as well as the reproductive structures.\n",
      "The vegetative (somatic) structures of vascular plants include two major organ systems: (1) a shoot system, composed of stems and leaves, and (2) a root system. These two systems are common to nearly all vascular plants, and provide a unifying theme for the study of plant morphology.\n",
      "-The study of plant organs is covered in plant morphology. Organs of plants can be divided into vegetative and reproductive. Vegetative plant organs include roots, stems, and leaves. The reproductive organs are variable. In flowering plants, they are represented by the flower, seed and fruit. In conifers, the organ that bears the reproductive structures is called a cone. In other divisions (phyla) of plants, the reproductive organs are called strobili, in Lycopodiophyta, or simply gametophores in mosses. Common organ system designations in plants include the differentiation of shoot and root. All parts of the plant above ground (in non-epiphytes), including the functionally distinct leaf and flower organs, may be classified together as the shoot organ system.The vegetative organs are essential for maintaining the life of a plant. While there can be 11 organ systems in animals, there are far fewer in plants, where some perform the vital functions, such as photosynthesis, while the reproductive organs are essential in reproduction. However, if there is asexual vegetative reproduction, the vegetative organs are those that create the new generation of plants (see clonal colony).\n",
      "\n",
      "\n",
      "\n",
      "What is the definition of anatomy?\n",
      "-Anatomy (from Ancient Greek ἀνατομή (anatomḗ) 'dissection') is the branch of biology concerned with the study of the structure of organisms and their parts. Anatomy is a branch of natural science that deals with the structural organization of living things. It is an old science, having its beginnings in prehistoric times. Anatomy is inherently tied to developmental biology, embryology, comparative anatomy, evolutionary biology, and phylogeny, as these are the processes by which anatomy is generated, both over immediate and long-term timescales. Anatomy and physiology, which study the structure and function of organisms and their parts respectively, make a natural pair of related disciplines, and are often studied together. Human anatomy is one of the essential basic sciences that are applied in medicine.Anatomy is a complex and dynamic field that is constantly evolving as new discoveries are made. In recent years, there has been a significant increase in the use of advanced imaging techniques, such as MRI and CT scans, which allow for more detailed and accurate visualizations of the body's structures.\n",
      "-The branch of biology dealing with the study of the bodies and their specific structural features called morphology. Anatomy is a branch of morphology that deals with the structure of the body at a level higher than tissue. Anatomy is closely related to histology, which studies the structure of tissues, as well as cytology, which studies the structure and function of the individual cells, from which the tissues and organs of the studied macroorganism are built. Taken together, anatomy, histology, cytology and embryology represent a morphology The study of functions and mechanisms in a body is physiology.\n",
      "-Derived from the Greek ἀνατομή anatomē \"dissection\" (from ἀνατέμνω anatémnō \"I cut up, cut open\" from ἀνά aná \"up\", and τέμνω témnō \"I cut\"), anatomy is the scientific study of the structure of organisms including their systems, organs and tissues. It includes the appearance and position of the various parts, the materials from which they are composed, and their relationships with other parts. Anatomy is quite distinct from physiology and biochemistry, which deal respectively with the functions of those parts and the chemical processes involved. For example, an anatomist is concerned with the shape, size, position, structure, blood supply and innervation of an organ such as the liver; while a physiologist is interested in the production of bile, the role of the liver in nutrition and the regulation of bodily functions.The discipline of anatomy can be subdivided into a number of branches, including gross or macroscopic anatomy and microscopic anatomy. Gross anatomy is the study of structures large enough to be seen with the naked eye, and also includes superficial anatomy or surface anatomy, the study by sight of the external body features. Microscopic anatomy is the study of structures on a microscopic scale, along with histology (the study of tissues), and embryology (the study of an organism in its immature condition). Regional anatomy is the study of the interrelationships of all of the structures in a specific body region, such as the abdomen. In contrast, systemic anatomy is the study of the structures that make up a discrete body system—that is, a group of structures that work together to perform a unique body function, such as the digestive system.Anatomy can be studied using both invasive and non-invasive methods with the goal of obtaining information about the structure and organization of organs and systems. Methods used include dissection, in which a body is opened and its organs studied, and endoscopy, in which a video camera-equipped instrument is inserted through a small incision in the body wall and used to explore the internal organs and other structures. Angiography using X-rays or magnetic resonance angiography are methods to visualize blood vessels.The term \"anatomy\" is commonly taken to refer to human anatomy. However, substantially similar structures and tissues are found throughout the rest of the animal kingdom, and the term also includes the anatomy of other animals. The term zootomy is also sometimes used to specifically refer to non-human animals. The structure and tissues of plants are of a dissimilar nature and they are studied in plant anatomy.\n",
      "-Morphology is a branch of biology dealing with the study of the form and structure of organisms and their specific structural features.This includes aspects of the outward appearance (shape, structure, colour, pattern, size), i.e. external morphology (or eidonomy), as well as the form and structure of the internal parts like bones and organs, i.e. internal morphology (or anatomy). This is in contrast to physiology, which deals primarily with function. Morphology is a branch of life science dealing with the study of gross structure of an organism or taxon and its component parts.\n",
      "-Comparative morphology is analysis of the patterns of the locus of structures within the body plan of an organism, and forms the basis of taxonomical categorization.\n",
      "Functional morphology is the study of the relationship between the structure and function of morphological features.\n",
      "Experimental morphology is the study of the effects of external factors upon the morphology of organisms under experimental conditions, such as the effect of genetic mutation.\n",
      "Anatomy is a \"branch of morphology that deals with the structure of organisms\".\n",
      "Molecular morphology is a rarely used term, usually referring to the superstructure of polymers such as fiber formation or to larger composite assemblies. The term is commonly not applied to the spatial structure of individual molecules.\n",
      "Gross morphology refers to the collective structures of an organism as a whole as a general description of the form and structure of an organism, taking into account all of its structures without specifying an individual structure.\n",
      "\n",
      "\n",
      "\n",
      "What is a trophic level in an ecological pyramid?\n",
      "-Trophic levels A trophic level (from Greek troph, τροφή, trophē, meaning \"food\" or \"feeding\") is \"a group of organisms acquiring a considerable majority of its energy from the lower adjacent level (according to ecological pyramids) nearer the abiotic source.\": 383  Links in food webs primarily connect feeding relations or trophism among species. Biodiversity within ecosystems can be organized into trophic pyramids, in which the vertical dimension represents feeding relations that become further removed from the base of the food chain up toward top predators, and the horizontal dimension represents the abundance or biomass at each level. When the relative abundance or biomass of each species is sorted into its respective trophic level, they naturally sort into a 'pyramid of numbers'.Species are broadly categorized as autotrophs (or primary producers), heterotrophs (or consumers), and Detritivores (or decomposers). Autotrophs are organisms that produce their own food (production is greater than respiration) by photosynthesis or chemosynthesis. Heterotrophs are organisms that must feed on others for nourishment and energy (respiration exceeds production). Heterotrophs can be further sub-divided into different functional groups, including primary consumers (strict herbivores), secondary consumers (carnivorous predators that feed exclusively on herbivores), and tertiary consumers (predators that feed on a mix of herbivores and predators). Omnivores do not fit neatly into a functional category because they eat both plant and animal tissues. It has been suggested that omnivores have a greater functional influence as predators because compared to herbivores, they are relatively inefficient at grazing.Trophic levels are part of the holistic or complex systems view of ecosystems. Each trophic level contains unrelated species that are grouped together because they share common ecological functions, giving a macroscopic view of the system. While the notion of trophic levels provides insight into energy flow and top-down control within food webs, it is troubled by the prevalence of omnivory in real ecosystems. This has led some ecologists to \"reiterate that the notion that species clearly aggregate into discrete, homogeneous trophic levels is fiction.\": 815  Nonetheless, recent studies have shown that real trophic levels do exist, but \"above the herbivore trophic level, food webs are better characterized as a tangled web of omnivores.\": 612 Keystone species A keystone species is a species that is connected to a disproportionately large number of other species in the food-web. Keystone species have lower levels of biomass in the trophic pyramid relative to the importance of their role. The many connections that a keystone species holds means that it maintains the organization and structure of entire communities. The loss of a keystone species results in a range of dramatic cascading effects (termed trophic cascades) that alters trophic dynamics, other food web connections, and can cause the extinction of other species. The term keystone species was coined by Robert Paine in 1969 and is a reference to the keystone architectural feature as the removal of a keystone species can result in a community collapse just as the removal of the keystone in an arch can result in the arch's loss of stability.Sea otters (Enhydra lutris) are commonly cited as an example of a keystone species because they limit the density of sea urchins that feed on kelp. If sea otters are removed from the system, the urchins graze until the kelp beds disappear, and this has a dramatic effect on community structure. Hunting of sea otters, for example, is thought to have led indirectly to the extinction of the Steller's sea cow (Hydrodamalis gigas). While the keystone species concept has been used extensively as a conservation tool, it has been criticized for being poorly defined from an operational stance. It is difficult to experimentally determine what species may hold a keystone role in each ecosystem. Furthermore, food web theory suggests that keystone species may not be common, so it is unclear how generally the keystone species model can be applied.\n",
      "-In above ground food webs, energy moves from producers (plants) to primary consumers (herbivores) and then to secondary consumers (predators). The phrase, trophic level, refers to the different levels or steps in the energy pathway. In other words, the producers, consumers, and decomposers are the main trophic levels. This chain of energy transferring from one species to another can continue several more times, but eventually ends. At the end of the food chain, decomposers such as bacteria and fungi break down dead plant and animal material into simple nutrients.\n",
      "-A pyramid of energy shows how much energy is retained in the form of new biomass from each trophic level, while a pyramid of biomass shows how much biomass (the amount of living or organic matter present in an organism) is present in the organisms. There is also a pyramid of numbers representing the number of individual organisms at each trophic level. Pyramids of energy are normally upright, but other pyramids can be inverted(pyramid of biomass for marine region) or take other shapes.(spindle shaped pyramid) Ecological pyramids begin with producers on the bottom (such as plants) and proceed through the various trophic levels (such as herbivores that eat plants, then carnivores that eat flesh, then omnivores that eat both plants and flesh, and so on). The highest level is the top of the food chain.\n",
      "-Trophic level A species’ trophic level is their position in the food chain or web. At the bottom of the food web are autotrophs, also known as primary producer. Producers provide their own energy through photosynthesis or chemosynthesis, plants are primary producers. The next level is herbivores (primary consumers), these species feed on vegetation for their energy source. Herbivores are consumed by omnivores or carnivores. These species are secondary and tertiary consumers. Additional levels to the trophic scale come when smaller omnivores or carnivores are eaten by larger ones. At the top of the food web is the apex predator, this animal species is not consumed by any other in the community. Herbivores, omnivores and carnivores are all heterotrophs.A basic example of a food chain is; grass → rabbit → fox. Food chains become more complex when more species are present, often being food webs. Energy is passed up through trophic levels. Energy is lost at each level, due to ecological inefficiencies.The trophic level of an organism can change based on the other species present. For example, tuna can be an apex predator eating the smaller fish, such as mackerel. However, in a community where a shark species is present the shark becomes the apex predator, feeding on the tuna.Decomposers play a role in the trophic pyramid. They provide energy source and nutrients to the plant species in the community. Decomposers such as fungi and bacteria recycle energy back to the base of the food web by feeding on dead organisms from all trophic levels.\n",
      "-Ecological pyramids In a pyramid of numbers, the number of consumers at each level decreases significantly, so that a single top consumer, (e.g., a polar bear or a human), will be supported by a much larger number of separate producers. There is usually a maximum of four or five links in a food chain, although food chains in aquatic ecosystems are more often longer than those on land. Eventually, all the energy in a food chain is dispersed as heat.Ecological pyramids place the primary producers at the base. They can depict different numerical properties of ecosystems, including numbers of individuals per unit of area, biomass (g/m2), and energy (k cal m−2 yr−1). The emergent pyramidal arrangement of trophic levels with amounts of energy transfer decreasing as species become further removed from the source of production is one of several patterns that is repeated amongst the planets ecosystems. The size of each level in the pyramid generally represents biomass, which can be measured as the dry weight of an organism. Autotrophs may have the highest global proportion of biomass, but they are closely rivaled or surpassed by microbes.Pyramid structure can vary across ecosystems and across time. In some instances biomass pyramids can be inverted. This pattern is often identified in aquatic and coral reef ecosystems. The pattern of biomass inversion is attributed to different sizes of producers. Aquatic communities are often dominated by producers that are smaller than the consumers that have high growth rates. Aquatic producers, such as planktonic algae or aquatic plants, lack the large accumulation of secondary growth as exists in the woody trees of terrestrial ecosystems. However, they are able to reproduce quickly enough to support a larger biomass of grazers. This inverts the pyramid. Primary consumers have longer lifespans and slower growth rates that accumulates more biomass than the producers they consume. Phytoplankton live just a few days, whereas the zooplankton eating the phytoplankton live for several weeks and the fish eating the zooplankton live for several consecutive years. Aquatic predators also tend to have a lower death rate than the smaller consumers, which contributes to the inverted pyramidal pattern. Population structure, migration rates, and environmental refuge for prey are other possible causes for pyramids with biomass inverted. Energy pyramids, however, will always have an upright pyramid shape if all sources of food energy are included and this is dictated by the second law of thermodynamics.\n",
      "\n",
      "\n",
      "\n",
      "What is a crossover experiment?\n",
      "-In chemistry, a crossover experiment is a method used to study the mechanism of a chemical reaction. In a crossover experiment, two similar but distinguishable reactants simultaneously undergo a reaction as part of the same reaction mixture. The products formed will either correspond directly to one of the two reactants (non-crossover products) or will include components of both reactants (crossover products). The aim of a crossover experiment is to determine whether or not a reaction process involves a stage where the components of each reactant have an opportunity to exchange with each other.\n",
      "-In medicine, a crossover study or crossover trial is a longitudinal study in which subjects receive a sequence of different treatments (or exposures). While crossover studies can be observational studies, many important crossover studies are controlled experiments, which are discussed in this article. Crossover designs are common for experiments in many scientific disciplines, for example psychology, pharmaceutical science, and medicine.\n",
      "-The results of crossover experiments are often straightforward to analyze, making them one of the most useful and most frequently applied methods of mechanistic study. In organic chemistry, crossover experiments are most often used to distinguish between intramolecular and intermolecular reactions.Inorganic and organometallic chemists rely heavily on crossover experiments, and in particular isotopic labeling experiments, for support or contradiction of proposed mechanisms. When the mechanism being investigated is more complicated than an intra- or intermolecular substitution or rearrangement, crossover experiment design can itself become a challenging question. A well-designed crossover experiment can lead to conclusions about a mechanism that would otherwise be impossible to make. Many mechanistic studies include both crossover experiments and measurements of rate and kinetic isotope effects.\n",
      "-Crossover experiments allow for experimental study of a reaction mechanism. Mechanistic studies are of interest to theoretical and experimental chemists for a variety of reasons including prediction of stereochemical outcomes, optimization of reaction conditions for rate and selectivity, and design of improved catalysts for better turnover number, robustness, etc. Since a mechanism cannot be directly observed or determined solely based on the reactants or products, mechanisms are challenging to study experimentally. Only a handful of experimental methods are capable of providing information about the mechanism of a reaction, including crossover experiments, studies of the kinetic isotope effect, and rate variations by substituent. The crossover experiment has the advantage of being conceptually straightforward and relatively easy to design, carry out, and interpret. In modern mechanistic studies, crossover experiments and KIE studies are commonly used in conjunction with computational methods.\n",
      "-In designing a crossover experiment the first task is to propose possible mechanisms for the reaction being studied. Based on these possible mechanisms, the goal is to determine either a traditional crossover experiment or an isotope scrambling experiment that will enable the researcher to distinguish between the two or more possible mechanisms. Often many methods of mechanistic study will have to be employed to support or discount all of the mechanisms proposed. However, in some cases a crossover experiment alone will be able to distinguish between the main possibilities, for example in the case of intramolecular vs. intermolecular organic reaction mechanisms.\n",
      "\n",
      "\n",
      "\n",
      "What is the role of IL-10 in the formation of Tr1 cells and tolerogenic DCs?\n",
      "-During a tolerant state potential effector cells remain but are tightly regulated by induced antigen-specific CD4+ regulatory T cells (iTregs). Many subsets of iTregs play a part in this process, but CD4+CD25+FoxP3+ Tregs play a key role, because they have the ability to convert conventional T cells into iTregs directly by secretion of the suppressive cytokines TGF-β, IL-10 or IL-35, or indirectly via dendritic cells (DCs). Production of IL-10 induces the formation of another population of regulatory T cells called Tr1. Tr1 cells are dependent on IL-10 and TGF-β as well as Tregs, but differ from them by lacking expression of Foxp3. High IL-10 production is characteristic for Tr1 cells themselves and they also produce TGF-β. In the presence of IL-10 can be also induced tolerogenic DCs from monocytes, whose production of IL-10 is also important for Tr1 formation. These interactions lead to the production of enzymes such as IDO (indolamine 2,3-dioxygenase) that catabolize essential amino acids. This microenvironment with a lack of essential amino acids together with other signals results in mTOR (mammalian target of rapamycin) inhibition which, particularly in synergy with TGF-β, direct the induction of new FoxP3 (forkhead box protein 3) expressing Tregs.\n",
      "-IL-27, together with TGF-β induces IL-10–producing regulatory T cells with Tr1-like properties cells. IL-27 alone can induce IL-10-producing Tr1 cells, but in the absence of TGF-β, the cells produce large quantities of both IFN-γ and IL-10. IL-6 and IL-21 also plays a role in differentiation as they regulate expression of transcription factors necessary for IL-10 production, which is believed to start up the differentiation itself later on.\n",
      "-The suppressing and tolerance-inducing effect of Tr1 cells is mediated mainly by cytokines. The other mechanism as cell to cell contact, modulation of dendritic cells, metabolic disruption and cytolysis is however also available to them. In vivo Tr1 cells need to be activated, to be able to exert their regulatory effects.\n",
      "-Studies have suggested a role for tolerogenic dendritic cells in the treatment of diseases like type 1 diabetes mellitus and multiple sclerosis.In animal models of Diabetes mellitus (NOD mice), GM-CSF induces resistance by increasing the frequency of regulatory T cells which can suppress T cell proliferation through their T-cell receptors. GM-CSF treated mice were found to have a semi-mature phenotype of dendritic cells which were inefficient at inducing antigen specific cytotoxic T cells compared to controls.In multiple sclerosis research, EAE mice were completely protected from symptoms when injected with dendritic cells matured with TNF-α and antigen specific peptide compared to controls. T regulatory cells of mice treated with TNF-α produced IL-10, a cytokine which is able to inhibit the Th1 response therefore protecting against the Th1 dependent autoimmune EAE.Mouse models of autoimmune thyroiditis showed that a semi-mature phenotype of dendritic cells is maintained after mouse thyroglobulin immunization in GM-CSF treated but not control mice. IL-10 produced by T regulatory cells was important in suppressing the mouse thyroglobulin specific T cell response and therefore protecting against Experimental autoimmune thyroiditis in mice.Phase I studies into the safety and efficacy of tolerogenic DC therapy in humans have demonstrated the appropriateness of the therapy for further research. Future research will consider the effectiveness of tolerogenic therapies in a number of planned clinical trials into autoimmune diseases.\n",
      "-Tolerogenic DCs are essential in maintenance of central and peripheral tolerance through induction of T cell clonal deletion, T cell anergy and generation and activation of regulatory T (Treg) cells. For that reason, tolerogenic DCs are possible candidates for specific cellular therapy for treatment of allergic diseases, autoimmune diseases (e.g. type 1 diabetes, multiple sclerosis, rheumatoid arthritis) or transplant rejections.Tolerogenic DCs often display an immature or semi-mature phenotype with characteristically low expression of costimulatory (e.g. CD80, CD86) and MHC molecules on their surface. Tolerogenic DCs also produce different cytokines as mature DCs (e.g. anti-inflammatory cytokines interleukin (IL)-10, transforming growth factor-β (TGF-β)). Moreover, tolerogenic DCs may also express various inhibitory surface molecules (e.g. programmed cell death ligand (PDL)-1, PDL-2) or can modulate metabolic parameters and change T cell response. For example, tolerogenic DCs can release or induce enzymes such as indoleamine 2,3-dioxygenase (IDO) or heme oxygenase-1 (HO-1). IDO promotes the degradation of tryptophan to N-formylkynurenin leading to reduced T cell proliferation, whereas HO- 1 catalyzes degradation of hemoglobin resulting in production of monoxide and lower DC immunogenicity. Besides that, tolerogenic DCs also may produce retinoic acid (RA), which induces Treg differentiation.Human tolerogenic DCs may be induced by various immunosuppressive drugs or biomediators. Immunosuppressive drugs, e.g. corticosteroid dexamethasone, rapamycin, cyclosporine or acetylsalicylic acid, cause low expression of costimulatory molecules, reduced expression of MHC, higher expression of inhibitory molecules (e.g. PDL-1) or higher secretion of IL-10 or IDO. In addition, incubation with inhibitory cytokines IL-10 or TGF-β leads to generation of tolerogenic phenotype. Other mediators also affect generation of tolerogenic DC, e.g. vitamin D3, vitamin D2, hepatocyte growth factor or vasoactive intestinal peptide. The oldest and mostly used cytokine cocktail for in vitro DC generation is GM-CSF/IL-4.Tolerogenic DCs may be a potential candidate for specific immunotherapy and are studied for using them for treatment of inflammatory, autoimmune and allergic diseases and also in transplant medicine. Important and interesting feature of tolerogenic DCs is also the migratory capacity toward secondary lymph organs, leading to T-cell mediated immunosuppression. The first trial to transfer tolerogenic DCs to humans was undertaken by Ralph Steinman's group in 2001. Relating to the DC administration, various application have been used in humans in last years. Tolerogenic DCs have been injected e.g. intraperitoneally in patients with Crohn's disease, intradermally in diabetes and rheumatoid arthritis patients, subcutaneously in rheumatoid arthritis patients and via arthroscopic injections in joints of patient with rheumatoid and inflammatory arthritis.Therefore, it is necessary to test tolerogenic DCs for a stable phenotype to exclude a loss of the regulatory function and a switch to an immunostimulatory activity.\n",
      "\n",
      "\n",
      "\n",
      "What is the reason behind the designation of Class L dwarfs, and what is their color and composition?\n",
      "-Class L Class L dwarfs get their designation because they are cooler than M stars and L is the remaining letter alphabetically closest to M. Some of these objects have masses large enough to support hydrogen fusion and are therefore stars, but most are of substellar mass and are therefore brown dwarfs. They are a very dark red in color and brightest in infrared. Their atmosphere is cool enough to allow metal hydrides and alkali metals to be prominent in their spectra.Due to low surface gravity in giant stars, TiO- and VO-bearing condensates never form. Thus, L-type stars larger than dwarfs can never form in an isolated environment. However, it may be possible for these L-type supergiants to form through stellar collisions, an example of which is V838 Monocerotis while in the height of its luminous red nova eruption.\n",
      "-Spectral class L The defining characteristic of spectral class M, the coolest type in the long-standing classical stellar sequence, is an optical spectrum dominated by absorption bands of titanium(II) oxide (TiO) and vanadium(II) oxide (VO) molecules. However, GD 165B, the cool companion to the white dwarf GD 165, had none of the hallmark TiO features of M dwarfs. The subsequent identification of many objects like GD 165B ultimately led to the definition of a new spectral class, the L dwarfs, defined in the red optical region of the spectrum not by metal-oxide absorption bands (TiO, VO), but by metal hydride emission bands (FeH, CrH, MgH, CaH) and prominent atomic lines of alkali metals (Na, K, Rb, Cs). As of 2013, over 900 L dwarfs have been identified, most by wide-field surveys: the Two Micron All Sky Survey (2MASS), the Deep Near Infrared Survey of the Southern Sky (DENIS), and the Sloan Digital Sky Survey (SDSS). This spectral class contains not only the brown dwarfs, because the coolest main-sequence stars above brown dwarfs (> 80 MJ) have the spectral class L2 to L6.\n",
      "-Brown dwarfs and sub-stellar objects Protostars with masses less than roughly 0.08 M☉ (1.6×1029 kg) never reach temperatures high enough for nuclear fusion of hydrogen to begin. These are known as brown dwarfs. The International Astronomical Union defines brown dwarfs as stars massive enough to fuse deuterium at some point in their lives (13 Jupiter masses (MJ), 2.5 × 1028 kg, or 0.0125 M☉). Objects smaller than 13 MJ are classified as sub-brown dwarfs (but if they orbit around another stellar object they are classified as planets). Both types, deuterium-burning and not, shine dimly and fade away slowly, cooling gradually over hundreds of millions of years.\n",
      "-Cool red and brown dwarf classes The new spectral types L, T, and Y were created to classify infrared spectra of cool stars. This includes both red dwarfs and brown dwarfs that are very faint in the visible spectrum.Brown dwarfs, stars that do not undergo hydrogen fusion, cool as they age and so progress to later spectral types. Brown dwarfs start their lives with M-type spectra and will cool through the L, T, and Y spectral classes, faster the less massive they are; the highest-mass brown dwarfs cannot have cooled to Y or even T dwarfs within the age of the universe. Because this leads to an unresolvable overlap between spectral types' effective temperature and luminosity for some masses and ages of different L-T-Y types, no distinct temperature or luminosity values can be given.\n",
      "-Classification of brown dwarfs Spectral class M These are brown dwarfs with a spectral class of M5.5 or later; they are also called late-M dwarfs. These can be considered red dwarfs in the eyes of some scientists. Many brown dwarfs with spectral type M are young objects, such as Teide 1.\n",
      "\n",
      "\n",
      "\n",
      "What was Isaac Newton's explanation for rectilinear propagation of light?\n",
      "-Isaac Newton rejected the wave explanation of rectilinear propagation, believing that if light consisted of waves, it would \"bend and spread every way\" into the shadows. His corpuscular theory of light explained rectilinear propagation more simply, and it accounted for the ordinary laws of refraction and reflection, including TIR, on the hypothesis that the corpuscles of light were subject to a force acting perpendicular to the interface. In this model, for dense-to-rare incidence, the force was an attraction back towards the denser medium, and the critical angle was the angle of incidence at which the normal velocity of the approaching corpuscle was just enough to reach the far side of the force field; at more oblique incidence, the corpuscle would be turned back. Newton gave what amounts to a formula for the critical angle, albeit in words: \"as the Sines are which measure the Refraction, so is the Sine of Incidence at which the total Reflexion begins, to the Radius of the Circle\".Newton went beyond Huygens in two ways. First, not surprisingly, Newton pointed out the relationship between TIR and dispersion: when a beam of white light approaches a glass-to-air interface at increasing obliquity, the most strongly-refracted rays (violet) are the first to be \"taken out\" by \"total Reflexion\", followed by the less-refracted rays. Second, he observed that total reflection could be frustrated (as we now say) by laying together two prisms, one plane and the other slightly convex; and he explained this simply by noting that the corpuscles would be attracted not only to the first prism, but also to the second.In two other ways, however, Newton's system was less coherent. First, his explanation of partial reflection depended not only on the supposed forces of attraction between corpuscles and media, but also on the more nebulous hypothesis of \"Fits of easy Reflexion\" and \"Fits of easy Transmission\". Second, although his corpuscles could conceivably have \"sides\" or \"poles\", whose orientations could conceivably determine whether the corpuscles suffered ordinary or extraordinary refraction in \"Island-Crystal\", his geometric description of the extraordinary refraction was theoretically unsupported and empirically inaccurate.\n",
      "-Wave theory of light Beginning in 1670 and progressing over three decades, Isaac Newton developed and championed his corpuscular theory, arguing that the perfectly straight lines of reflection demonstrated light's particle nature, as at that time no wave theory demonstrated travel in straight lines.: 19  He explained refraction by positing that particles of light accelerated laterally upon entering a denser medium. Around the same time, Newton's contemporaries Robert Hooke and Christiaan Huygens, and later Augustin-Jean Fresnel, mathematically refined the wave viewpoint, showing that if light traveled at different speeds in different media, refraction could be easily explained as the medium-dependent propagation of light waves. The resulting Huygens–Fresnel principle was extremely successful at reproducing light's behaviour and was consistent with Thomas Young's discovery of wave interference of light by his double-slit experiment in 1801. The wave view did not immediately displace the ray and particle view, but began to dominate scientific thinking about light in the mid 19th century, since it could explain polarization phenomena that the alternatives could not.James Clerk Maxwell discovered that he could apply his previously discovered Maxwell's equations, along with a slight modification to describe self-propagating waves of oscillating electric and magnetic fields. It quickly became apparent that visible light, ultraviolet light, and infrared light were all electromagnetic waves of differing frequency.: 272  This theory became a critical ingredient in the beginning of quantum mechanics.\n",
      "-According to Laplace's elaboration of Newton's theory of refraction, a corpuscle incident on a plane interface between two homogeneous isotropic media was subject to a force field that was symmetrical about the interface. If both media were transparent, total reflection would occur if the corpuscle were turned back before it exited the field in the second medium. But if the second medium were opaque, reflection would not be total unless the corpuscle were turned back before it left the first medium; this required a larger critical angle than the one given by Snell's law, and consequently impugned the validity of Wollaston's method for opaque media. Laplace combined the two cases into a single formula for the relative refractive index in terms of the critical angle (minimum angle of incidence for TIR). The formula contained a parameter which took one value for a transparent external medium and another value for an opaque external medium. Laplace's theory further predicted a relationship between refractive index and density for a given substance.\n",
      "-Nonetheless, he continued to develop his ideas. He believed that a wave model could much better explain many aspects of light propagation than the corpuscular model: A very extensive class of phenomena leads us still more directly to the same conclusion; they consist chiefly of the production of colours by means of transparent plates, and by diffraction or inflection, none of which have been explained upon the supposition of emanation, in a manner sufficiently minute or comprehensive to satisfy the most candid even of the advocates for the projectile system; while on the other hand all of them may be at once understood, from the effect of the interference of double lights, in a manner nearly similar to that which constitutes in sound the sensation of a beat, when two strings forming an imperfect unison, are heard to vibrate together.\n",
      "-During this period, many scientists proposed a wave theory of light based on experimental observations, including Robert Hooke, Christiaan Huygens and Leonhard Euler. However, Isaac Newton, who did many experimental investigations of light, had rejected the wave theory of light and developed his corpuscular theory of light according to which light is emitted from a luminous body in the form of tiny particles. This theory held sway until the beginning of the nineteenth century despite the fact that many phenomena, including diffraction effects at edges or in narrow apertures, colours in thin films and insect wings, and the apparent failure of light particles to crash into one another when two light beams crossed, could not be adequately explained by the corpuscular theory which, nonetheless, had many eminent supporters, including Pierre-Simon Laplace and Jean-Baptiste Biot.\n",
      "\n",
      "\n",
      "\n",
      "What is the relationship between chemical potential and quarks/antiquarks?\n",
      "-The phase diagram of quark matter is not well known, either experimentally or theoretically. A commonly conjectured form of the phase diagram is shown in the figure to the right. It is applicable to matter in a compact star, where the only relevant thermodynamic potentials are quark chemical potential μ and temperature T.  For guidance it also shows the typical values of μ and T in heavy-ion collisions and in the early universe. For readers who are not familiar with the concept of a chemical potential, it is helpful to think of μ as a measure of the imbalance between quarks and antiquarks in the system. Higher μ means a stronger bias favoring quarks over antiquarks. At low temperatures there are no antiquarks, and then higher μ generally means a higher density of quarks.\n",
      "-It is common in electrochemistry and solid-state physics to discuss both the chemical potential and the electrochemical potential of the electrons. However, in the two fields, the definitions of these two terms are sometimes swapped. In electrochemistry, the electrochemical potential of electrons (or any other species) is the total potential, including both the (internal, nonelectrical) chemical potential and the electric potential, and is by definition constant across a device in equilibrium, whereas the chemical potential of electrons is equal to the electrochemical potential minus the local electric potential energy per electron. In solid-state physics, the definitions are normally compatible with this, but occasionally  the definitions are swapped.\n",
      "-Chemical potential Assuming that the concentration of fermions does not change with temperature, then the total chemical potential µ (Fermi level) of the three-dimensional ideal Fermi gas is related to the zero temperature Fermi energy EF by a Sommerfeld expansion (assuming  kBT≪EF ): where T is the temperature.Hence, the internal chemical potential, µ-E0, is approximately equal to the Fermi energy at temperatures that are much lower than the characteristic Fermi temperature TF. This characteristic temperature is on the order of 105 K for a metal, hence at room temperature (300 K), the Fermi energy and internal chemical potential are essentially equivalent.\n",
      "-Contact potentials When solids of two different materials are in contact, thermodynamic equilibrium requires that one of the solids assume a higher electrical potential than the other. This is called the contact potential. Dissimilar metals in contact produce what is known also as a contact electromotive force or Galvani potential. The magnitude of this potential difference is often expressed as a difference in Fermi levels in the two solids when they are at charge neutrality, where the Fermi level (a name for the chemical potential of an electron system) describes the energy necessary to remove an electron from the body to some common point (such as ground). If there is an energy advantage in taking an electron from one body to the other, such a transfer will occur. The transfer causes a charge separation, with one body gaining electrons and the other losing electrons. This charge transfer causes a potential difference between the bodies, which partly cancels the potential originating from the contact, and eventually equilibrium is reached. At thermodynamic equilibrium, the Fermi levels are equal (the electron removal energy is identical) and there is now a built-in electrostatic potential between the bodies.\n",
      "-Chemical potential plays an especially important role in solid-state physics and is closely related to the concepts of work function, Fermi energy, and Fermi level. For example, n-type silicon has a higher internal chemical potential of electrons than p-type silicon. In a p–n junction diode at equilibrium the chemical potential (internal chemical potential) varies from the p-type to the n-type side, while the total chemical potential (electrochemical potential, or, Fermi level) is constant throughout the diode.\n",
      "\n",
      "\n",
      "\n",
      "What is the American Petroleum Institute (API) gravity?\n",
      "-API gravity is thus an inverse measure of a petroleum liquid's density relative to that of water (also known as specific gravity). It is used to compare densities of petroleum liquids. For example, if one petroleum liquid is less dense than another, it has a greater API gravity. Although API gravity is mathematically a dimensionless quantity (see the formula below), it is referred to as being in 'degrees'. API gravity is graduated in degrees on a hydrometer instrument. API gravity values of most petroleum liquids fall between 10 and 70 degrees.\n",
      "-The American Petroleum Institute gravity, or API gravity, is a measure of how heavy or light a petroleum liquid is compared to water: if its API gravity is greater than 10, it is lighter and floats on water; if less than 10, it is heavier and sinks.\n",
      "-Light crude oil has an API gravity higher than 31.1° (i.e., less than 870 kg/m3) Medium oil has an API gravity between 22.3 and 31.1° (i.e., 870 to 920 kg/m3) Heavy crude oil has an API gravity below 22.3° (i.e., 920 to 1000 kg/m3) Extra heavy oil has an API gravity below 10.0° (i.e., greater than 1000 kg/m3)However, not all parties use the same grading. The United States Geological Survey uses slightly different ranges.Crude oil with API gravity less than 10° is referred to as extra heavy oil or bitumen. Bitumen derived from oil sands deposits in Alberta, Canada, has an API gravity of around 8°. It can be diluted with lighter hydrocarbons to produce diluted bitumen, which has an API gravity of less than 22.3°, or further \"upgraded\" to an API gravity of 31 to 33° as synthetic crude.\n",
      "-1.1 API gravity Density has always been an important criterion of oils, generally an oil with low density is considered to be more valuable than an oil with higher density due to the fact that if contains more light fractions (i.e. gasoline). Thus, the API gravity or specific gravity is widely used for the classification of crude oils, based on a scheme proposed by the American Petroleum Institute (Table 1).\n",
      "-The formula to calculate API gravity from specific gravity (SG) is: API gravity 141.5 SG 131.5 Conversely, the specific gravity of petroleum liquids can be derived from their API gravity value as SG at 60 141.5 API gravity 131.5 Thus, a heavy oil with a specific gravity of 1.0 (i.e., with the same density as pure water at 60 °F) has an API gravity of: 141.5 1.0 131.5 10.0 API \n",
      "\n",
      "\n",
      "\n",
      "What are the two main factors that cause resistance in a metal?\n",
      "-Most metals have electrical resistance. In simpler models (non quantum mechanical models) this can be explained by replacing electrons and the crystal lattice by a wave-like structure. When the electron wave travels through the lattice, the waves interfere, which causes resistance. The more regular the lattice is, the less disturbance happens and thus the less resistance. The amount of resistance is thus mainly caused by two factors. First, it is caused by the temperature and thus amount of vibration of the crystal lattice. Higher temperatures cause bigger vibrations, which act as irregularities in the lattice. Second, the purity of the metal is relevant as a mixture of different ions is also an irregularity. The small decrease in conductivity on melting of pure metals is due to the loss of long range crystalline order. The short range order remains and strong correlation between positions of ions results in coherence between waves diffracted by adjacent ions.\n",
      "-Metals In general, electrical resistivity of metals increases with temperature. Electron–phonon interactions can play a key role. At high temperatures, the resistance of a metal increases linearly with temperature. As the temperature of a metal is reduced, the temperature dependence of resistivity follows a power law function of temperature. Mathematically the temperature dependence of the resistivity ρ of a metal can be approximated through the Bloch–Grüneisen formula: where  ρ(0) is the residual resistivity due to defect scattering, A is a constant that depends on the velocity of electrons at the Fermi surface, the Debye radius and the number density of electrons in the metal.  ΘR is the Debye temperature as obtained from resistivity measurements and matches very closely with the values of Debye temperature obtained from specific heat measurements. n is an integer that depends upon the nature of interaction: n = 5 implies that the resistance is due to scattering of electrons by phonons (as it is for simple metals) n = 3 implies that the resistance is due to s-d electron scattering (as is the case for transition metals) n = 2 implies that the resistance is due to electron–electron interaction.The Bloch–Grüneisen formula is an approximation obtained assuming that the studied metal has spherical Fermi surface inscribed within the first Brillouin zone and a Debye phonon spectrum.If more than one source of scattering is simultaneously present, Matthiessen's rule (first formulated by Augustus Matthiessen in the 1860s) states that the total resistance can be approximated by adding up several different terms, each with the appropriate value of n.\n",
      "-As temperatures change, the electrical resistivity of amorphous metals behaves very different than that of regular metals. While the resistivity in regular metals generally increases with temperature, following the Matthiessen's rule, the resistivity in a large number of amorphous metals is found to decrease with increasing temperature. This is effect can be observed in amorphous metals of high resistivities between 150 and 300 microohm-centimeters. In these metals, the scattering events causing the resistivity of the metal can no longer be considered statistically independent, thus explaining the breakdown of the Matthiessen's rule. The fact that the thermal change of the resistivity in amorphous metals can be negative over a large range of temperatures and correlated to their absolute resistivity values was first observed by Mooij in 1973, hence coining the term \"Mooij-rule\".The alloys of boron, silicon, phosphorus, and other glass formers with magnetic metals (iron, cobalt, nickel) have high magnetic susceptibility, with low coercivity and high electrical resistance. Usually the electrical conductivity of a metallic glass is of the same low order of magnitude as of a molten metal just above the melting point. The high resistance leads to low losses by eddy currents when subjected to alternating magnetic fields, a property useful for e.g. transformer magnetic cores. Their low coercivity also contributes to low loss.\n",
      "-Metal resistance PUF The metal resistance-based PUF derives its entropy from random physical variations in the metal contacts, vias and wires that define the power grid and interconnect of an IC. There are several important advantages to leveraging random resistance variations in the metal resources of an IC including: Temperature and voltage stability: Temperature and voltage (TV) variations represent one of the most significant challenges for PUFs in applications that require re-generation of exactly the same bitstring later in time, e.g., encryption. Metal resistance (unlike transistors) varies linearly with temperature and is independent of voltage. Therefore, metal resistance provides a very high level of robustness to changing environmental conditions.\n",
      "-Temperature The effect of temperature on thermal conductivity is different for metals and nonmetals. In metals, heat conductivity is primarily due to free electrons. Following the Wiedemann–Franz law, thermal conductivity of metals is approximately proportional to the absolute temperature (in kelvins) times electrical conductivity. In pure metals the electrical conductivity decreases with increasing temperature and thus the product of the two, the thermal conductivity, stays approximately constant. However, as temperatures approach absolute zero, the thermal conductivity decreases sharply. In alloys the change in electrical conductivity is usually smaller and thus thermal conductivity increases with temperature, often proportionally to temperature. Many pure metals have a peak thermal conductivity between 2 K and 10 K.\n",
      "\n",
      "\n",
      "\n",
      "What is the significance of the redshift-distance relationship in determining the expansion history of the universe?\n",
      "-Extragalactic observations The most distant objects exhibit larger redshifts corresponding to the Hubble flow of the universe. The largest-observed redshift, corresponding to the greatest distance and furthest back in time, is that of the cosmic microwave background radiation; the numerical value of its redshift is about z = 1089 (z = 0 corresponds to present time), and it shows the state of the universe about 13.8 billion years ago, and 379,000 years after the initial moments of the Big Bang.The luminous point-like cores of quasars were the first \"high-redshift\" (z > 0.1) objects discovered before the improvement of telescopes allowed for the discovery of other high-redshift galaxies.For galaxies more distant than the Local Group and the nearby Virgo Cluster, but within a thousand megaparsecs or so, the redshift is approximately proportional to the galaxy's distance. This correlation was first observed by Edwin Hubble and has come to be known as Hubble's law. Vesto Slipher was the first to discover galactic redshifts, in about the year 1912, while Hubble correlated Slipher's measurements with distances he measured by other means to formulate his Law. In the widely accepted cosmological model based on general relativity, redshift is mainly a result of the expansion of space: this means that the farther away a galaxy is from us, the more the space has expanded in the time since the light left that galaxy, so the more the light has been stretched, the more redshifted the light is, and so the faster it appears to be moving away from us. Hubble's law follows in part from the Copernican principle. Because it is usually not known how luminous objects are, measuring the redshift is easier than more direct distance measurements, so redshift is sometimes in practice converted to a crude distance measurement using Hubble's law.Gravitational interactions of galaxies with each other and clusters cause a significant scatter in the normal plot of the Hubble diagram. The peculiar velocities associated with galaxies superimpose a rough trace of the mass of virialized objects in the universe. This effect leads to such phenomena as nearby galaxies (such as the Andromeda Galaxy) exhibiting blueshifts as we fall towards a common barycenter, and redshift maps of clusters showing a fingers of god effect due to the scatter of peculiar velocities in a roughly spherical distribution. This added component gives cosmologists a chance to measure the masses of objects independent of the mass-to-light ratio (the ratio of a galaxy's mass in solar masses to its brightness in solar luminosities), an important tool for measuring dark matter.The Hubble law's linear relationship between distance and redshift assumes that the rate of expansion of the universe is constant. However, when the universe was much younger, the expansion rate, and thus the Hubble \"constant\", was larger than it is today. For more distant galaxies, then, whose light has been travelling to us for much longer times, the approximation of constant expansion rate fails, and the Hubble law becomes a non-linear integral relationship and dependent on the history of the expansion rate since the emission of the light from the galaxy in question. Observations of the redshift-distance relationship can be used, then, to determine the expansion history of the universe and thus the matter and energy content.While it was long believed that the expansion rate has been continuously decreasing since the Big Bang, observations beginning in 1988 of the redshift-distance relationship using Type Ia supernovae have suggested that in comparatively recent times the expansion rate of the universe has begun to accelerate.\n",
      "-Distance measures are used in physical cosmology to give a natural notion of the distance between two objects or events in the universe. They are often used to tie some observable quantity (such as the luminosity of a distant quasar, the redshift of a distant galaxy, or the angular size of the acoustic peaks in the cosmic microwave background (CMB) power spectrum) to another quantity that is not directly observable, but is more convenient for calculations (such as the comoving coordinates of the quasar, galaxy, etc.). The distance measures discussed here all reduce to the common notion of Euclidean distance at low redshift.\n",
      "-Expansion of space In the earlier part of the twentieth century, Slipher, Wirtz and others made the first measurements of the redshifts and blueshifts of galaxies beyond the Milky Way. They initially interpreted these redshifts and blueshifts as being due to random motions, but later Lemaître (1927) and Hubble (1929), using previous data, discovered a roughly linear correlation between the increasing redshifts of, and distances to, galaxies. Lemaître realized that these observations could be explained by a mechanism of producing redshifts seen in Friedmann's solutions to Einstein's equations of general relativity. The correlation between redshifts and distances is required by all such models that have a metric expansion of space. As a result, the wavelength of photons propagating through the expanding space is stretched, creating the cosmological redshift.\n",
      "-The observational result of Hubble's Law, the proportional relationship between distance and the speed with which a galaxy is moving away from us (usually referred to as redshift) is a product of the cosmic distance ladder. Edwin Hubble observed that fainter galaxies are more redshifted. Finding the value of the Hubble constant was the result of decades of work by many astronomers, both in amassing the measurements of galaxy redshifts and in calibrating the steps of the distance ladder. Hubble's Law is the primary means we have for estimating the distances of quasars and distant galaxies in which individual distance indicators cannot be seen.\n",
      "-The redshift observed in astronomy can be measured because the emission and absorption spectra for atoms are distinctive and well known, calibrated from spectroscopic experiments in laboratories on Earth. When the redshift of various absorption and emission lines from a single astronomical object is measured, z is found to be remarkably constant. Although distant objects may be slightly blurred and lines broadened, it is by no more than can be explained by thermal or mechanical motion of the source. For these reasons and others, the consensus among astronomers is that the redshifts they observe are due to some combination of the three established forms of Doppler-like redshifts. Alternative hypotheses and explanations for redshift such as tired light are not generally considered plausible.Spectroscopy, as a measurement, is considerably more difficult than simple photometry, which measures the brightness of astronomical objects through certain filters. When photometric data is all that is available (for example, the Hubble Deep Field and the Hubble Ultra Deep Field), astronomers rely on a technique for measuring photometric redshifts. Due to the broad wavelength ranges in photometric filters and the necessary assumptions about the nature of the spectrum at the light-source, errors for these sorts of measurements can range up to δz = 0.5, and are much less reliable than spectroscopic determinations. However, photometry does at least allow a qualitative characterization of a redshift. For example, if a Sun-like spectrum had a redshift of z = 1, it would be brightest in the infrared(1000nm) rather than at the blue-green(500nm) color associated with the peak of its blackbody spectrum, and the light intensity will be reduced in the filter by a factor of four, (1 + z)2. Both the photon count rate and the photon energy are redshifted. (See K correction for more details on the photometric consequences of redshift.) Local observations In nearby objects (within our Milky Way galaxy) observed redshifts are almost always related to the line-of-sight velocities associated with the objects being observed. Observations of such redshifts and blueshifts have enabled astronomers to measure velocities and parametrize the masses of the orbiting stars in spectroscopic binaries, a method first employed in 1868 by British astronomer William Huggins. Similarly, small redshifts and blueshifts detected in the spectroscopic measurements of individual stars are one way astronomers have been able to diagnose and measure the presence and characteristics of planetary systems around other stars and have even made very detailed differential measurements of redshifts during planetary transits to determine precise orbital parameters. Finely detailed measurements of redshifts are used in helioseismology to determine the precise movements of the photosphere of the Sun. Redshifts have also been used to make the first measurements of the rotation rates of planets, velocities of interstellar clouds, the rotation of galaxies, and the dynamics of accretion onto neutron stars and black holes which exhibit both Doppler and gravitational redshifts. Additionally, the temperatures of various emitting and absorbing objects can be obtained by measuring Doppler broadening—effectively redshifts and blueshifts over a single emission or absorption line. By measuring the broadening and shifts of the 21-centimeter hydrogen line in different directions, astronomers have been able to measure the recessional velocities of interstellar gas, which in turn reveals the rotation curve of our Milky Way. Similar measurements have been performed on other galaxies, such as Andromeda. As a diagnostic tool, redshift measurements are one of the most important spectroscopic measurements made in astronomy.\n",
      "\n",
      "\n",
      "\n",
      "What is the Evans balance?\n",
      "-An Evans balance, also known as a Johnson's balance (after a commercial producer of the Evans balance) is a device for measuring magnetic susceptibility. Magnetic susceptibility is related to the force experienced by a substance in a magnetic field. Various practical devices are available for the measurement of susceptibility, which differ in the shape of the magnetic field and the way the force is measured.The Evans balance employs a similar sample configuration but measures the force on the magnet.\n",
      "-The Evans balance. is a torsion balance which uses a sample in a fixed position and a variable secondary magnet to bring the magnets back to their initial position. It, too, is calibrated against HgCo(NCS)4.\n",
      "With a Faraday balance the sample is placed in a magnetic field of constant gradient, and weighed on a torsion balance. This method can yield information on magnetic anisotropy.\n",
      "SQUID is a very sensitive magnetometer.\n",
      "For substances in solution NMR may be used to measure susceptibility.\n",
      "-Volume magnetic susceptibility is measured by the force change felt upon a substance when a magnetic field gradient is applied. Early measurements are made using the Gouy balance where a sample is hung between the poles of an electromagnet. The change in weight when the electromagnet is turned on is proportional to the susceptibility. Today, high-end measurement systems use a superconductive magnet. An alternative is to measure the force change on a strong compact magnet upon insertion of the sample. This system, widely used today, is called the Evans balance. For liquid samples, the susceptibility can be measured from the dependence of the NMR frequency of the sample on its shape or orientation.Another method using NMR techniques measures the magnetic field distortion around a sample immersed in water inside an MR scanner. This method is highly accurate for diamagnetic materials with susceptibilities similar to water.\n",
      "-Advantages vs alternative magnetic balances The main advantage of this system is that it is cheap to construct as it does not require a precision weighing device. Moreover, using a Evans balance is less time-consuming than using a Gouy or Faraday balances, although it is not sensitive and accurate in comparison to these last two systems. One reason that they were time-consuming is that the sample had to be suspended between the two poles of a very powerful magnet. The tube had to be suspended in the same place every time for the apparatus constant to be accurate. In the case of the Gouy balance, the static charge on the glass tube often caused the tube to stick to magnets. With the Evans balance, a reading could be taken in a matter of seconds with only small sacrifices in sensitivity and accuracy. A Johnson-Matthey balance has a range from 0.001 x 10−7 to 1.99 x 10−7 c.g.s. volume susceptibility units. The original Evans balance had an accuracy within 1% of literature values for diamagnetic solutions and within 2% of literature values of paramagnetic solids.The system allows for measurements of solid, liquid, and gaseous forms of a wide range of paramagnetic and diamagnetic materials. For each measurement, only around 250 mg of sample is required (50 mg can be used for a thin-bore sample tube).\n",
      "-The Evans balance measures susceptibility indirectly by referring to a calibration standard of known susceptibility. The most convenient compound for this purpose is mercury cobalt thiocyanate, HgCo(NCS)4, which has a susceptibility of 16.44×10−6 (±0.5%) CGS at 20 °C. Another common calibration standard is [Ni(en)3]S2O3 which has a susceptibility of 1.104 x 10−5 erg G−2 cm−3. Three readings of the meter are needed, of an empty tube, R0 of the tube filled with calibrant and of the tube filled with the sample, Rs. Some balances have an auto-tare feature that eliminates the need for the R0 measurement. Accuracy depends somewhat on the homogeneous packing of the sample. The first two provide a calibration constant, C. The mass susceptibility in grams is calculated as 10 9m where L is the length of the sample, C is the calibration constant (usually 1 if it has been calibrated), and m is its mass in grams. The reading for the empty tube is needed because the tube glass is diamagnetic. There is a V term multiplied by an A term in the most general form of the equation. These two terms (V∗A) are collectively added to the numerator in the above equation. The V term is the volume susceptibility of air (0.029 x 10−6 erg G−2 cm−3) and A is the cross-sectional area of the sample. These two terms can be ignored for solid samples, yielding the original equation written above.To calculate the volume magnetic susceptibility (χ) instead of the weight susceptibility (χg), such as in a liquid sample, the equation would have the extra V term added to the numerator and instead of being divided by m, the equation would be divided by d for the density of the solution.\n",
      "\n",
      "\n",
      "\n",
      "What is the definition of dimension in mathematics?\n",
      "-In mathematics, the dimension of an object is, roughly speaking, the number of degrees of freedom of a point that moves on this object. In other words, the dimension is the number of independent parameters or coordinates that are needed for defining the position of a point that is constrained to be on the object. For example, the dimension of a point is zero; the dimension of a line is one, as a point can move on a line in only one direction (or its opposite); the dimension of a plane is two etc.\n",
      "-The dimension is an intrinsic property of an object, in the sense that it is independent of the dimension of the space in which the object is or can be embedded. For example, a curve, such as a circle, is of dimension one, because the position of a point on a curve is determined by its signed distance along the curve to a fixed point on the curve. This is independent from the fact that a curve cannot be embedded in a Euclidean space of dimension lower than two, unless it is a line.\n",
      "-In physics and mathematics, the dimension of a mathematical space (or object) is informally defined as the minimum number of coordinates needed to specify any point within it. Thus, a line has a dimension of one (1D) because only one coordinate is needed to specify a point on it – for example, the point at 5 on a number line. A surface, such as the boundary of a cylinder or sphere, has a dimension of two (2D) because two coordinates are needed to specify a point on it – for example, both a latitude and longitude are required to locate a point on the surface of a sphere. A two-dimensional Euclidean space is a two-dimensional space on the plane. The inside of a cube, a cylinder or a sphere is three-dimensional (3D) because three coordinates are needed to locate a point within these spaces.\n",
      "-The intuitive concept of dimension of a geometric object X is the number of independent parameters one needs to pick out a unique point inside. However, any point specified by two parameters can be instead specified by one, because the cardinality of the real plane is equal to the cardinality of the real line (this can be seen by an argument involving interweaving the digits of two numbers to yield a single number encoding the same information). The example of a space-filling curve shows that one can even map the real line to the real plane surjectively (taking one real number into a pair of real numbers in a way so that all pairs of numbers are covered) and continuously, so that a one-dimensional object completely fills up a higher-dimensional object.\n",
      "-Dimension/Hierarchy. Dimension is a dimension of a cube. A dimension is a primary organizer of measure and attribute information in a cube. MDX does not know of, nor does it assume any, dependencies between dimensions - they are assumed to be mutually independent. A dimension will contain some members (see below) organized in some hierarchy or hierarchies containing levels. It can be specified by its unique name, e.g. [Time] or it can be returned by an MDX function, e.g. .Dimension. Hierarchy is a dimension hierarchy of a cube. It can be specified by its unique name, e.g. [Time].[Fiscal] or it can be returned by an MDX function, e.g. .Hierarchy. Hierarchies are contained within dimensions. (OLEDB for OLAP MDX specification does not distinguish between dimension and hierarchy data types. Some implementations, such as Microsoft Analysis Services, treat them differently.) Level. Level is a [[:sjjhhikt:level|level]] in a dimension hierarchy. It can be specified by its unique name, e.g. [Time].[Fiscal].[Month] or it can be returned by an MDX function, e.g. .Level.\n",
      "\n",
      "\n",
      "\n",
      "What is accelerator-based light-ion fusion?\n",
      "-Beam–beam or beam–target fusion Accelerator-based light-ion fusion is a technique using particle accelerators to achieve particle kinetic energies sufficient to induce light-ion fusion reactions.Accelerating light ions is relatively easy, and can be done in an efficient manner—requiring only a vacuum tube, a pair of electrodes, and a high-voltage transformer; fusion can be observed with as little as 10 kV between the electrodes. The system can be arranged to accelerate ions into a static fuel-infused target, known as beam–target fusion, or by accelerating two streams of ions towards each other, beam–beam fusion. The key problem with accelerator-based fusion (and with cold targets in general) is that fusion cross sections are many orders of magnitude lower than Coulomb interaction cross-sections. Therefore, the vast majority of ions expend their energy emitting bremsstrahlung radiation and the ionization of atoms of the target. Devices referred to as sealed-tube neutron generators are particularly relevant to this discussion. These small devices are miniature particle accelerators filled with deuterium and tritium gas in an arrangement that allows ions of those nuclei to be accelerated against hydride targets, also containing deuterium and tritium, where fusion takes place, releasing a flux of neutrons. Hundreds of neutron generators are produced annually for use in the petroleum industry where they are used in measurement equipment for locating and mapping oil reserves.A number of attempts to recirculate the ions that \"miss\" collisions have been made over the years. One of the better-known attempts in the 1970s was Migma, which used a unique particle storage ring to capture ions into circular orbits and return them to the reaction area. Theoretical calculations made during funding reviews pointed out that the system would have significant difficulty scaling up to contain enough fusion fuel to be relevant as a power source. In the 1990s, a new arrangement using a field-reverse configuration (FRC) as the storage system was proposed by Norman Rostoker and continues to be studied by TAE Technologies as of 2021. A closely related approach is to merge two FRC's rotating in opposite directions, which is being actively studied by Helion Energy. Because these approaches all have ion energies well beyond the Coulomb barrier, they often suggest the use of alternative fuel cycles like p-11B that are too difficult to attempt using conventional approaches.\n",
      "-Beam–beam or beam–target fusion Accelerator-based light-ion fusion is a technique using particle accelerators to achieve particle kinetic energies sufficient to induce light-ion fusion reactions.Accelerating light ions is relatively easy, and can be done in an efficient manner—requiring only a vacuum tube, a pair of electrodes, and a high-voltage transformer; fusion can be observed with as little as 10 kV between the electrodes. The system can be arranged to accelerate ions into a static fuel-infused target, known as beam–target fusion, or by accelerating two streams of ions towards each other, beam–beam fusion. The key problem with accelerator-based fusion (and with cold targets in general) is that fusion cross sections are many orders of magnitude lower than Coulomb interaction cross-sections. Therefore, the vast majority of ions expend their energy emitting bremsstrahlung radiation and the ionization of atoms of the target. Devices referred to as sealed-tube neutron generators are particularly relevant to this discussion. These small devices are miniature particle accelerators filled with deuterium and tritium gas in an arrangement that allows ions of those nuclei to be accelerated against hydride targets, also containing deuterium and tritium, where fusion takes place, releasing a flux of neutrons. Hundreds of neutron generators are produced annually for use in the petroleum industry where they are used in measurement equipment for locating and mapping oil reserves.A number of attempts to recirculate the ions that \"miss\" collisions have been made over the years. One of the better-known attempts in the 1970s was Migma, which used a unique particle storage ring to capture ions into circular orbits and return them to the reaction area. Theoretical calculations made during funding reviews pointed out that the system would have significant difficulty scaling up to contain enough fusion fuel to be relevant as a power source. In the 1990s, a new arrangement using a field-reverse configuration (FRC) as the storage system was proposed by Norman Rostoker and continues to be studied by TAE Technologies as of 2021. A closely related approach is to merge two FRC's rotating in opposite directions, which is being actively studied by Helion Energy. Because these approaches all have ion energies well beyond the Coulomb barrier, they often suggest the use of alternative fuel cycles like p-11B that are too difficult to attempt using conventional approaches.\n",
      "-Cyclotron particle accelerators, linear particle accelerators, and synchrotron particle accelerators can accelerate positively charged hydrogen ions (protons) until their velocity approaches the speed of light. Each ion has a kinetic energy range of 100-1000+ MeV. The resulting high energy protons can capture electrons from electron emitter electrodes, and be thus electrically neutralized. This creates an electrically neutral beam of high energy hydrogen atoms, that can proceed in a straight line at near the speed of light to smash into its target and damage it.\n",
      "-A particle accelerator is a machine that uses electromagnetic fields to propel charged particles to very high speeds and energies, and to contain them in well-defined beams.Large accelerators are used for fundamental research in particle physics. The largest accelerator currently active is the Large Hadron Collider (LHC) near Geneva, Switzerland, operated by the CERN. It is a collider accelerator, which can accelerate two beams of protons to an energy of 6.5 TeV and cause them to collide head-on, creating center-of-mass energies of 13 TeV. Other powerful accelerators are, RHIC at Brookhaven National Laboratory in New York and, formerly, the Tevatron at Fermilab, Batavia, Illinois. Accelerators are also used as synchrotron light sources for the study of condensed matter physics. Smaller particle accelerators are used in a wide variety of applications, including particle therapy for oncological purposes, radioisotope production for medical diagnostics, ion implanters for the manufacture of semiconductors, and accelerator mass spectrometers for measurements of rare isotopes such as radiocarbon. There are currently more than 30,000 accelerators in operation around the world.There are two basic classes of accelerators: electrostatic and electrodynamic (or electromagnetic) accelerators. Electrostatic particle accelerators use static electric fields to accelerate particles. The most common types are the Cockcroft–Walton generator and the Van de Graaff generator. A small-scale example of this class is the cathode ray tube in an ordinary old television set. The achievable kinetic energy for particles in these devices is determined by the accelerating voltage, which is limited by electrical breakdown. Electrodynamic or electromagnetic accelerators, on the other hand, use changing electromagnetic fields (either magnetic induction or oscillating radio frequency fields) to accelerate particles. Since in these types the particles can pass through the same accelerating field multiple times, the output energy is not limited by the strength of the accelerating field. This class, which was first developed in the 1920s, is the basis for most modern large-scale accelerators.\n",
      "-Heavy ion fusion is a fusion energy concept that uses a stream of high-energy ions from a particle accelerator to rapidly heat and compress a small pellet of fusion fuel. It is a subclass of the larger inertial confinement fusion (ICF) approach, replacing the more typical laser systems with an accelerator.\n",
      "\n",
      "\n",
      "\n",
      "What is the interstellar medium (ISM)?\n",
      "-In astronomy, the interstellar medium (ISM) is the matter and radiation that exist in the space between the star systems in a galaxy. This matter includes gas in ionic, atomic, and molecular form, as well as dust and cosmic rays. It fills interstellar space and blends smoothly into the surrounding intergalactic space. The energy that occupies the same volume, in the form of electromagnetic radiation, is the interstellar radiation field. Although the density of atoms in the ISM is usually far below that in the best laboratory vacuums, the mean free path between collisions is short compared to typical interstellar lengths, so on these scales the ISM behaves as a gas (more precisely, as a plasma: it is everywhere at least slightly ionized), responding to pressure forces, and not as a collection of non-interacting particles.\n",
      "-X-ray Quantum Calorimeter (XQC) project In astronomy, the interstellar medium (or ISM) is the gas and cosmic dust that pervade interstellar space: the matter that exists between the star systems within a galaxy. It fills interstellar space and blends smoothly into the surrounding intergalactic medium. The interstellar medium consists of an extremely dilute (by terrestrial standards) mixture of ions, atoms, molecules, larger dust grains, cosmic rays, and (galactic) magnetic fields. The energy that occupies the same volume, in the form of electromagnetic radiation, is the interstellar radiation field.\n",
      "-The interstellar medium is matter that occupies the space between star systems in a galaxy. 99% of this matter is gaseous – hydrogen, helium, and smaller quantities of other ionized elements such as oxygen. The other 1% is dust particles, thought to be mainly graphite, silicates, and ices. Clouds of the dust and gas are referred to as nebulae.\n",
      "-Interstellar space Interstellar space is the physical space within a galaxy beyond the influence each star has upon the encompassed plasma. The contents of interstellar space are called the interstellar medium. Approximately 70% of the mass of the interstellar medium consists of lone hydrogen atoms; most of the remainder consists of helium atoms. This is enriched with trace amounts of heavier atoms formed through stellar nucleosynthesis. These atoms are ejected into the interstellar medium by stellar winds or when evolved stars begin to shed their outer envelopes such as during the formation of a planetary nebula. The cataclysmic explosion of a supernova generates an expanding shock wave consisting of ejected materials that further enrich the medium. The density of matter in the interstellar medium can vary considerably: the average is around 106 particles per m3, but cold molecular clouds can hold 108–1012 per m3.A number of molecules exist in interstellar space, as can tiny 0.1 μm dust particles. The tally of molecules discovered through radio astronomy is steadily increasing at the rate of about four new species per year. Large regions of higher density matter known as molecular clouds allow chemical reactions to occur, including the formation of organic polyatomic species. Much of this chemistry is driven by collisions. Energetic cosmic rays penetrate the cold, dense clouds and ionize hydrogen and helium, resulting, for example, in the trihydrogen cation. An ionized helium atom can then split relatively abundant carbon monoxide to produce ionized carbon, which in turn can lead to organic chemical reactions.The local interstellar medium is a region of space within 100 parsecs (pc) of the Sun, which is of interest both for its proximity and for its interaction with the Solar System. This volume nearly coincides with a region of space known as the Local Bubble, which is characterized by a lack of dense, cold clouds. It forms a cavity in the Orion Arm of the Milky Way galaxy, with dense molecular clouds lying along the borders, such as those in the constellations of Ophiuchus and Taurus. (The actual distance to the border of this cavity varies from 60 to 250 pc or more.) This volume contains about 104–105 stars and the local interstellar gas counterbalances the astrospheres that surround these stars, with the volume of each sphere varying depending on the local density of the interstellar medium. The Local Bubble contains dozens of warm interstellar clouds with temperatures of up to 7,000 K and radii of 0.5–5 pc.When stars are moving at sufficiently high peculiar velocities, their astrospheres can generate bow shocks as they collide with the interstellar medium. For decades it was assumed that the Sun had a bow shock. In 2012, data from Interstellar Boundary Explorer (IBEX) and NASA's Voyager probes showed that the Sun's bow shock does not exist. Instead, these authors argue that a subsonic bow wave defines the transition from the solar wind flow to the interstellar medium. A bow shock is the third boundary of an astrosphere after the termination shock and the astropause (called the heliopause in the Solar System).\n",
      "-The interstellar medium is composed of multiple phases distinguished by whether matter is ionic, atomic, or molecular, and the temperature and density of the matter. The interstellar medium is composed primarily of hydrogen, followed by helium with trace amounts of carbon, oxygen, and nitrogen. The thermal pressures of these phases are in rough equilibrium with one another. Magnetic fields and turbulent motions also provide pressure in the ISM, and are typically more important, dynamically, than the thermal pressure. In the interstellar medium, matter is primarily in molecular form and reaches number densities of 1012 molecules per m3 (1 trillion molecules per m3). In hot, diffuse regions, gas is highly ionized, and the density may be as low as 100 ions per m3. Compare this with a number density of roughly 1025 molecules per m3 for air at sea level, and 1016 molecules per m3 (10 quadrillion molecules per m3) for a laboratory high-vacuum chamber. By mass, 99% of the ISM is gas in any form, and 1% is dust. Of the gas in the ISM, by number 91% of atoms are hydrogen and 8.9% are helium, with 0.1% being atoms of elements heavier than hydrogen or helium, known as \"metals\" in astronomical parlance. By mass this amounts to 70% hydrogen, 28% helium, and 1.5% heavier elements. The hydrogen and helium are primarily a result of primordial nucleosynthesis, while the heavier elements in the ISM are mostly a result of enrichment (due to stellar nucleosynthesis) in the process of stellar evolution.\n",
      "\n",
      "\n",
      "\n",
      "What is the significance of the change in slope of the pinched hysteresis curves in ReRAM and other forms of two-terminal resistance memory?\n",
      "-Pinched hysteresis One of the resulting properties of memristors and memristive systems is the existence of a pinched hysteresis effect. For a current-controlled memristive system, the input u(t) is the current i(t), the output y(t) is the voltage v(t), and the slope of the curve represents the electrical resistance. The change in slope of the pinched hysteresis curves demonstrates switching between different resistance states which is a phenomenon central to ReRAM and other forms of two-terminal resistance memory. At high frequencies, memristive theory predicts the pinched hysteresis effect will degenerate, resulting in a straight line representative of a linear resistor. It has been proven that some types of non-crossing pinched hysteresis curves (denoted Type-II) cannot be described by memristors.\n",
      "-As the frequency tends to infinity, the hysteresis loop degenerates to a straight line through the origin, whose slope depends on the amplitude and shape of the forcing signal.According to Chua all resistive switching memories including ReRAM, MRAM and phase-change memory meet these criteria and are memristors. However, the lack of data for the Lissajous curves over a range of initial conditions or over a range of frequencies complicates assessments of this claim.\n",
      "-Experimental evidence shows that redox-based resistance memory (ReRAM) includes a nanobattery effect that is contrary to Chua's memristor model. This indicates that the memristor theory needs to be extended or corrected to enable accurate ReRAM modeling.\n",
      "-The above-mentioned thermodynamic principle furthermore implies that the operation of two-terminal non-volatile memory devices (e.g. \"resistance-switching\" memory devices (ReRAM)) cannot be associated with the memristor concept, i.e., such devices cannot by itself remember their current or voltage history. Transitions between distinct internal memory or resistance states are of probabilistic nature. The probability for a transition from state {i} to state {j} depends on the height of the free-energy barrier between both states. The transition probability can thus be influenced by suitably driving the memory device, i.e., by \"lowering\" the free-energy barrier for the transition {i} → {j} by means of, for example, an externally applied bias.\n",
      "-Let HM = y, AM = x, NH = u, and HI = w = dx. Let the tangent at each point on the curve,  p=−dydx=uw . The reduction of the resistance of the sloping ring NI compared to the vertical ring NH rotated about AB is  r=yp31+p2dx=yu3u2+w2 (2) Let the minimum resistance solid be replaced by an identical one, except that the arc between points I and K is shifted by a small distance to the right  IJ=KL=o>0 , or to the left  Ij=Kl=o<0 , as shown in more detail in Fig. 5. In either case, HI becomes  HJ,Hj=w+o The resistance of the arcs of the curve DN and SG are unchanged. Also, the resistance of the arc IK is not changed by being shifted, since the slope remains the same along its length. The only change to the overall resistance of DNSG is due to the change to the gradient of arcs NI and KS. The 2 displacements have to be equal for the slope of the arc IK to be unaffected, and the new curve to end at G.\n",
      "\n",
      "\n",
      "\n",
      "What is geometric quantization in mathematical physics?\n",
      "-In mathematical physics, geometric quantization is a mathematical approach to defining a quantum theory corresponding to a given classical theory. It attempts to carry out quantization, for which there is in general no exact recipe, in such a way that certain analogies between the classical theory and the quantum theory remain manifest. For example, the similarity between the Heisenberg equation in the Heisenberg picture of quantum mechanics and the Hamilton equation in classical physics should be built in.\n",
      "-In mathematical physics, geometric quantization is a mathematical approach to defining a quantum theory corresponding to a given classical theory. It attempts to carry out quantization, for which there is in general no exact recipe, in such a way that certain analogies between the classical theory and the quantum theory remain manifest. For example, the similarity between the Heisenberg equation in the Heisenberg picture of quantum mechanics and the Hamilton equation in classical physics should be built in.\n",
      "-Quantization is the process of constraining an input from a continuous or otherwise large set of values (such as the real numbers) to a discrete set (such as the integers). The term quantization may refer to: \n",
      "-In physics, quantisation (in American English quantization) is the systematic transition procedure from a classical understanding of physical phenomena to a newer understanding known as quantum mechanics. It is a procedure for constructing quantum mechanics from classical mechanics. A generalization involving infinite degrees of freedom is field quantization, as in the \"quantization of the electromagnetic field\", referring to photons as field \"quanta\" (for instance as light quanta). This procedure is basic to theories of atomic physics, chemistry, particle physics, nuclear physics, condensed matter physics, and quantum optics.\n",
      "-Quantization (physics) Canonical quantization Geometric quantization Discrete spectrum, or otherwise discrete quantity Spatial quantization Charge quantization \n",
      "\n",
      "\n",
      "\n",
      "What is the definition of an improper rotation?\n",
      "-An improper rotation is the composition of a rotation about an axis, and reflection in a plane perpendicular to that axis. The order in which the rotation and reflection are performed does not matter (that is, these operations commute). Improper rotation is also defined as the composition of a rotation about an axis, and inversion about a point on the axis. These definitions are equivalent because inversion about a point is equivalent to rotation by 180° about any axis, followed by mirroring about a plane perpendicular to that axis. The symmetry elements for improper rotation are the rotation axis, and either the mirror plane, the inversion point, or both. The improper rotation group of order 2n is denoted S2n.\n",
      "-In geometry, an improper rotation (also called rotation-reflection, rotoreflection, rotary reflection, or rotoinversion) is an isometry in Euclidean space that is a combination of a rotation about an axis and a reflection in a plane perpendicular to that axis. Reflection and inversion are each special case of improper rotation. Any improper rotation is an affine transformation and, in cases that keep the coordinate origin fixed, a linear transformation.\n",
      "-In 3 dimensions, improper rotation is equivalently defined as a combination of rotation about an axis and inversion in a point on the axis. For this reason it is also called a rotoinversion or rotary inversion. The two definitions are equivalent because rotation by an angle θ followed by reflection is the same transformation as rotation by θ + 180° followed by inversion (taking the point of inversion to be in the plane of reflection). In both definitions, the operations commute.\n",
      "-Improper rotation, also called rotation-reflection: rotation about an axis by an angle θ, combined with reflection in the plane through the origin perpendicular to the axis. Rotation-reflection by θ = 360°/n for any positive integer n is denoted Sn (from the Schoenflies notation for the group Sn that it generates if n is even).Inversion is a special case of rotation-reflection (i = S2), as is reflection (σ = S1), so these operations are often considered to be improper rotations.\n",
      "-Improper rotation operations An improper rotation involves two operation steps: a proper rotation followed by reflection through a plane perpendicular to the rotation axis. The improper rotation is represented by the symbol Sn where n is the order. Since the improper rotation is the combination of a proper rotation and a reflection, Sn will always exist whenever Cn and a perpendicular plane exist separately. S1 is usually denoted as σ, a reflection operation about a mirror plane. S2 is usually denoted as i, an inversion operation about an inversion center. When n is an even number  Snn=E, but when n is odd  Sn2n=E.\n",
      "\n",
      "\n",
      "\n",
      "What is power density in the context of energy systems, and how does it differ between renewable and non-renewable energy sources?\n",
      "-Measured in W/m2 it describes the amount of power obtained per unit of Earth surface area used by a specific energy system, including all supporting infrastructure, manufacturing, mining of fuel (if applicable) and decommissioning., Fossil fuels and nuclear power are characterized by high power density which means large power can be drawn from power plants occupying relatively small area. Renewable energy sources have power density at least three orders of magnitude smaller and for the same energy output they need to occupy accordingly larger area, which has been already highlighted as a limiting factor of renewable energy in German Energiewende.The following table shows median surface power density of renewable and non-renewable energy sources.\n",
      "-Power density is the amount of power (time rate of energy transfer) per unit volume.In energy transformers including batteries, fuel cells, motors, power supply units etc., power density refers to a volume, where it is often called volume power density, expressed as W/m3.\n",
      "In reciprocating internal combustion engines, power density (power per swept volume or brake horsepower per cubic centimeter) is an important metric, based on the internal capacity of the engine, not its external size.\n",
      "-Surface power density is an important factor in comparison of industrial energy sources. The concept was popularised by geographer Vaclav Smil. The term is usually shortened to \"power density\" in the relevant literature, which can lead to confusion with homonymous or related terms.\n",
      "-In physics and engineering, surface power density is power per unit area.\n",
      "-Wind Power Density (WPD) is a quantitative measure of wind energy available at any location. It is the mean annual power available per square meter of swept area of a turbine, and is calculated for different heights above ground. Calculation of wind power density includes the effect of wind velocity and air density.Wind turbines are classified by the wind speed they are designed for, from class I to class III, with A to C referring to the turbulence intensity of the wind.\n",
      "\n",
      "\n",
      "\n",
      "What is Modified Newtonian Dynamics (MOND)?\n",
      "-Modified Newtonian dynamics (MOND) is a hypothesis that proposes a modification of Newton's law of universal gravitation to account for observed properties of galaxies. It is an alternative to the hypothesis of dark matter in terms of explaining why galaxies do not appear to obey the currently understood laws of physics.\n",
      "-MOND Modified Newtonian Dynamics (MOND) is a relatively modern proposal to explain the galaxy rotation problem based on a variation of Newton's Second Law of Dynamics at low accelerations. This would produce a large-scale variation of Newton's universal theory of gravity. A modification of Newton's theory would also imply a modification of general relativistic cosmology in as much as Newtonian cosmology is the limit of Friedman cosmology. While almost all astrophysicists today reject MOND in favor of dark matter, a small number of researchers continue to enhance it, recently incorporating Brans–Dicke theories into treatments that attempt to account for cosmological observations.\n",
      "-MOND is a phenomenological modification of the Newtonian acceleration law. In Newtonian gravity theory, the gravitational acceleration in the spherically symmetric, static field of a point mass  M at distance  r from the source can be written as a=−GMr2, where  G is Newton's constant of gravitation. The corresponding force acting on a test mass  m is F=ma.\n",
      "To account for the anomalous rotation curves of spiral galaxies, Milgrom proposed a modification of this force law in the form F=μ(aa0)ma, where  μ(x) is an arbitrary function subject to the following conditions: μ(x)={1|x|≫1x|x|≪1 In this form, MOND is not a complete theory: for instance, it violates the law of momentum conservation.\n",
      "However, such conservation laws are automatically satisfied for physical theories that are derived using an action principle. This led Bekenstein to a first, nonrelativistic generalization of MOND. This theory, called AQUAL (for A QUAdratic Lagrangian) is based on the Lagrangian L=−a028πGf(|∇Φ|2a02)−ρΦ, where  Φ is the Newtonian gravitational potential,  ρ is the mass density, and  f(y) is a dimensionless function.\n",
      "In the case of a spherically symmetric, static gravitational field, this Lagrangian reproduces the MOND acceleration law after the substitutions  a=−∇Φ and  μ(y)=df(y)/dy are made.\n",
      "-MOND is an example of a class of theories known as modified gravity, and is an alternative to the hypothesis that the dynamics of galaxies are determined by massive, invisible dark matter halos. Since Milgrom's original proposal, proponents of MOND have claimed to successfully predict a variety of galactic phenomena that they state are difficult to understand as consequences of dark matter.Though MOND explains the anomalously great rotational velocities of galaxies at their perimeters, it does not fully explain the velocity dispersions of individual galaxies within galaxy clusters. MOND reduces the discrepancy between the velocity dispersions and clusters' observed missing baryonic mass from a factor of around 10 to a factor of about 2. However, the residual discrepancy cannot be accounted for by MOND, requiring that other explanations close the gap such as the presence of as-yet undetected missing baryonic matter.The accurate measurement of the speed of gravitational waves compared to the speed of light in 2017 ruled out a certain class of modified gravity theories but concluded that other MOND theories that dispense with the need for dark matter remained viable. Two years later, theories put forth by Constantinos Skordis and Tom Zlosnik were consistent with gravitational waves that always travel at the speed of light. Later still in 2021, Skordis and Zlosnik developed a subclass of their theory called \"RMOND\", for \"relativistic MOND\", which had \"been shown to reproduce in great detail the main observations in cosmology, including the cosmic-microwave-background power spectrum, and the matter structure power spectrum.\" \n",
      "-Relativistic MOND The original theory of MOND by Milgrom was developed in 1983 as an alternative to \"dark matter\". Departures from Newton's law of gravitation are governed by an acceleration scale, not a distance scale. MOND successfully explains the Tully–Fisher observation that the luminosity of a galaxy should scale as the fourth power of the rotation speed. It also explains why the rotation discrepancy in dwarf galaxies is particularly large.\n",
      "\n",
      "\n",
      "\n",
      "What is linear frame dragging?\n",
      "-Linear frame dragging is the similarly inevitable result of the general principle of relativity, applied to linear momentum. Although it arguably has equal theoretical legitimacy to the \"rotational\" effect, the difficulty of obtaining an experimental verification of the effect means that it receives much less discussion and is often omitted from articles on frame-dragging (but see Einstein, 1921).Static mass increase is a third effect noted by Einstein in the same paper. The effect is an increase in inertia of a body when other masses are placed nearby. While not strictly a frame dragging effect (the term frame dragging is not used by Einstein), it is demonstrated by Einstein that it derives from the same equation of general relativity. It is also a tiny effect that is difficult to confirm experimentally.\n",
      "-This \"mutual\" effect, and the ability of an accelerated mass to warp lightbeam geometry and lightbeam-based coordinate systems, is referred to as frame-dragging.\n",
      "Frame-dragging removes the usual distinction between accelerated frames (which show gravitational effects) and inertial frames (where the geometry is supposedly free from gravitational fields). When a forcibly-accelerated body physically \"drags\" a coordinate system, the problem becomes an exercise in warped spacetime for all observers.\n",
      "-Rotational frame-dragging (the Lense–Thirring effect) appears in the general principle of relativity and similar theories in the vicinity of rotating massive objects. Under the Lense–Thirring effect, the frame of reference in which a clock ticks the fastest is one which is revolving around the object as viewed by a distant observer. This also means that light traveling in the direction of rotation of the object will move past the massive object faster than light moving against the rotation, as seen by a distant observer. It is now the best known frame-dragging effect, partly thanks to the Gravity Probe B experiment. Qualitatively, frame-dragging can be viewed as the gravitational analog of electromagnetic induction.\n",
      "-Frame-dragging is an effect on spacetime, predicted by Albert Einstein's general theory of relativity, that is due to non-static stationary distributions of mass–energy. A stationary field is one that is in a steady state, but the masses causing that field may be non-static ⁠— rotating, for instance. More generally, the subject that deals with the effects caused by mass–energy currents is known as gravitoelectromagnetism, which is analogous to the magnetism of classical electromagnetism.\n",
      "-Frame-dragging may be illustrated most readily using the Kerr metric, which describes the geometry of spacetime in the vicinity of a mass M rotating with angular momentum J, and Boyer–Lindquist coordinates (see the link for the transformation): sin sin sin 2⁡θρ2dϕdt where rs is the Schwarzschild radius rs=2GMc2 and where the following shorthand variables have been introduced for brevity α=JMc cos 2⁡θ Λ2=r2−rsr+α2 In the non-relativistic limit where M (or, equivalently, rs) goes to zero, the Kerr metric becomes the orthogonal metric for the oblate spheroidal coordinates sin 2⁡θdϕ2 We may rewrite the Kerr metric in the following form c2dτ2=(gtt−gtϕ2gϕϕ)dt2+grrdr2+gθθdθ2+gϕϕ(dϕ+gtϕgϕϕdt)2 This metric is equivalent to a co-rotating reference frame that is rotating with angular speed Ω that depends on both the radius r and the colatitude θ sin 2⁡θ In the plane of the equator this simplifies to: Ω=rsαcr3+α2r+rsα2 Thus, an inertial reference frame is entrained by the rotating central mass to participate in the latter's rotation; this is frame-dragging.\n",
      "\n",
      "\n",
      "\n",
      "What is explicit symmetry breaking in theoretical physics?\n",
      "-In theoretical physics, explicit symmetry breaking is the breaking of a symmetry of a theory by terms in its defining equations of motion (most typically, to the Lagrangian or the Hamiltonian) that do not respect the symmetry. Usually this term is used in situations where these symmetry-breaking terms are small, so that the symmetry is approximately respected by the theory. An example is the spectral line splitting in the Zeeman effect, due to a magnetic interaction perturbation in the Hamiltonian of the atoms involved.\n",
      "-In explicit symmetry breaking (ESB), the equations of motion describing a system are variant under the broken symmetry. In Hamiltonian mechanics or Lagrangian Mechanics, this happens when there is at least one term in the Hamiltonian (or Lagrangian) that explicitly breaks the given symmetry.\n",
      "-Explicit symmetry breaking differs from spontaneous symmetry breaking. In the latter, the defining equations respect the symmetry but the ground state (vacuum) of the theory breaks it.Explicit symmetry breaking is also associated with electromagnetic radiation. A system of accelerated charges results in electromagnetic radiation when the geometric symmetry of the electric field in free space is explicitly broken by the associated electrodynamic structure under time varying excitation of the given system. This is quite evident in an antenna where the electric lines of field curl around or have rotational geometry around the radiating terminals in contrast to linear geometric orientation within a pair of transmission lines which does not radiate even under time varying excitation.\n",
      "-A common setting for explicit symmetry breaking is perturbation theory in quantum mechanics. The symmetry is evident in a base Hamiltonian  H0 . This  H0 is often an integrable Hamiltonian, admitting symmetries which in some sense make the Hamiltonian integrable. The base Hamiltonian might be chosen to provide a starting point close to the system being modelled.\n",
      "-Vector symmetry description If all quarks had non-zero but equal masses, then this chiral symmetry is broken to the vector symmetry of the \"diagonal flavour group\" SU(Nf), which applies the same transformation to both helicities of the quarks. This reduction of symmetry is a form of explicit symmetry breaking. The strength of explicit symmetry breaking is controlled by the current quark masses in QCD.\n",
      "\n",
      "\n",
      "\n",
      "What is the role of the Higgs boson in the Standard Model?\n",
      "-The Higgs boson plays a unique role in the Standard Model, by explaining why the other elementary particles, except the photon and gluon, are massive. In particular, the Higgs boson explains why the photon has no mass, while the W and Z bosons are very heavy. Elementary-particle masses and the differences between electromagnetism (mediated by the photon) and the weak force (mediated by the W and Z bosons) are critical to many aspects of the structure of microscopic (and hence macroscopic) matter. In electroweak theory, the Higgs boson generates the masses of the leptons (electron, muon, and tau) and quarks. As the Higgs boson is massive, it must interact with itself.\n",
      "-Higgs boson One of the most important goals of ATLAS was to investigate a missing piece of the Standard Model, the Higgs boson. The Higgs mechanism, which includes the Higgs boson, gives mass to elementary particles, leading to differences between the weak force and electromagnetism by giving the W and Z bosons mass while leaving the photon massless.\n",
      "-The Higgs boson The Higgs boson, sometimes called the Higgs particle, is an elementary particle in the Standard Model of particle physics produced by the quantum excitation of the Higgs field, one of the fields in particle physics theory. In the Standard Model, the Higgs particle is a massive scalar boson with zero spin, even (positive) parity, no electric charge, and no colour charge, that couples to (interacts with) mass. It is also very unstable, decaying into other particles almost immediately.\n",
      "-The Higgs boson, sometimes called the Higgs particle, is an elementary particle in the Standard Model of particle physics produced by the quantum excitation of the Higgs field, one of the fields in particle physics theory. In the Standard Model, the Higgs particle is a massive scalar boson with zero spin, even (positive) parity, no electric charge, and no colour charge that couples to (interacts with) mass. It is also very unstable, decaying into other particles almost immediately upon generation.\n",
      "-Higgs boson As of 2011, the Higgs boson, the quantum of a field that is thought to provide particles with rest masses, remained the only particle of the Standard Model to be verified.\n",
      "\n",
      "\n",
      "\n",
      "What is Lorentz symmetry or Lorentz invariance in relativistic physics?\n",
      "-In relativistic physics, Lorentz symmetry or Lorentz invariance, named after the Dutch physicist Hendrik Lorentz, is an equivalence of observation or observational symmetry due to special relativity implying that the laws of physics stay the same for all observers that are moving with respect to one another within an inertial frame. It has also been described as \"the feature of nature that says experimental results are independent of the orientation or the boost velocity of the laboratory through space\".Lorentz covariance, a related concept, is a property of the underlying spacetime manifold. Lorentz covariance has two distinct, but closely related meanings: A physical quantity is said to be Lorentz covariant if it transforms under a given representation of the Lorentz group. According to the representation theory of the Lorentz group, these quantities are built out of scalars, four-vectors, four-tensors, and spinors. In particular, a Lorentz covariant scalar (e.g., the space-time interval) remains the same under Lorentz transformations and is said to be a Lorentz invariant (i.e., they transform under the trivial representation).\n",
      "-For example, the following laws, equations, and theories respect Lorentz symmetry: The kinematical laws of special relativity Maxwell's field equations in the theory of electromagnetism The Dirac equation in the theory of the electron The Standard Model of particle physicsThe Lorentz group expresses the fundamental symmetry of space and time of all known fundamental laws of nature. In small enough regions of spacetime where gravitational variances are negligible, physical laws are Lorentz invariant in the same manner as special relativity.\n",
      "-Lorentz invariance measures the universal features in hypothetical loop quantum gravity universes. The various hypothetical multiverse loop quantum gravity universe design models could have various general covariant principle results.\n",
      "Because loop quantum gravity models universes, space gravity theories are contenders to build and answer unification theory; the Lorentz invariance helps grade the spread of universal features throughout a proposed multiverse in time.\n",
      "-In physics, Lorentz transformations became known at the beginning of the 20th century, when it was discovered that they exhibit the symmetry of Maxwell's equations. Subsequently, they became fundamental to all of physics, because they formed the basis of special relativity in which they exhibit the symmetry of Minkowski spacetime, making the speed of light invariant between different inertial frames. They relate the spacetime coordinates of two arbitrary inertial frames of reference with constant relative speed v. In one frame, the position of an event is given by x,y,z and time t, while in the other frame the same event has coordinates x′,y′,z′ and t′.\n",
      "-Similar to the approximate Lorentz symmetry of phonons in a lattice (where the speed of sound plays the role of the critical speed), the Lorentz symmetry of special relativity (with the speed of light as the critical speed in vacuum) is only a low-energy limit of the laws of physics, which involve new phenomena at some fundamental scale. Bare conventional \"elementary\" particles are not point-like field-theoretical objects at very small distance scales, and a nonzero fundamental length must be taken into account. Lorentz symmetry violation is governed by an energy-dependent parameter which tends to zero as momentum decreases. Such patterns require the existence of a privileged local inertial frame (the \"vacuum rest frame\"). They can be tested, at least partially, by ultra-high energy cosmic ray experiments like the Pierre Auger Observatory.\n",
      "\n",
      "\n",
      "\n",
      "What is the significance of Baryon Acoustic Oscillations (BAOs) in the study of the universe?\n",
      "-Sky surveys and baryon acoustic oscillations Baryon acoustic oscillations (BAO) are fluctuations in the density of the visible baryonic matter (normal matter) of the universe on large scales. These are predicted to arise in the Lambda-CDM model due to acoustic oscillations in the photon–baryon fluid of the early universe, and can be observed in the cosmic microwave background angular power spectrum. BAOs set up a preferred length scale for baryons. As the dark matter and baryons clumped together after recombination, the effect is much weaker in the galaxy distribution in the nearby universe, but is detectable as a subtle (≈1 percent) preference for pairs of galaxies to be separated by 147 Mpc, compared to those separated by 130–160 Mpc. This feature was predicted theoretically in the 1990s and then discovered in 2005, in two large galaxy redshift surveys, the Sloan Digital Sky Survey and the 2dF Galaxy Redshift Survey. Combining the CMB observations with BAO measurements from galaxy redshift surveys provides a precise estimate of the Hubble constant and the average matter density in the Universe. The results support the Lambda-CDM model.\n",
      "-In cosmology, baryon acoustic oscillations (BAO) are fluctuations in the density of the visible baryonic matter (normal matter) of the universe, caused by acoustic density waves in the primordial plasma of the early universe. In the same way that supernovae provide a \"standard candle\" for astronomical observations, BAO matter clustering provides a \"standard ruler\" for length scale in cosmology.  The length of this standard ruler is given by the maximum distance the acoustic waves could travel in the primordial plasma before the plasma cooled to the point where it became neutral atoms (the epoch of recombination), which stopped the expansion of the plasma density waves, \"freezing\" them into place. The length of this standard ruler (≈490 million light years in today's universe) can be measured by looking at the large scale structure of matter using astronomical surveys. BAO measurements help cosmologists understand more about the nature of dark energy (which causes the accelerating expansion of the universe) by constraining cosmological parameters.\n",
      "-Another class of physical distance indicator is the standard ruler. In 2008, galaxy diameters have been proposed as a possible standard ruler for cosmological parameter determination. More recently the physical scale imprinted by baryon acoustic oscillations (BAO) in the early universe has been used.  In the early universe (before recombination) the baryons and photons scatter off each other, and form a tightly-coupled fluid that can support sound waves. The waves are sourced by primordial density perturbations, and travel at speed that can be predicted from the baryon density and other cosmological parameters. The total distance that these sound waves can travel before recombination determines a fixed scale, which simply expands with the universe after recombination. BAO therefore provide a standard ruler that can be measured in galaxy surveys from the effect of baryons on the clustering of galaxies. The method requires an extensive galaxy survey in order to make this scale visible, but has been measured with percent-level precision (see baryon acoustic oscillations). The scale does depend on cosmological parameters like the baryon and matter densities, and the number of neutrinos, so distances based on BAO are more dependent on cosmological model than those based on local measurements.\n",
      "-Baryon acoustic oscillations The signature of baryon acoustic oscillations (BAO) can be observed in the distribution of tracers of the matter density field and used to measure the expansion history of the Universe. BAO can also be measured using purely photometric data, though at less significance. DES team observation samples consists of 7 million galaxies distributed over a footprint of 4100 deg2 with 0.6 < zphoto < 1.1 and a typical redshift uncertainty of 0.03(1+z). From their statistics, they combine the likelihoods derived from angular correlations and spherical harmonics to constrain the ratio of comoving angular diameter distance  0.835 18.92 0.51 at the effective redshift of our sample to the sound horizon scale at the drag epoch.\n",
      "-DESI will measure the expansion history of the universe using the baryon acoustic oscillations (BAO) imprinted in the clustering of galaxies, quasars, and the intergalactic medium. The BAO technique is a robust way to extract cosmological distance information from the clustering of matter and galaxies. It relies only on very large-scale structure and it does so in a manner that enables scientists to separate the acoustic peak of the BAO signature from uncertainties in most systematic errors in the data. BAO was identified in the 2006 Dark Energy Task Force report as one of the key methods for studying dark energy. In May 2014, the High-Energy Physics Advisory Panel, a federal advisory committee, commissioned by the US Department of Energy (DOE) and the National Science Foundation (NSF) endorsed DESI.\n",
      "\n",
      "\n",
      "\n",
      "What can be inferred about the electronic entropy of insulators and metals based on their densities of states at the Fermi level?\n",
      "-Insulators have zero density of states at the Fermi level due to their band gaps. Thus, the density of states-based electronic entropy is essentially zero in these systems.\n",
      "-Metals have non-zero density of states at the Fermi level. Metals with free-electron-like band structures (e.g. alkali metals, alkaline earth metals, Cu, and Al) generally exhibit relatively low density of states at the Fermi level, and therefore exhibit fairly low electronic entropies. Transition metals, wherein the flat d-bands lie close to the Fermi level, generally exhibit much larger electronic entropies than the free-electron like metals.\n",
      "-Useful approximation It is useful to recognize that the only states within ~±kBT of the Fermi level contribute significantly to the entropy. Other states are either fully occupied, f = 1, or completely unoccupied, f = 0. In either case, these states do not contribute to the entropy. If one assumes that the density of states is constant within ±kBT of the Fermi level, one can derive that the electron heat capacity, equal to: CV=T(∂S∂T)T,V=π23kB2Tn(EF) where n(EF) is the density of states (number of levels per unit energy) at the Fermi level. Several other approximations can be made, but they all indicate that the electronic entropy should, to first order, be proportional to the temperature and the density of states at the Fermi level. As the density of states at the Fermi level varies widely between systems, this approximation is a reasonable heuristic for inferring when it may be necessary to include electronic entropy in the thermodynamic description of a system; only systems with large densities of states at the Fermi level should exhibit non-negligible electronic entropy (where large may be approximately defined as n(EF) ≥ (k2BT)−1).\n",
      "-Typical values Metals Under the free electron model, the electrons in a metal can be considered to form a uniform Fermi gas. The number density  N/V of conduction electrons in metals ranges between approximately 1028 and 1029 electrons per m3, which is also the typical density of atoms in ordinary solid matter. This number density produces a Fermi energy of the order: where me is the electron rest mass. This Fermi energy corresponds to a Fermi temperature of the order of 106 kelvins, much higher than the temperature of the sun surface. Any metal will boil before reaching this temperature under atmospheric pressure. Thus for any practical purpose, a metal can be considered as a Fermi gas at zero temperature as a first approximation (normal temperatures are small compared to TF).\n",
      "-Electronic entropy is the entropy of a system attributable to electrons' probabilistic occupation of states. This entropy can take a number of forms. The first form can be termed a density of states based entropy. The Fermi–Dirac distribution implies that each eigenstate of a system, i, is occupied with a certain probability, pi. As the entropy is given by a sum over the probabilities of occupation of those states, there is an entropy associated with the occupation of the various electronic states. In most molecular systems, the energy spacing between the highest occupied molecular orbital and the lowest unoccupied molecular orbital is usually large, and thus the probabilities associated with the occupation of the excited states are small. Therefore, the electronic entropy in molecular systems can safely be neglected. Electronic entropy is thus most relevant for the thermodynamics of condensed phases, where the density of states at the Fermi level can be quite large, and the electronic entropy can thus contribute substantially to thermodynamic behavior. A second form of electronic entropy can be attributed to the configurational entropy associated with localized electrons and holes. This entropy is similar in form to the configurational entropy associated with the mixing of atoms on a lattice.\n",
      "\n",
      "\n",
      "\n",
      "What are permutation-inversion groups?\n",
      "-Symmetry operations, point groups and permutation-inversion groups A molecule at equilibrium in a certain electronic state usually has some geometrical symmetry. This symmetry is described by a certain point group which consists of operations (called symmetry operations) that produce a spatial orientation of the molecule that is indistinguishable from the starting configuration. There are five types of point group symmetry operation: identity, rotation, reflection, inversion and improper rotation or rotation-reflection. Common to all symmetry operations is that the geometrical center-point of the molecule does not change its position; hence the name point group. One can determine the elements of the point group for a particular molecule by considering the geometrical symmetry of its molecular model. However, when one uses a point group, the elements are not to be interpreted in the same way. Instead the elements rotate and/or reflect the vibronic (vibration-electronic) coordinates and these elements commute with the vibronic Hamiltonian. The point group is used to classify by symmetry the vibronic eigenstates. The symmetry classification of the rotational levels, the eigenstates of the full (rovibronic nuclear spin) Hamiltonian, requires the use of the appropriate permutation-inversion group as introduced by Longuet-Higgins. See the Section Inversion symmetry and nuclear permutation symmetry below. The elements of permutation-inversion groups commute with the full molecular Hamiltonian. In addition to point groups, there exists another kind of group important in crystallography, where translation in 3-D also needs to be taken care of. They are known as space groups.\n",
      "-As discussed above in the section Point groups and permutation-inversion groups, point groups are useful for classifying the vibrational and electronic states of rigid molecules (sometimes called semi-rigid molecules) which undergo only small oscillations about a single equilibrium geometry. Longuet-Higgins introduced a more general type of symmetry group suitable not only for classifying the vibrational and electronic states of rigid molecules but also for classifying their rotational and nuclear spin states. Further, such groups can be used to classify the states of non-rigid (or fluxional) molecules that tunnel between equivalent geometries (called versions) and to allow for the distorting effects of molecular rotation. These groups are known as permutation-inversion groups, because the symmetry operations in them are energetically feasible permutations of identical nuclei, or inversion with respect to the center of mass (the parity operation), or a combination of the two.\n",
      "-In mathematics, a permutation group is a group G whose elements are permutations of a given set M and whose group operation is the composition of permutations in G (which are thought of as bijective functions from the set M to itself). The group of all permutations of a set M is the symmetric group of M, often written as Sym(M). The term permutation group thus means a subgroup of the symmetric group. If M = {1, 2, ..., n} then Sym(M) is usually denoted by Sn, and may be called the symmetric group on n letters.\n",
      "-A subgroup of a symmetric group is called a permutation group.\n",
      "-One can determine the symmetry operations of the point group for a particular molecule by considering the geometrical symmetry of its molecular model. However, when one uses a point group to classify molecular states, the operations in it are not to be interpreted in the same way. Instead the operations are interpreted as rotating and/or reflecting the vibronic (vibration-electronic) coordinates and these operations commute with the vibronic Hamiltonian. They are \"symmetry operations\" for that vibronic Hamiltonian. The point group is used to classify by symmetry the vibronic eigenstates of a rigid molecule. The symmetry classification of the rotational levels, the eigenstates of the full (rotation-vibration-electronic) Hamiltonian, requires the use of the appropriate permutation-inversion group as introduced by Longuet-Higgins. Point groups describe the geometrical symmetry of a molecule whereas permutation-inversion groups describe the energy-invariant symmetry.\n",
      "\n",
      "\n",
      "\n",
      "What is the relationship between dielectric loss and the transparency of a material?\n",
      "-Absorption and reflection Some materials cause absorption of electromagnetic waves, preventing it from reaching the receiver, in other cases, particularly with metallic or conductive materials reflection occurs. This can cause dead zones where no reception is available. Aluminium foiled thermal isolation in modern homes can easily reduce indoor mobile signals by 10 dB frequently leading to complaints about the bad reception of long-distance rural cell signals.\n",
      "-If the object is transparent, then the light waves are passed on to neighboring atoms through the bulk of the material and re-emitted on the opposite side of the object. Such frequencies of light waves are said to be transmitted.\n",
      "Transparency in insulators An object may be not transparent either because it reflects the incoming light or because it absorbs the incoming light. Almost all solids reflect a part and absorb a part of the incoming light.\n",
      "-Attenuation causes the reflected wave to decrease in power as distance from the reflective material increases. As the power of the reflective wave decreases compared to the power of the incident wave, interference also decreases. And as interference decreases, so does the phase difference between sound pressure and particle velocity. At a large enough distance from the reflective material, there is no interference left anymore. At this distance one can speak of the far field.\n",
      "-The change in permittivity occurs because of the disruption in the atomic structure of the materials. That is, the changes are due to the breaking of bonds and re-bonding within the atomic structure of the amorphous or crystalline structures. This modification in turn modifies the carrier traps within the band structure, reducing them, and hence ensuing the decrement in the permittivityThis contrasts with the photorefractive effect where the change is induced by the alteration in the electron distribution due to the photon-absorption.\n",
      "-Influence of wavelength The refractive index of a given sample varies with wavelength for all materials. This dispersion relation is nonlinear and is characteristic for every material. In the visible range, a decrease of the refractive index comes with increasing wavelength. In glass prisms very little absorption is observable. In the infrared wavelength range several absorption maxima and fluctuations in the refractive index appear. To guarantee a high quality measurement with an accuracy of up to 0.00002 in the refractive index the wavelength has to be determined correctly. Therefore, in modern refractometers the wavelength is tuned to a bandwidth of +/-0.2 nm to ensure correct results for samples with different dispersions.\n",
      "\n",
      "\n",
      "\n",
      "What is the purpose of measuring the Larmor precession fields at about 100 microtesla with highly sensitive superconducting quantum interference devices (SQUIDs) in ultra-low field MRI?\n",
      "-T1 and T2 Each tissue returns to its equilibrium state after excitation by the independent relaxation processes of T1 (spin-lattice; that is, magnetization in the same direction as the static magnetic field) and T2 (spin-spin; transverse to the static magnetic field).\n",
      "To create a T1-weighted image, magnetization is allowed to recover before measuring the MR signal by changing the repetition time (TR). This image weighting is useful for assessing the cerebral cortex, identifying fatty tissue, characterizing focal liver lesions, and in general, obtaining morphological information, as well as for post-contrast imaging.\n",
      "To create a T2-weighted image, magnetization is allowed to decay before measuring the MR signal by changing the echo time (TE). This image weighting is useful for detecting edema and inflammation, revealing white matter lesions, and assessing zonal anatomy in the prostate and uterus.\n",
      "-T1 and T2 Each tissue returns to its equilibrium state after excitation by the independent relaxation processes of T1 (spin-lattice; that is, magnetization in the same direction as the static magnetic field) and T2 (spin-spin; transverse to the static magnetic field).\n",
      "To create a T1-weighted image, magnetization is allowed to recover before measuring the MR signal by changing the repetition time (TR). This image weighting is useful for assessing the cerebral cortex, identifying fatty tissue, characterizing focal liver lesions, and in general, obtaining morphological information, as well as for post-contrast imaging.\n",
      "To create a T2-weighted image, magnetization is allowed to decay before measuring the MR signal by changing the echo time (TE). This image weighting is useful for detecting edema and inflammation, revealing white matter lesions, and assessing zonal anatomy in the prostate and uterus.\n",
      "The standard display of MR images is to represent fluid characteristics in black-and-white images, where different tissues turn out as follows: \n",
      "-In MRI scanners, sections of the body are exposed to a strong magnetic field causing primarily the hydrogen nuclei (\"spins\") of water in tissues to be polarized in the direction of the magnetic field. An intense radiofrequency pulse is applied that tips the magnetization generated by the hydrogen nuclei in the direction of the receiver coil where the spin polarization can be detected. Random molecular rotational oscillations matching the resonance frequency of the nuclear spins provide the \"relaxation\" mechanisms that bring the net magnetization back to its equilibrium position in alignment with the applied magnetic field. The magnitude of the spin polarization detected by the receiver is used to form the MR image but decays with a characteristic time constant known as the T1 relaxation time. Water protons in different tissues have different T1 values, which is one of the main sources of contrast in MR images. A contrast agent usually shortens, but in some instances increases, the value of T1 of nearby water protons thereby altering the contrast in the image.\n",
      "-In MRI, the static magnetic field is augmented by a field gradient coil to vary across the scanned region, so that different spatial locations become associated with different precession frequencies. Only those regions where the field is such that the precession frequencies match the RF frequency will experience excitation. Usually, these field gradients are modulated to sweep across the region to be scanned, and it is the almost infinite variety of RF and gradient pulse sequences that gives MRI its versatility. Change of field gradient spreads the responding FID signal in the frequency domain, but this can be recovered and measured by a refocusing gradient (to create a so-called \"gradient echo\"), or by a radio frequency pulse (to create a so-called \"spin-echo\"), or in digital post-processing of the spread signal. The whole process can be repeated when some T1-relaxation has occurred and the thermal equilibrium of the spins has been more or less restored. The repetition time (TR) is the time between two successive excitations of the same slice.Typically, in soft tissues T1 is around one second while T2 and T*2 are a few tens of milliseconds. However, these values can vary widely between different tissues, as well as between different external magnetic fields. This behavior is one factor giving MRI its tremendous soft tissue contrast.\n",
      "-T2*-weighted imaging is built from the basic physics of magnetic resonance imaging where there is spin–spin relaxation, that is, the transverse component of the magnetization vector exponentially decays towards its equilibrium value. It is characterized by the spin–spin relaxation time, known as T2. In an idealized system, all nuclei in a given chemical environment, in a magnetic field, relax with the same frequency. However, in real systems, there are minor differences in chemical environment which can lead to a distribution of resonance frequencies around the ideal. Over time, this distribution can lead to a dispersion of the tight distribution of magnetic spin vectors, and loss of signal (Free Induction Decay). In fact, for most magnetic resonance experiments, this \"relaxation\" dominates. This results in dephasing.\n",
      "\n",
      "\n",
      "\n",
      "What is the difference between illuminance and luminance?\n",
      "-Illuminance Illuminance is a measure of how much luminous flux is spread over a given area. One can think of luminous flux (with the unit lumen) as a measure of the total \"amount\" of visible light present, and the illuminance as a measure of the intensity of illumination on a surface. A given amount of light will illuminate a surface more dimly if it is spread over a larger area, so illuminance is inversely proportional to area when the luminous flux is held constant.  One lux is equal to one lumen per square metre: 1 lx = 1 lm/m2 = 1 cd·sr/m2.A flux of 1000 lumens, spread uniformly over an area of 1 square metre, lights up that square metre with an illuminance of 1000 lux. However, the same 1000 lumens spread out over 10 square metres produces a dimmer illuminance of only 100 lux.\n",
      "-Luminance is a photometric measure of the luminous intensity per unit area of light travelling in a given direction. It describes the amount of light that passes through, is emitted from, or is reflected from a particular area, and falls within a given solid angle.  The procedure for conversion from spectral radiance to luminance is standardized by the CIE and ISO.Brightness is the term for the subjective impression of the objective luminance measurement standard (see Objectivity (science) § Objectivity in measurement for the importance of this contrast).\n",
      "-In photometry, illuminance is the total luminous flux incident on a surface, per unit area. It is a measure of how much the incident light illuminates the surface, wavelength-weighted by the luminosity function to correlate with human brightness perception. Similarly, luminous emittance is the luminous flux per unit area emitted from a surface. Luminous emittance is also known as luminous exitance.In SI units illuminance is measured in lux (lx), or equivalently in lumens per square metre (lm·m−2). Luminous exitance is measured in lm·m−2 only, not lux. In the CGS system, the unit of illuminance is the phot, which is equal to 10000 lux. The foot-candle is a non-metric unit of illuminance that is used in photography.Illuminance was formerly often called brightness, but this leads to confusion with other uses of the word, such as to mean luminance. \"Brightness\" should never be used for quantitative description, but only for nonquantitative references to physiological sensations and perceptions of light.\n",
      "-The United States Federal Trade Commission (FTC) has assigned an unconventional meaning to brightness when applied to lamps. When appearing on light bulb packages, brightness means luminous flux, while in other contexts it means luminance. Luminous flux is the total amount of light coming from a source, such as a lighting device. Luminance, the original meaning of brightness, is the amount of light per solid angle coming from an area, such as the sky. The table below shows the standard ways of indicating the amount of light.\n",
      "-Brightness: The amount of light emitted from the display. It is sometimes synonymous with the term luminance, which is defined as the amount of light per area and is measured in SI units as candela per square meter.\n",
      "\n",
      "\n",
      "\n",
      "What is a magnetic monopole in particle physics?\n",
      "-In particle physics, a magnetic monopole is a hypothetical elementary particle that is an isolated magnet with only one magnetic pole (a north pole without a south pole or vice versa). A magnetic monopole would have a net north or south \"magnetic charge\". Modern interest in the concept stems from particle theories, notably the grand unified and superstring theories, which predict their existence. The known elementary particles that have electric charge are electric monopoles.\n",
      "-The pole model usually treats magnetic charge as a mathematical abstraction, rather than a physical property of particles. However, a magnetic monopole is a hypothetical particle (or class of particles) that physically has only one magnetic pole (either a north pole or a south pole). In other words, it would possess a \"magnetic charge\" analogous to an electric charge. Magnetic field lines would start or end on magnetic monopoles, so if they exist, they would give exceptions to the rule that magnetic field lines neither start nor end. Some theories (such as Grand Unified Theories) have predicted the existence of magnetic monopoles, but so far, none have been observed.\n",
      "-Magnetic monopoles Since a bar magnet gets its ferromagnetism from electrons distributed evenly throughout the bar, when a bar magnet is cut in half, each of the resulting pieces is a smaller bar magnet. Even though a magnet is said to have a north pole and a south pole, these two poles cannot be separated from each other. A monopole—if such a thing exists—would be a new and fundamentally different kind of magnetic object. It would act as an isolated north pole, not attached to a south pole, or vice versa. Monopoles would carry \"magnetic charge\" analogous to electric charge. Despite systematic searches since 1931, as of 2010, they have never been observed, and could very well not exist.Nevertheless, some theoretical physics models predict the existence of these magnetic monopoles. Paul Dirac observed in 1931 that, because electricity and magnetism show a certain symmetry, just as quantum theory predicts that individual positive or negative electric charges can be observed without the opposing charge, isolated South or North magnetic poles should be observable. Using quantum theory Dirac showed that if magnetic monopoles exist, then one could explain the quantization of electric charge—that is, why the observed elementary particles carry charges that are multiples of the charge of the electron.\n",
      "-In physics, a neutral particle is a particle without an electric charge, such as a neutron.  The term neutral particles should not be confused with truly neutral particles, the subclass of neutral particles that are also identical to their own antiparticles.\n",
      "-For many magnets the first non-zero term is the magnetic dipole moment. (To date, no isolated magnetic monopoles have been experimentally detected.) A magnetic dipole is the limit of either a current loop or a pair of poles as the dimensions of the source are reduced to zero while keeping the moment constant. As long as these limits only apply to fields far from the sources, they are equivalent. However, the two models give different predictions for the internal field (see below).\n",
      "\n",
      "\n",
      "\n",
      "What is the difference between redshift due to the expansion of the universe and Doppler redshift?\n",
      "-Redshift is also used to measure the expansion of space, but this is not truly a Doppler effect. Rather, redshifting due to the expansion of space is known as cosmological redshift, which can be derived purely from the Robertson-Walker metric under the formalism of general relativity. Having said this, it also happens that there are detectable Doppler effects on cosmological scales, which, if incorrectly interpreted as cosmological in origin, lead to the observation of redshift-space distortions.\n",
      "-There is a distinction between a redshift in cosmological context as compared to that witnessed when nearby objects exhibit a local Doppler-effect redshift. Rather than cosmological redshifts being a consequence of the relative velocities that are subject to the laws of special relativity (and thus subject to the rule that no two locally separated objects can have relative velocities with respect to each other faster than the speed of light), the photons instead increase in wavelength and redshift because of a global feature of the spacetime through which they are traveling. One interpretation of this effect is the idea that space itself is expanding. Due to the expansion increasing as distances increase, the distance between two remote galaxies can increase at more than 3×108 m/s, but this does not imply that the galaxies move faster than the speed of light at their present location (which is forbidden by Lorentz covariance).\n",
      "-At the time of discovery and development of Hubble's law, it was acceptable to explain redshift phenomenon as a Doppler shift in the context of special relativity, and use the Doppler formula to associate redshift z with velocity. Today, in the context of general relativity, velocity between distant objects depends on the choice of coordinates used, and therefore, the redshift can be equally described as a Doppler shift or a cosmological shift (or gravitational) due to the expanding space, or some combination of the two.\n",
      "-Distinguishing between cosmological and local effects For cosmological redshifts of z < 0.01 additional Doppler redshifts and blueshifts due to the peculiar motions of the galaxies relative to one another cause a wide scatter from the standard Hubble Law. The resulting situation can be illustrated by the Expanding Rubber Sheet Universe, a common cosmological analogy used to describe the expansion of space. If two objects are represented by ball bearings and spacetime by a stretching rubber sheet, the Doppler effect is caused by rolling the balls across the sheet to create peculiar motion. The cosmological redshift occurs when the ball bearings are stuck to the sheet and the sheet is stretched.The redshifts of galaxies include both a component related to recessional velocity from expansion of the universe, and a component related to peculiar motion (Doppler shift). The redshift due to expansion of the universe depends upon the recessional velocity in a fashion determined by the cosmological model chosen to describe the expansion of the universe, which is very different from how Doppler redshift depends upon local velocity. Describing the cosmological expansion origin of redshift, cosmologist Edward Robert Harrison said, \"Light leaves a galaxy, which is stationary in its local region of space, and is eventually received by observers who are stationary in their own local region of space. Between the galaxy and the observer, light travels through vast regions of expanding space. As a result, all wavelengths of the light are stretched by the expansion of space. It is as simple as that...\" Steven Weinberg clarified, \"The increase of wavelength from emission to absorption of light does not depend on the rate of change of a(t) [here a(t) is the Robertson–Walker scale factor] at the times of emission or absorption, but on the increase of a(t) in the whole period from emission to absorption.\"Popular literature often uses the expression \"Doppler redshift\" instead of \"cosmological redshift\" to describe the redshift of galaxies dominated by the expansion of spacetime, but the cosmological redshift is not found using the relativistic Doppler equation which is instead characterized by special relativity; thus v ≥ c is impossible while, in contrast, v ≥ c is possible for cosmological redshifts because the space which separates the objects (for example, a quasar from the Earth) can expand faster than the speed of light. More mathematically, the viewpoint that \"distant galaxies are receding\" and the viewpoint that \"the space between galaxies is expanding\" are related by changing coordinate systems. Expressing this precisely requires working with the mathematics of the Friedmann–Robertson–Walker metric.If the universe were contracting instead of expanding, we would see distant galaxies blueshifted by an amount proportional to their distance instead of redshifted.\n",
      "-Doppler effect If a source of the light is moving away from an observer, then redshift (z > 0) occurs; if the source moves towards the observer, then blueshift (z < 0) occurs. This is true for all electromagnetic waves and is explained by the Doppler effect. Consequently, this type of redshift is called the Doppler redshift. If the source moves away from the observer with velocity v, which is much less than the speed of light (v ≪ c), the redshift is given by z≈vc (since  γ≈1 )where c is the speed of light. In the classical Doppler effect, the frequency of the source is not modified, but the recessional motion causes the illusion of a lower frequency.\n",
      "\n",
      "\n",
      "\n",
      "What is the relationship between Coordinated Universal Time (UTC) and Universal Time (UT1)?\n",
      "-Universal Time (UT1) is the Earth Rotation Angle (ERA) linearly scaled to match historical definitions of mean solar time at 0° longitude. At high precision, Earth's rotation is irregular and is determined from the positions of distant quasars using long baseline interferometry, laser ranging of the Moon and artificial satellites, as well as GPS satellite orbits.\n",
      "Coordinated Universal Time (UTC) is an atomic time scale designed to approximate UT1. UTC differs from TAI by an integral number of seconds. UTC is kept within 0.9 second of UT1 by the introduction of one-second steps to UTC, the \"leap second\". To date these steps (and difference \"TAI-UTC\") have always been positive.\n",
      "The Global Positioning System broadcasts a very precise time signal worldwide, along with instructions for converting GPS time (GPST) to UTC. It was defined with a constant offset from TAI: GPST = TAI - 19 s. The GPS time standard is maintained independently but regularly synchronized with or from, UTC time.\n",
      "-International Atomic Time (TAI) is the primary international time standard from which other time standards are calculated. Universal Time (UT1) is mean solar time at 0° longitude, computed from astronomical observations. It varies from TAI because of the irregularities in Earth's rotation. Coordinated Universal Time (UTC) is an atomic time scale designed to approximate Universal Time. UTC differs from TAI by an integral number of seconds. UTC is kept within 0.9 second of UT1 by the introduction of one-second steps to UTC, the \"leap second\". The Global Positioning System broadcasts a very precise time signal based on UTC time.\n",
      "-Contrary to TAI, UTC is a discontinuous time scale. It is occasionally adjusted by leap seconds. Between these adjustments, it is composed of segments that are mapped to atomic time by a constant offset. From its beginning in 1961 through December 1971, the adjustments were made regularly in fractional leap seconds so that UTC approximated UT2. Afterward, these adjustments were made only in whole seconds to approximate UT1. This was a compromise arrangement in order to enable a publicly broadcast time scale. The less frequent whole-second adjustments meant that the time scale would be more stable and easier to synchronize internationally. The fact that it continues to approximate UT1 means that tasks such as navigation which require a source of Universal Time continue to be well served by the public broadcast of UTC.\n",
      "-The TAI and UT1 time scales are precisely defined, the former by atomic clocks (and thus independent of Earth's rotation) and the latter by astronomical observations (that measure actual planetary rotation and thus the solar time at the Greenwich meridian). UTC (on which civil time is usually based) is a compromise, stepping with atomic seconds but periodically reset by a leap second to match UT1.\n",
      "-A set of atomic clocks throughout the world keeps time by consensus: the clocks \"vote\" on the correct time, and all voting clocks are steered to agree with the consensus, which is called International Atomic Time (TAI). TAI \"ticks\" atomic seconds.: 207–218 Civil time is defined to agree with the rotation of the Earth. The international standard for timekeeping is Coordinated Universal Time (UTC). This time scale \"ticks\" the same atomic seconds as TAI, but inserts or omits leap seconds as necessary to correct for variations in the rate of rotation of the Earth.: 16–17, 207 A time scale in which the seconds are not exactly equal to atomic seconds is UT1, a form of universal time. UT1 is defined by the rotation of the Earth with respect to the Sun, and does not contain any leap seconds.: 68, 232  UT1 always differs from UTC by less than a second.\n",
      "\n",
      "\n",
      "\n",
      "What is the reason for heating metals to a temperature just above the upper critical temperature?\n",
      "-Metallic materials consist of a microstructure of small crystals called \"grains\" or crystallites. The nature of the grains (i.e. grain size and composition) is one of the most effective factors that can determine the overall mechanical behavior of the metal. Heat treatment provides an efficient way to manipulate the properties of the metal by controlling the rate of diffusion and the rate of cooling within the microstructure. Heat treating is often used to alter the mechanical properties of a metallic alloy, manipulating properties such as the hardness, strength, toughness, ductility, and elasticity.There are two mechanisms that may change an alloy's properties during heat treatment: the formation of martensite causes the crystals to deform intrinsically, and the diffusion mechanism causes changes in the homogeneity of the alloy.The crystal structure consists of atoms that are grouped in a very specific arrangement, called a lattice. In most elements, this order will rearrange itself, depending on conditions like temperature and pressure. This rearrangement called allotropy or polymorphism, may occur several times, at many different temperatures for a particular metal. In alloys, this rearrangement may cause an element that will not normally dissolve into the base metal to suddenly become soluble, while a reversal of the allotropy will make the elements either partially or completely insoluble.When in the soluble state, the process of diffusion causes the atoms of the dissolved element to spread out, attempting to form a homogenous distribution within the crystals of the base metal. If the alloy is cooled to an insoluble state, the atoms of the dissolved constituents (solutes) may migrate out of the solution. This type of diffusion, called precipitation, leads to nucleation, where the migrating atoms group together at the grain-boundaries. This forms a microstructure generally consisting of two or more distinct phases. For instance, steel that has been heated above the austenizing temperature (red to orange-hot, or around 1,500 °F (820 °C) to 1,600 °F (870 °C) depending on carbon content), and then cooled slowly, forms a laminated structure composed of alternating layers of ferrite and cementite, becoming soft pearlite. After heating the steel to the austenite phase and then quenching it in water, the microstructure will be in the martensitic phase. This is due to the fact that the steel will change from the austenite phase to the martensite phase after quenching. Some pearlite or ferrite may be present if the quench did not rapidly cool off all the steel.Unlike iron-based alloys, most heat-treatable alloys do not experience a ferrite transformation. In these alloys, the nucleation at the grain-boundaries often reinforces the structure of the crystal matrix. These metals harden by precipitation. Typically a slow process, depending on temperature, this is often referred to as \"age hardening\".Many metals and non-metals exhibit a martensite transformation when cooled quickly (with external media like oil, polymer, water, etc.). When a metal is cooled very quickly, the insoluble atoms may not be able to migrate out of the solution in time. This is called a \"diffusionless transformation.\" When the crystal matrix changes to its low-temperature arrangement, the atoms of the solute become trapped within the lattice. The trapped atoms prevent the crystal matrix from completely changing into its low-temperature allotrope, creating shearing stresses within the lattice. When some alloys are cooled quickly, such as steel, the martensite transformation hardens the metal, while in others, like aluminum, the alloy becomes softer.\n",
      "-Because a smaller grain size usually enhances mechanical properties, such as toughness, shear strength and tensile strength, these metals are often heated to a temperature that is just above the upper critical temperature, in order to prevent the grains of solution from growing too large. For instance, when steel is heated above the upper critical-temperature, small grains of austenite form. These grow larger as the temperature is increased. When cooled very quickly, during a martensite transformation, the austenite grain-size directly affects the martensitic grain-size. Larger grains have large grain-boundaries, which serve as weak spots in the structure. The grain size is usually controlled to reduce the probability of breakage.The diffusion transformation is very time-dependent. Cooling a metal will usually suppress the precipitation to a much lower temperature. Austenite, for example, usually only exists above the upper critical temperature. However, if the austenite is cooled quickly enough, the transformation may be suppressed for hundreds of degrees below the lower critical temperature. Such austenite is highly unstable and, if given enough time, will precipitate into various microstructures of ferrite and cementite. The cooling rate can be used to control the rate of grain growth or can even be used to produce partially martensitic microstructures. However, the martensite transformation is time-independent. If the alloy is cooled to the martensite transformation (Ms) temperature before other microstructures can fully form, the transformation will usually occur at just under the speed of sound.When austenite is cooled slow enough that a martensite transformation does not occur, the austenite grain size will have an effect on the rate of nucleation, but it is generally temperature and the rate of cooling that controls the grain size and microstructure. When austenite is cooled extremely slowly, it will form large ferrite crystals filled with spherical inclusions of cementite. This microstructure is referred to as \"sphereoidite\". If cooled a little faster, then coarse pearlite will form. Even faster, and fine pearlite will form. If cooled even faster, bainite will form. Similarly, these microstructures will also form, if cooled to a specific temperature and then held there for a certain time.Most non-ferrous alloys are also heated in order to form a solution. Most often, these are then cooled very quickly to produce a martensite transformation, putting the solution into a supersaturated state. The alloy, being in a much softer state, may then be cold worked. This causes work hardening that increases the strength and hardness of the alloy. Moreover, the defects caused by plastic deformation tend to speed up precipitation, increasing the hardness beyond what is normal for the alloy. Even if not cold worked, the solutes in these alloys will usually precipitate, although the process may take much longer. Sometimes these metals are then heated to a temperature that is below the lower critical (A1) temperature, preventing recrystallization, in order to speed-up the precipitation.\n",
      "-Initial grain size affects the critical temperature. Grain boundaries are good sites for nuclei to form. Since an increase in grain size results in fewer boundaries this results in a decrease in the nucleation rate and hence an increase in the recrystallization temperature Deformation affects the final grain size. Increasing the deformation, or reducing the deformation temperature, increases the rate of nucleation faster than it increases the rate of growth. As a result, the final grain size is reduced by increased deformation.\n",
      "-In materials science, grain growth is the increase in size of grains (crystallites) in a material at high temperature. This occurs when recovery and recrystallisation are complete and further reduction in the internal energy can only be achieved by reducing the total area of grain boundary. The term is commonly used in metallurgy but is also used in reference to ceramics and minerals. The behaviors of grain growth is analogous to the coarsening behaviors of grains, which implied that both of grain growth and coarsening may be dominated by the same physical mechanism.\n",
      "-Grain Boundary Precipitation In superalloys strengthened by metal carbides, increasingly large carbide particles form preferentially at grain boundaries, preventing grain boundary sliding at high temperatures. This leads to an increase in the yield strength, and thus a yield strength anomaly.\n",
      "\n",
      "\n",
      "\n",
      "What is the cause of the observed change in the periods of moons orbiting a distant planet when measured from Earth?\n",
      "-Ole Christensen Rømer used an astronomical measurement to make the first quantitative estimate of the speed of light in the year 1676. When measured from Earth, the periods of moons orbiting a distant planet are shorter when the Earth is approaching the planet than when the Earth is receding from it. The distance travelled by light from the planet (or its moon) to Earth is shorter when the Earth is at the point in its orbit that is closest to its planet than when the Earth is at the farthest point in its orbit, the difference in distance being the diameter of the Earth's orbit around the Sun. The observed change in the moon's orbital period is caused by the difference in the time it takes light to traverse the shorter or longer distance. Rømer observed this effect for Jupiter's innermost major moon Io and deduced that light takes 22 minutes to cross the diameter of the Earth's orbit.\n",
      "-The instantaneous lunar distance is constantly changing. The actual distance between the Moon and Earth can change as quickly as 75 meters per second, or more than 1,000 km (620 mi) in just 6 hours, due to its non-circular orbit. There are other effects that also influence the lunar distance. Some factors include: Perturbations and eccentricity The distance to the Moon can be measured to an accuracy of 2 mm over a 1-hour sampling period, which results in an overall uncertainty of a decimeter for the semi-major axis. However, due to its elliptical orbit with varying eccentricity, the instantaneous distance varies with monthly periodicity. Furthermore, the distance is perturbed by the gravitational effects of various astronomical bodies – most significantly the Sun and less so Venus and Jupiter. Other forces responsible for minute perturbations are: gravitational attraction to other planets in the Solar System and to asteroids; tidal forces; and relativistic effects. The effect of radiation pressure from the Sun contributes an amount of ±3.6 mm to the lunar distance.Although the instantaneous uncertainty is a few millimeters, the measured lunar distance can change by more than 21,000 km (13,000 mi) from the mean value throughout a typical month. These perturbations are well understood and the lunar distance can be accurately modeled over thousands of years.\n",
      "-Orbit eccentricity causes the planet/Sun distance to change during the year: The higher is the eccentricity, the higher is the change; Sun rays intensity in various moments of the year changes as the planet/Sun distance changes. Earth eccentricity is very low (0.0167 in a scale from 0 to 1.0000), hence it does not affect so much temperature changes during the year.\n",
      "-The instantaneous Earth–Moon distance, or distance to the Moon, is the distance from the center of Earth to the center of the Moon. Lunar distance (LD or  Δ ⊕ L {\\textstyle \\Delta _{\\oplus L}} ), or Earth–Moon characteristic distance, is a unit of measure in astronomy. More technically, it is the semi-major axis of the geocentric lunar orbit. The lunar distance is on average approximately 385,000 km (239,000 mi), or 1.28 light-seconds; this is roughly 30 times Earth's diameter or 9.5 times Earth's circumference. A little less than 400 lunar distances make up an astronomical unit.\n",
      "-Since Earth's diameter is 3.7 times the Moon's, the length of the planet's umbra is correspondingly 3.7 times that of the lunar umbra: roughly 1,400,000 km (870,000 mi).\n",
      "\n",
      "\n",
      "\n",
      "What is the origin of the radio emission observed from supernova remnants?\n",
      "-Supernova remnants A supernova occurs when a high-mass star reaches the end of its life. When nuclear fusion in the core of the star stops, the star collapses. The gas falling inward either rebounds or gets so strongly heated that it expands outwards from the core, thus causing the star to explode. The expanding shell of gas forms a supernova remnant, a special diffuse nebula. Although much of the optical and X-ray emission from supernova remnants originates from ionized gas, a great amount of the radio emission is a form of non-thermal emission called synchrotron emission. This emission originates from high-velocity electrons oscillating within magnetic fields.\n",
      "-In Supernovae When a star explodes in a supernova, the fastest ejecta move at semi-relativistic speeds approximately 10% the speed of light. This blast wave gyrates electrons in ambient magnetic fields and generates synchrotron emission, revealing the radius of the blast wave at the location of the emission. Synchrotron emission can also reveal the strength of the magnetic field at the front of the shock wave, as well as the circumstellar density it encounters, but strongly depends on the choice of energy partition between the magnetic field, proton kinetic energy, and electron kinetic energy. Radio synchrotron emission has allowed astronomers to shed light on mass loss and stellar winds that occur just prior to stellar death.\n",
      "-Energy output Although supernovae are primarily known as luminous events, the electromagnetic radiation they release is almost a minor side-effect. Particularly in the case of core collapse supernovae, the emitted electromagnetic radiation is a tiny fraction of the total energy released during the event.There is a fundamental difference between the balance of energy production in the different types of supernova. In type Ia white dwarf detonations, most of the energy is directed into heavy element synthesis and the kinetic energy of the ejecta. In core collapse supernovae, the vast majority of the energy is directed into neutrino emission, and while some of this apparently powers the observed destruction, 99%+ of the neutrinos escape the star in the first few minutes following the start of the collapse.Standard type Ia supernovae derive their energy from a runaway nuclear fusion of a carbon-oxygen white dwarf. The details of the energetics are still not fully understood, but the result is the ejection of the entire mass of the original star at high kinetic energy. Around half a solar mass of that mass is 56Ni generated from silicon burning. 56Ni is radioactive and decays into 56Co by beta plus decay (with a half life of six days) and gamma rays. 56Co itself decays by the beta plus (positron) path with a half life of 77 days into stable 56Fe. These two processes are responsible for the electromagnetic radiation from type Ia supernovae. In combination with the changing transparency of the ejected material, they produce the rapidly declining light curve.Core collapse supernovae are on average visually fainter than type Ia supernovae, but the total energy released is far higher, as outlined in the following table.\n",
      "-The majority of these emissions are thought to be produced by a mechanism called \"cyclotron maser instability\", which develops close to the auroral regions. Electrons moving parallel to the magnetic field precipitate into the atmosphere while those with a sufficient perpendicular velocity are reflected by the converging magnetic field. This results in an unstable velocity distribution. This velocity distribution spontaneously generates radio waves at the local electron cyclotron frequency. The electrons involved in the generation of radio waves are probably those carrying currents from the poles of the planet to the magnetodisk. The intensity of Jovian radio emissions usually varies smoothly with time. However, there are short and powerful bursts (S bursts) of emission superimposed on the more gradual variations and which can outshine all other components. The total emitted power of the DAM component is about 100 GW, while the power of all other HOM/KOM components is about 10 GW. In comparison, the total power of Earth's radio emissions is about 0.1 GW.Jupiter's radio and particle emissions are strongly modulated by its rotation, which makes the planet somewhat similar to a pulsar. This periodical modulation is probably related to asymmetries in the Jovian magnetosphere, which are caused by the tilt of the magnetic moment with respect to the rotational axis as well as by high-latitude magnetic anomalies. The physics governing Jupiter's radio emissions is similar to that of radio pulsars. They differ only in the scale, and Jupiter can be considered a very small radio pulsar too. In addition, Jupiter's radio emissions strongly depend on solar wind pressure and, hence, on solar activity.In addition to relatively long-wavelength radiation, Jupiter also emits synchrotron radiation (also known as the Jovian decimetric radiation or DIM radiation) with frequencies in the range of 0.1–15 GHz (wavelength from 3 m to 2 cm),. These emissions are from relativistic electrons trapped in the inner radiation belts of the planet. The energy of the electrons that contribute to the DIM emissions is from 0.1 to 100 MeV, while the leading contribution comes from the electrons with energy in the range 1–20 MeV. This radiation is well understood and was used since the beginning of the 1960s to study the structure of the planet's magnetic field and radiation belts. The particles in the radiation belts originate in the outer magnetosphere and are adiabatically accelerated, when they are transported to the inner magnetosphere. However, this requires a source population of moderately high energy electrons (>> 1 keV), and the origin of this population is not well understood.\n",
      "-Other sources include: Sun Jupiter Sagittarius A, the Galactic Center of the Milky Way, with one portion Sagittarius A* thought to be a radio wave emitting supermassive black hole Active galactic nuclei and pulsars have jets of charged particles which emit synchrotron radiation Merging galaxy clusters often show diffuse radio emission Supernova remnants can also show diffuse radio emission; pulsars are a type of supernova remnant that shows highly synchronous emission.\n",
      "\n",
      "\n",
      "\n",
      "What is the relationship between the Hamiltonians and eigenstates in supersymmetric quantum mechanics?\n",
      "-SUSY quantum mechanics involves pairs of Hamiltonians which share a particular mathematical relationship, which are called partner Hamiltonians. (The potential energy terms which occur in the Hamiltonians are then known as partner potentials.) An introductory theorem shows that for every eigenstate of one Hamiltonian, its partner Hamiltonian has a corresponding eigenstate with the same energy. This fact can be exploited to deduce many properties of the eigenstate spectrum. It is analogous to the original description of SUSY, which referred to bosons and fermions. We can imagine a \"bosonic Hamiltonian\", whose eigenstates are the various bosons of our theory. The SUSY partner of this Hamiltonian would be \"fermionic\", and its eigenstates would be the theory's fermions. Each boson would have a fermionic partner of equal energy.\n",
      "-SUSY quantum mechanics involves pairs of Hamiltonians which share a particular mathematical relationship, which are called partner Hamiltonians. (The potential energy terms which occur in the Hamiltonians are then called partner potentials.) An introductory theorem shows that for every eigenstate of one Hamiltonian, its partner Hamiltonian has a corresponding eigenstate with the same energy (except possibly for zero energy eigenstates). This fact can be exploited to deduce many properties of the eigenstate spectrum. It is analogous to the original description of SUSY, which referred to bosons and fermions. We can imagine a \"bosonic Hamiltonian\", whose eigenstates are the various bosons of our theory. The SUSY partner of this Hamiltonian would be \"fermionic\", and its eigenstates would be the theory's fermions. Each boson would have a fermionic partner of equal energy—but, in the relativistic world, energy and mass are interchangeable, so we can just as easily say that the partner particles have equal mass.\n",
      "-Mathematically, the relation of degeneracy with symmetry can be clarified as follows. Consider a symmetry operation associated with a unitary operator S. Under such an operation, the new Hamiltonian is related to the original Hamiltonian by a similarity transformation generated by the operator S, such that  H′=SHS−1=SHS† , since S is unitary. If the Hamiltonian remains unchanged under the transformation operation S, we have  SHS†=H SHS−1=H SH=HS [S,H]=0 Now, if  |α⟩ is an energy eigenstate, H|α⟩=E|α⟩ where E is the corresponding energy eigenvalue.\n",
      "-In quantum mechanics, the Hamiltonian of a system is an operator corresponding to the total energy of that system, including both kinetic energy and potential energy. Its spectrum, the system's energy spectrum or its set of energy eigenvalues, is the set of possible outcomes obtainable from a measurement of the system's total energy. Due to its close relation to the energy spectrum and time-evolution of a system, it is of fundamental importance in most formulations of quantum theory.\n",
      "-Notice that if a system is in an eigenstate of a given Hamiltonian, the system remains in that state.\n",
      "\n",
      "\n",
      "\n",
      "What is the proposed name for the field that is responsible for cosmic inflation and the metric expansion of space?\n",
      "-Cosmology Inflaton There has been considerable scientific research on possible links between the Higgs field and the inflaton – a hypothetical field suggested as the explanation for the expansion of space during the first fraction of a second of the universe (known as the \"inflationary epoch\"). Some theories suggest that a fundamental scalar field might be responsible for this phenomenon; the Higgs field is such a field, and its existence has led to papers analysing whether it could also be the inflaton responsible for this exponential expansion of the universe during the Big Bang. Such theories are highly tentative and face significant problems related to unitarity, but may be viable if combined with additional features such as large non-minimal coupling, a Brans–Dicke scalar, or other \"new\" physics, and they have received treatments suggesting that Higgs inflation models are still of interest theoretically.\n",
      "-The inflaton field is a hypothetical scalar field which is conjectured to have driven cosmic inflation in the very early universe.\n",
      "The field, originally postulated by Alan Guth, provides a mechanism by which a period of rapid expansion from 10−35 to 10−34 seconds after the initial expansion can be generated, forming a universe consistent with observed spatial isotropy and homogeneity.\n",
      "-Scalar fields are hypothesized to have caused the high accelerated expansion of the early universe (inflation), helping to solve the horizon problem and giving a hypothetical reason for the non-vanishing cosmological constant of cosmology. Massless (i.e. long-ranged) scalar fields in this context are known as inflatons. Massive (i.e. short-ranged) scalar fields are proposed, too, using for example Higgs-like fields.\n",
      "-In physical cosmology, cosmic inflation, cosmological inflation, or just inflation, is a theory of exponential expansion of space in the early universe. The inflationary epoch is believed to have lasted from 10−36 seconds to between 10−33 and 10−32 seconds after the Big Bang. Following the inflationary period, the universe continued to expand, but at a slower rate. The acceleration of this expansion due to dark energy began after the universe was already over 7.7 billion years old (5.4 billion years ago).Inflation theory was developed in the late 1970s and early 80s, with notable contributions by several theoretical physicists, including Alexei Starobinsky at Landau Institute for Theoretical Physics, Alan Guth at Cornell University, and Andrei Linde at Lebedev Physical Institute. Alexei Starobinsky, Alan Guth, and Andrei Linde won the 2014 Kavli Prize \"for pioneering the theory of cosmic inflation\". It was developed further in the early 1980s. It explains the origin of the large-scale structure of the cosmos. Quantum fluctuations in the microscopic inflationary region, magnified to cosmic size, become the seeds for the growth of structure in the Universe (see galaxy formation and evolution and structure formation). Many physicists also believe that inflation explains why the universe appears to be the same in all directions (isotropic), why the cosmic microwave background radiation is distributed evenly, why the universe is flat, and why no magnetic monopoles have been observed.\n",
      "-Cosmic expansion is a key feature of Big Bang cosmology. It can be modeled mathematically with the Friedmann–Lemaître–Robertson–Walker metric, where it corresponds to an increase in the scale of the spatial part of the universe's spacetime metric (which governs the size and geometry of spacetime). Within this framework, stationary objects separate over time because space is expanding. However, this is not a generally covariant description but rather only a choice of coordinates. Contrary to common misconception, it is equally valid to adopt a description in which space does not expand and objects simply move apart under the influence of their mutual gravity. Although cosmic expansion is often framed as a consequence of general relativity, it is also predicted by Newtonian gravity.According to inflation theory, during the inflationary epoch about 10−32 of a second after the Big Bang, the universe suddenly expanded, and its volume increased by a factor of at least 1078 (an expansion of distance by a factor of at least 1026 in each of the three dimensions). This would be equivalent to expanding an object 1 nanometer (10−9 m, about half the width of a molecule of DNA) in length to one approximately 10.6 light years (about 1017 m or 62 trillion miles) long. Cosmic expansion subsequently decelerated down to much slower rates, until at around 9.8 billion years after the Big Bang (4 billion years ago) it began to gradually expand more quickly, and is still doing so. Physicists have postulated the existence of dark energy, appearing as a cosmological constant in the simplest gravitational models, as a way to explain this late-time acceleration. According to the simplest extrapolation of the currently favored cosmological model, the Lambda-CDM model, this acceleration becomes more dominant into the future.\n",
      "\n",
      "\n",
      "\n",
      "Which of the following statements accurately describes the characteristics of gravitational waves?\n",
      "-The effects of a passing gravitational wave, in an extremely exaggerated form, can be visualized by imagining a perfectly flat region of spacetime with a group of motionless test particles lying in a plane, e.g., the surface of a computer screen. As a gravitational wave passes through the particles along a line perpendicular to the plane of the particles, i.e., following the observer's line of vision into the screen, the particles will follow the distortion in spacetime, oscillating in a \"cruciform\" manner, as shown in the animations. The area enclosed by the test particles does not change and there is no motion along the direction of propagation.The oscillations depicted in the animation are exaggerated for the purpose of discussion – in reality a gravitational wave has a very small amplitude (as formulated in linearized gravity). However, they help illustrate the kind of oscillations associated with gravitational waves as produced by a pair of masses in a circular orbit. In this case the amplitude of the gravitational wave is constant, but its plane of polarization changes or rotates at twice the orbital rate, so the time-varying gravitational wave size, or 'periodic spacetime strain', exhibits a variation as shown in the animation. If the orbit of the masses is elliptical then the gravitational wave's amplitude also varies with time according to Einstein's quadrupole formula.As with other waves, there are a number of characteristics used to describe a gravitational wave: Amplitude: Usually denoted h, this is the size of the wave – the fraction of stretching or squeezing in the animation. The amplitude shown here is roughly h = 0.5 (or 50%). Gravitational waves passing through the Earth are many sextillion times weaker than this – h ≈ 10−20.\n",
      "-Gravitational waves Predicted in 1916 by Albert Einstein, there are gravitational waves: ripples in the metric of spacetime that propagate at the speed of light. These are one of several analogies between weak-field gravity and electromagnetism in that, they are analogous to electromagnetic waves. On 11 February 2016, the Advanced LIGO team announced that they had directly detected gravitational waves from a pair of black holes merging.The simplest type of such a wave can be visualized by its action on a ring of freely floating particles. A sine wave propagating through such a ring towards the reader distorts the ring in a characteristic, rhythmic fashion (animated image to the right). Since Einstein's equations are non-linear, arbitrarily strong gravitational waves do not obey linear superposition, making their description difficult. However, linear approximations of gravitational waves are sufficiently accurate to describe the exceedingly weak waves that are expected to arrive here on Earth from far-off cosmic events, which typically result in relative distances increasing and decreasing by  10 21 or less. Data analysis methods routinely make use of the fact that these linearized waves can be Fourier decomposed.Some exact solutions describe gravitational waves without any approximation, e.g., a wave train traveling through empty space or Gowdy universes, varieties of an expanding cosmos filled with gravitational waves. But for gravitational waves produced in astrophysically relevant situations, such as the merger of two black holes, numerical methods are presently the only way to construct appropriate models.\n",
      "-Speed: This is the speed at which a point on the wave (for example, a point of maximum stretch or squeeze) travels. For gravitational waves with small amplitudes, this wave speed is equal to the speed of light (c).The speed, wavelength, and frequency of a gravitational wave are related by the equation c = λf, just like the equation for a light wave. For example, the animations shown here oscillate roughly once every two seconds. This would correspond to a frequency of 0.5 Hz, and a wavelength of about 600 000 km, or 47 times the diameter of the Earth.\n",
      "-The direct detection of gravitational waves is complicated by the extraordinarily small effect the waves produce on a detector. The amplitude of a spherical wave falls off as the inverse of the distance from the source. Thus, even waves from extreme systems such as merging binary black holes die out to a very small amplitude by the time they reach the Earth. Astrophysicists predicted that some gravitational waves passing the Earth might produce differential motion on the order 10−18 m in a LIGO-size instrument.\n",
      "-Gravitational waves Gravitational waves, a direct consequence of Einstein's theory, are distortions of geometry that propagate at the speed of light, and can be thought of as ripples in spacetime. They should not be confused with the gravity waves of fluid dynamics, which are a different concept.\n",
      "\n",
      "\n",
      "\n",
      "What is the difference between the coevolution of myrmecophytes and the mutualistic symbiosis of mycorrhiza?\n",
      "-Mutualism With Mycorrhizae The relationship between plants and mycorrhizal fungi is an example of mutualism because plants provides fungi with carbohydrates and mycorrhizal fungi help plants absorb more water and nutrients. Since mycorrhizal fungi increase plants' uptake of below-ground resources, plants who form a mutualistic relationship with fungi have stimulated shoot growth and a higher shoot to root ratio.\n",
      "-Mycorrhizal fungi form a mutualistic relationship with the roots of most plant species. In such a relationship, both the plants themselves and those parts of the roots that host the fungi, are said to be mycorrhizal. Relatively few of the mycorrhizal relationships between plant species and fungi have been examined to date, but 95% of the plant families investigated are predominantly mycorrhizal either in the sense that most of their species associate beneficially with mycorrhizae, or are absolutely dependent on mycorrhizae. The Orchidaceae are notorious as a family in which the absence of the correct mycorrhizae is fatal even to germinating seeds.Recent research into ectomycorrhizal plants in boreal forests has indicated that mycorrhizal fungi and plants have a relationship that may be more complex than simply mutualistic. This relationship was noted when mycorrhizal fungi were unexpectedly found to be hoarding nitrogen from plant roots in times of nitrogen scarcity. Researchers argue that some mycorrhizae distribute nutrients based upon the environment with surrounding plants and other mycorrhizae. They go on to explain how this updated model could explain why mycorrhizae do not alleviate plant nitrogen limitation, and why plants can switch abruptly from a mixed strategy with both mycorrhizal and nonmycorrhizal roots to a purely mycorrhizal strategy as soil nitrogen availability declines. It has also been suggested that evolutionary and phylogenetic relationships can explain much more variation in the strength of mycorrhizal mutualisms than ecological factors.\n",
      "-Coevolution Ecological interactions can be classified broadly into a host and an associate relationship. A host is any entity that harbours another that is called the associate. Relationships between species that are mutually or reciprocally beneficial are called mutualisms. Examples of mutualism include fungus-growing ants employing agricultural symbiosis, bacteria living in the guts of insects and other organisms, the fig wasp and yucca moth pollination complex, lichens with fungi and photosynthetic algae, and corals with photosynthetic algae. If there is a physical connection between host and associate, the relationship is called symbiosis. Approximately 60% of all plants, for example, have a symbiotic relationship with arbuscular mycorrhizal fungi living in their roots forming an exchange network of carbohydrates for mineral nutrients.Indirect mutualisms occur where the organisms live apart. For example, trees living in the equatorial regions of the planet supply oxygen into the atmosphere that sustains species living in distant polar regions of the planet. This relationship is called commensalism because many others receive the benefits of clean air at no cost or harm to trees supplying the oxygen. If the associate benefits while the host suffers, the relationship is called parasitism. Although parasites impose a cost to their host (e.g., via damage to their reproductive organs or propagules, denying the services of a beneficial partner), their net effect on host fitness is not necessarily negative and, thus, becomes difficult to forecast. Co-evolution is also driven by competition among species or among members of the same species under the banner of reciprocal antagonism, such as grasses competing for growth space. The Red Queen Hypothesis, for example, posits that parasites track down and specialize on the locally common genetic defense systems of its host that drives the evolution of sexual reproduction to diversify the genetic constituency of populations responding to the antagonistic pressure.\n",
      "-Some plants, called legumes, can form simultaneous symbiotic relationships with both AM fungi and the nitrogen-fixing bacteria Rhizobia. In fact, both organisms trigger the same pathways in plants during early colonization, indicating that the two very different responses could share a common origin. While the bacteria can supply nitrogen, they cannot provide other benefits of AM fungi; AM actually enhances bacterial colonization, probably by supplying extra phosphorus for the formation of the bacterial habitat within the plant, and thus contributing indirectly to the plant's nitrogen status. It is not known if there is signaling between the two, or only between the plant and each microbe. There is almost certainly competition between the bacterial and fungal partners, whether directly or indirectly, due to the fact that both are dependent on the plant as their sole source of energy. The plant must strive to strike a delicate balance between the maintenance of both partners based on its nutrient status.\n",
      "-Mycorrhizae – Mycorrhizae are similar to rhizobia in that they interact with plants at their roots. Whereas rhizobia are bacteria that fix nitrogen, mycorrhizae are fungi that bring nutrients to the plants in return for carbon. Mycorrhizas are also capable of improving water uptake and communicating to their hosts to resist to pathogens. Three main types of mycorrhizae exist:Arbuscula: found in non-woody and tropical plants Ectomycorrhiza: found in boreal and temperate forests Ericoid: found in species of the heathland.Digestive symbiotes – Digestive symbiotes are an example of an important trophic mutualism that does not occur between an autotroph and heterotroph. Bacteria known as \"extracellular symbionts\" live within the gastrointestinal tracts of vertebrates, where they aid in the digestion of food. The bacteria benefits by extracting substrates from the eaten food, while the animal’s assimilation is increased by being able to digest certain foods that its natural system cannot. (book) In addition, these bacteria create short-chain fatty acids (SCFA), providing the vertebrate with energy totaling up to anywhere from 29%-79% of the vertebrate’s maintenance energy depending on the species.\n",
      "\n",
      "\n",
      "\n",
      "What is the Kelvin-Helmholtz instability and how does it affect Earth's magnetosphere?\n",
      "-The Kelvin–Helmholtz instability (after Lord Kelvin and Hermann von Helmholtz) is a fluid instability that occurs when there is velocity shear in a single continuous fluid or a velocity difference across the interface between two fluids. Kelvin-Helmholtz instabilities are visible in the atmospheres of planets and moons, such as in cloud formations on Earth or the Red Spot on Jupiter, and the atmospheres of the Sun and other stars.\n",
      "-The research characterised variances in formation of the interplanetary magnetic field (IMF) largely influenced by Kelvin–Helmholtz instability (which occur at the interface of two fluids) as a result of differences in thickness and numerous other characteristics of the boundary layer. Experts believe that this was the first occasion that the appearance of Kelvin–Helmholtz waves at the magnetopause had been displayed at high latitude downward orientation of the IMF. These waves are being seen in unforeseen places under solar wind conditions that were formerly believed to be undesired for their generation. These discoveries show how Earth's magnetosphere can be penetrated by solar particles under specific IMF circumstances. The findings are also relevant to studies of magnetospheric progressions around other planetary bodies. This study suggests that Kelvin–Helmholtz waves can be a somewhat common, and possibly constant, instrument for the entrance of solar wind into terrestrial magnetospheres under various IMF orientations.\n",
      "-Earth's magnetosphere Over Earth's equator, the magnetic field lines become almost horizontal, then return to reconnect at high latitudes. However, at high altitudes, the magnetic field is significantly distorted by the solar wind and its solar magnetic field. On the dayside of Earth, the magnetic field is significantly compressed by the solar wind to a distance of approximately 65,000 kilometers (40,000 mi). Earth's bow shock is about 17 kilometers (11 mi) thick and located about 90,000 kilometers (56,000 mi) from Earth. The magnetopause exists at a distance of several hundred kilometers above Earth's surface. Earth's magnetopause has been compared to a sieve because it allows solar wind particles to enter. Kelvin–Helmholtz instabilities occur when large swirls of plasma travel along the edge of the magnetosphere at a different velocity from the magnetosphere, causing the plasma to slip past. This results in magnetic reconnection, and as the magnetic field lines break and reconnect, solar wind particles are able to enter the magnetosphere. On Earth's nightside, the magnetic field extends in the magnetotail, which lengthwise exceeds 6,300,000 kilometers (3,900,000 mi). Earth's magnetotail is the primary source of the polar aurora. Also, NASA scientists have suggested that Earth's magnetotail might cause \"dust storms\" on the Moon by creating a potential difference between the day side and the night side.\n",
      "-Kelvin–Helmholtz instability The Kelvin–Helmholtz instability (KHI) is an application of hydrodynamic stability that can be seen in nature. It occurs when there are two fluids flowing at different velocities. The difference in velocity of the fluids causes a shear velocity at the interface of the two layers. The shear velocity of one fluid moving induces a shear stress on the other which, if greater than the restraining surface tension, then results in an instability along the interface between them. This motion causes the appearance of a series of overturning ocean waves, a characteristic of the Kelvin–Helmholtz instability. Indeed, the apparent ocean wave-like nature is an example of vortex formation, which are formed when a fluid is rotating about some axis, and is often associated with this phenomenon.\n",
      "-If the density and velocity vary continuously in space (with the lighter layers uppermost, so that the fluid is RT-stable), the dynamics of the Kelvin-Helmholtz instability is described by the Taylor–Goldstein equation:  where  {\\textstyle N={\\sqrt {g/L_{\\rho }}}} denotes the Brunt–Väisälä frequency, U is the horizontal parallel velocity, k is the wave number, c is the eigenvalue parameter of the problem,  ϕ~ is complex amplitude of the stream function. Its onset is given by the Richardson number  Ri . Typically the layer is unstable for  0.25 . These effects are common in cloud layers. The study of this instability is applicable in plasma physics, for example in inertial confinement fusion and the plasma–beryllium interface. In situations where there is a state of static stability, evident by heavier fluids found below than the lower fluid, the Rayleigh-Taylor instability can be ignored as the Kelvin–Helmholtz instability is sufficient given the conditions.Numerically, the Kelvin–Helmholtz instability is simulated in a temporal or a spatial approach. In the temporal approach, the flow is considered in a periodic (cyclic) box \"moving\" at mean speed (absolute instability). In the spatial approach, simulations mimic a lab experiment with natural inlet and outlet conditions (convective instability).\n",
      "\n",
      "\n",
      "\n",
      "What is the significance of the high degree of fatty-acyl disorder in the thylakoid membranes of plants?\n",
      "-Plant thylakoid membranes maintain high fluidity, even at relatively cold environmental temperatures, due to the abundance of 18-carbon fatty acyl chains with three double bonds, linolenic acid, as has been revealed by 13-C NMR studies.\n",
      "-Membrane The thylakoid membrane is the site of the light-dependent reactions of photosynthesis with the photosynthetic pigments embedded directly in the membrane. It is an alternating pattern of dark and light bands measuring each 1 nanometre. The thylakoid lipid bilayer shares characteristic features with prokaryotic membranes and the inner chloroplast membrane. For example, acidic lipids can be found in thylakoid membranes, cyanobacteria and other photosynthetic bacteria and are involved in the functional integrity of the photosystems. The thylakoid membranes of higher plants are composed primarily of phospholipids and galactolipids that are asymmetrically arranged along and across the membranes. Thylakoid membranes are richer in galactolipids rather than phospholipids; also they predominantly consist of hexagonal phase II forming monogalacotosyl diglyceride lipid. Despite this unique composition, plant thylakoid membranes have been shown to assume largely lipid-bilayer dynamic organization. Lipids forming the thylakoid membranes, richest in high-fluidity linolenic acid are synthesized in a complex pathway involving exchange of lipid precursors between the endoplasmic reticulum and inner membrane of the plastid envelope and transported from the inner membrane to the thylakoids via vesicles.\n",
      "-Phase transitions in biological systems Phase transitions play many important roles in biological systems. Examples include the lipid bilayer formation, the coil-globule transition in the process of protein folding and DNA melting, liquid crystal-like transitions in the process of DNA condensation, and cooperative ligand binding to DNA and proteins with the character of phase transition.In biological membranes, gel to liquid crystalline phase transitions play a critical role in physiological functioning of biomembranes. In gel phase, due to low fluidity of membrane lipid fatty-acyl chains, membrane proteins have restricted movement and thus are restrained in exercise of their physiological role. Plants depend critically on photosynthesis by chloroplast thylakoid membranes which are exposed cold environmental temperatures. Thylakoid membranes retain innate fluidity even at relatively low temperatures because of high degree of fatty-acyl disorder allowed by their high content of linolenic acid, 18-carbon chain with 3-double bonds. Gel-to-liquid crystalline phase transition temperature of biological membranes can be determined by many techniques including calorimetry, fluorescence, spin label electron paramagnetic resonance and NMR by recording measurements of the concerned parameter by at series of sample temperatures. A simple method for its determination from 13-C NMR line intensities has also been proposed.It has been proposed that some biological systems might lie near critical points. Examples include neural networks in the salamander retina, bird flocks gene expression networks in Drosophila, and protein folding. However, it is not clear whether or not alternative reasons could explain some of the phenomena supporting arguments for criticality. It has also been suggested that biological organisms share two key properties of phase transitions: the change of macroscopic behavior and the coherence of a system at a critical point. Phase transitions are prominent feature of motor behavior in biological systems. Spontaneous gait transitions, as well as fatigue-induced motor task disengagements, show typical critical behavior as an intimation of the sudden qualitative change of the previously stable motor behavioral pattern.\n",
      "-A study of central linewidths of electron spin resonance spectra of thylakoid membranes and aqueous dispersions of their total extracted lipids, labeled with stearic acid spin label (having spin or doxyl moiety at 5,7,9,12,13,14 and 16th carbons, with reference to carbonyl group), reveals a fluidity gradient. Decreasing linewidth from 5th to 16th carbons represents increasing degree of motional freedom (fluidity gradient) from headgroup-side to methyl terminal in both native membranes and their aqueous lipid extract (a multilamellar liposomal structure, typical of lipid bilayer organization). This pattern points at similarity of lipid bilayer organization in both native membranes and liposomes. This observation is critical, as thylakoid membranes comprising largely galactolipids, contain only 10% phospholipid, unlike other biological membranes consisting largely of phospholipids. Proteins in chloroplast thylakoid membranes, apparently, restrict lipid fatty acyl chain segmental mobility from 9th to 16th carbons vis a vis their liposomal counterparts. Surprisingly, liposomal fatty acyl chains are more restricted at 5th and 7th carbon positions as compared at these positions in thylakoid membranes. This is explainable as due to motional restricting effect at these positions, because of steric hindrance by large chlorophyll headgroups, specially so, in liposomes. However, in native thylakoid membranes, chlorophylls are mainly complexed with proteins as light-harvesting complexes and may not largely be free to restrain lipid fluidity, as such.\n",
      "-Cholesterol is normally found dispersed in varying degrees throughout cell membranes, in the irregular spaces between the hydrophobic tails of the membrane lipids, where it confers a stiffening and strengthening effect on the membrane. Additionally, the amount of cholesterol in biological membranes varies between organisms, cell types, and even in individual cells. Cholesterol, a major component of plasma membranes, regulates the fluidity of the overall membrane, meaning that cholesterol controls the amount of movement of the various cell membrane components based on its concentrations. In high temperatures, cholesterol inhibits the movement of phospholipid fatty acid chains, causing a reduced permeability to small molecules and reduced membrane fluidity. The opposite is true for the role of cholesterol in cooler temperatures. Cholesterol production, and thus concentration, is up-regulated (increased) in response to cold temperature. At cold temperatures, cholesterol interferes with fatty acid chain interactions. Acting as antifreeze, cholesterol maintains the fluidity of the membrane. Cholesterol is more abundant in cold-weather animals than warm-weather animals. In plants, which lack cholesterol, related compounds called sterols perform the same function as cholesterol.\n",
      "\n",
      "\n",
      "\n",
      "What is the explanation for the effective supersymmetry in quark-diquark models?\n",
      "-Nonetheless, color-charged particles may combine to form color neutral composite particles called hadrons. A quark may pair up with an antiquark: the quark has a color and the antiquark has the corresponding anticolor. The color and anticolor cancel out, forming a color neutral meson. Alternatively, three quarks can exist together, one quark being \"red\", another \"blue\", another \"green\". These three colored quarks together form a color-neutral baryon. Symmetrically, three antiquarks with the colors \"antired\", \"antiblue\" and \"antigreen\" can form a color-neutral antibaryon.\n",
      "-Quarks carry not only electric charge, but also charges such as color charge and weak isospin. Because of a phenomenon known as color confinement, a hadron cannot have a net color charge; that is, the total color charge of a particle has to be zero (\"white\"). A quark can have one of three \"colors\", dubbed \"red\", \"green\", and \"blue\"; while an antiquark may be either \"anti-red\", \"anti-green\" or \"anti-blue\".For normal hadrons, a white color can thus be achieved in one of three ways:  A quark of one color with an antiquark of the corresponding anticolor, giving a meson with baryon number 0, Three quarks of different colors, giving a baryon with baryon number +1, Three antiquarks of different anticolors, giving an antibaryon with baryon number −1.The baryon number was defined long before the quark model was established, so rather than changing the definitions, particle physicists simply gave quarks one third the baryon number. Nowadays it might be more accurate to speak of the conservation of quark number.\n",
      "-The quarks are bound together by the strong force, which acts in such a way as to cancel the colour charges within the particle. In a meson, this means a quark is partnered with an antiquark with an opposite colour charge – blue and antiblue, for example – while in a baryon, the three quarks have between them all three colour charges – red, blue, and green. In a pentaquark, the colours also need to cancel out, and the only feasible combination is to have one quark with one colour (e.g. red), one quark with a second colour (e.g. green), two quarks with the third colour (e.g. blue), and one antiquark to counteract the surplus colour (e.g. antiblue).The binding mechanism for pentaquarks is not yet clear. They may consist of five quarks tightly bound together, but it is also possible that they are more loosely bound and consist of a three-quark baryon and a two-quark meson interacting relatively weakly with each other via pion exchange (the same force that binds atomic nuclei) in a \"meson-baryon molecule\".\n",
      "-According to the quark model, the properties of hadrons are primarily determined by their so-called valence quarks. For example, a proton is composed of two up quarks (each with electric charge ++2⁄3, for a total of +4⁄3 together) and one down quark (with electric charge −+1⁄3). Adding these together yields the proton charge of +1. Although quarks also carry color charge, hadrons must have zero total color charge because of a phenomenon called color confinement. That is, hadrons must be \"colorless\" or \"white\". The simplest ways for this to occur are with a quark of one color and an antiquark of the corresponding anticolor, or three quarks of different colors. Hadrons with the first arrangement are a type of meson, and those with the second arrangement are a type of baryon.\n",
      "-All three colours mixed together, or any one of these colours and its complement (or negative), is \"colourless\" or \"white\" and has a net colour charge of zero. Due to a property of the strong interaction called colour confinement, free particles must have a colour charge of zero.  A baryon is composed of three quarks, which must be one each of red, green, and blue colours; likewise an antibaryon is composed of three antiquarks, one each of antired, antigreen and antiblue. A meson is made from one quark and one antiquark; the quark can be any colour, and the antiquark has the matching anticolor.\n",
      "\n",
      "\n",
      "\n",
      "What is the relationship between the complete electromagnetic Hamiltonian of a molecule and the parity operation?\n",
      "-Molecules The complete (rotational-vibrational-electronic-nuclear spin) electromagnetic Hamiltonian of any molecule commutes with (or is invariant to) the parity operation P (or E*, in the notation introduced by Longuet-Higgins) and its eigenvalues can be given the parity symmetry label + or - as they are even or odd, respectively. The parity operation involves the inversion of electronic and nuclear spatial coordinates at the molecular center of mass.\n",
      "-Centrosymmetric molecules at equilibrium have a centre of symmetry at their midpoint (the nuclear center of mass). This includes all homonuclear diatomic molecules as well as certain symmetric molecules such as ethylene, benzene, xenon tetrafluoride and sulphur hexafluoride. For centrosymmetric molecules, the point group contains the operation i which is not to be confused with the parity operation. The operation i involves the inversion of the electronic and vibrational displacement coordinates at the nuclear centre of mass. For centrosymmetric molecules the operation i commutes with the rovibronic (rotation-vibration-electronic) Hamiltonian and can be used to label such states. Electronic and vibrational states of centrosymmetric molecules are either unchanged by the operation i, or they are changed in sign by i. The former are denoted by the subscript g and are called gerade, while the latter are denoted by the subscript u and are called ungerade. The complete Hamiltonian of a centrosymmetric molecule does not commute with the point group inversion operation i because of the effect of the nuclear hyperfine Hamiltonian. The nuclear hyperfine Hamiltonian can mix the rotational levels of g and u vibronic states (called ortho-para mixing) and give rise to ortho-para transitions Nuclei In atomic nuclei, the state of each nucleon (proton or neutron) has even or odd parity, and nucleon configurations can be predicted using the nuclear shell model. As for electrons in atoms, the nucleon state has odd overall parity if and only if the number of nucleons in odd-parity states is odd. The parity is usually written as a + (even) or − (odd) following the nuclear spin value. For example, the isotopes of oxygen include 17O(5/2+), meaning that the spin is 5/2 and the parity is even. The shell model explains this because the first 16 nucleons are paired so that each pair has spin zero and even parity, and the last nucleon is in the 1d5/2 shell, which has even parity since ℓ = 2 for a d orbital.\n",
      "-The complete Hamiltonian of a diatomic molecule (as for all molecules) commutes with the parity operation P or E* and rovibronic (rotation-vibration-electronic) energy levels (often called rotational levels) can be given the parity symmetry label + or -. The complete Hamiltonian of a homonuclear diatomic molecule also commutes with the operation of permuting (or exchanging) the coordinates of the two (identical) nuclei and rotational levels  gain the additional label s or a depending on whether the total wavefunction is unchanged (symmetric) or changed in sign (antisymmetric) by the permutation operation. Thus, the rotational levels of heteronuclear diatomic molecules are labelled + or -, whereas those of homonuclear diatomic molecules are labelled +s, +a, -s or -a. The rovibronic nuclear spin states are classified using the appropriate permutation-inversion group.The complete Hamiltonian of a homonuclear diatomic molecule (as for all centro-symmetric molecules) does not commute with the point group inversion operation i because of the effect of the nuclear hyperfine Hamiltonian. The nuclear hyperfine Hamiltonian can mix the rotational levels of g and u vibronic states (called ortho-para mixing) and give rise to ortho-para transitions Spin and total angular momentum If S denotes the resultant of the individual electron spins,  s(s+1)ℏ2 are the eigenvalues of S and as in the case of atoms, each electronic term of the molecule is also characterised by the value of S. If spin-orbit coupling is neglected, there is a degeneracy of order  2s+1 associated with each  s for a given  Λ . Just as for atoms, the quantity  2s+1 is called the multiplicity of the term and.is written as a (left) superscript, so that the term symbol is written as  2s+1Λ . For example, the symbol  3Π denotes a term such that  Λ=1 and  s=1 . It is worth noting that the ground state (often labelled by the symbol  X ) of most diatomic molecules is such that  s=0 and exhibits maximum symmetry. Thus, in most cases it is a  1Σ+ state (written as  X1Σ+ , excited states are written with  A,B,C,...\n",
      "-All fundamental interactions of elementary particles, with the exception of the weak interaction, are symmetric under parity. The weak interaction is chiral and thus provides a means for probing chirality in physics. In interactions that are symmetric under parity, such as electromagnetism in atomic and molecular physics, parity serves as a powerful controlling principle underlying quantum transitions.\n",
      "A matrix representation of P (in any number of dimensions) has determinant equal to −1, and hence is distinct from a rotation, which has a determinant equal to 1. In a two-dimensional plane, a simultaneous flip of all coordinates in sign is not a parity transformation; it is the same as a 180° rotation.\n",
      "In quantum mechanics, wave functions that are unchanged by a parity transformation are described as even functions, while those that change sign under a parity transformation are odd functions.\n",
      "-In quantum mechanics, Hamiltonians are invariant (symmetric) under a parity transformation if  P^ commutes with the Hamiltonian. In non-relativistic quantum mechanics, this happens for any scalar potential, i.e.,  V=V(r) , hence the potential is spherically symmetric. The following facts can be easily proven: If  |φ⟩ and  |ψ⟩ have the same parity, then  ⟨φ|X^|ψ⟩=0 where  X^ is the position operator.\n",
      "For a state  |L→,Lz⟩ of orbital angular momentum  L→ with z-axis projection  Lz , then  P^|L→,Lz⟩=(−1)L|L→,Lz⟩ If  [H^,P^]=0 , then atomic dipole transitions only occur between states of opposite parity.\n",
      "\n",
      "\n",
      "\n",
      "What is the difference between active and passive transport in cells?\n",
      "-Unlike passive transport, which uses the kinetic energy and natural entropy of molecules moving down a gradient, active transport uses cellular energy to move them against a gradient, polar repulsion, or other resistance. Active transport is usually associated with accumulating high concentrations of molecules that the cell needs, such as ions, glucose and amino acids. Examples of active transport include the uptake of glucose in the intestines in humans and the uptake of mineral ions into root hair cells of plants.\n",
      "-Passive transport is a type of membrane transport that does not require energy to move substances across cell membranes. Instead of using cellular energy, like active transport, passive transport relies on the second law of thermodynamics to drive the movement of substances across cell membranes. Fundamentally, substances follow Fick's first law, and move from an area of high concentration to one of low concentration because this movement increases the entropy of the overall system. The rate of passive transport depends on the permeability of the cell membrane, which, in turn, depends on the organization and characteristics of the membrane lipids and proteins. The four main kinds of passive transport are simple diffusion, facilitated diffusion, filtration, and/or osmosis.\n",
      "-In cellular biology, active transport is the movement of molecules or ions across a cell membrane from a region of lower concentration to a region of higher concentration—against the concentration gradient. Active transport requires cellular energy to achieve this movement. There are two types of active transport: primary active transport that uses adenosine triphosphate (ATP), and secondary active transport that uses an electrochemical gradient. This process is in contrast to passive transport, which allows molecules or ions to move down their concentration gradient, from an area of high concentration to an area of low concentration, without energy.\n",
      "-An example of active transport of ions is the Na+-K+-ATPase (NKA). NKA is powered by the hydrolysis of ATP into ADP and an inorganic phosphate; for every molecule of ATP hydrolized, three Na+ are transported outside and two K+ are transported inside the cell. This makes the inside of the cell more negative than the outside and more specifically generates a membrane potential Vmembrane of about −60 mV.An example of passive transport is ion fluxes through Na+, K+, Ca2+, and Cl− channels. Unlike active transport, passive transport is powered by the arithmetic sum of osmosis (a concentration gradient) and an electric field (the transmembrane potential). Formally, the molar Gibbs free energy change associated with successful transport is where R represents the gas constant, T represents absolute temperature, z is the charge per ion, and F represents the Faraday constant.: 464–465 In the example of Na+, both terms tend to support transport: the negative electric potential inside the cell attracts the positive ion and since Na+ is concentrated outside the cell, osmosis supports diffusion through the Na+ channel into the cell. In the case of K+, the effect of osmosis is reversed: although external ions are attracted by the negative intracellular potential, entropy seeks to diffuse the ions already concentrated inside the cell. The converse phenomenon (osmosis supports transport, electric potential opposes it) can be achieved for Na+ in cells with abnormal transmembrane potentials: at +70 mV, the Na+ influx halts; at higher potentials, it becomes an efflux.\n",
      "-active transport In cellular biology, the movement of molecules across a membrane from a region of their lower concentration to a region of their higher concentration—against the concentration gradient. Active transport requires cellular energy to achieve this movement. There are two types of active transport: primary active transport that uses ATP, and secondary active transport that uses an electrochemical gradient.\n",
      "\n",
      "\n",
      "\n",
      "What is the Heisenberg uncertainty principle and how does it relate to angular momentum in quantum mechanics?\n",
      "-In quantum mechanics, angular momentum (like other quantities) is expressed as an operator, and its one-dimensional projections have quantized eigenvalues. Angular momentum is subject to the Heisenberg uncertainty principle, implying that at any time, only one projection (also called \"component\") can be measured with definite precision; the other two then remain uncertain. Because of this, the axis of rotation of a quantum particle is undefined. Quantum particles do possess a type of non-orbital angular momentum called \"spin\", but this angular momentum does not correspond to a spinning motion. In relativistic quantum mechanics the above relativistic definition becomes a tensorial operator.\n",
      "-Spin projection quantum number and multiplicity In classical mechanics, the angular momentum of a particle possesses not only a magnitude (how fast the body is rotating), but also a direction (either up or down on the axis of rotation of the particle). Quantum-mechanical spin also contains information about direction, but in a more subtle form. Quantum mechanics states that the component of angular momentum for a spin-s particle measured along any direction can only take on the values Si=ℏsi,si∈{−s,−(s−1),…,s−1,s}, where Si is the spin component along the i-th axis (either x, y, or z), si is the spin projection quantum number along the i-th axis, and s is the principal spin quantum number (discussed in the previous section). Conventionally the direction chosen is the z axis: Sz=ℏsz,sz∈{−s,−(s−1),…,s−1,s}, where Sz is the spin component along the z axis, sz is the spin projection quantum number along the z axis.\n",
      "-Angular momentum and spin Similarly for the angular momentum which implies that the angular momentum of the photon is the quantum interpretation of this expression is that the photon has a probability of  |ψR|2 of having an angular momentum of  ℏ and a probability of  |ψL|2 of having an angular momentum of  −ℏ . We can therefore think of the angular momentum of the photon being quantized as well as the energy. This has indeed been experimentally verified. Photons have only been observed to have angular momenta of  ±ℏ Spin operator The spin of the photon is defined as the coefficient of  ℏ in the angular momentum calculation. A photon has spin 1 if it is in the  |R⟩ state and -1 if it is in the  |L⟩ state. The spin operator is defined as the outer product The eigenvectors of the spin operator are  |R⟩ and  |L⟩ with eigenvalues 1 and -1, respectively.\n",
      "-While many introductory texts treat photons using the mathematical techniques of non-relativistic quantum mechanics, this is in some ways an awkward oversimplification, as photons are by nature intrinsically relativistic. Because photons have zero rest mass, no wave function defined for a photon can have all the properties familiar from wave functions in non-relativistic quantum mechanics. In order to avoid these difficulties, physicists employ the second-quantized theory of photons described below, quantum electrodynamics, in which photons are quantized excitations of electromagnetic modes.Another difficulty is finding the proper analogue for the uncertainty principle, an idea frequently attributed to Heisenberg, who introduced the concept in analyzing a thought experiment involving an electron and a high-energy photon. However, Heisenberg did not give precise mathematical definitions of what the \"uncertainty\" in these measurements meant. The precise mathematical statement of the position–momentum uncertainty principle is due to Kennard, Pauli, and Weyl. The uncertainty principle applies to situations where an experimenter has a choice of measuring either one of two \"canonically conjugate\" quantities, like the position and the momentum of a particle. According to the uncertainty principle, no matter how the particle is prepared, it is not possible to make a precise prediction for both of the two alternative measurements: if the outcome of the position measurement is made more certain, the outcome of the momentum measurement becomes less so, and vice versa. A coherent state minimizes the overall uncertainty as far as quantum mechanics allows. Quantum optics makes use of coherent states for modes of the electromagnetic field. There is a tradeoff, reminiscent of the position–momentum uncertainty relation, between measurements of an electromagnetic wave's amplitude and its phase. This is sometimes informally expressed in terms of the uncertainty in the number of photons present in the electromagnetic wave,  ΔN , and the uncertainty in the phase of the wave,  Δϕ . However, this cannot be an uncertainty relation of the Kennard–Pauli–Weyl type, since unlike position and momentum, the phase  ϕ cannot be represented by a Hermitian operator.\n",
      "-Spin, orbital, and total angular momentum The classical definition of angular momentum as  L=r×p can be carried over to quantum mechanics, by reinterpreting r as the quantum position operator and p as the quantum momentum operator. L is then an operator, specifically called the orbital angular momentum operator. The components of the angular momentum operator satisfy the commutation relations of the Lie algebra so(3). Indeed, these operators are precisely the infinitesimal action of the rotation group on the quantum Hilbert space. (See also the discussion below of the angular momentum operators as the generators of rotations.) However, in quantum physics, there is another type of angular momentum, called spin angular momentum, represented by the spin operator S. Spin is often depicted as a particle literally spinning around an axis, but this is a misleading and inaccurate picture: spin is an intrinsic property of a particle, unrelated to any sort of motion in space and fundamentally different from orbital angular momentum. All elementary particles have a characteristic spin (possibly zero), and almost all elementary particles have nonzero spin. For example electrons have \"spin 1/2\" (this actually means \"spin ħ/2\"), photons have \"spin 1\" (this actually means \"spin ħ\"), and pi-mesons have spin 0.Finally, there is total angular momentum J, which combines both the spin and orbital angular momentum of all particles and fields. (For one particle, J = L + S.) Conservation of angular momentum applies to J, but not to L or S; for example, the spin–orbit interaction allows angular momentum to transfer back and forth between L and S, with the total remaining constant. Electrons and photons need not have integer-based values for total angular momentum, but can also have half-integer values.In molecules the total angular momentum F is the sum of the rovibronic (orbital) angular momentum N, the electron spin angular momentum S, and the nuclear spin angular momentum I. For electronic singlet states the rovibronic angular momentum is denoted J rather than N. As explained by Van Vleck, the components of the molecular rovibronic angular momentum referred to molecule-fixed axes have different commutation relations from those for the components about space-fixed axes.\n",
      "\n",
      "\n",
      "\n",
      "What is the difference between natural convection and forced convection?\n",
      "-Forced convection: when a fluid is forced to flow over the surface by an internal source such as fans, by stirring, and pumps, creating an artificially induced convection current.In many real-life applications (e.g. heat losses at solar central receivers or cooling of photovoltaic panels), natural and forced convection occur at the same time (mixed convection).Internal and external flow can also classify convection. Internal flow occurs when a fluid is enclosed by a solid boundary such as when flowing through a pipe. An external flow occurs when a fluid extends indefinitely without encountering a solid surface. Both of these types of convection, either natural or forced, can be internal or external because they are independent of each other. The bulk temperature, or the average fluid temperature, is a convenient reference point for evaluating properties related to convective heat transfer, particularly in applications related to flow in pipes and ducts.\n",
      "-Free, or natural, convection occurs when bulk fluid motions (streams and currents) are caused by buoyancy forces that result from density variations due to variations of temperature in the fluid. Forced convection is a term used when the streams and currents in the fluid are induced by external means—such as fans, stirrers, and pumps—creating an artificially induced convection current.\n",
      "-Convection can be \"forced\" by movement of a fluid by means other than buoyancy forces (for example, a water pump in an automobile engine). Thermal expansion of fluids may also force convection. In other cases, natural buoyancy forces alone are entirely responsible for fluid motion when the fluid is heated, and this process is called \"natural convection\". An example is the draft in a chimney or around any fire. In natural convection, an increase in temperature produces a reduction in density, which in turn causes fluid motion due to pressures and forces when fluids of different densities are affected by gravity (or any g-force). For example, when water is heated on a stove, hot water from the bottom of the pan is displaced (or forced up) by the colder denser liquid, which falls. After heating has stopped, mixing and conduction from this natural convection eventually result in a nearly homogeneous density, and even temperature. Without the presence of gravity (or conditions that cause a g-force of any type), natural convection does not occur, and only forced-convection modes operate.\n",
      "-In fluid thermodynamics, combined forced convection and natural convection, or mixed convection, occurs when natural convection and forced convection mechanisms act together to transfer heat. This is also defined as situations where both pressure forces and buoyant forces interact. How much each form of convection contributes to the heat transfer is largely determined by the flow, temperature, geometry, and orientation. The nature of the fluid is also influential, since the Grashof number increases in a fluid as temperature increases, but is maximized at some point for a gas.\n",
      "-Heat convection occurs when the bulk flow of a fluid (gas or liquid) carries its heat through the fluid. All convective processes also move heat partly by diffusion, as well. The flow of fluid may be forced by external processes, or sometimes (in gravitational fields) by buoyancy forces caused when thermal energy expands the fluid (for example in a fire plume), thus influencing its own transfer. The latter process is often called \"natural convection\". The former process is often called \"forced convection.\" In this case, the fluid is forced to flow by use of a pump, fan, or other mechanical means.\n",
      "\n",
      "\n",
      "\n",
      "What is magnetic susceptibility?\n",
      "-In electromagnetism, the magnetic susceptibility (from Latin susceptibilis 'receptive'; denoted χ, chi) is a measure of how much a material will become magnetized in an applied magnetic field. It is the ratio of magnetization M (magnetic moment per unit volume) to the applied magnetizing field intensity H. This allows a simple classification, into two categories, of most materials' responses to an applied magnetic field: an alignment with the magnetic field, χ > 0, called paramagnetism, or an alignment against the field, χ < 0, called diamagnetism.\n",
      "-Volume susceptibility Magnetic susceptibility is a dimensionless proportionality constant that indicates the degree of magnetization of a material in response to an applied magnetic field. A related term is magnetizability, the proportion between magnetic moment and magnetic flux density. A closely related parameter is the permeability, which expresses the total magnetization of material and volume.\n",
      "-Magnetic susceptibility indicates whether a material is attracted into or repelled out of a magnetic field. Paramagnetic materials align with the applied field and are attracted to regions of greater magnetic field. Diamagnetic materials are anti-aligned and are pushed away, toward regions of lower magnetic fields. On top of the applied field, the magnetization of the material adds its own magnetic field, causing the field lines to concentrate in paramagnetism, or be excluded in diamagnetism. Quantitative measures of the magnetic susceptibility also provide insights into the structure of materials, providing insight into bonding and energy levels. Furthermore, it is widely used in geology for paleomagnetic studies and structural geology.The magnetizability of materials comes from the atomic-level magnetic properties of the particles of which they are made. Usually, this is dominated by the magnetic moments of electrons. Electrons are present in all materials, but without any external magnetic field, the magnetic moments of the electrons are usually either paired up or random so that the overall magnetism is zero (the exception to this usual case is ferromagnetism). The fundamental reasons why the magnetic moments of the electrons line up or do not are very complex and cannot be explained by classical physics. However, a useful simplification is to measure the magnetic susceptibility of a material and apply the macroscopic form of Maxwell's equations. This allows classical physics to make useful predictions while avoiding the underlying quantum mechanical details.\n",
      "-In electricity (electromagnetism), the electric susceptibility ( χe ; Latin: susceptibilis \"receptive\") is a dimensionless proportionality constant that indicates the degree of polarization of a dielectric material in response to an applied electric field. The greater the electric susceptibility, the greater the ability of a material to polarize in response to the field, and thereby reduce the total electric field inside the material(and store energy). It is in this way that the electric susceptibility influences the electric permittivity of the material and thus influences many other phenomena in that medium, from the capacitance of capacitors to the speed of light.\n",
      "-In physics the susceptibility is a quantification for the change of an extensive property under variation of an intensive property. The word may refer to: In physics, the susceptibility of a material or substance describes its response to an applied field. For example: Magnetic susceptibility Electric susceptibility The two types of susceptibility above are examples of a linear response function; sometimes the terms susceptibility and linear response function are used interchangeably.\n",
      "\n",
      "\n",
      "\n",
      "What is a transient condensation cloud, also known as a Wilson cloud?\n",
      "-A transient condensation cloud, also called a Wilson cloud, is observable surrounding large explosions in humid air.\n",
      "-The lifetime of the Wilson cloud during nuclear air bursts can be shortened by the thermal radiation from the fireball, which heats the cloud above to the dew point and evaporates the droplets.\n",
      "Non-nuclear explosions Any sufficiently large explosion, such as one caused by a large quantity of conventional explosives or a volcanic eruption, can create a condensation cloud, as seen in Operation Sailor Hat or in the 2020 Beirut explosion, where a very large Wilson cloud expanded outwards from the blast.\n",
      "-Similar condensation effects can be observed as Wilson clouds, also called condensation clouds, at large explosions in humid air and other Prandtl–Glauert singularity effects.\n",
      "-Condensation effects Nuclear mushroom clouds are often accompanied by short-lived vapour clouds, known variously as \"Wilson clouds\", condensation clouds, or vapor rings. The \"negative phase\" following the positive overpressure behind a shock front causes a sudden rarefaction of the surrounding medium. This low pressure region causes an adiabatic drop in temperature, causing moisture in the air to condense in an outward moving shell surrounding the explosion. When the pressure and temperature return to normal, the Wilson cloud dissipates. Scientists observing the Operation Crossroads nuclear tests in 1946 at Bikini Atoll named that transitory cloud a \"Wilson cloud\" because of its visual similarity to a Wilson cloud chamber; the cloud chamber uses condensation from a rapid pressure drop to mark the tracks of electrically charged subatomic particles. Analysts of later nuclear bomb tests used the more general term \"condensation cloud\" in preference to \"Wilson cloud\".\n",
      "-When a nuclear weapon or a large amount of a conventional explosive is detonated in sufficiently humid air, the \"negative phase\" of the shock wave causes a rarefaction of the air surrounding the explosion, but not contained within it. This rarefaction results in a temporary cooling of that air, which causes a condensation of some of the water vapor contained in it. When the pressure and the temperature return to normal, the Wilson cloud dissipates.\n",
      "\n",
      "\n",
      "\n",
      "What is a uniform tiling in the hyperbolic plane?\n",
      "-In hyperbolic geometry, a uniform hyperbolic tiling (or regular, quasiregular or semiregular hyperbolic tiling) is an edge-to-edge filling of the hyperbolic plane which has regular polygons as faces and is vertex-transitive (transitive on its vertices, isogonal, i.e. there is an isometry mapping any vertex onto any other). It follows that all vertices are congruent, and the tiling has a high degree of rotational and translational symmetry.\n",
      "-Tessellations in non-Euclidean geometries It is possible to tessellate in non-Euclidean geometries such as hyperbolic geometry. A uniform tiling in the hyperbolic plane (that may be regular, quasiregular, or semiregular) is an edge-to-edge filling of the hyperbolic plane, with regular polygons as faces; these are vertex-transitive (transitive on its vertices), and isogonal (there is an isometry mapping any vertex onto any other).A uniform honeycomb in hyperbolic space is a uniform tessellation of uniform polyhedral cells. In three-dimensional (3-D) hyperbolic space there are nine Coxeter group families of compact convex uniform honeycombs, generated as Wythoff constructions, and represented by permutations of rings of the Coxeter diagrams for each family.\n",
      "-In geometry, a uniform tiling is a tessellation of the plane by regular polygon faces with the restriction of being vertex-transitive.\n",
      "Uniform tilings can exist in both the Euclidean plane and hyperbolic plane. Uniform tilings are related to the finite uniform polyhedra which can be considered uniform tilings of the sphere.\n",
      "Most uniform tilings can be made from a Wythoff construction starting with a symmetry group and a singular generator point inside of the fundamental domain. A planar symmetry group has a polygonal fundamental domain and can be represented by the group name represented by the order of the mirrors in sequential vertices.\n",
      "A fundamental domain triangle is (p q r), and a right triangle (p q 2), where p, q, r are whole numbers greater than 1. The triangle may exist as a spherical triangle, a Euclidean plane triangle, or a hyperbolic plane triangle, depending on the values of p, q and r.\n",
      "-There are an infinite number of dual uniform tilings in hyperbolic plane with isogonal irregular pentagonal faces. They have face configurations as V3.3.p.3.q.\n",
      "The binary tiling can be made into a pentagonal tiling if one replaces the horocyclic edges by line segments.\n",
      "-This tiling is one of 10 uniform tilings constructed from [8,3] hyperbolic symmetry and three subsymmetries [1+,8,3], [8,3+] and [8,3]+.\n",
      "This tiling can be considered a member of a sequence of uniform patterns with vertex figure (4.6.2p) and Coxeter-Dynkin diagram . For p < 6, the members of the sequence are omnitruncated polyhedra (zonohedrons), shown below as spherical tilings. For p > 6, they are tilings of the hyperbolic plane, starting with the truncated triheptagonal tiling.\n",
      "\n",
      "\n",
      "\n",
      "What is the relation between the three moment theorem and the bending moments at three successive supports of a continuous beam?\n",
      "-In civil engineering and structural analysis Clapeyron's theorem of three moments is a relationship among the bending moments at three consecutive supports of a horizontal beam.\n",
      "Let A,B,C-D be the three consecutive points of support, and denote by- l the length of AB and  l′ the length of BC, by w and  w′ the weight per unit of length in these segments. Then the bending moments  MA,MB,MC at the three points are related by: MAl+2MB(l+l′)+MCl′=14wl3+14w′(l′)3.\n",
      "-Mohr's first theorem The change in slope of a deflection curve between two points of a beam is equal to the area of the M/EI diagram between those two points.(Figure 02) Mohr's second theorem Consider two points k1 and k2 on a beam. The deflection of k1 and k2 relative to the point of intersection between tangent at k1 and k2 and vertical through k1 is equal to the moment of M/EI diagram between k1 and k2 about k1.(Figure 03) The three moment equation expresses the relation between bending moments at three successive supports of a continuous beam, subject to a loading on a two adjacent span with or without settlement of the supports.\n",
      "-Let A' B' and C' be the final positions of the beam ABC due to support settlements.\n",
      "Derivation of three moment theorem PB'Q is a tangent drawn at B' for final Elastic Curve A'B'C' of the beam ABC. RB'S is a horizontal line drawn through B'.  Consider, Triangles RB'P and QB'S.\n",
      "PRRB′=SQB′S, From (1), (2), and (3), ΔB−ΔA+PA′L1=ΔC−ΔB−QC′L2 Draw the M/EI diagram to find the PA' and QC'.\n",
      "From Mohr's Second Theorem  PA' = First moment of area of M/EI diagram between A and B about A.\n",
      "PA′=(12×M1E1I1×L1)×L1×13+(12×M2E2I2×L1)×L1×23+A1X1E1I1 QC' = First moment of area of M/EI diagram between B and C about C.\n",
      "QC′=(12×M3E2I2×L2)×L2×13+(12×M2E2I2×L2)×L2×23+A2X2E2I2 Substitute in PA' and QC' on equation (a), the Three Moment Theorem (TMT) can be obtained.\n",
      "-The moment-area theorem is an engineering tool to derive the slope, rotation and deflection of beams and frames. This theorem was developed by Mohr and later stated namely by Charles Ezra Greene in 1873. This method is advantageous when we solve problems involving beams, especially for those subjected to a series of concentrated loadings or having segments with different moments of inertia.\n",
      "-K=pw where  p is the applied force and  w is the deflection. According to elementary beam theory, the relationship between the applied bending moment  M and the resulting curvature  κ of the beam is: M=EIκ=EId2wdx2 where  w is the deflection of the beam and  x is the distance along the beam. Double integration of the above equation leads to computing the deflection of the beam, and in turn, the bending stiffness of the beam.\n",
      "\n",
      "\n",
      "\n",
      "What is the throttling process, and why is it important?\n",
      "-Throttling One of the simple applications of the concept of enthalpy is the so-called throttling process, also known as Joule–Thomson expansion. It concerns a steady adiabatic flow of a fluid through a flow resistance (valve, porous plug, or any other type of flow resistance) as shown in the figure. This process is very important, since it is at the heart of domestic refrigerators, where it is responsible for the temperature drop between ambient temperature and the interior of the refrigerator. It is also the final stage in many types of liquefiers.\n",
      "-The gas-cooling throttling process is commonly exploited in refrigeration processes such as liquefiers in air separation industrial process. In hydraulics, the warming effect from Joule–Thomson throttling can be used to find internally leaking valves as these will produce heat which can be detected by thermocouple or thermal-imaging camera. Throttling is a fundamentally irreversible process. The throttling due to the flow resistance in supply lines, heat exchangers, regenerators, and other components of (thermal) machines is a source of losses that limits their performance.\n",
      "-Throttling valves control the amount or pressure of a fluid allowed to pass through and are designed to withstand the stress and wear caused by this operation. Because they may wear out in this usage, they are often installed alongside isolation valves which can temporarily disconnect a failing throttling valve from the rest of the system, so it can be refurbished or replaced.\n",
      "-A very convenient way to get a quantitative understanding of the throttling process is by using diagrams such as h-T diagrams, h-P diagrams, and others. Commonly used are the so-called T-s diagrams. Figure 2 shows the T-s diagram of nitrogen as an example. Various points are indicated as follows: As shown before, throttling keeps h constant. E.g. throttling from 200 bar and 300 K (point a in fig. 2) follows the isenthalpic (line of constant specific enthalpy) of 430 kJ/kg. At 1 bar it results in point b which has a temperature of 270 K. So throttling from 200 bar to 1 bar gives a cooling from room temperature to below the freezing point of water. Throttling from 200 bar and an initial temperature of 133 K (point c in fig. 2) to 1 bar results in point d, which is in the two-phase region of nitrogen at a temperature of 77.2 K. Since the enthalpy is an extensive parameter the enthalpy in d (hd) is equal to the enthalpy in e (he) multiplied with the mass fraction of the liquid in d (xd) plus the enthalpy in f (hf) multiplied with the mass fraction of the gas in d (1 − xd). So hd=xdhe+(1−xd)hf.\n",
      "-Thermally Enhanced Grout High performance grout with a greater TC than more commonly used products Throttling Valve Most commonly used to restrict flow between desuperheaters and buffer tanks, throttling valves allow an operator to achieve higher delta T between entering and leaving water by slowing the GPM flow.\n",
      "Ton of Refrigeration A measure of the amount of heat absorption required to melt I ton of ice in 24 hours. A ton of refrigeration is a measure of the amount of cooling delivered by a heat pump (or other air conditioning system). One ton of refrigeration is equivalent to a cooling rate of 12,000 Btu per hour.\n",
      "Total Cooling Load The total amount of heat energy that must be removed from a space to keep it at the thermostat set point temperature as well as at the desired humidity level, defined to be the sum of the sensible cooling load and the latent cooling load.\n",
      "Tremie Line The pipe used to pump an appropriate grouting material into a borehole from the bottom of the hole to the top. A tremie line will commonly be made of I-inch or I-IL-inch diameter HDPE pipe.\n",
      "Turbulent Flow Regime The flow condition where fluid flow becomes chaotic and disordered. The mixing effect caused by turbulent flow maximizes heat transfer between the fluid and pipe walls in the closed-loop GHEX while also increasing the system pumping pressure.\n",
      "\n",
      "\n",
      "\n",
      "What happens to excess base metal as a solution cools from the upper transformation temperature towards an insoluble state?\n",
      "-Similarly, a hypoeutectoid alloy has two critical temperatures, called \"arrests\". Between these two temperatures, the alloy will exist partly as the solution and partly as a separate crystallizing phase, called the \"pro eutectoid phase\". These two temperatures are called the upper (A3) and lower (A1) transformation temperatures. As the solution cools from the upper transformation temperature toward an insoluble state, the excess base metal will often be forced to \"crystallize-out\", becoming the pro eutectoid. This will occur until the remaining concentration of solutes reaches the eutectoid level, which will then crystallize as a separate microstructure.\n",
      "-Hypereutectoid alloys A hypereutectic alloy also has different melting points. However, between these points, it is the constituent with the higher melting point that will be solid. Similarly, a hypereutectoid alloy has two critical temperatures. When cooling a hypereutectoid alloy from the upper transformation temperature, it will usually be the excess solutes that crystallize-out first, forming the pro-eutectoid. This continues until the concentration in the remaining alloy becomes eutectoid, which then crystallizes into a separate microstructure.\n",
      "-Geochemical fractionations Certain isotopes of trace metals are preferentially oxidized or reduced; thus, transitions between redox species of the metal ions (e.g., Fe2+ → Fe3+) are fractionating, resulting in different isotopic compositions between the different redox pools in the environment. Additionally, at high temperatures, metals ions can evaporate (and subsequently condense upon cooling), and the relative differences in isotope masses of a given heavy metal leads to fractionation during these evaporation and condensation processes. Diffusion of isotopes through a solution or material can also result in fractionations, as the lighter mass isotopes are able to diffuse at a faster rate. Additionally, isotopes can have slight variations in their solubility and other chemical and physical properties, which can also drive fractionation.\n",
      "-Like oil and water, a molten metal may not always mix with another element. For example, pure iron is almost completely insoluble with copper. Even when the constituents are soluble, each will usually have a saturation point, beyond which no more of the constituent can be added. Iron, for example, can hold a maximum of 6.67% carbon. Although the elements of an alloy usually must be soluble in the liquid state, they may not always be soluble in the solid state. If the metals remain soluble when solid, the alloy forms a solid solution, becoming a homogeneous structure consisting of identical crystals, called a phase. If as the mixture cools the constituents become insoluble, they may separate to form two or more different types of crystals, creating a heterogeneous microstructure of different phases, some with more of one constituent than the other. However, in other alloys, the insoluble elements may not separate until after crystallization occurs. If cooled very quickly, they first crystallize as a homogeneous phase, but they are supersaturated with the secondary constituents. As time passes, the atoms of these supersaturated alloys can separate from the crystal lattice, becoming more stable, and forming a second phase that serves to reinforce the crystals internally.\n",
      "-For example, a hypoeutectoid steel contains less than 0.77% carbon. Upon cooling a hypoeutectoid steel from the austenite transformation temperature, small islands of proeutectoid-ferrite will form. These will continue to grow and the carbon will recede until the eutectoid concentration in the rest of the steel is reached. This eutectoid mixture will then crystallize as a microstructure of pearlite. Since ferrite is softer than pearlite, the two microstructures combine to increase the ductility of the alloy. Consequently, the hardenability of the alloy is lowered.\n",
      "\n",
      "\n",
      "\n",
      "What is the relationship between mass, force, and acceleration, according to Sir Isaac Newton's laws of motion?\n",
      "-Mass is (among other properties) an inertial property; that is, the tendency of an object to remain at constant velocity unless acted upon by an outside force. Under Sir Isaac Newton's 336-year-old laws of motion and an important formula that sprang from his work, F = ma, an object with a mass, m, of one kilogram accelerates, a, at one meter per second per second (about one-tenth the acceleration due to earth's gravity) when acted upon by a force, F, of one newton.\n",
      "-Mass – is both a property of a physical body and a measure of its resistance to acceleration (rate of change of velocity with respect to time) when a net force is applied. An object's mass also determines the strength of its gravitational attraction to other bodies. The SI base unit of mass is the kilogram (kg). In physics, mass is not the same as weight, even though mass is often determined by measuring the object's weight using a spring scale, rather than balance scale comparing it directly with known masses. An object on the Moon would weigh less than it does on Earth because of the lower gravity, but it would still have the same mass. This is because weight is a force, while mass is the property that (along with gravity) determines the strength of this force.\n",
      "-Mass Mass refers to the intrinsic property of all material objects to resist changes in their momentum. Weight, on the other hand, refers to the downward force produced when a mass is in a gravitational field. In free fall, (no net gravitational forces) objects lack weight but retain their mass. The Imperial units of mass include the ounce, pound, and ton. The metric units gram and kilogram are units of mass.\n",
      "-In more formal terms, Newton's second law of motion states that the force exerted on an object is directly proportional to the acceleration hence acquired by that object, thus: F=ma, where  m represents the mass of the object undergoing an acceleration  a . As a result, the newton may be defined in terms of the kilogram ( kg ), metre ( m ), and second ( s ) as kg ⋅ms2.\n",
      "-In scientific contexts, mass is the amount of \"matter\" in an object (though \"matter\" may be difficult to define), but weight is the force exerted on an object's matter by gravity. At the Earth's surface, an object whose mass is exactly one kilogram weighs approximately 9.81 newtons, the product of its mass and the gravitational field strength there. The object's weight is less on Mars, where gravity is weaker; more on Saturn, where gravity is stronger; and very small in space, far from significant sources of gravity, but it always has the same mass.\n",
      "\n",
      "\n",
      "\n",
      "What did Arthur Eddington discover about two of Einstein's types of gravitational waves?\n",
      "-The possibility of gravitational waves was discussed in 1893 by Oliver Heaviside, using the analogy between the inverse-square law of gravitation and the electrostatic force. In 1905, Henri Poincaré proposed gravitational waves, emanating from a body and propagating at the speed of light, as being required by the Lorentz transformations and suggested that, in analogy to an accelerating electrical charge producing electromagnetic waves, accelerated masses in a relativistic field theory of gravity should produce gravitational waves. When Einstein published his general theory of relativity in 1915, he was skeptical of Poincaré's idea since the theory implied there were no \"gravitational dipoles\". Nonetheless, he still pursued the idea and based on various approximations came to the conclusion there must, in fact, be three types of gravitational waves (dubbed longitudinal–longitudinal, transverse–longitudinal, and transverse–transverse by Hermann Weyl).However, the nature of Einstein's approximations led many (including Einstein himself) to doubt the result. In 1922, Arthur Eddington showed that two of Einstein's types of waves were artifacts of the coordinate system he used, and could be made to propagate at any speed by choosing appropriate coordinates, leading Eddington to jest that they \"propagate at the speed of thought\".: 72  This also cast doubt on the physicality of the third (transverse–transverse) type that Eddington showed always propagate at the speed of light regardless of coordinate system. In 1936, Einstein and Nathan Rosen submitted a paper to Physical Review in which they claimed gravitational waves could not exist in the full general theory of relativity because any such solution of the field equations would have a singularity. The journal sent their manuscript to be reviewed by Howard P. Robertson, who anonymously reported that the singularities in question were simply the harmless coordinate singularities of the employed cylindrical coordinates. Einstein, who was unfamiliar with the concept of peer review, angrily withdrew the manuscript, never to publish in Physical Review again. Nonetheless, his assistant Leopold Infeld, who had been in contact with Robertson, convinced Einstein that the criticism was correct, and the paper was rewritten with the opposite conclusion and published elsewhere.: 79ff  In 1956, Felix Pirani remedied the confusion caused by the use of various coordinate systems by rephrasing the gravitational waves in terms of the manifestly observable Riemann curvature tensor.At the time, Pirani's work was overshadowed by the community's focus on a different question: whether gravitational waves could transmit energy. This matter was settled by a thought experiment proposed by Richard Feynman during the first \"GR\" conference at Chapel Hill in 1957. In short, his argument known as the \"sticky bead argument\" notes that if one takes a rod with beads then the effect of a passing gravitational wave would be to move the beads along the rod; friction would then produce heat, implying that the passing wave had done work. Shortly after, Hermann Bondi, published a detailed version of the \"sticky bead argument\". This later lead to a series of articles (1959 to 1989)  by Bondi and Pirani that established the existence of plane wave solutions for gravitational waves.After the Chapel Hill conference, Joseph Weber started designing and building the first gravitational wave detectors now known as Weber bars. In 1969, Weber claimed to have detected the first gravitational waves, and by 1970 he was \"detecting\" signals regularly from the Galactic Center; however, the frequency of detection soon raised doubts on the validity of his observations as the implied rate of energy loss of the Milky Way would drain our galaxy of energy on a timescale much shorter than its inferred age. These doubts were strengthened when, by the mid-1970s, repeated experiments from other groups building their own Weber bars across the globe failed to find any signals, and by the late 1970s consensus was that Weber's results were spurious.In the same period, the first indirect evidence of gravitational waves was discovered. In 1974, Russell Alan Hulse and Joseph Hooton Taylor, Jr. discovered the first binary pulsar, which earned them the 1993 Nobel Prize in Physics. Pulsar timing observations over the next decade showed a gradual decay of the orbital period of the Hulse–Taylor pulsar that matched the loss of energy and angular momentum in gravitational radiation predicted by general relativity.This indirect detection of gravitational waves motivated further searches, despite Weber's discredited result. Some groups continued to improve Weber's original concept, while others pursued the detection of gravitational waves using laser interferometers. The idea of using a laser interferometer for this seems to have been floated independently by various people, including M. E. Gertsenshtein and V. I. Pustovoit in 1962, and Vladimir B. Braginskiĭ in 1966. The first prototypes were developed in the 1970s by Robert L. Forward and Rainer Weiss. In the decades that followed, ever more sensitive instruments were constructed, culminating in the construction of GEO600, LIGO, and Virgo.After years of producing null results, improved detectors became operational in 2015. On 11 February 2016, the LIGO-Virgo collaborations announced the first observation of gravitational waves, from a signal (dubbed GW150914) detected at 09:50:45 GMT on 14 September 2015 of two black holes with masses of 29 and 36 solar masses merging about 1.3 billion light-years away. During the final fraction of a second of the merger, it released more than 50 times the power of all the stars in the observable universe combined. The signal increased in frequency from 35 to 250 Hz over 10 cycles (5 orbits) as it rose in strength for a period of 0.2 second. The mass of the new merged black hole was 62 solar masses. Energy equivalent to three solar masses was emitted as gravitational waves. The signal was seen by both LIGO detectors in Livingston and Hanford, with a time difference of 7 milliseconds due to the angle between the two detectors and the source. The signal came from the Southern Celestial Hemisphere, in the rough direction of (but much farther away than) the Magellanic Clouds. The confidence level of this being an observation of gravitational waves was 99.99994%.A year earlier, the BICEP2 collaboration claimed that they had detected the imprint of gravitational waves in the cosmic microwave background. However, they were later forced to retract this result.In 2017, the Nobel Prize in Physics was awarded to Rainer Weiss, Kip Thorne and Barry Barish for their role in the detection of gravitational waves.In 2023, NANOGrav, EPTA, PPTA, and IPTA announced that they found evidence of a universal gravitational wave background. North American Nanohertz Observatory for Gravitational Waves states, that they were created over cosmological time scales by supermassive black holes, identifying the distinctive Hellings-Downs curve in 15 years of radio observations of 25 pulsars.\n",
      "-In 1922, Arthur Stanley Eddington wrote a paper expressing (apparently for the first time) the view that gravitational waves are in essence ripples in coordinates, and have no physical meaning. He did not appreciate Einstein's arguments that the waves are real.In 1936, together with Nathan Rosen, Einstein rediscovered the Beck vacuums, a family of exact gravitational wave solutions with cylindrical symmetry (sometimes also called Einstein–Rosen waves). While investigating the motion of test particles in these solutions, Einstein and Rosen became convinced that gravitational waves were unstable to collapse. Einstein reversed himself and declared that gravitational radiation was not after all a prediction of his theory. Einstein wrote to his friend Max Born Together with a young collaborator, I arrived at the interesting result that gravitational waves do not exist, though they had been assumed a certainty to the first approximation. This shows that the nonlinear field equations can show us more, or rather limit us more, than we have believed up till now.\n",
      "-Gravitational waves Predicted in 1916 by Albert Einstein, there are gravitational waves: ripples in the metric of spacetime that propagate at the speed of light. These are one of several analogies between weak-field gravity and electromagnetism in that, they are analogous to electromagnetic waves. On 11 February 2016, the Advanced LIGO team announced that they had directly detected gravitational waves from a pair of black holes merging.The simplest type of such a wave can be visualized by its action on a ring of freely floating particles. A sine wave propagating through such a ring towards the reader distorts the ring in a characteristic, rhythmic fashion (animated image to the right). Since Einstein's equations are non-linear, arbitrarily strong gravitational waves do not obey linear superposition, making their description difficult. However, linear approximations of gravitational waves are sufficiently accurate to describe the exceedingly weak waves that are expected to arrive here on Earth from far-off cosmic events, which typically result in relative distances increasing and decreasing by  10 21 or less. Data analysis methods routinely make use of the fact that these linearized waves can be Fourier decomposed.Some exact solutions describe gravitational waves without any approximation, e.g., a wave train traveling through empty space or Gowdy universes, varieties of an expanding cosmos filled with gravitational waves. But for gravitational waves produced in astrophysically relevant situations, such as the merger of two black holes, numerical methods are presently the only way to construct appropriate models.\n",
      "-Albert Einstein originally predicted the existence of gravitational waves in 1916, on the basis of his theory of general relativity. General relativity interprets gravity as a consequence of distortions in space-time, caused by mass. Therefore, Einstein also predicted that events in the cosmos would cause \"ripples\" in space-time – distortions of space-time itself – which would spread outward, although they would be so minuscule that they would be nearly impossible to detect by any technology foreseen at that time. It was also predicted that objects moving in an orbit would lose energy for this reason (a consequence of the law of conservation of energy), as some energy would be given off as gravitational waves, although this would be insignificantly small in all but the most extreme cases.One case where gravitational waves would be strongest is during the final moments of the merger of two compact objects such as neutron stars or black holes. Over a span of millions of years, binary neutron stars, and binary black holes lose energy, largely through gravitational waves, and as a result, they spiral in towards each other. At the very end of this process, the two objects will reach extreme velocities, and in the final fraction of a second of their merger a substantial amount of their mass would theoretically be converted into gravitational energy, and travel outward as gravitational waves, allowing a greater than usual chance for detection. However, since little was known about the number of compact binaries in the universe and reaching that final stage can be very slow, there was little certainty as to how often such events might happen.\n",
      "-At the end of the 19th century, the electromagnetic field was understood as a collection of two vector fields in space. Nowadays, one recognizes this as a single antisymmetric 2nd-rank tensor field in spacetime.\n",
      "Gravitation in general relativity Einstein's theory of gravity, called general relativity, is another example of a field theory. Here the principal field is the metric tensor, a symmetric 2nd-rank tensor field in spacetime. This replaces Newton's law of universal gravitation.\n",
      "Waves as fields Waves can be constructed as physical fields, due to their finite propagation speed and causal nature when a simplified physical model of an isolated closed system is set. They are also subject to the inverse-square law.\n",
      "For electromagnetic waves, there are optical fields, and terms such as near- and far-field limits for diffraction. In practice though, the field theories of optics are superseded by the electromagnetic field theory of Maxwell.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 뽑힌 context의 내용 확인하기 \n",
    "\n",
    "context=df['context'].tolist()\n",
    "\n",
    "for idx,q in enumerate(df['prompt'].tolist()):\n",
    "    print (q)\n",
    "    print(context[idx])\n",
    "    print()\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4f7abe1-20f3-46c6-86bc-e088eff4a92a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81d9a561-75a1-4051-9c0e-543bc193aa00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9562, 3858, 6799, 4290, 3539, 2936, 3111, 3021, 6251, 2681, 7054, 4999, 2457, 3272, 4448, 4768, 3332, 3100, 4957, 1840, 6876, 2498, 3524, 8186, 3273, 2832, 3692, 3777, 4511, 4766, 4283, 2735, 3589, 4186, 3943, 3983, 3342, 5039, 4856, 4949, 4480, 4083, 6015, 6461, 4481, 3866, 4414, 4485, 7593, 5835, 6118, 4296, 2887, 3045, 2768, 3039, 6931, 5938, 4752, 3111, 3753, 5259, 4809, 4598, 4959, 4977, 2875, 3318, 5459, 8200, 5055, 8067, 3393, 6227, 2798, 5116, 2871, 3624, 4826, 6056, 3589, 2312, 5429, 3992, 1977, 3621, 3811, 2766, 3817, 2927, 2003, 2826, 2427, 5226, 4841, 3139, 2931, 3607, 3777, 3720, 5573, 4040, 4078, 7686, 3759, 4121, 5226, 4844, 3474, 5653, 4289, 6084, 3535, 4864, 2780, 2004, 4768, 4803, 4502, 3094, 3208, 1837, 3053, 3962, 5512, 2271, 4660, 2996, 8884, 6903, 3261, 7200, 6230, 2998, 4039, 14339, 5961, 2901, 5034, 3586, 5015, 7332, 3157, 3879, 5621, 2322, 5950, 9691, 3634, 6744, 3670, 6665, 3962, 2391, 5690, 9661, 4928, 3631, 8049, 6471, 3198, 2036, 2774, 2086, 4411, 3602, 2383, 2224, 3682, 5094, 3885, 4247, 2680, 5229, 3121, 3221, 5531, 3719, 8224, 3404, 6321, 3006, 5067, 4790, 6278, 5042, 6675, 4003, 6080, 3965, 7052, 3647, 3718, 2233, 2884, 2682, 3978, 3934, 2589, 12471]\n",
      "4491.66\n"
     ]
    }
   ],
   "source": [
    "## context의 길이 알아보기 \n",
    "\n",
    "lengths=[]\n",
    "for c in context:\n",
    "    lengths.append(len(c))\n",
    "\n",
    "hap=0\n",
    "for l in lengths:\n",
    "    hap+=l\n",
    "\n",
    "print(lengths)\n",
    "print(hap/200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4603442-3630-4172-a0cf-0f4c596b1da7",
   "metadata": {},
   "source": [
    "### all-MInLM-L6-v2 embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f746ced-9eec-45de-be9d-7fdc8972a826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name output/all-MiniLM-L6-v2. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prompt embedding, t=0.0s\n",
      "Loading faiss index, t=0.6s\n",
      "Starting text search, t=5.2s\n",
      "Starting context extraction, t=5.2s\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"../dataset/test.csv\", index_col=\"id\")\n",
    "\n",
    "## all-MiniLM-L6-v2 embedding \n",
    "\n",
    "NUM_TITLES = 5\n",
    "MAX_SEQ_LEN = 512\n",
    "MODEL_PATH = \"output/all-MiniLM-L6-v2\"\n",
    "\n",
    "## load embedding model\n",
    "start = time()\n",
    "print(f\"Starting prompt embedding, t={time() - start :.1f}s\")\n",
    "model = SentenceTransformer(MODEL_PATH, device=\"cuda:2\")\n",
    "\n",
    "## Get query embedding\n",
    "f = lambda row : \" \".join([row[\"prompt\"], row[\"A\"], row[\"B\"], row[\"C\"], row[\"D\"], row[\"E\"]])\n",
    "inputs = df.apply(f, axis=1).values # better results than prompt only\n",
    "prompt_embeddings = model.encode(inputs, show_progress_bar=False)\n",
    "\n",
    "## faiss wikipedia index 불러오기\n",
    "print(f\"Loading faiss index, t={time() - start :.1f}s\")\n",
    "faiss_index = faiss.read_index(MODEL_PATH + '/faiss.index')\n",
    "faiss_index = faiss.index_cpu_to_all_gpus(faiss_index) # OOM이 일어날 때는 지우기 \n",
    "\n",
    "## top-5의 관련있는 인덱스 가져오기 \n",
    "print(f\"Starting text search, t={time() - start :.1f}s\")\n",
    "search_index = faiss_index.search(np.float32(prompt_embeddings), NUM_TITLES)[1]\n",
    "\n",
    "## 인덱스를 찾아서 실제 문서 가져오기 \n",
    "print(f\"Starting context extraction, t={time() - start :.1f}s\")\n",
    "dataset = load_from_disk(\"dataset/all-paraphs-parsed-expanded\")\n",
    "for i in range(len(df)):\n",
    "    df.loc[i, \"context\"] = \"-\" + \"\\n-\".join([dataset[int(j)][\"text\"] for j in search_index[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86968433-f950-49a9-82a3-bfa292499403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which of the following statements accurately d...</td>\n",
       "      <td>MOND is a theory that reduces the observed mis...</td>\n",
       "      <td>MOND is a theory that increases the discrepanc...</td>\n",
       "      <td>MOND is a theory that explains the missing bar...</td>\n",
       "      <td>MOND is a theory that reduces the discrepancy ...</td>\n",
       "      <td>MOND is a theory that eliminates the observed ...</td>\n",
       "      <td>-MOND is an example of a class of theories kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which of the following is an accurate definiti...</td>\n",
       "      <td>Dynamic scaling refers to the evolution of sel...</td>\n",
       "      <td>Dynamic scaling refers to the non-evolution of...</td>\n",
       "      <td>Dynamic scaling refers to the evolution of sel...</td>\n",
       "      <td>Dynamic scaling refers to the non-evolution of...</td>\n",
       "      <td>Dynamic scaling refers to the evolution of sel...</td>\n",
       "      <td>-In such systems we can define a certain time-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which of the following statements accurately d...</td>\n",
       "      <td>The triskeles symbol was reconstructed as a fe...</td>\n",
       "      <td>The triskeles symbol is a representation of th...</td>\n",
       "      <td>The triskeles symbol is a representation of a ...</td>\n",
       "      <td>The triskeles symbol represents three interloc...</td>\n",
       "      <td>The triskeles symbol is a representation of th...</td>\n",
       "      <td>-Classical Antiquity The triskeles proper, com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the significance of regularization in ...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>-Regularization: Classical physics theory brea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which of the following statements accurately d...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>-Several qualitative observations can be made ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Which of the following statements accurately d...</td>\n",
       "      <td>Gauss's law holds only for situations involvin...</td>\n",
       "      <td>Gauss's law holds in all cases, but it is most...</td>\n",
       "      <td>Gauss's law, which applies equally to all elec...</td>\n",
       "      <td>Gauss's law only holds for electric fields wit...</td>\n",
       "      <td>Gauss's law, which holds for all situations, i...</td>\n",
       "      <td>-While the electric flux is not affected by ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Which of the following statements accurately d...</td>\n",
       "      <td>The dimension of an object in a CW complex is ...</td>\n",
       "      <td>The dimension of an object in a CW complex is ...</td>\n",
       "      <td>The dimension of an object in a CW complex is ...</td>\n",
       "      <td>The dimension of an object in a CW complex is ...</td>\n",
       "      <td>The dimension of an object in a CW complex dep...</td>\n",
       "      <td>-An inductive dimension may be defined inducti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Which of the following statements accurately d...</td>\n",
       "      <td>The blocking temperature of an antiferromagnet...</td>\n",
       "      <td>The blocking temperature of an antiferromagnet...</td>\n",
       "      <td>The blocking temperature of an antiferromagnet...</td>\n",
       "      <td>The blocking temperature of an antiferromagnet...</td>\n",
       "      <td>The blocking temperature of an antiferromagnet...</td>\n",
       "      <td>-Antiferromagnets can couple to ferromagnets, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is the term used in astrophysics to descr...</td>\n",
       "      <td>Blueshifting</td>\n",
       "      <td>Redshifting</td>\n",
       "      <td>Reddening</td>\n",
       "      <td>Whitening</td>\n",
       "      <td>Yellowing</td>\n",
       "      <td>-The interactions and phenomena summarized in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What is the role of axioms in a formal theory?</td>\n",
       "      <td>Basis statements called axioms form the founda...</td>\n",
       "      <td>Axioms are supplementary statements added to a...</td>\n",
       "      <td>Axioms are redundant statements that can be de...</td>\n",
       "      <td>The axioms in a theory are used for experiment...</td>\n",
       "      <td>The axioms in a formal theory are added to pro...</td>\n",
       "      <td>-In mathematics and logic, an axiomatic system...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               prompt  \\\n",
       "id                                                      \n",
       "0   Which of the following statements accurately d...   \n",
       "1   Which of the following is an accurate definiti...   \n",
       "2   Which of the following statements accurately d...   \n",
       "3   What is the significance of regularization in ...   \n",
       "4   Which of the following statements accurately d...   \n",
       "5   Which of the following statements accurately d...   \n",
       "6   Which of the following statements accurately d...   \n",
       "7   Which of the following statements accurately d...   \n",
       "8   What is the term used in astrophysics to descr...   \n",
       "9      What is the role of axioms in a formal theory?   \n",
       "\n",
       "                                                    A  \\\n",
       "id                                                      \n",
       "0   MOND is a theory that reduces the observed mis...   \n",
       "1   Dynamic scaling refers to the evolution of sel...   \n",
       "2   The triskeles symbol was reconstructed as a fe...   \n",
       "3   Regularizing the mass-energy of an electron wi...   \n",
       "4   The angular spacing of features in the diffrac...   \n",
       "5   Gauss's law holds only for situations involvin...   \n",
       "6   The dimension of an object in a CW complex is ...   \n",
       "7   The blocking temperature of an antiferromagnet...   \n",
       "8                                        Blueshifting   \n",
       "9   Basis statements called axioms form the founda...   \n",
       "\n",
       "                                                    B  \\\n",
       "id                                                      \n",
       "0   MOND is a theory that increases the discrepanc...   \n",
       "1   Dynamic scaling refers to the non-evolution of...   \n",
       "2   The triskeles symbol is a representation of th...   \n",
       "3   Regularizing the mass-energy of an electron wi...   \n",
       "4   The angular spacing of features in the diffrac...   \n",
       "5   Gauss's law holds in all cases, but it is most...   \n",
       "6   The dimension of an object in a CW complex is ...   \n",
       "7   The blocking temperature of an antiferromagnet...   \n",
       "8                                         Redshifting   \n",
       "9   Axioms are supplementary statements added to a...   \n",
       "\n",
       "                                                    C  \\\n",
       "id                                                      \n",
       "0   MOND is a theory that explains the missing bar...   \n",
       "1   Dynamic scaling refers to the evolution of sel...   \n",
       "2   The triskeles symbol is a representation of a ...   \n",
       "3   Regularizing the mass-energy of an electron wi...   \n",
       "4   The angular spacing of features in the diffrac...   \n",
       "5   Gauss's law, which applies equally to all elec...   \n",
       "6   The dimension of an object in a CW complex is ...   \n",
       "7   The blocking temperature of an antiferromagnet...   \n",
       "8                                           Reddening   \n",
       "9   Axioms are redundant statements that can be de...   \n",
       "\n",
       "                                                    D  \\\n",
       "id                                                      \n",
       "0   MOND is a theory that reduces the discrepancy ...   \n",
       "1   Dynamic scaling refers to the non-evolution of...   \n",
       "2   The triskeles symbol represents three interloc...   \n",
       "3   Regularizing the mass-energy of an electron wi...   \n",
       "4   The angular spacing of features in the diffrac...   \n",
       "5   Gauss's law only holds for electric fields wit...   \n",
       "6   The dimension of an object in a CW complex is ...   \n",
       "7   The blocking temperature of an antiferromagnet...   \n",
       "8                                           Whitening   \n",
       "9   The axioms in a theory are used for experiment...   \n",
       "\n",
       "                                                    E  \\\n",
       "id                                                      \n",
       "0   MOND is a theory that eliminates the observed ...   \n",
       "1   Dynamic scaling refers to the evolution of sel...   \n",
       "2   The triskeles symbol is a representation of th...   \n",
       "3   Regularizing the mass-energy of an electron wi...   \n",
       "4   The angular spacing of features in the diffrac...   \n",
       "5   Gauss's law, which holds for all situations, i...   \n",
       "6   The dimension of an object in a CW complex dep...   \n",
       "7   The blocking temperature of an antiferromagnet...   \n",
       "8                                           Yellowing   \n",
       "9   The axioms in a formal theory are added to pro...   \n",
       "\n",
       "                                              context  \n",
       "id                                                     \n",
       "0   -MOND is an example of a class of theories kno...  \n",
       "1   -In such systems we can define a certain time-...  \n",
       "2   -Classical Antiquity The triskeles proper, com...  \n",
       "3   -Regularization: Classical physics theory brea...  \n",
       "4   -Several qualitative observations can be made ...  \n",
       "5   -While the electric flux is not affected by ch...  \n",
       "6   -An inductive dimension may be defined inducti...  \n",
       "7   -Antiferromagnets can couple to ferromagnets, ...  \n",
       "8   -The interactions and phenomena summarized in ...  \n",
       "9   -In mathematics and logic, an axiomatic system...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfb4db32-4c16-4ad7-931f-012a4bf50007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed \"missing baryonic mass\" discrepancy in galaxy clusters?\n",
      "-MOND is an example of a class of theories known as modified gravity, and is an alternative to the hypothesis that the dynamics of galaxies are determined by massive, invisible dark matter halos. Since Milgrom's original proposal, proponents of MOND have claimed to successfully predict a variety of galactic phenomena that they state are difficult to understand as consequences of dark matter.Though MOND explains the anomalously great rotational velocities of galaxies at their perimeters, it does not fully explain the velocity dispersions of individual galaxies within galaxy clusters. MOND reduces the discrepancy between the velocity dispersions and clusters' observed missing baryonic mass from a factor of around 10 to a factor of about 2. However, the residual discrepancy cannot be accounted for by MOND, requiring that other explanations close the gap such as the presence of as-yet undetected missing baryonic matter.The accurate measurement of the speed of gravitational waves compared to the speed of light in 2017 ruled out a certain class of modified gravity theories but concluded that other MOND theories that dispense with the need for dark matter remained viable. Two years later, theories put forth by Constantinos Skordis and Tom Zlosnik were consistent with gravitational waves that always travel at the speed of light. Later still in 2021, Skordis and Zlosnik developed a subclass of their theory called \"RMOND\", for \"relativistic MOND\", which had \"been shown to reproduce in great detail the main observations in cosmology, including the cosmic-microwave-background power spectrum, and the matter structure power spectrum.\" \n",
      "-Outstanding problems for MOND The most serious problem facing Milgrom's law is that it cannot eliminate the need for dark matter in all astrophysical systems: galaxy clusters show a residual mass discrepancy even when analyzed using MOND. The fact that some form of unseen mass must exist in these systems detracts from the adequacy of MOND as a solution to the missing mass problem, although the amount of extra mass required is a fifth that of a Newtonian analysis, and there is no requirement that the missing mass be non-baryonic. It has been speculated that 2 eV neutrinos could account for the cluster observations in MOND while preserving the hypothesis's successes at the galaxy scale. Indeed, analysis of sharp lensing data for the galaxy cluster Abell 1689 shows that MOND only becomes distinctive at Mpc distance from the center, so that Zwicky's conundrum remains, and 1.8 eV neutrinos are needed in clusters.The 2006 observation of a pair of colliding galaxy clusters known as the \"Bullet Cluster\", poses a significant challenge for all theories proposing a modified gravity solution to the missing mass problem, including MOND. Astronomers measured the distribution of stellar and gas mass in the clusters using visible and X-ray light, respectively, and in addition mapped the inferred dark matter density using gravitational lensing. In MOND, one would expect the \"missing mass\" to be centred on regions of visible mass which experience accelerations lower than a0 (assuming the external field effect is negligible). In ΛCDM, on the other hand, one would expect the dark matter to be significantly offset from the visible mass because the halos of the two colliding clusters would pass through each other (assuming, as is conventional, that dark matter is collisionless), whilst the cluster gas would interact and end up at the centre. An offset is clearly seen in the observations. It has been suggested, however, that MOND-based models may be able to generate such an offset in strongly non-spherically symmetric systems, such as the Bullet Cluster.A significant piece of evidence in favor of standard dark matter is the observed anisotropies in the cosmic microwave background. While ΛCDM is able to explain the observed angular power spectrum, MOND has a much harder time, though recently it has been shown that MOND can fit the observations too. MOND also encounters difficulties explaining structure formation, with density perturbations in MOND perhaps growing so rapidly that too much structure is formed by the present epoch. However, forming galaxies more rapidly than in ΛCDM can be a good thing to some extent.Several other studies have noted observational difficulties with MOND. For example, it has been claimed that MOND offers a poor fit to the velocity dispersion profile of globular clusters and the temperature profile of galaxy clusters, that different values of a0 are required for agreement with different galaxies' rotation curves, and that MOND is naturally unsuited to forming the basis of cosmology. Furthermore, many versions of MOND predict that the speed of light is different from the speed of gravity, but in 2017 the speed of gravitational waves was measured to be equal to the speed of light to high precision. This is well understood in modern relativistic theories of MOND, with the constraint from gravitational waves actually helping by substantially restricting how a covariant theory might be constructed.Besides these observational issues, MOND and its relativistic generalizations are plagued by theoretical difficulties. Several ad hoc and inelegant additions to general relativity are required to create a theory compatible with a non-Newtonian non-relativistic limit, though the predictions in this limit are rather clear. This is the case for the more commonly used modified gravity versions of MOND, but some formulations (most prominently those based on modified inertia) have long suffered from poor compatibility with cherished physical principles such as conservation laws. Researchers working on MOND generally do not interpret it as a modification of inertia, with only very limited work done on this area.\n",
      "-Since MOND modifies Newtonian dynamics in an acceleration-dependent way, it predicts a specific relationship between the acceleration of a star at any radius from the centre of a galaxy and the amount of unseen (dark matter) mass within that radius that would be inferred in a Newtonian analysis. This is known as the mass discrepancy-acceleration relation, and has been measured observationally. One aspect of the MOND prediction is that the mass of the inferred dark matter goes to zero when the stellar centripetal acceleration becomes greater than a0, where MOND reverts to Newtonian mechanics. In a dark matter hypothesis, it is a challenge to understand why this mass should correlate so closely with acceleration, and why there appears to be a critical acceleration above which dark matter is not required.\n",
      "-The majority of astronomers, astrophysicists, and cosmologists accept dark matter as the explanation for galactic rotation curves (based on general relativity, and hence Newtonian mechanics), and are committed to a dark matter solution of the missing-mass problem. The primary difference between supporters of ΛCDM and MOND is in the observations for which they demand a robust, quantitative explanation, and those for which they are satisfied with a qualitative account, or are prepared to leave for future work. Proponents of MOND emphasize predictions made on galaxy scales (where MOND enjoys its most notable successes) and believe that a cosmological model consistent with galaxy dynamics has yet to be discovered. Proponents of ΛCDM require high levels of cosmological accuracy (which concordance cosmology provides) and argue that a resolution of galaxy-scale issues will follow from a better understanding of the complicated baryonic astrophysics underlying galaxy formation.\n",
      "-In MOND, all gravitationally bound objects with a < a0 – regardless of their origin – should exhibit a mass discrepancy when analyzed using Newtonian mechanics, and should lie on the BTFR. Under the dark matter hypothesis, objects formed from baryonic material ejected during the merger or tidal interaction of two galaxies (\"tidal dwarf galaxies\") are expected to be devoid of dark matter and hence show no mass discrepancy. Three objects unambiguously identified as Tidal Dwarf Galaxies appear to have mass discrepancies in close agreement with the MOND prediction.\n",
      "\n",
      "\n",
      "\n",
      "Which of the following is an accurate definition of dynamic scaling in self-similar systems?\n",
      "-In such systems we can define a certain time-dependent stochastic variable  x . We are interested in computing the probability distribution of  x at various instants of time i.e.  f(x,t) . The numerical value of  f and the typical or mean value of  x generally changes over time. The question is: what happens to the corresponding dimensionless variables? If the numerical values of the dimensional quantities change, but corresponding dimensionless quantities remain invariant then we can argue that snapshots of the system at different times are similar. When this happens we say that the system is self-similar.  One way of verifying dynamic scaling is to plot dimensionless variables  f/tθ as a function of  x/tz of the data extracted at various different time. Then if all the plots of  f vs  x obtained at different times collapse onto a single universal curve then it is said that the systems at different time are similar and it obeys dynamic scaling. The idea of data collapse is deeply rooted to the Buckingham Pi theorem. Essentially such systems can be termed as temporal self-similarity since the same system is similar at different times.\n",
      "-Here the exponent  θ is fixed by the dimensional requirement  [f]=[tθ] . The numerical value of  f/tθ should remain invariant despite the unit of measurement of  t is changed by some factor since  φ is a dimensionless quantity.  Many of these systems evolve in a self-similar fashion in the sense that data obtained from the snapshot at any fixed time is similar to the respective data taken from the snapshot of any earlier or later time. That is, the system is similar to itself at different times. The litmus test of such self-similarity is provided by the dynamic scaling.\n",
      "-A time developing phenomenon is said to exhibit self-similarity if the numerical value of certain observable quantity  f(x,t) measured at different times are different but the corresponding dimensionless quantity at given value of  x/tz remain invariant. It happens if the quantity  f(x,t) exhibits dynamic scaling. The idea is just an extension of the idea of similarity of two triangles. Note that two triangles are similar if the numerical values of their sides are different however the corresponding dimensionless quantities, such as their angles, coincide.\n",
      "-Dynamic scaling (sometimes known as Family-Vicsek scaling) is a litmus test that shows whether an evolving system exhibits self-similarity. In general a function is said to exhibit dynamic scaling if it satisfies: f(x,t)∼tθφ(xtz).\n",
      "-Many phenomena investigated by physicists are not static but evolve probabilistically with time (i.e. Stochastic process). The universe itself is perhaps one of the best examples. It has been expanding ever since the Big Bang. Similarly, growth of networks like the Internet are also ever growing systems. Another example is polymer degradation where degradation does not occur in a blink of an eye but rather over quite a long time. Spread of biological and computer viruses too does not happen over night.  Many other seemingly disparate systems which are found to exhibit dynamic scaling. For example: kinetics of aggregation described by Smoluchowski coagulation equation,  complex networks described by Barabasi–Albert model, the kinetic and stochastic Cantor set, the growth model within the Kardar–Parisi–Zhang (KPZ) universality class; one find that the width of the surface  W(L,t) exhibits dynamic scaling.\n",
      "\n",
      "\n",
      "\n",
      "Which of the following statements accurately describes the origin and significance of the triskeles symbol?\n",
      "-Classical Antiquity The triskeles proper, composed of three human legs, is younger than the triple spiral, found in decorations on Greek pottery especially as a design shown on hoplite shields, and later also minted on Greek and Anatolian coinage.\n",
      "An early example is found on the shield of Achilles in an Attic hydria of the late 6th century BCE.\n",
      "It is found on coinage in Lycia, and on staters of Pamphylia (at Aspendos, 370–333 BCE) and Pisidia. The meaning of the Greek triskeles is not recorded directly.\n",
      "The Duc de Luynes in his 1835 study noted the co-occurrence of the symbol with the eagle, the cockerel, the head of Medusa, Perseus, three crescent moons, three ears of corn, and three grains of corn.\n",
      "From this, he reconstructed feminine divine triad which he identified with the \"triple goddess\" Hecate.\n",
      "The triskeles was adopted as emblem by the rulers of Syracuse. It is possible that this usage is related with the Greek name of the island of Sicily, Trinacria (Τρινακρία 'having three headlands').\n",
      "The Sicilian triskeles is shown with the head of Medusa at the center.\n",
      "The ancient symbol has been re-introduced in modern flags of Sicily since 1848. The oldest find of a triskeles in Sicily is a vase dated to 700 BCE, for which researchers assume a Minoan-Mycenaean origin.\n",
      "Roman period and Late Antiquity Late examples of the triple spiral symbols are found in Iron Age Europe, e.g. carved in rock in Castro Culture settlement in Galicia, Asturias and Northern Portugal.\n",
      "In Ireland before the 5th century, in Celtic Christianity the symbol took on new meaning, as a symbol of the Trinity (Father, Son, and Holy Spirit).\n",
      "-A triskelion or triskeles is an ancient motif consisting of a triple spiral exhibiting rotational symmetry or other patterns in triplicate that emanate from a common center.\n",
      "The spiral design can be based on interlocking Archimedean spirals, or represent three bent human legs. It is found in artifacts of the European Neolithic and Bronze Age with continuation into the Iron Age especially in the context of the La Tène culture and related Celtic traditions.\n",
      "The actual triskeles symbol of three human legs is found especially in Greek antiquity, beginning in archaic pottery and continued in coinage of the classical period.\n",
      "In the Hellenistic period, the symbol becomes associated with the island of Sicily, appearing on coins minted under Dionysius I of Syracuse beginning in c. 382 BCE.\n",
      "It later appears in heraldry, and, other than in the flag of Sicily, came to be used in the flag of the Isle of Man (known as ny tree cassyn 'the three legs').Greek τρισκελής (triskelḗs) means 'three-legged'.\n",
      "While the Greek adjective τρισκελής 'three-legged (e.g., of a table)' is ancient, use of the term for the symbol is modern, introduced in 1835 by Honoré Théodoric d'Albert de Luynes as French triskèle, and adopted in the spelling triskeles following Otto Olshausen (1886).\n",
      "The form triskelion (as it were Greek τρισκέλιον) is a diminutive which entered English usage in numismatics in the late 19th century.\n",
      "The form consisting of three human legs (as opposed to the triple spiral) has also been called a \"triquetra of legs\", also triskelos or triskel.\n",
      "-The triskeles was included in the design of the Army Gold Medal awarded to British Army majors and above who had taken a key part in the Battle of Maida (1806).\n",
      "An early flag of Sicily, proposed in 1848, included the Sicilian triskeles or \"Trinacria symbol\".\n",
      "Later versions of Sicilian flags have retained the emblem, including the one officially adopted in 2000.\n",
      "The Flag of the Isle of Man (1932) shows a heraldic design of a triskeles of three armoured legs.\n",
      "-In the Bavarian town of Füssen, Germany the flag and coat of arms of the town contains a triskele, as does the flag of the Russian autonomous region of Ust-Orda Buryat Okrug.The spiral is used by some polytheistic reconstructionist or neopagan groups. As a \"Celtic symbol\", it is used primarily by groups with a Celtic cultural orientation and, less frequently, can also be found in use by various eclectic or syncretic traditions such as Neopaganism. The spiral triskele is one of the primary symbols of Celtic Reconstructionist Paganism, used to represent a variety of triplicities in cosmology and theology; it is also a favored symbol due to its association with the god Manannán mac Lir.Other uses of triskelion-like emblems include the logo for the Trisquel Linux distribution and the seal of the United States Department of Transportation.A specific version of the triskele featuring a conglomeration composed of three sevens has been adopted by neo-nazis, having been originally used (although without the sevens) by the 27th SS Volunteer Division Langemarck on their shoulder strap. In South Africa the Afrikaner Weerstandsbeweging (AWB), an Afrikaner nationalist, neo-Nazi organization and political party (founded 1973), uses a triskele composed of three sevens as its symbol in place of a swastika. The Blood & Honour group also utilises it. Usage of the symbol can be a prosecutable offence under German law, depending on the context in which the symbol is used.\n",
      "-The triple spiral design is found as a decorative element in Gothic architecture. The three legs (triskeles) symbol is rarely found as a charge in late medieval heraldry, notably as the arms of the King of Mann (Armorial Wijnbergen, c. 1280), and as canting arms in the city seal of the Bavarian city of Füssen (dated 1317).\n",
      "\n",
      "\n",
      "\n",
      "What is the significance of regularization in terms of renormalization problems in physics?\n",
      "-Regularization: Classical physics theory breaks down at small scales, e.g., the difference between an electron and a point particle shown above. Addressing this problem requires new kinds of additional physical constraints. For instance, in this case, assuming a finite electron radius (i.e., regularizing the electron mass-energy) suffices to explain the system below a certain size. Similar regularization arguments work in other renormalization problems. For example, a theory may hold under one narrow set of conditions, but due to calculations involving infinities or singularities, it may breakdown under other conditions or scales. In the case of the electron, another way to avoid infinite mass-energy while retaining the point nature of the particle is to postulate tiny additional dimensions over which the particle could 'spread out' rather than restrict its motion solely over 3D space. This is precisely the motivation behind string theory and other multi-dimensional models including multiple time dimensions. Rather than the existence of unknown new physics, assuming the existence of particle interactions with other surrounding particles in the environment, renormalization offers an alternatively strategy to resolve infinities in such classical problems.\n",
      "-Renormalization is distinct from regularization, another technique to control infinities by assuming the existence of new unknown physics at new scales.\n",
      "-However, the result usually includes terms proportional to expressions like  1/ϵ which are not well-defined in the limit  ϵ→0 . Regularization is the first step towards obtaining a completely finite and meaningful result; in quantum field theory it must be usually followed by a related, but independent technique called renormalization. Renormalization is based on the requirement that some physical quantities — expressed by seemingly divergent expressions such as  1/ϵ — are equal to the observed values. Such a constraint allows one to calculate a finite value for many other quantities that looked divergent.\n",
      "-In physics, especially quantum field theory, regularization is a method of modifying observables which have singularities in order to make them finite by the introduction of a suitable parameter called the regulator. The regulator, also known as a \"cutoff\", models our lack of knowledge about physics at unobserved scales (e.g. scales of small size or large energy levels). It compensates for (and requires) the possibility that \"new physics\" may be discovered at those scales which the present theory is unable to model, while enabling the current theory to give accurate predictions as an \"effective theory\" within its intended scale of use.\n",
      "-Opinions Paul Dirac was persistently, extremely critical about procedures of renormalization. In 1963, he wrote, \"… in the renormalization theory we have a theory that has defied all the attempts of the mathematician to make it sound. I am inclined to suspect that the renormalization theory is something that will not survive in the future,…\" He further observed that \"One can distinguish between two main procedures for a theoretical physicist. One of them is to work from the experimental basis ... The other procedure is to work from the mathematical basis. One examines and criticizes the existing theory. One tries to pin-point the faults in it and then tries to remove them. The difficulty here is to remove the faults without destroying the very great successes of the existing theory.\"Abdus Salam remarked in 1972, \"Field-theoretic infinities first encountered in Lorentz's computation of electron have persisted in classical electrodynamics for seventy and in quantum electrodynamics for some thirty-five years. These long years of frustration have left in the subject a curious affection for the infinities and a passionate belief that they are an inevitable part of nature; so much so that even the suggestion of a hope that they may after all be circumvented - and finite values for the renormalization constants computed - is considered irrational.\"However, in Gerard ’t Hooft’s opinion, \"History tells us that if we hit upon some obstacle, even if it looks like a pure formality or just a technical complication, it should be carefully scrutinized. Nature might be telling us something, and we should find out what it is.\"The difficulty with a realistic regularization is that so far there is none, although nothing could be destroyed by its bottom-up approach; and there is no experimental basis for it.\n",
      "\n",
      "\n",
      "\n",
      "Which of the following statements accurately describes the relationship between the dimensions of a diffracting object and the angular spacing of features in the diffraction pattern?\n",
      "-Several qualitative observations can be made of diffraction in general: The angular spacing of the features in the diffraction pattern is inversely proportional to the dimensions of the object causing the diffraction. In other words: The smaller the diffracting object, the 'wider' the resulting diffraction pattern, and vice versa. (More precisely, this is true of the sines of the angles.) The diffraction angles are invariant under scaling; that is, they depend only on the ratio of the wavelength to the size of the diffracting object.\n",
      "-Several qualitative observations can be made of diffraction in general: The angular spacing of the features in the diffraction pattern is inversely proportional to the dimensions of the object causing the diffraction. In other words: the smaller the diffracting object, the wider the resulting diffraction pattern, and vice versa. (More precisely, this is true of the sines of the angles.) The diffraction angles are invariant under scaling; that is, they depend only on the ratio of the wavelength to the size of the diffracting object.\n",
      "-When the diffracting object has a periodic structure, for example in a diffraction grating, the features generally become sharper. The third figure, for example, shows a comparison of a double-slit pattern with a pattern formed by five slits, both sets of slits having the same spacing, between the center of one slit and the next.\n",
      "-In optics, any optical instrument or system – a microscope, telescope, or camera – has a principal limit to its resolution due to the physics of diffraction. An optical instrument is said to be diffraction-limited if it has reached this limit of resolution performance. Other factors may affect an optical system's performance, such as lens imperfections or aberrations, but these are caused by errors in the manufacture or calculation of a lens, whereas the diffraction limit is the maximum resolution possible for a theoretically perfect, or ideal, optical system.The diffraction-limited angular resolution, in radians, of an instrument is proportional to the wavelength of the light being observed, and inversely proportional to the diameter of its objective's entrance aperture. For telescopes with circular apertures, the size of the smallest feature in an image that is diffraction limited is the size of the Airy disk. As one decreases the size of the aperture of a telescopic lens, diffraction proportionately increases. At small apertures, such as f/22, most modern lenses are limited only by diffraction and not by aberrations or other imperfections in the construction.\n",
      "-The imaging system's resolution can be limited either by aberration or by diffraction causing blurring of the image. These two phenomena have different origins and are unrelated. Aberrations can be explained by geometrical optics and can in principle be solved by increasing the optical quality of the system. On the other hand, diffraction comes from the wave nature of light and is determined by the finite aperture of the optical elements. The lens' circular aperture is analogous to a two-dimensional version of the single-slit experiment. Light passing through the lens interferes with itself creating a ring-shape diffraction pattern, known as the Airy pattern, if the wavefront of the transmitted light is taken to be spherical or plane over the exit aperture.\n",
      "\n",
      "\n",
      "\n",
      "Which of the following statements accurately depicts the relationship between Gauss's law, electric flux, electric field, and symmetry in electric fields?\n",
      "-While the electric flux is not affected by charges that are not within the closed surface, the net electric field, E can be affected by charges that lie outside the closed surface. While Gauss's law holds for all situations, it is most useful for \"by hand\" calculations when high degrees of symmetry exist in the electric field. Examples include spherical and cylindrical symmetry.\n",
      "-(The quantity 4π appears because 4πr2 is the surface area of the sphere of radius r, which reflects the geometry of the configuration. For details, see the articles Relation between Gauss's law and Coulomb's law and Inverse-square law.) Unit of charge A major difference between the Gaussian system and the ISQ is in the respective definitions of the quantity charge. In the ISQ, a separate base dimension, electric current, with the associated SI unit, the ampere, is associated with electromagnetic phenomena, with the consequence that a unit of electrical charge (1 coulomb = 1 ampere × 1 second) is a physical quantity that cannot be expressed purely in terms of the mechanical units (kilogram, metre, second). On the other hand, in the Gaussian system, the unit of electric charge (the statcoulomb, statC) can be written entirely as a dimensional combination of the non-electrical base units (gram, centimetre, second), as: 1 statC = 1 g1/2⋅cm3/2⋅s−1.For example, Coulomb's law in Gaussian units has no constant: F=Q1GQ2Gr2, where F is the repulsive force between two electrical charges, QG1 and QG2 are the two charges in question, and r is the distance separating them. If QG1 and QG2 are expressed in statC and r in cm, then the unit of F that is coherent with these units is the dyne.\n",
      "-For a closed Gaussian surface, electric flux is given by: where E is the electric field, S is any closed surface, Q is the total electric charge inside the surface S, ε0 is the electric constant (a universal constant, also called the \"permittivity of free space\") (ε0 ≈ 8.854187817×10−12 F/m)This relation is known as Gauss' law for electric fields in its integral form and it is one of Maxwell's equations.\n",
      "-In physics and electromagnetism, Gauss's law, also known as Gauss's flux theorem, (or sometimes simply called Gauss's theorem) is a law relating the distribution of electric charge to the resulting electric field. In its integral form, it states that the flux of the electric field out of an arbitrary closed surface is proportional to the electric charge enclosed by the surface, irrespective of how that charge is distributed. Even though the law alone is insufficient to determine the electric field across a surface enclosing any charge distribution, this may be possible in cases where symmetry mandates uniformity of the field. Where no such symmetry exists, Gauss's law can be used in its differential form, which states that the divergence of the electric field is proportional to the local density of charge.  The law was first formulated by Joseph-Louis Lagrange in 1773, followed by Carl Friedrich Gauss in 1835, both in the context of the attraction of ellipsoids. It is one of Maxwell's equations, which forms the basis of classical electrodynamics. Gauss's law can be used to derive Coulomb's law, and vice versa.\n",
      "-By way of contrast, Gauss's law for electric fields, another of Maxwell's equations, is ΦE= S E⋅dS=Qε0 where E is the electric field, S is any closed surface, Q is the total electric charge inside the surface S, ε0 is the electric constant (a universal constant, also called the \"permittivity of free space\").The flux of E through a closed surface is not always zero; this indicates the presence of \"electric monopoles\", that is, free positive or negative charges.\n",
      "\n",
      "\n",
      "\n",
      "Which of the following statements accurately describes the dimension of an object in a CW complex?\n",
      "-An inductive dimension may be defined inductively as follows. Consider a discrete set of points (such as a finite collection of points) to be 0-dimensional. By dragging a 0-dimensional object in some direction, one obtains a 1-dimensional object. By dragging a 1-dimensional object in a new direction, one obtains a 2-dimensional object. In general one obtains an (n + 1)-dimensional object by dragging an n-dimensional object in a new direction. The inductive dimension of a topological space may refer to the small inductive dimension or the large inductive dimension, and is based on the analogy that, in the case of metric spaces, (n + 1)-dimensional balls have n-dimensional boundaries, permitting an inductive definition based on the dimension of the boundaries of open sets. Moreover, the boundary of a discrete set of points is the empty set, and therefore the empty set can be taken to have dimension -1.Similarly, for the class of CW complexes, the dimension of an object is the largest n for which the n-skeleton is nontrivial. Intuitively, this can be described as follows: if the original space can be continuously deformed into a collection of higher-dimensional triangles joined at their faces with a complicated surface, then the dimension of the object is the dimension of those triangles.\n",
      "-In physics and mathematics, the dimension of a mathematical space (or object) is informally defined as the minimum number of coordinates needed to specify any point within it. Thus, a line has a dimension of one (1D) because only one coordinate is needed to specify a point on it – for example, the point at 5 on a number line. A surface, such as the boundary of a cylinder or sphere, has a dimension of two (2D) because two coordinates are needed to specify a point on it – for example, both a latitude and longitude are required to locate a point on the surface of a sphere. A two-dimensional Euclidean space is a two-dimensional space on the plane. The inside of a cube, a cylinder or a sphere is three-dimensional (3D) because three coordinates are needed to locate a point within these spaces.\n",
      "-The construction, in words The CW complex construction is a straightforward generalization of the following process: A 0-dimensional CW complex is just a set of zero or more discrete points (with the discrete topology).\n",
      "-One sees from the cellular chain complex that the  n -skeleton determines all lower-dimensional homology modules: Hk(X)≅Hk(Xn) for  k<n An important consequence of this cellular perspective is that if a CW-complex has no cells in consecutive dimensions, then all of its homology modules are free. For example, the complex projective space  CPn has a cell structure with one cell in each even dimension; it follows that for  0≤k≤n ,H2k(CPn;Z)≅Z and 0.\n",
      "-The standard CW structure on the real numbers has as 0-skeleton the integers  Z and as 1-cells the intervals  {[n,n+1]:n∈Z} . Similarly, the standard CW structure on  Rn has cubical cells that are products of the 0 and 1-cells from  R . This is the standard cubic lattice cell structure on  Rn Finite-dimensional CW complexes Some examples of finite-dimensional CW complexes are: An n-dimensional sphere. It admits a CW structure with two cells, one 0-cell and one n-cell. Here the n-cell  Dn is attached by the constant mapping from its boundary  Sn−1 to the single 0-cell. An alternative cell decomposition has one (n-1)-dimensional sphere (the \"equator\") and two n-cells that are attached to it (the \"upper hemi-sphere\" and the \"lower hemi-sphere\"). Inductively, this gives  Sn a CW decomposition with two cells in every dimension k such that  0≤k≤n The n-dimensional real projective space. It admits a CW structure with one cell in each dimension.\n",
      "\n",
      "\n",
      "\n",
      "Which of the following statements accurately describes the blocking temperature of an antiferromagnetic layer in a spin valve?\n",
      "-Antiferromagnets can couple to ferromagnets, for instance, through a mechanism known as exchange bias, in which the ferromagnetic film is either grown upon the antiferromagnet or annealed in an aligning magnetic field, causing the surface atoms of the ferromagnet to align with the surface atoms of the antiferromagnet. This provides the ability to \"pin\" the orientation of a ferromagnetic film, which provides one of the main uses in so-called spin valves, which are the basis of magnetic sensors including modern hard disk drive read heads. The temperature at or above which an antiferromagnetic layer loses its ability to \"pin\" the magnetization direction of an adjacent ferromagnetic layer is called the blocking temperature of that layer and is usually lower than the Néel temperature.\n",
      "-A spin valve is a device, consisting of two or more conducting magnetic materials, whose electrical resistance can change between two values depending on the relative alignment of the magnetization in the layers. The resistance change is a result of the giant magnetoresistive effect. The magnetic layers of the device align \"up\" or \"down\" depending on an external magnetic field. In the simplest case, a spin valve consists of a non-magnetic material sandwiched between two ferromagnets, one of which is fixed (pinned) by an antiferromagnet which acts to raise its magnetic coercivity and behaves as a \"hard\" layer, while the other is free (unpinned) and behaves as a \"soft\" layer. Due to the difference in coercivity, the soft layer changes polarity at lower applied magnetic field strength than the hard one. Upon application of a magnetic field of appropriate strength, the soft layer switches polarity, producing two distinct states: a parallel, low-resistance state, and an antiparallel, high-resistance state.\n",
      "-Antiferromagnetic and non-magnetic layers An antiferromagnetic layer is required to pin one of the ferromagnetic layers (i.e., make it fixed or magnetically hard). This results from a large negative exchange coupling energy between ferromagnets and antiferromagnets in contact.\n",
      "The non-magnetic layer is required to decouple the two ferromagnetic layers so that at least one of them remains free (magnetically soft).\n",
      "-Pseudo spin valves The basic operating principles of a pseudo spin valve are identical to that of an ordinary spin valve, but instead of changing the magnetic coercivity of the different ferromagnetic layers by pinning one with an antiferromagnetic layer, the two layers are made of different ferromagnets with different coercivities e.g., NiFe and Co. Note that coercivities are largely an extrinsic property of materials and thus determined by processing conditions.\n",
      "-Equivalently, blocking temperature is the temperature below which a material shows slow relaxation of magnetization.\n",
      "\n",
      "\n",
      "\n",
      "What is the term used in astrophysics to describe light-matter interactions resulting in energy shifts in the radiation field?\n",
      "-The interactions and phenomena summarized in the subjects of radiative transfer and physical optics can result in shifts in the wavelength and frequency of electromagnetic radiation. In such cases, the shifts correspond to a physical energy transfer to matter or other photons rather than being by a transformation between reference frames. Such shifts can be from such physical phenomena as coherence effects or the scattering of electromagnetic radiation whether from charged elementary particles, from particulates, or from fluctuations of the index of refraction in a dielectric medium as occurs in the radio phenomenon of radio whistlers. While such phenomena are sometimes referred to as \"redshifts\" and \"blueshifts\", in astrophysics light-matter interactions that result in energy shifts in the radiation field are generally referred to as \"reddening\" rather than \"redshifting\" which, as a term, is normally reserved for the effects discussed above.In many circumstances scattering causes radiation to redden because entropy results in the predominance of many low-energy photons over few high-energy ones (while conserving total energy). Except possibly under carefully controlled conditions, scattering does not produce the same relative change in wavelength across the whole spectrum; that is, any calculated z is generally a function of wavelength. Furthermore, scattering from random media generally occurs at many angles, and z is a function of the scattering angle. If multiple scattering occurs, or the scattering particles have relative motion, then there is generally distortion of spectral lines as well.In interstellar astronomy, visible spectra can appear redder due to scattering processes in a phenomenon referred to as interstellar reddening—similarly Rayleigh scattering causes the atmospheric reddening of the Sun seen in the sunrise or sunset and causes the rest of the sky to have a blue color. This phenomenon is distinct from redshifting because the spectroscopic lines are not shifted to other wavelengths in reddened objects and there is an additional dimming and distortion associated with the phenomenon due to photons being scattered in and out of the line of sight.\n",
      "-The opposite of a redshift is a blueshift. A blueshift is any decrease in wavelength (increase in energy), with a corresponding increase in frequency, of an electromagnetic wave. In visible light, this shifts a color towards the blue end of the spectrum.\n",
      "-In physics, a redshift is an increase in the wavelength, and corresponding decrease in the frequency and photon energy, of electromagnetic radiation (such as light). The opposite change, a decrease in wavelength and simultaneous increase in frequency and energy, is known as a negative redshift, or blueshift. The terms derive from the colours red and blue which form the extremes of the visible light spectrum. The three main causes of electromagnetic redshift in astronomy and cosmology are, first, radiation traveling between objects that are moving apart (\"relativistic\" redshift, an example of the relativistic Doppler effect); second, the gravitational redshift due to radiation traveling towards an object in a weaker gravitational potential; and third, the cosmological redshift due to radiation traveling through expanding space. All sufficiently distant light sources show redshift for a velocity proportionate to their distance from Earth, a fact known as Hubble's law.\n",
      "-Examples of strong redshifting are a gamma ray perceived as an X-ray, or initially visible light perceived as radio waves. Subtler redshifts are seen in the spectroscopic observations of astronomical objects, and are used in terrestrial technologies such as Doppler radar and radar guns.\n",
      "Other physical processes exist that can lead to a shift in the frequency of electromagnetic radiation, including scattering and optical effects; however, the resulting changes are distinguishable from (astronomical) redshift and are not generally referred to as such (see section on physical optics and radiative transfer).\n",
      "-In physics and general relativity, gravitational redshift (known as Einstein shift in older literature) is the phenomenon that electromagnetic waves or photons travelling out of a gravitational well (seem to) lose energy. This loss of energy corresponds to a decrease in the wave frequency and increase in the wavelength, known more generally as a redshift. The opposite effect, in which photons (seem to) gain energy when travelling into a gravitational well, is known as a gravitational blueshift (a type of blueshift). The effect was first described by Einstein in 1907, eight years before his publication of the full theory of relativity.\n",
      "\n",
      "\n",
      "\n",
      "What is the role of axioms in a formal theory?\n",
      "-In mathematics and logic, an axiomatic system is any set of axioms from which some or all axioms can be used in conjunction to logically derive theorems. A theory is a consistent, relatively-self-contained body of knowledge which usually contains an axiomatic system and all its derived theorems. An axiomatic system that is completely described is a special kind of formal system. A formal theory is an axiomatic system (usually formulated within model theory) that describes a set of sentences that is closed under logical implication. A formal proof is a complete rendition of a mathematical proof within a formal system.\n",
      "-It is common in mathematics to choose a number of hypotheses within a given language and declare that the theory consists of all statements provable from these hypotheses. These hypotheses form the foundational basis of the theory and are called axioms or postulates. The field of mathematics known as proof theory studies formal languages, axioms and the structure of proofs.\n",
      "-Versus theorems Theories are distinct from theorems. A theorem is derived deductively from axioms (basic assumptions) according to a formal system of rules, sometimes as an end in itself and sometimes as a first step toward being tested or applied in a concrete situation; theorems are said to be true in the sense that the conclusions of a theorem are logical consequences of the axioms. Theories are abstract and conceptual, and are supported or challenged by observations in the world. They are 'rigorously tentative', meaning that they are proposed as true and expected to satisfy careful examination to account for the possibility of faulty inference or incorrect observation. Sometimes theories are incorrect, meaning that an explicit set of observations contradicts some fundamental objection or application of the theory, but more often theories are corrected to conform to new observations, by restricting the class of phenomena the theory applies to or changing the assertions made. An example of the former is the restriction of classical mechanics to phenomena involving macroscopic length scales and particle speeds much lower than the speed of light.\n",
      "-The theorems are the logical consequences of the axioms, that is, the statements that can be obtained from the axioms by using the laws of deductive logic.An interpretation of an axiomatic system is some particular way of giving concrete meaning to the primitives of that system. If this association of meanings makes the axioms of the system true statements, then the interpretation is called a model of the system. In a model, all the theorems of the system are automatically true statements.\n",
      "-Thus, an axiom is an elementary basis for a formal logic system that together with the rules of inference define a deductive system.\n",
      "Examples This section gives examples of mathematical theories that are developed entirely from a set of non-logical axioms (axioms, henceforth). A rigorous treatment of any of these topics begins with a specification of these axioms.\n",
      "\n",
      "\n",
      "\n",
      "What did Fresnel predict and verify with regards to total internal reflections?\n",
      "-Similarly, Fresnel calculated and verified the angle of incidence that would give a 90° phase difference after three reflections at the same angle, and four reflections at the same angle. In each case there were two solutions, and in each case he reported that the larger angle of incidence gave an accurate circular polarization (for an initial linear polarization at 45° to the plane of reflection). For the case of three reflections he also tested the smaller angle, but found that it gave some coloration due to the proximity of the critical angle and its slight dependence on wavelength. (Compare Fig. 2 above, which shows that the phase difference δ is more sensitive to the refractive index for smaller angles of incidence.) For added confidence, Fresnel predicted and verified that four total internal reflections at 68°27' would give an accurate circular polarization if two of the reflections had water as the external medium while the other two had air, but not if the reflecting surfaces were all wet or all dry.\n",
      "-For glass with a refractive index of 1.51, Fresnel calculated that a 45° phase difference between the two reflection coefficients (hence a 90° difference after two reflections) required an angle of incidence of 48°37' or 54°37'. He cut a rhomb to the latter angle and found that it performed as expected. Thus the specification of the Fresnel rhomb was completed. Similarly, Fresnel calculated and verified the angle of incidence that would give a 90° phase difference after three reflections at the same angle, and four reflections at the same angle. In each case there were two solutions, and in each case he reported that the larger angle of incidence gave an accurate circular polarization (for an initial linear polarization at 45° to the plane of reflection). For the case of three reflections he also tested the smaller angle, but found that it gave some coloration due to the proximity of the critical angle and its slight dependence on wavelength. (Compare Fig. 13 above, which shows that the phase difference δ is more sensitive to the refractive index for smaller angles of incidence.) For added confidence, Fresnel predicted and verified that four total internal reflections at 68°27' would give an accurate circular polarization if two of the reflections had water as the external medium while the other two had air, but not if the reflecting surfaces were all wet or all dry.Fresnel's deduction of the phase shift in TIR is thought to have been the first occasion on which a physical meaning was attached to the argument of a complex number. Although this reasoning was applied without the benefit of knowing that light waves were electromagnetic, it passed the test of experiment, and survived remarkably intact after James Clerk Maxwell changed the presumed nature of the waves. Meanwhile, Fresnel's success inspired James MacCullagh and Augustin-Louis Cauchy, beginning in 1836, to analyze reflection from metals by using the Fresnel equations with a complex refractive index. The imaginary part of the complex index represents absorption.The term critical angle, used for convenience in the above narrative, is anachronistic: it apparently dates from 1873.In the 20th century, quantum electrodynamics reinterpreted the amplitude of an electromagnetic wave in terms of the probability of finding a photon. In this framework, partial transmission and frustrated TIR concern the probability of a photon crossing a boundary, and attenuated total reflectance concerns the probability of a photon being absorbed on the other side.\n",
      "-In 1816, Fresnel offered his first attempt at a wave-based theory of chromatic polarization. Without (yet) explicitly invoking transverse waves, his theory treated the light as consisting of two perpendicularly polarized components. In 1817 he noticed that plane-polarized light seemed to be partly depolarized by total internal reflection, if initially polarized at an acute angle to the plane of incidence. By including total internal reflection in a chromatic-polarization experiment, he found that the apparently depolarized light was a mixture of components polarized parallel and perpendicular to the plane of incidence, and that the total reflection introduced a phase difference between them. Choosing an appropriate angle of incidence (not yet exactly specified) gave a phase difference of 1/8 of a cycle. Two such reflections from the \"parallel faces\" of \"two coupled prisms\" gave a phase difference of 1/4 of a cycle. In that case, if the light was initially polarized at 45° to the plane of incidence and reflection, it appeared to be completely depolarized after the two reflections. These findings were reported in a memoir submitted and read to the French Academy of Sciences in November 1817.In 1821, Fresnel derived formulae equivalent to his sine and tangent laws (Eqs. (19) and (20), above)  by modeling light waves as transverse elastic waves with vibrations perpendicular to what had previously been called the plane of polarization. Using old experimental data, he promptly confirmed that the equations correctly predicted the direction of polarization of the reflected beam when the incident beam was polarized at 45° to the plane of incidence, for light incident from air onto glass or water. The experimental confirmation was reported in a \"postscript\" to the work in which Fresnel expounded his mature theory of chromatic polarization, introducing transverse waves. Details of the derivation were given later, in a memoir read to the academy in January 1823. The derivation combined conservation of energy with continuity of the tangential vibration at the interface, but failed to allow for any condition on the normal component of vibration.Meanwhile, in a memoir submitted in December 1822, Fresnel coined the terms linear polarization, circular polarization, and elliptical polarization. For circular polarization, the two perpendicular components were a quarter-cycle (±90°) out of phase.\n",
      "-Stage 3: Calculation of angles (1823) The concept of circular polarization was useful in the memoir of January 1823, containing the detailed derivations of the sine and tangent laws: in that same memoir, Fresnel found that for angles of incidence greater than the critical angle, the resulting reflection coefficients were complex with unit magnitude. Noting that the magnitude represented the amplitude ratio as usual, he guessed that the argument represented the phase shift, and verified the hypothesis by experiment. The verification involved calculating the angle of incidence that would introduce a total phase difference of 90° between the s and p components, for various numbers of total internal reflections at that angle (generally there were two solutions), subjecting light to that number of total internal reflections at that angle of incidence, with an initial linear polarization at 45° to the plane of incidence, and checking that the final polarization was circular.This procedure was necessary because, with the technology of the time, one could not measure the s and p phase-shifts directly, and one could not measure an arbitrary degree of ellipticality of polarization, such as might be caused by the difference between the phase shifts. But one could verify that the polarization was circular, because the brightness of the light was then insensitive to the orientation of the analyzer.\n",
      "-The new terminology was useful in the memoir of January 1823, containing the detailed derivations of the sine and tangent laws: in that same memoir, Fresnel found that for angles of incidence greater than the critical angle, the resulting reflection coefficients were complex with unit magnitude. Noting that the magnitude represented the amplitude ratio as usual, he guessed that the argument represented the phase shift, and verified the hypothesis by experiment. The verification involved calculating the angle of incidence that would introduce a total phase difference of 90° between the s and p components, for various numbers of total internal reflections at that angle (generally there were two solutions), subjecting light to that number of total internal reflections at that angle of incidence, with an initial linear polarization at 45° to the plane of incidence, and checking that the final polarization was circular.This procedure was necessary because, with the technology of the time, one could not measure the s and p phase-shifts directly, and one could not measure an arbitrary degree of ellipticality of polarization, such as might be caused by the difference between the phase shifts. But one could verify that the polarization was circular, because the brightness of the light was then insensitive to the orientation of the analyzer.\n",
      "\n",
      "\n",
      "\n",
      "What is the relationship between the Wigner function and the density matrix operator?\n",
      "-The density matrix operator may also be realized in phase space. Under the Wigner map, the density matrix transforms into the equivalent Wigner function, W(x,p)=def1πℏ∫−∞∞ψ∗(x+y)ψ(x−y)e2ipy/ℏdy.\n",
      "The equation for the time evolution of the Wigner function, known as Moyal equation, is then the Wigner-transform of the above von Neumann equation, ∂W(x,p,t)∂t=−{{W(x,p,t),H(x,p)}}, where  H(x,p) is the Hamiltonian, and  {{⋅,⋅}} is the Moyal bracket, the transform of the quantum commutator.\n",
      "The evolution equation for the Wigner function is then analogous to that of its classical limit, the Liouville equation of classical physics. In the limit of vanishing Planck's constant  ℏ ,  W(x,p,t) reduces to the classical Liouville probability density function in phase space.\n",
      "-The Wigner transformation is a general invertible transformation of an operator Ĝ on a Hilbert space to a function g(x, p) on phase space and is given by g(x,p)=∫−∞∞dseips/ℏ⟨x−s2|G^|x+s2⟩.\n",
      "Hermitian operators map to real functions. The inverse of this transformation, from phase space to Hilbert space, is called the Weyl transformation: ⟨x|G^|y⟩=∫−∞∞dpheip(x−y)/ℏg(x+y2,p) (not to be confused with the distinct Weyl transformation in differential geometry).\n",
      "The Wigner function W(x, p) discussed here is thus seen to be the Wigner transform of the density matrix operator ρ̂. Thus the trace of an operator with the density matrix Wigner-transforms to the equivalent phase-space integral overlap of g(x, p) with the Wigner function.\n",
      "-It is symmetric in x and p: W(x,p)=1πℏ∫−∞∞φ∗(p+q)φ(p−q)e−2ixq/ℏdq, where φ is the normalized momentum-space wave function, proportional to the Fourier transform of ψ.\n",
      "In 3D, W(r→,p→)=1(2π)3∫ψ∗(r→+ℏs→/2)ψ(r→−ℏs→/2)eip→⋅s→d3s.\n",
      "In the general case, which includes mixed states, it is the Wigner transform of the density matrix: where ⟨x|ψ⟩ = ψ(x). This Wigner transformation (or map) is the inverse of the Weyl transform, which maps phase-space functions to Hilbert-space operators, in Weyl quantization.\n",
      "Thus, the Wigner function is the cornerstone of quantum mechanics in phase space.\n",
      "In 1949, José Enrique Moyal elucidated how the Wigner function provides the integration measure (analogous to a probability density function) in phase space, to yield expectation values from phase-space c-number functions g(x, p) uniquely associated to suitably ordered operators Ĝ through Weyl's transform (see Wigner–Weyl transform and property 7 below), in a manner evocative of classical probability theory.\n",
      "Specifically, an operator's Ĝ expectation value is a \"phase-space average\" of the Wigner transform of that operator: \n",
      "-The Wigner transform of the von Neumann evolution equation of the density matrix in the Schrödinger picture is Moyal's evolution equation for the Wigner function: where H(x, p) is the Hamiltonian, and {{⋅, ⋅}} is the Moyal bracket. In the classical limit, ħ → 0, the Moyal bracket reduces to the Poisson bracket, while this evolution equation reduces to the Liouville equation of classical statistical mechanics.\n",
      "-This bound disappears in the classical limit, ħ → 0. In this limit, W(x, p) reduces to the probability density in coordinate space x, usually highly localized, multiplied by δ-functions in momentum: the classical limit is \"spiky\". Thus, this quantum-mechanical bound precludes a Wigner function which is a perfectly localized δ-function in phase space, as a reflection of the uncertainty principle.10. The Wigner transformation is simply the Fourier transform of the antidiagonals of the density matrix, when that matrix is expressed in a position basis.\n",
      "\n",
      "\n",
      "\n",
      "What is one of the examples of the models proposed by cosmologists and theoretical physicists without the cosmological or Copernican principles that can be used to address specific issues in the Lambda-CDM model and distinguish between current models and other possible models?\n",
      "-Violations of the cosmological principle The ΛCDM model has been shown to satisfy the cosmological principle, which states that, on a large-enough scale, the universe looks the same in all directions (isotropy) and from every location (homogeneity); \"the universe looks the same whoever and wherever you are.\" The cosmological principle exists because when the predecessors of the ΛCDM model were first being developed, there was not sufficient data available to distinguish between more complex anisotropic or inhomogeneous models, so homogeneity and isotropy were assumed to simplify the models, and the assumptions were carried over into the ΛCDM model. However, recent findings have suggested that violations of the cosmological principle, especially of isotropy, exist. These violations have called the ΛCDM model into question, with some authors suggesting that the cosmological principle is now obsolete or that the Friedmann–Lemaître–Robertson–Walker metric breaks down in the late universe. This has additional implications for the validity of the cosmological constant in the ΛCDM model, as dark energy is implied by observations only if the cosmological principle is true.\n",
      "-The standard model of cosmology, the Lambda-CDM model, assumes the Copernican principle and the more general cosmological principle. Some cosmologists and theoretical physicists have created models without the cosmological or Copernican principles to constrain the values of observational results, to address specific known issues in the Lambda-CDM model, and to propose tests to distinguish between current models and other possible models.\n",
      "-The ΛCDM (Lambda cold dark matter) or Lambda-CDM model is a parameterization of the Big Bang cosmological model in which the universe contains three major components: first, a cosmological constant denoted by Lambda (Greek Λ) associated with dark energy; second, the postulated cold dark matter (abbreviated CDM); and third, ordinary matter. It is frequently referred to as the standard model of Big Bang cosmology because it is the simplest model that provides a reasonably good account of the following properties of the cosmos: the existence and structure of the cosmic microwave background the large-scale structure in the distribution of galaxies the observed abundances of hydrogen (including deuterium), helium, and lithium the accelerating expansion of the universe observed in the light from distant galaxies and supernovaeThe model assumes that general relativity is the correct theory of gravity on cosmological scales. It emerged in the late 1990s as a concordance cosmology, after a period of time when disparate observed properties of the universe appeared mutually inconsistent, and there was no consensus on the makeup of the energy density of the universe.\n",
      "-The model is inconsistent with the emerging Lambda-CDM model of cosmology. Contentions include the absence of an explanation in the Standard Model of particle physics for the observed amount of cold dark matter (CDM) and its contributions to dark energy, which are many orders of magnitude too large. It is also difficult to accommodate the observed predominance of matter over antimatter (matter/antimatter asymmetry). The isotropy and homogeneity of the visible universe over large distances seems to require a mechanism like cosmic inflation, which would also constitute an extension of the Standard Model.Currently, no proposed theory of everything has been widely accepted or verified.\n",
      "-The ΛCDM model can be extended by adding cosmological inflation, quintessence, and other elements that are current areas of speculation and research in cosmology.\n",
      "Some alternative models challenge the assumptions of the ΛCDM model. Examples of these are modified Newtonian dynamics, entropic gravity, modified gravity, theories of large-scale variations in the matter density of the universe, bimetric gravity, scale invariance of empty space, and decaying dark matter (DDM).\n",
      "\n",
      "\n",
      "\n",
      "What is the Roche limit?\n",
      "-In celestial mechanics, the Roche limit, also called Roche radius, is the distance from a celestial body within which a second celestial body, held together only by its own force of gravity, will disintegrate because the first body's tidal forces exceed the second body's self-gravitation. Inside the Roche limit, orbiting material disperses and forms rings, whereas outside the limit, material tends to coalesce. The Roche radius depends on the radius of the first body and on the ratio of the bodies' densities.\n",
      "-The Roche limit typically applies to a satellite's disintegrating due to tidal forces induced by its primary, the body around which it orbits. Parts of the satellite that are closer to the primary are attracted more strongly by gravity from the primary than parts that are farther away; this disparity effectively pulls the near and far parts of the satellite apart from each other, and if the disparity (combined with any centrifugal effects due to the object's spin) is larger than the force of gravity holding the satellite together, it can pull the satellite apart. Some real satellites, both natural and artificial, can orbit within their Roche limits because they are held together by forces other than gravitation. Objects resting on the surface of such a satellite would be lifted away by tidal forces. A weaker satellite, such as a comet, could be broken up when it passes within its Roche limit.\n",
      "-The limiting distance to which a satellite can approach without breaking up depends on the rigidity of the satellite. At one extreme, a completely rigid satellite will maintain its shape until tidal forces break it apart. At the other extreme, a highly fluid satellite gradually deforms leading to increased tidal forces, causing the satellite to elongate, further compounding the tidal forces and causing it to break apart more readily.\n",
      "-But note that, as defined above, the Roche limit refers to a body held together solely by the gravitational forces which cause otherwise unconnected particles to coalesce, thus forming the body in question. The Roche limit is also usually calculated for the case of a circular orbit, although it is straightforward to modify the calculation to apply to the case (for example) of a body passing the primary on a parabolic or hyperbolic trajectory.\n",
      "-When a body (body 1) is acted on by the gravity of another body (body 2), the field can vary significantly on body 1 between the side of the body facing body 2 and the side facing away from body 2. Figure 4 shows the differential force of gravity on a spherical body (body 1) exerted by another body (body 2). These so-called tidal forces cause strains on both bodies and may distort them or even, in extreme cases, break one or the other apart. The Roche limit is the distance from a planet at which tidal effects would cause an object to disintegrate because the differential force of gravity from the planet overcomes the attraction of the parts of the object for one another. These strains would not occur if the gravitational field were uniform, because a uniform field only causes the entire body to accelerate together in the same direction and at the same rate.\n",
      "\n",
      "\n",
      "\n",
      "What is Martin Heidegger's view on the relationship between time and human existence?\n",
      "-Henri Bergson believed that time was neither a real homogeneous medium nor a mental construct, but possesses what he referred to as Duration. Duration, in Bergson's view, was creativity and memory as an essential component of reality.According to Martin Heidegger we do not exist inside time, we are time. Hence, the relationship to the past is a present awareness of having been, which allows the past to exist in the present. The relationship to the future is the state of anticipating a potential possibility, task, or engagement. It is related to the human propensity for caring and being concerned, which causes \"being ahead of oneself\" when thinking of a pending occurrence. Therefore, this concern for a potential occurrence also allows the future to exist in the present. The present becomes an experience, which is qualitative instead of quantitative. Heidegger seems to think this is the way that a linear relationship with time, or temporal existence, is broken or transcended.\n",
      "-In Being and Time (1927; transl. 1962), Martin Heidegger argues that the concept of time prevalent in all Western thought has largely remained unchanged since the definition offered by Aristotle in the Physics. Heidegger says, \"Aristotle's essay on time is the first detailed Interpretation of this phenomenon [time] which has come down to us. Every subsequent account of time, including Henri Bergson's, has been essentially determined by it.\" Aristotle defined time as \"the number of movement in respect of before and after\". By defining time in this way Aristotle privileges what is present-at-hand, namely the \"presence\" of time. Heidegger argues in response that \"entities are grasped in their Being as 'presence'; this means that they are understood with regard to a definite mode of time – the 'Present'\". Central to Heidegger's own philosophical project is the attempt to gain a more authentic understanding of time. Heidegger considers time to be the unity of three ecstases: the past, the present, and the future.\n",
      "-The presence to which Heidegger refers is both a presence as in a \"now\" and also a presence as in an eternal present, as one might associate with God or the \"eternal\" laws of science. This hypostatized (underlying) belief in presence is undermined by novel phenomenological ideas, such that presence itself does not subsist, but comes about primordially through the action of our futural projection, our realization of finitude and the reception or rejection of the traditions of our time.In his short work Intuition of the Instant, Gaston Bachelard attempts to navigate beyond, or parallel to, the Western concept of 'time as duration' – as the imagined trajectorial space of movement. He distinguishes between two foundations of time: time viewed as a duration, and time viewed as an instant. Bachelard then follows this second phenomenon of time and concludes that time as a duration does not exist, but is created as a necessary mediation for increasingly complex beings to persist. The reality of time for existence, though, is in fact a reprisal of the instant, the gestation of all existence every instant, the eternal death that gives life.\n",
      "-Heidegger's being-for-death The German philosopher Martin Heidegger wrote about death as something conclusively determined, in the sense that it is inevitable for every human being, while on the other hand, it unmasks its indeterminate nature via the truth that one never knows when or how death is going to come. Heidegger does not engage in speculation about whether being after death is possible. He argues that all human existence is embedded in time: past, present, future, and when considering the future, we encounter the notion of death. This then creates angst. Angst can create a clear understanding in one that death is a possible mode of existence, which Heidegger described as \"clearing\". Thus, angst can lead to a freedom about existence, but only if people can stop denying their mortality (as expressed in Heidegger's terminology as \"stop denying being-for-death\").\n",
      "-In philosophy, temporality refers to the idea of a linear progression of past, present, and future. The term is frequently used, however, in the context of critiques of commonly held ideas of linear time. In social sciences, temporality is studied with respect to the human perception of time and the social organization of time. The perception of time underwent significant changes in the three hundred years between the Middle Ages and modernity.Examples in continental philosophy of philosophers raising questions of temporality include Edmund Husserl's analysis of internal time consciousness, Martin Heidegger's Being and Time, J. M. E. McTaggart's article \"The Unreality of Time\", George Herbert Mead's Philosophy of the Present, and Jacques Derrida's criticisms of Husserl's analysis.  Temporality is \"deeply intertwined with the rhetorical act of harnessing and subverting power in the unfolding struggle for justice.\" Temporalities, particularly in European settler colonialism, have been observed in critical theory as a tool for both subjugation and oppression of Indigenous communities, and Native resistance to that oppression.\n",
      "\n",
      "\n",
      "\n",
      "What is the \"ultraviolet catastrophe\"?\n",
      "-Since the first use of this term, it has also been used for other predictions of a similar nature, as in quantum electrodynamics and such cases as ultraviolet divergence.\n",
      "-The ultraviolet catastrophe, also called the Rayleigh–Jeans catastrophe, was the prediction of late 19th century/early 20th century classical physics that an ideal black body at thermal equilibrium would emit an unbounded quantity of energy as wavelength decreased into the ultraviolet range.: 6–7 The term \"ultraviolet catastrophe\" was first used in 1911 by Paul Ehrenfest, but the concept originated with the 1900 statistical derivation of the Rayleigh–Jeans law.  The phrase refers to the fact that the empirically derived Rayleigh–Jeans law, which accurately predicted experimental results at large wavelengths, failed to do so for short wavelengths. (See the image for further elaboration.) As the theory diverged from empirical observations when these frequencies reached the ultraviolet region of the electromagnetic spectrum, there was a problem. This problem was later found to be due to a property of quanta as proposed by Max Planck: There could be no fraction of a discrete energy package already carrying minimal energy.\n",
      "-This formula is obtained from the equipartition theorem of classical statistical mechanics which states that all harmonic oscillator modes (degrees of freedom) of a system at equilibrium have an average energy of  kBT The \"ultraviolet catastrophe\" is the expression of the fact that the formula misbehaves at higher frequencies, i.e.  Bν(T)→∞ as  ν→∞ An example, from Mason's A History of the Sciences, illustrates multi-mode vibration via a piece of string. As a natural vibrator, the string will oscillate with specific modes (the standing waves of a string in harmonic resonance), dependent on the length of the string. In classical physics, a radiator of energy will act as a natural vibrator. Additionally, since each mode will have the same energy, most of the energy in a natural vibrator will be in the smaller wavelengths and higher frequencies, where most of the modes are.\n",
      "-The name comes from the earliest example of such a divergence, the \"ultraviolet catastrophe\" first encountered in understanding blackbody radiation. According to classical physics at the end of the nineteenth century, the quantity of radiation in the form of light released at any specific wavelength should increase with decreasing wavelength—in particular, there should be considerably more ultraviolet light released from a blackbody radiator than infrared light. Measurements showed the opposite, with maximal energy released at intermediate wavelengths, suggesting a failure of classical mechanics. This problem eventually led to the development of quantum mechanics.\n",
      "-According to classical electromagnetism, the number of electromagnetic modes in a 3-dimensional cavity, per unit frequency, is proportional to the square of the frequency. This implies that the radiated power per unit frequency should be proportional to frequency squared. Thus, both the power at a given frequency and the total radiated power is unlimited as higher and higher frequencies are considered: this is unphysical as the total radiated power of a cavity is not observed to be infinite, a point that was made independently by Einstein and by Lord Rayleigh and Sir James Jeans in 1905.\n",
      "\n",
      "\n",
      "\n",
      "What is the most popular explanation for the shower-curtain effect?\n",
      "-Bernoulli effect hypothesis The most popular explanation given for the shower-curtain effect is Bernoulli's principle. Bernoulli's principle states that an increase in velocity results in a decrease in pressure. This theory presumes that the water flowing out of a shower head causes the air through which the water moves to start flowing in the same direction as the water. This movement would be parallel to the plane of the shower curtain. If air is moving across the inside surface of the shower curtain, Bernoulli's principle says the air pressure there will drop. This would result in a pressure differential between the inside and outside, causing the curtain to move inward. It would be strongest when the gap between the bather and the curtain is smallest, resulting in the curtain attaching to the bather.\n",
      "-Coandă effect The Coandă effect, also known as \"boundary layer attachment\", is the tendency of a moving fluid to adhere to an adjacent wall.\n",
      "Condensation A hot shower will produce steam that condenses on the shower side of the curtain, lowering the pressure there. In a steady state the steam will be replaced by new steam delivered by the shower but in reality the water temperature will fluctuate and lead to times when the net steam production is negative.\n",
      "Air pressure Colder dense air outside and hot less dense air inside causes higher air pressure on the outside to force the shower curtain inwards to equalise the air pressure, this can be observed simply when the bathroom door is open allowing cold air into the bathroom.\n",
      "-The shower-curtain effect in physics describes the phenomenon of a shower curtain being blown inward when a shower is running. The problem of identifying the cause of this effect has been featured in Scientific American magazine, with several theories given to explain the phenomenon but no definite conclusion.\n",
      "The shower-curtain effect may also be used to describe the observation how nearby phase front distortions of an optical wave are more severe than remote distortions of the same amplitude.\n",
      "-Buoyancy hypothesis Also called Chimney effect or Stack effect, observes that warm air (from the hot shower) rises out over the shower curtain as cooler air (near the floor) pushes in under the curtain to replace the rising air. By pushing the curtain in towards the shower, the (short range) vortex and Coandă effects become more significant. However, the shower-curtain effect persists when cold water is used, implying that this cannot be the only mechanism at work.\n",
      "-Many shower curtains come with features to reduce the shower-curtain effect. They may have adhesive suction cups on the bottom edges of the curtain, which are then pushed onto the sides of the shower when in use. Others may have magnets at the bottom, though these are not effective on acrylic or fiberglass tubs.\n",
      "It is possible to use a telescopic shower curtain rod to block the curtain on its lower part and to prevent it from sucking inside.\n",
      "Hanging the curtain rod higher or lower, or especially further away from the shower head, can reduce the effect. A (convex) curved shower rod can also be used to hold the curtain against the inside wall of a tub.\n",
      "\n",
      "\n",
      "\n",
      "What is the butterfly effect?\n",
      "-A related way to interpret the butterfly effect is to see it as highlighting the difference between the application of the notion of causality in physics and a more general use of causality as represented by Mackie's INUS conditions. In classical (Newtonian) physics, in general, only those conditions are (explicitly) taken into account, that are both necessary and sufficient. For instance, when a massive sphere is caused to roll down a slope starting from a point of unstable equilibrium, then its velocity is assumed to be caused by the force of gravity accelerating it; the small push that was needed to set it into motion is not explicitly dealt with as a cause. In order to be a physical cause there must be a certain proportionality with the ensuing effect. A distinction is drawn between triggering and causation of the ball's motion. By the same token the butterfly can be seen as triggering a tornado, its cause being assumed to be seated in the atmospherical energies already present beforehand, rather than in the movements of a butterfly.\n",
      "-In chaos theory, the butterfly effect is the sensitive dependence on initial conditions in which a small change in one state of a deterministic nonlinear system can result in large differences in a later state.\n",
      "-Sensitivity to initial conditions Sensitivity to initial conditions means that each point in a chaotic system is arbitrarily closely approximated by other points that have significantly different future paths or trajectories. Thus, an arbitrarily small change or perturbation of the current trajectory may lead to significantly different future behavior.Sensitivity to initial conditions is popularly known as the \"butterfly effect\", so-called because of the title of a paper given by Edward Lorenz in 1972 to the American Association for the Advancement of Science in Washington, D.C., entitled Predictability: Does the Flap of a Butterfly's Wings in Brazil set off a Tornado in Texas?. The flapping wing represents a small change in the initial condition of the system, which causes a chain of events that prevents the predictability of large-scale phenomena. Had the butterfly not flapped its wings, the trajectory of the overall system could have been vastly different.\n",
      "-Theories in physics like the butterfly effect from chaos theory open up the possibility of a type of distributed parameter systems in causality. The butterfly effect theory proposes: \"Small variations of the initial condition of a nonlinear dynamical system may produce large variations in the long term behavior of the system.\" This opens up the opportunity to understand a distributed causality.\n",
      "-Recurrence, the approximate return of a system toward its initial conditions, together with sensitive dependence on initial conditions, are the two main ingredients for chaotic motion. They have the practical consequence of making complex systems, such as the weather, difficult to predict past a certain time range (approximately a week in the case of weather) since it is impossible to measure the starting atmospheric conditions completely accurately.\n",
      "\n",
      "\n",
      "\n",
      "What is the 'reactive Leidenfrost effect' observed in non-volatile materials?\n",
      "-The Leidenfrost effect is a physical phenomenon in which a liquid, close to a surface that is significantly hotter than the liquid's boiling point, produces an insulating vapor layer that keeps the liquid from boiling rapidly. Because of this repulsive force, a droplet hovers over the surface, rather than making physical contact with it. The effect is named after the German doctor Johann Gottlob Leidenfrost, who described it in A Tract About Some Qualities of Common Water.\n",
      "-The effect can be seen as drops of water are sprinkled onto a pan at various times as it heats up. Initially, as the temperature of the pan is just below 100 °C (212 °F), the water flattens out and slowly evaporates, or if the temperature of the pan is well below 100 °C (212 °F), the water stays liquid. As the temperature of the pan rises above 100 °C (212 °F), the water droplets hiss when touching the pan, and these droplets evaporate quickly. When the temperature exceeds the Leidenfrost point, the Leidenfrost effect appears. On contact with the pan, the water droplets bunch up into small balls of water and skitter around, lasting much longer than when the temperature of the pan was lower. This effect works until a much higher temperature causes any further drops of water to evaporate too quickly to cause this effect.\n",
      "-Non-volatile materials were discovered in 2015 to also exhibit a 'reactive Leidenfrost effect', whereby solid particles were observed to float above hot surfaces and skitter around erratically. Detailed characterization of the reactive Leidenfrost effect was completed for small particles of cellulose (~0.5 mm) on high temperature polished surfaces by high speed photography. Cellulose was shown to decompose to short-chain oligomers which melt and wet smooth surfaces with increasing heat transfer associated with increasing surface temperature. Above 675 °C (1,247 °F), cellulose was observed to exhibit transition boiling with violent bubbling and associated reduction in heat transfer. Liftoff of the cellulose droplet (depicted at the right) was observed to occur above about 750 °C (1,380 °F), associated with a dramatic reduction in heat transfer.High speed photography of the reactive Leidenfrost effect of cellulose on porous surfaces (macroporous alumina) was also shown to suppress the reactive Leidenfrost effect and enhance overall heat transfer rates to the particle from the surface. The new phenomenon of a 'reactive Leidenfrost (RL) effect' was characterized by a dimensionless quantity, (φRL= τconv/τrxn), which relates the time constant of solid particle heat transfer to the time constant of particle reaction, with the reactive Leidenfrost effect occurring for 10−1< φRL< 10+1. The reactive Leidenfrost effect with cellulose will occur in numerous high temperature applications with carbohydrate polymers, including biomass conversion to biofuels, preparation and cooking of food, and tobacco use.The Leidenfrost effect has also been used as a means to promote chemical change of various organic liquids through their conversion by thermal decomposition into various products. Examples include decomposition of ethanol, diethyl carbonate, and glycerol.\n",
      "-The Leidenfrost point signifies the onset of stable film boiling. It represents the point on the boiling curve where the heat flux is at the minimum and the surface is completely covered by a vapor blanket. Heat transfer from the surface to the liquid occurs by conduction and radiation through the vapour. In 1756, Leidenfrost observed that water droplets supported by the vapor film slowly evaporate as they move about on the hot surface. As the surface temperature is increased, radiation through the vapor film becomes more significant and the heat flux increases with increasing excess temperature.\n",
      "-This is most commonly seen when cooking, when drops of water are sprinkled onto a hot pan. If the pan's temperature is at or above the Leidenfrost point, which is approximately 193 °C (379 °F) for water, the water skitters across the pan and takes longer to evaporate than it would take if the water droplets had been sprinkled onto a cooler pan.\n",
      "\n",
      "\n",
      "\n",
      "What is reciprocal length or inverse length?\n",
      "-Reciprocal length or inverse length is a quantity or measurement used in several branches of science and mathematics. As the reciprocal of length, common units used for this measurement include the reciprocal metre or inverse metre (symbol: m−1), the reciprocal centimetre or inverse centimetre (symbol: cm−1).\n",
      "-Quantities measured in reciprocal length include: absorption coefficient or attenuation coefficient, in materials science curvature of a line, in mathematics gain, in laser physics magnitude of vectors in reciprocal space, in crystallography more generally any spatial frequency e.g. in cycles per unit length optical power of a lens, in optics rotational constant of a rigid rotor, in quantum mechanics wavenumber, or magnitude of a wavevector, in spectroscopy density of a linear feature in hydrology and other fields; see kilometre per square kilometre surface area to volume ratioIn optics, the dioptre is a unit equivalent to reciprocal metre.\n",
      "-The energy is inversely proportional to the size of the unit of which the reciprocal is used, and is proportional to the number of reciprocal length units. For example, in terms of energy, one reciprocal metre equals 10−2 (one hundredth) as much as a reciprocal centimetre. Five reciprocal metres are five times as much energy as one reciprocal metre.\n",
      "-In some branches of physics, the universal constants c, the speed of light, and ħ, the reduced Planck constant, are treated as being unity (i.e. that c = ħ = 1), which leads to mass, energy, momentum, frequency and reciprocal length all having the same unit. As a result, reciprocal length is used as a measure of energy. The frequency of a photon yields a certain photon energy, according to the Planck–Einstein relation, and the frequency of a photon is related to its spatial frequency via the speed of light. Spatial frequency is a reciprocal length, which can thus be used as a measure of energy, usually of a particle. For example, the reciprocal centimetre, cm−1, is an energy unit equal to the energy of a photon with a wavelength of 1 cm. That energy amounts to approximately 1.24×10−4 eV or 1.986×10−23 J.\n",
      "-Non-SI In the centimetre–gram–second system of units, the basic unit of length is the centimetre, or 1⁄100 of a metre.\n",
      "Other non-SI units are derived from decimal multiples of the metre.\n",
      "\n",
      "\n",
      "\n",
      "Which of the following statements is true about the categorization of planetary systems according to their orbital dynamics?\n",
      "-Orbital dynamics Planetary systems can be categorized according to their orbital dynamics as resonant, non-resonant-interacting, hierarchical, or some combination of these. In resonant systems the orbital periods of the planets are in integer ratios. The Kepler-223 system contains four planets in an 8:6:4:3 orbital resonance.\n",
      "Giant planets are found in mean-motion resonances more often than smaller planets.\n",
      "In interacting systems the planets orbits are close enough together that they perturb the orbital parameters. The Solar System could be described as weakly interacting. In strongly interacting systems Kepler's laws do not hold.\n",
      "In hierarchical systems the planets are arranged so that the system can be gravitationally considered as a nested system of two-bodies, e.g. in a star with a close-in hot jupiter with another gas giant much further out, the star and hot jupiter form a pair that appears as a single object to another planet that is far enough out.\n",
      "Other, as yet unobserved, orbital possibilities include: double planets; various co-orbital planets such as quasi-satellites, trojans and exchange orbits; and interlocking orbits maintained by precessing orbital planes.\n",
      "-Classification Planetary system architectures may be partitioned into four classes based on how the mass of the planets is distributed around the host star: Similar: The masses of all planets in a system are similar to each other. This architecture class is the most commonly-observed in our galaxy. Examples include Trappist-1. The planets in these systems are said to be like 'peas in a pod'.\n",
      "-A planetary system is a set of gravitationally bound non-stellar objects in or out of orbit around a star or star system. Generally speaking, systems with one or more planets constitute a planetary system, although such systems may also consist of bodies such as dwarf planets, asteroids, natural satellites, meteoroids, comets, planetesimals and circumstellar disks. The Sun together with the planetary system revolving around it, including Earth, forms the Solar System. The term exoplanetary system is sometimes used in reference to other planetary systems.\n",
      "-Under this definition, the Solar System is considered to have eight planets. Bodies that fulfill the first two conditions but not the third are classified as dwarf planets, provided they are not natural satellites of other planets. Originally an IAU committee had proposed a definition that would have included a larger number of planets as it did not include (c) as a criterion. After much discussion, it was decided via a vote that those bodies should instead be classified as dwarf planets.This definition is based in modern theories of planetary formation, in which planetary embryos initially clear their orbital neighborhood of other smaller objects. As described below, planets form by material accreting together in a disk of matter surrounding a protostar. This process results in a collection of relatively substantial objects, each of which has either \"swept up\" or scattered away most of the material that had been orbiting near it. These objects do not collide with one another because they are too far apart, sometimes in orbital resonance.\n",
      "-Although each planet has unique physical characteristics, a number of broad commonalities do exist among them. Some of these characteristics, such as rings or natural satellites, have only as yet been observed in planets in the Solar System, whereas others are commonly observed in extrasolar planets.\n",
      "\n",
      "\n",
      "\n",
      "What is the propagation constant in sinusoidal waves?\n",
      "-The propagation constant's value is expressed logarithmically, almost universally to the base e, rather than the more usual base 10 that is used in telecommunications in other situations. The quantity measured, such as voltage, is expressed as a sinusoidal phasor. The phase of the sinusoid varies with distance which results in the propagation constant being a complex number, the imaginary part being caused by the phase change.\n",
      "-The propagation constant of a sinusoidal electromagnetic wave is a measure of the change undergone by the amplitude and phase of the wave as it propagates in a given direction. The quantity being measured can be the voltage, the current in a circuit, or a field vector such as electric field strength or flux density. The propagation constant itself measures the change per unit length, but it is otherwise dimensionless. In the context of two-port networks and their cascades, propagation constant measures the change undergone by the source quantity as it propagates from one port to the next.\n",
      "-In electromagnetic theory, the phase constant, also called phase change constant, parameter or coefficient is the imaginary component of the propagation constant for a plane wave. It represents the change in phase per unit length along the path travelled by the wave at any instant and is equal to the real part of the angular wavenumber of the wave. It is represented by the symbol β and is measured in units of radians per unit length.\n",
      "-Propagation constant The propagation constant of the sinusoidal electromagnetic wave is a measure of the change undergone by the amplitude and phase of the wave as it propagates in a given direction. The quantity being measured can be the voltage, the current in a circuit, or a field vector such as electric field strength or flux density. The propagation constant itself measures the change per unit length, but it is otherwise dimensionless. In the context of two-port networks and their cascades, propagation constant measures the change undergone by the source quantity as it propagates from one port to the next.\n",
      "-Wavelength, phase velocity, and skin depth have simple relationships to the components of the propagation constant: \n",
      "\n",
      "\n",
      "\n",
      "What is the gravitomagnetic interaction?\n",
      "-Some higher-order gravitomagnetic effects can reproduce effects reminiscent of the interactions of more conventional polarized charges. For instance, if two wheels are spun on a common axis, the mutual gravitational attraction between the two wheels will be greater if they spin in opposite directions than in the same direction. This can be expressed as an attractive or repulsive gravitomagnetic component.\n",
      "-This approximate reformulation of gravitation as described by general relativity in the weak field limit makes an apparent field appear in a frame of reference different from that of a freely moving inertial body. This apparent field may be described by two components that act respectively like the electric and magnetic fields of electromagnetism, and by analogy these are called the gravitoelectric and gravitomagnetic fields, since these arise in the same way around a mass that a moving electric charge is the source of electric and magnetic fields. The main consequence of the gravitomagnetic field, or velocity-dependent acceleration, is that a moving object near a massive, non-axisymmetric, rotating object will experience acceleration not predicted by a purely Newtonian (gravitoelectric) gravity field. More subtle predictions, such as induced rotation of a falling object and precession of a spinning object are among the last basic predictions of general relativity to be directly tested.\n",
      "-Gravitoelectromagnetism, abbreviated GEM, refers to a set of formal analogies between the equations for electromagnetism and relativistic gravitation; specifically: between Maxwell's field equations and an approximation, valid under certain conditions, to the Einstein field equations for general relativity. Gravitomagnetism is a widely used term referring specifically to the kinetic effects of gravity, in analogy to the magnetic effects of moving electric charge. The most common version of GEM is valid only far from isolated sources, and for slowly moving test particles.\n",
      "-Gravity Gravitation is the weakest of the four interactions at the atomic scale, where electromagnetic interactions dominate.\n",
      "-According to general relativity, in its weak-field and slow-motion linearized approximation, a slowly spinning body induces an additional component of the gravitational field that acts on a freely-falling test particle with a non-central, gravitomagnetic Lorentz-like force.\n",
      "\n",
      "\n",
      "\n",
      "What did Newton's manuscripts of the 1660s show?\n",
      "-In regard to evidence that still survives of the earlier history, manuscripts written by Newton in the 1660s show that Newton himself had, by 1669, arrived at proofs that in a circular case of planetary motion, \"endeavour to recede\" (what was later called centrifugal force) had an inverse-square relation with distance from the center. After his 1679–1680 correspondence with Hooke, Newton adopted the language of inward or centripetal force. According to Newton scholar J. Bruce Brackenridge, although much has been made of the change in language and difference of point of view, as between centrifugal or centripetal forces, the actual computations and proofs remained the same either way. They also involved the combination of tangential and radial displacements, which Newton was making in the 1660s. The lesson offered by Hooke to Newton here, although significant, was one of perspective and did not change the analysis. This background shows there was basis for Newton to deny deriving the inverse square law from Hooke.\n",
      "-Newton's early work on motion In the 1660s Newton studied the motion of colliding bodies and deduced that the centre of mass of two colliding bodies remains in uniform motion. Surviving manuscripts of the 1660s also show Newton's interest in planetary motion and that by 1669 he had shown, for a circular case of planetary motion, that the force he called \"endeavour to recede\" (now called centrifugal force) had an inverse-square relation with distance from the center. After his 1679–1680 correspondence with Hooke, described below, Newton adopted the language of inward or centripetal force. According to Newton scholar J. Bruce Brackenridge, although much has been made of the change in language and difference of point of view, as between centrifugal or centripetal forces, the actual computations and proofs remained the same either way. They also involved the combination of tangential and radial displacements, which Newton was making in the 1660s. The difference between the centrifugal and centripetal points of view, though a significant change of perspective, did not change the analysis. Newton also clearly expressed the concept of linear inertia in the 1660s: for this Newton was indebted to Descartes' work published 1644.\n",
      "-c.1665 – The British scientist Robert Boyle reveals his scientific methods in his writings, and commends that a subject be generally researched before detailed experiments are undertaken; that results that are inconsistent with current theories are reported; that experiments should be regarded as 'provisional' in nature; and that experiments are shown to be repeatable.\n",
      "1665 – Academic journals are published for the first time, in France and Great Britain.\n",
      "1675 – To encourage the publicising of new discoveries in science, the German-born Henry Oldenburg pioneers the practice now known as peer reviewing, by sending scientific manuscripts to experts to judge their quality.\n",
      "1687 – Sir Isaac Newton's book Philosophiæ Naturalis Principia Mathematica (Mathematical Principles of Natural Philosophy), is first published. It laid the foundations of classical mechanics. Newton also made seminal contributions to optics, and shares credit with Gottfried Wilhelm Leibniz for developing the infinitesimal calculus.\n",
      "-Plagiarism dispute In 1686, when the first book of Newton's Principia was presented to the Royal Society, Robert Hooke accused Newton of plagiarism by claiming that he had taken from him the \"notion\" of \"the rule of the decrease of Gravity, being reciprocally as the squares of the distances from the Center\". At the same time (according to Edmond Halley's contemporary report) Hooke agreed that \"the Demonstration of the Curves generated thereby\" was wholly Newton's.\n",
      "-Newton's law of universal gravitation In 1679, Robert Hooke wrote to Isaac Newton of his hypothesis concerning orbital motion, which partly depends on an inverse-square force. In 1684, both Hooke and Newton told Edmond Halley that they had proven the inverse-square law of planetary motion, in January and August, respectively. While Hooke refused to produce his proofs, Newton was prompted to compose De motu corporum in gyrum ('On the motion of bodies in an orbit'), in which he mathematically derives Kepler's laws of planetary motion. In 1687, with Halley's support (and to Hooke's dismay), Newton published Philosophiæ Naturalis Principia Mathematica (Mathematical Principles of Natural Philosophy), which hypothesizes the inverse-square law of universal gravitation. In his own words:I deduced that the forces which keep the planets in their orbs must be reciprocally as the squares of their distances from the centres about which they revolve; and thereby compared the force requisite to keep the moon in her orb with the force of gravity at the surface of the earth; and found them to answer pretty nearly.\n",
      "\n",
      "\n",
      "\n",
      "What is the decay energy for the free neutron decay process?\n",
      "-For the free neutron, the decay energy for this process (based on the rest masses of the neutron, proton and electron) is 0.782343 MeV. That is the difference between the rest mass of the neutron and the sum of the rest masses of the products. That difference has to be carried away as kinetic energy. The maximal energy of the beta decay electron (in the process wherein the neutrino receives a vanishingly small amount of kinetic energy) has been measured at 0.782±0.013 MeV. The latter number is not well-enough measured to determine the comparatively tiny rest mass of the neutrino (which must in theory be subtracted from the maximal electron kinetic energy); furthermore, neutrino mass is constrained by many other methods.\n",
      "-For the free neutron the decay energy for this process (based on the masses of the neutron, proton, and electron) is 0.782343 MeV. The maximal energy of the beta decay electron (in the process wherein the neutrino receives a vanishingly small amount of kinetic energy) has been measured at 0.782±0.013 MeV. The latter number is not well-enough measured to determine the comparatively tiny rest mass of the neutrino (which must in theory be subtracted from the maximal electron kinetic energy) as well as neutrino mass is constrained by many other methods.\n",
      "-A small fraction (about 1 in 1,000) of free neutrons decay with the same products, but add an extra particle in the form of an emitted gamma ray: n0 → p+ + e− + νe + γ This gamma ray may be thought of as a sort of \"internal bremsstrahlung\" that arises as the emitted beta particle (electron) interacts with the charge of the proton in an electromagnetic way. In this process, some of the decay energy is carried away as photon energy. Gamma rays produced in this way are also a minor feature of beta decays of bound neutrons, that is, those within a nucleus.\n",
      "-A very small minority of neutron decays (about four per million) are so-called \"two-body (neutron) decays\", in which a proton, electron and antineutrino are produced as usual, but the electron fails to gain the 13.6 eV necessary energy to escape the proton (the ionization energy of hydrogen), and therefore simply remains bound to it, as a neutral hydrogen atom (one of the \"two bodies\"). In this type of free neutron decay, in essence all of the neutron decay energy is carried off by the antineutrino (the other \"body\").\n",
      "-The total energy released from uranium-235 to lead-207, including the energy lost to neutrinos, is 46.4 MeV.\n",
      "\n",
      "\n",
      "\n",
      "What is Hesse's principle of transfer in geometry?\n",
      "-In geometry, Hesse's principle of transfer (German: Übertragungsprinzip) states that if the points of the projective line P1 are depicted by a rational normal curve in Pn, then the group of the projective transformations of Pn that preserve the curve is isomorphic to the group of the projective transformations of P1 (this is a generalization of the original Hesse's principle, in a form suggested by Wilhelm Franz Meyer). It was originally introduced by Otto Hesse in 1866, in a more restricted form. It influenced Felix Klein in the development of the Erlangen program. Since its original conception, it was generalized by many mathematicians, including Klein, Fano, and Cartan.\n",
      "-In geometry, the Hesse configuration is a configuration of 9 points and 12 lines with three points per line and four lines through each point. It can be realized in the complex projective plane as the set of inflection points of an elliptic curve, but it has no realization in the Euclidean plane. It was introduced by Colin Maclaurin and studied by Hesse (1844), and is also known as Young's geometry, named after the later work of John Wesley Young on finite geometry.\n",
      "-In mathematics, transformation geometry (or transformational geometry) is the name of a mathematical and pedagogic take on the study of geometry by focusing on groups of geometric transformations, and properties that are invariant under them. It is opposed to the classical synthetic geometry approach of Euclidean geometry, that focuses on proving theorems.\n",
      "-In model theory, a transfer principle states that all statements of some language that are true for some structure are true for another structure. One of the first examples was the Lefschetz principle, which states that any sentence in the first-order language of fields that is true for the complex numbers is also true for any algebraically closed field of characteristic 0.\n",
      "-6. The Hesse group is the group of automorphisms of the Hesse configuration, of order 216.\n",
      "hexad A set of 6 points homaloid An element of a homaloidal system, in particular the image of a hyperlpane under a Cremona transformation.\n",
      "homaloidal 1. A homaloidal linear system of divisors is a linear system of grade 1, such as the image of the linear system of hyperplanes of projective space under a Cremona transformation. (Semple & Roth 1949, p.45) (Coolidge 1931, p. 442) When the linear system has dimension 2 or 3 it is called a homaloidal net or homaloidal web.\n",
      "2. Homaloidal means similar to a flat plane.  homographic 1. Having the same invariants. See Salmon (1879, p.232).\n",
      "2. A homographic transformation is an automorphism of projective space over a field, in other words an element of the projective general linear group. (Salmon 1879, p.283) homography 1. An isomorphism between projective spaces induced by an isomorphism of vector spaces.\n",
      "2. An axis of homography is a line associated to two related ranges of a conic. (Baker 1922b, vol 2, p. 16)  homology 1. As in homology group 2. A collineation fixing all lines through a point (the center) and all points through a line (the axis) not containing the center. See elation. This terminology was introduced by Lie.\n",
      "3. An automorphism of projective space with a hyperplane of fixed points (called the axis). It is called a harmonic homology if it has order 2, in which case it has an isolated fixed point called its center.\n",
      "Hurwitz curve Hurwitz surface A Hurwitz curve is a complex algebraic curve of genus g>0 with the maximum possible number 84(g–1) of automorphisms.\n",
      "hyperbolism Essentially a blow-up of a curve at a point. See Salmon (1879, p.175).\n",
      "hypercusp A singularity of a curve of some multiplicity r whose tangent cone is a single line meeting the curve with order r+1. (Coolidge 1931, p. 18) hyperelliptic A hyperelliptic curve is a curve with a degree 2 map to the projective line.\n",
      "hyperflex Same as point of undulation: a point of a curve where the tangent line has contact of order at least 4.\n",
      "hyperosculating point A point where the tangent space meets with order higher than normal.  hyperplane A linear subspace of projective space of codimension 1. Same as prime.\n",
      "\n",
      "\n",
      "\n",
      "What is the relationship between the Cauchy momentum equation and the Navier-Stokes equation?\n",
      "-All non-relativistic momentum conservation equations, such as the Navier–Stokes equation, can be derived by beginning with the Cauchy momentum equation and specifying the stress tensor through a constitutive relation. By expressing the shear tensor in terms of viscosity and fluid velocity, and assuming constant density and viscosity, the Cauchy momentum equation will lead to the Navier–Stokes equations. By assuming inviscid flow, the Navier–Stokes equations can further simplify to the Euler equations.\n",
      "-All non-relativistic balance equations, such as the Navier–Stokes equations, can be derived by beginning with the Cauchy equations and specifying the stress tensor through a constitutive relation. By expressing the deviatoric (shear) stress tensor in terms of viscosity and the fluid velocity gradient, and assuming constant viscosity, the above Cauchy equations will lead to the Navier–Stokes equations below.\n",
      "-The motivation for doing this is that pressure is typically a variable of interest, and also this simplifies application to specific fluid families later on since the rightmost tensor τ in the equation above must be zero for a fluid at rest. Note that τ is traceless. The Cauchy equation may now be written in another more explicit form: ρDuDt=−∇p+∇⋅τ+f This equation is still incomplete. For completion, one must make hypotheses on the forms of τ and p, that is, one needs a constitutive law for the stress tensor which can be obtained for specific fluid families and on the pressure. Some of these hypotheses lead to the Euler equations (fluid dynamics), other ones lead to the Navier–Stokes equations. Additionally, if the flow is assumed compressible an equation of state will be required, which will likely further require a conservation of energy formulation.\n",
      "-The Cauchy momentum equation is a vector partial differential equation put forth by Cauchy that describes the non-relativistic momentum transport in any continuum.\n",
      "-The generic density of the momentum source s seen previously is made specific first by breaking it up into two new terms, one to describe internal stresses and one for external forces, such as gravity. By examining the forces acting on a small cube in a fluid, it may be shown that ρDuDt=∇⋅σ+f where σ is the Cauchy stress tensor, and f accounts for body forces present. This equation is called the Cauchy momentum equation and describes the non-relativistic momentum conservation of any continuum that conserves mass. σ is a rank two symmetric tensor given by its covariant components. In orthogonal coordinates in three dimensions it is represented as the 3 × 3 matrix: σij=(σxxτxyτxzτyxσyyτyzτzxτzyσzz) where the σ are normal stresses and τ shear stresses. This matrix is split up into two terms: σij=(σxxτxyτxzτyxσyyτyzτzxτzyσzz)=−(p000p000p)+(σxx+pτxyτxzτyxσyy+pτyzτzxτzyσzz+p)=−pI+τ where I is the 3 × 3 identity matrix and τ is the deviatoric stress tensor. Note that the mechanical pressure p is equal to the negative of the mean normal stress: p=−13(σxx+σyy+σzz).\n",
      "\n",
      "\n",
      "\n",
      "What is X-ray pulsar-based navigation (XNAV)?\n",
      "-X-ray pulsar-based navigation and timing (XNAV) or simply pulsar navigation is a navigation technique whereby the periodic X-ray signals emitted from pulsars are used to determine the location of a vehicle, such as a spacecraft in deep space. A vehicle using XNAV would compare received X-ray signals with a database of known pulsar frequencies and locations. Similar to GPS, this comparison would allow the vehicle to calculate its position accurately (±5 km). The advantage of using X-ray signals over radio waves is that X-ray telescopes can be made smaller and lighter. Experimental demonstrations have been reported in 2018.\n",
      "-Pulsar navigation X-ray pulsar-based navigation and timing (XNAV) or simply pulsar navigation is a navigation technique whereby the periodic X-ray signals emitted from pulsars are used to determine the location of a vehicle, such as a spacecraft in deep space. A vehicle using XNAV would compare received X-ray signals with a database of known pulsar frequencies and locations. Similar to GPS, this comparison would allow the vehicle to calculate its position accurately (±5 km). The advantage of using X-ray signals over radio waves is that X-ray telescopes can be made smaller and lighter. Experimental demonstrations have been reported in 2018.\n",
      "-X-ray pulsar-based navigation and timing (XNAV) is an experimental navigation technique whereby the periodic X-ray signals emitted from pulsars are used to determine the location of a vehicle, such as a spacecraft in deep space. A vehicle using XNAV would compare received X-ray signals with a database of known pulsar frequencies and locations. Similar to GNSS, this comparison would allow the vehicle to triangulate its position accurately (±5 km). The advantage of using X-ray signals over radio waves is that X-ray telescopes can be made smaller and lighter. On 9 November 2016 the Chinese Academy of Sciences launched an experimental pulsar navigation satellite called XPNAV 1. SEXTANT (Station Explorer for X-ray Timing and Navigation Technology) is a NASA-funded project developed at the Goddard Space Flight Center that is testing XNAV on-orbit on board the International Space Station in connection with the NICER project, launched on 3 June 2017 on the SpaceX CRS-11 ISS resupply mission.\n",
      "-Studies The Advanced Concepts Team of ESA studied in 2003 the feasibility of x-ray pulsar navigation in collaboration with the Universitat Politecnica de Catalunya in Spain. After the study, the interest in the XNAV technology within the European Space Agency was consolidated leading, in 2012, to two different and more detailed studies performed by GMV AEROSPACE AND DEFENCE (ES) and the National Physical Laboratory (UK).\n",
      "-Another possibility that has been explored for deep space navigation is Pulsar navigation, which compares the X-ray bursts from a collection of known pulsars in order to determine the position of a spacecraft. This method has been tested by multiple space agencies, such as NASA and ESA.\n",
      "Electronic navigation Radio navigation A radio direction finder or RDF is a device for finding the direction to a radio source. Due to radio's ability to travel very long distances \"over the horizon\", it makes a particularly good navigation system for ships and aircraft that might be flying at a distance from land.\n",
      "\n",
      "\n",
      "\n",
      "What is the evidence for the existence of a supermassive black hole at the center of the Milky Way galaxy?\n",
      "-From the motion of star S2, the object's mass can be estimated as 4.0 million M☉, or about 7.96×1036 kg.\n",
      "The radius of the central object must be less than 17 light-hours, because otherwise S2 would collide with it. Observations of the star S14 indicate that the radius is no more than 6.25 light-hours, about the diameter of Uranus' orbit.\n",
      "-In the Milky Way Evidence indicates that the Milky Way galaxy has a supermassive black hole at its center, 26,000 light-years from the Solar System, in a region called Sagittarius A* because: The star S2 follows an elliptical orbit with a period of 15.2 years and a pericenter (closest distance) of 17 light-hours (1.8×1013 m or 120 AU) from the center of the central object.\n",
      "-Proper motions of stars orbiting Sagittarius A* The proper motions of stars near the centre of our own Milky Way provide strong observational evidence that these stars are orbiting a supermassive black hole. Since 1995, astronomers have tracked the motions of 90 stars orbiting an invisible object coincident with the radio source Sagittarius A*. By fitting their motions to Keplerian orbits, the astronomers were able to infer, in 1998, that a 2.6×106 M☉ object must be contained in a volume with a radius of 0.02 light-years to cause the motions of those stars. Since then, one of the stars—called S2—has completed a full orbit. From the orbital data, astronomers were able to refine the calculations of the mass to 4.3×106 M☉ and a radius of less than 0.002 light-years for the object causing the orbital motion of those stars. The upper limit on the object's size is still too large to test whether it is smaller than its Schwarzschild radius; nevertheless, these observations strongly suggest that the central object is a supermassive black hole as there are no other plausible scenarios for confining so much invisible mass into such a small volume. Additionally, there is some observational evidence that this object might possess an event horizon, a feature unique to black holes.\n",
      "-The nearby Andromeda Galaxy, 2.5 million light-years away, contains a 1.4+0.65−0.45×108 (140 million) M☉ central black hole, significantly larger than the Milky Way's. The largest supermassive black hole in the Milky Way's vicinity appears to be that of Messier 87 (i.e., M87*), at a mass of (6.5±0.7)×109 (c. 6.5 billion) M☉ at a distance of 48.92 million light-years. The supergiant elliptical galaxy NGC 4889, at a distance of 336 million light-years away in the Coma Berenices constellation, contains a black hole measured to be 2.1+3.5−1.3×1010 (21 billion) M☉.Masses of black holes in quasars can be estimated via indirect methods that are subject to substantial uncertainty. The quasar TON 618 is an example of an object with an extremely large black hole, estimated at 6.6×1010 (66 billion) M☉. Its redshift is 2.219. Other examples of quasars with large estimated black hole masses are the hyperluminous quasar APM 08279+5255, with an estimated mass of 1×1010 (10 billion) M☉, and the quasar SMSS J215728.21-360215.1, with a mass of (3.4±0.6)×1010 (34 billion) M☉, or nearly 10,000 times the mass of the black hole at the Milky Way's Galactic Center.Some galaxies, such as the galaxy 4C +37.11, appear to have two supermassive black holes at their centers, forming a binary system. If they collided, the event would create strong gravitational waves. Binary supermassive black holes are believed to be a common consequence of galactic mergers. The binary pair in OJ 287, 3.5 billion light-years away, contains the most massive black hole in a pair, with a mass estimated at 18.348 billion M☉. In 2011, a super-massive black hole was discovered in the dwarf galaxy Henize 2-10, which has no bulge. The precise implications for this discovery on black hole formation are unknown, but may indicate that black holes formed before bulges.\n",
      "-Outside the Milky Way Unambiguous dynamical evidence for supermassive black holes exists only for a handful of galaxies; these include the Milky Way, the Local Group galaxies M31 and M32, and a few galaxies beyond the Local Group, such as NGC 4395. In these galaxies, the root mean square (or rms) velocities of the stars or gas rises proportionally to 1/r near the center, indicating a central point mass. In all other galaxies observed to date, the rms velocities are flat, or even falling, toward the center, making it impossible to state with certainty that a supermassive black hole is present. Nevertheless, it is commonly accepted that the center of nearly every galaxy contains a supermassive black hole. The reason for this assumption is the M–sigma relation, a tight (low scatter) relation between the mass of the hole in the 10 or so galaxies with secure detections, and the velocity dispersion of the stars in the bulges of those galaxies. This correlation, although based on just a handful of galaxies, suggests to many astronomers a strong connection between the formation of the black hole and the galaxy itself.On March 28, 2011, a supermassive black hole was seen tearing a mid-size star apart. That is the only likely explanation of the observations that day of sudden X-ray radiation and the follow-up broad-band observations. The source was previously an inactive galactic nucleus, and from study of the outburst the galactic nucleus is estimated to be a SMBH with mass of the order of a million M☉. This rare event is assumed to be a relativistic outflow (material being emitted in a jet at a significant fraction of the speed of light) from a star tidally disrupted by the SMBH. A significant fraction of a solar mass of material is expected to have accreted onto the SMBH. Subsequent long-term observation will allow this assumption to be confirmed if the emission from the jet decays at the expected rate for mass accretion onto a SMBH.\n",
      "\n",
      "\n",
      "\n",
      "What is the function of the fibrous cardiac skeleton?\n",
      "-In cardiology, the cardiac skeleton, also known as the fibrous skeleton of the heart, is a high-density homogeneous structure of connective tissue that forms and anchors the valves of the heart, and influences the forces exerted by and through them. The cardiac skeleton separates and partitions the atria (the smaller, upper two chambers) from the ventricles (the larger, lower two chambers).The heart's cardiac skeleton comprises four dense connective tissue rings that encircle the mitral and tricuspid atrioventricular (AV) canals and extend to the origins of the pulmonary trunk and aorta. This provides crucial support and structure to the heart while also serving to electrically isolate the atria from the ventricles.The unique matrix of connective tissue within the cardiac skeleton isolates electrical influence within these defined chambers. In normal anatomy, there is only one conduit for electrical conduction from the upper chambers to the lower chambers, known as the atrioventricular node. The physiologic cardiac skeleton forms a firewall governing autonomic/electrical influence until bordering the bundle of His which further governs autonomic flow to the bundle branches of the ventricles. Understood as such, the cardiac skeleton efficiently centers and robustly funnels electrical energy from the atria to the ventricles.\n",
      "-The cardiac skeleton ensures that the electrical and autonomic energy generated above is ushered below and cannot return. The cardiac skeleton does this by establishing an electrically impermeable boundary to autonomic electrical influence within the heart. Simply put, the dense connective tissue within the cardiac skeleton does not conduct electricity and its deposition within the myocardial matrix is not accidental.\n",
      "-The anchored and electrically inert collagen framework of the four valves allows normal anatomy to house the atrioventricular node (AV node) in its center. The AV node is the only electrical conduit from the atria to the ventricles through the cardiac skeleton, which is why atrial fibrillation can never degrade into ventricular fibrillation.\n",
      "-The structure of the components of the heart has become an area of increasing interest. The cardiac skeleton binds several bands of dense connective tissue, as collagen, that encircle the bases of the pulmonary trunk, aorta, and all four heart valves. While not a traditionally or \"true\" or rigid skeleton, it does provide structure and support for the heart, as well as isolate the atria from the ventricles. This is why atrial fibrillation almost never degrades to ventricular fibrillation. In youth, this collagen structure is free of calcium adhesions and is quite flexible. With aging, calcium and other mineral accumulation occur within this skeleton. Distensibility of the ventricles is tied to variable accumulation of minerals which also contributes to the delay of the depolarization wave in geriatric patients that can take place from the AV node and the bundle of His.\n",
      "-The four cardiac valves are kept in their place partly because of the fibrous skeleton of the heart, which is a collection of connective tissue. It consists of the right fibrous trigone (which along with the membranous septum forms the central fibrous body), the left right fibrous trigone, and the conus tendon. The right fibrous trigone is the strongest part of the skeleton. It lies to the right of the aortic valve and connects it with the mitral and tricuspid valves. It is pierced by the Bundle of His. Lastly, the aortomitral curtain is also a part of the fibrous skeleton; it is formed by fibrous tissue connecting two of three of the aortic valve leaflets (the right and non-coronary leaflet) with anterior leaflet of the mitral valve.\n",
      "\n",
      "\n",
      "\n",
      "What is the Carnot engine?\n",
      "-Efficiency of real heat engines Carnot realized that, in reality, it is not possible to build a thermodynamically reversible engine. So, real heat engines are even less efficient than indicated by Equation 3. In addition, real engines that operate along the Carnot cycle style (isothermal expansion / isentropic expansion / isothermal compression / isentropic compression) are rare. Nevertheless, Equation 3 is extremely useful for determining the maximum efficiency that could ever be expected for a given set of thermal reservoirs.\n",
      "-Carnot's theorem is a formal statement of this fact: No engine operating between two heat reservoirs can be more efficient than a Carnot engine operating between those same reservoirs. Thus, Equation 3 gives the maximum efficiency possible for any engine using the corresponding temperatures. A corollary to Carnot's theorem states that: All reversible engines operating between the same heat reservoirs are equally efficient. Rearranging the right side of the equation gives what may be a more easily understood form of the equation, namely that the theoretical maximum efficiency of a heat engine equals the difference in temperature between the hot and cold reservoir divided by the absolute temperature of the hot reservoir. Looking at this formula an interesting fact becomes apparent: Lowering the temperature of the cold reservoir will have more effect on the ceiling efficiency of a heat engine than raising the temperature of the hot reservoir by the same amount. In the real world, this may be difficult to achieve since the cold reservoir is often an existing ambient temperature.\n",
      "-A Carnot cycle is an ideal thermodynamic cycle proposed by French physicist Sadi Carnot in 1824 and expanded upon by others in the 1830s and 1840s. By Carnot's theorem, it provides an upper limit on the efficiency of any classical thermodynamic engine during the conversion of heat into work, or conversely, the efficiency of a refrigeration system in creating a temperature difference through the application of work to the system.\n",
      "-Carnot's principle The historical origin of the second law of thermodynamics was in Sadi Carnot's theoretical analysis of the flow of heat in steam engines (1824). The centerpiece of that analysis, now known as a Carnot engine, is an ideal heat engine fictively operated in the limiting mode of extreme slowness known as quasi-static, so that the heat and work transfers are between subsystems that are always in their own internal states of thermodynamic equilibrium. It represents the theoretical maximum efficiency of a heat engine operating between any two given thermal or heat reservoirs at different temperatures. Carnot's principle was recognized by Carnot at a time when the caloric theory represented the dominant understanding of the nature of heat, before the recognition of the first law of thermodynamics, and before the mathematical expression of the concept of entropy. Interpreted in the light of the first law, Carnot's analysis is physically equivalent to the second law of thermodynamics, and remains valid today. Some samples from his book are: ...wherever there exists a difference of temperature, motive power can be produced.The production of motive power is then due in steam engines not to an actual consumption of caloric, but to its transportation from a warm body to a cold body ...The motive power of heat is independent of the agents employed to realize it; its quantity is fixed solely by the temperatures of the bodies between which is effected, finally, the transfer of caloric.In modern terms, Carnot's principle may be stated more precisely: The efficiency of a quasi-static or reversible Carnot cycle depends only on the temperatures of the two heat reservoirs, and is the same, whatever the working substance. A Carnot engine operated in this way is the most efficient possible heat engine using those two temperatures.\n",
      "-Carnot's theorem Carnot's theorem (1824) is a principle that limits the maximum efficiency for any possible engine. The efficiency solely depends on the temperature difference between the hot and cold thermal reservoirs. Carnot's theorem states: All irreversible heat engines between two heat reservoirs are less efficient than a Carnot engine operating between the same reservoirs.\n",
      "\n",
      "\n",
      "\n",
      "Which mathematical function is commonly used to characterize linear time-invariant systems?\n",
      "-Transfer functions do not properly exist for many non-linear systems. For example, they do not exist for relaxation oscillators; however, describing functions can sometimes be used to approximate such nonlinear time-invariant systems.\n",
      "-Transfer functions are commonly used in the analysis of systems such as single-input single-output filters in the fields of signal processing, communication theory, and control theory. The term is often used exclusively to refer to linear time-invariant (LTI) systems. Most real systems have non-linear input/output characteristics, but many systems, when operated within nominal parameters (not \"over-driven\") have behavior close enough to linear that LTI system theory is an acceptable representation of the input/output behavior.\n",
      "-In control systems theory, the describing function (DF) method, developed by Nikolay Mitrofanovich Krylov and Nikolay Bogoliubov in the 1930s, and extended by Ralph Kochenburger is an approximate procedure for analyzing certain nonlinear control problems. It is based on quasi-linearization, which is the approximation of the non-linear system under investigation by a linear time-invariant (LTI) transfer function that depends on the amplitude of the input waveform. By definition, a transfer function of a true LTI system cannot depend on the amplitude of the input function because an LTI system is linear. Thus, this dependence on amplitude generates a family of linear systems that are combined in an attempt to capture salient features of the non-linear system behavior. The describing function is one of the few widely applicable methods for designing nonlinear systems, and is very widely used as a standard mathematical tool for analyzing limit cycles in closed-loop controllers, such as industrial process controls, servomechanisms, and electronic oscillators.\n",
      "-The output of any general continuous-time linear system is related to the input by an integral which may be written over a doubly infinite range because of the causality condition: If the properties of the system do not depend on the time at which it is operated then it is said to be time-invariant and h is a function only of the time difference τ = t − t' which is zero for τ < 0 (namely t < t' ). By redefinition of h it is then possible to write the input-output relation equivalently in any of the ways, Linear time-invariant systems are most commonly characterized by the Laplace transform of the impulse response function called the transfer function which is: In applications this is usually a rational algebraic function of s. Because h(t) is zero for negative t, the integral may equally be written over the doubly infinite range and putting s = iω follows the formula for the frequency response function: \n",
      "-For time-invariant systems this is the basis of the impulse response or the frequency response methods (see LTI system theory), which describe a general input function x(t) in terms of unit impulses or frequency components.  Typical differential equations of linear time-invariant systems are well adapted to analysis using the Laplace transform in the continuous case, and the Z-transform in the discrete case (especially in computer implementations).\n",
      "\n",
      "\n",
      "\n",
      "What is the second law of thermodynamics?\n",
      "-The second law of thermodynamics is a physical law based on universal experience concerning heat and energy interconversions. One simple statement of the law is that heat always moves from hotter objects to colder objects (or \"downhill\"), unless energy in some form is supplied to reverse the direction of heat flow. Another definition is: \"Not all heat energy can be converted into work in a cyclic process.\"The second law of thermodynamics in other versions establishes the concept of entropy as a physical property of a thermodynamic system. It can be used to predict whether processes are forbidden despite obeying the requirement of conservation of energy as expressed in the first law of thermodynamics and provides necessary criteria for spontaneous processes. The second law may be formulated by the observation that the entropy of isolated systems left to spontaneous evolution cannot decrease, as they always arrive at a state of thermodynamic equilibrium where the entropy is highest at the given internal energy. An increase in the combined entropy of system and surroundings accounts for the irreversibility of natural processes, often referred to in the concept of the arrow of time.Historically, the second law was an empirical finding that was accepted as an axiom of thermodynamic theory. Statistical mechanics provides a microscopic explanation of the law in terms of probability distributions of the states of large assemblies of atoms or molecules. The second law has been expressed in many ways. Its first formulation, which preceded the proper definition of entropy and was based on caloric theory, is Carnot's theorem, formulated by the French scientist Sadi Carnot, who in 1824 showed that the efficiency of conversion of heat to work in a heat engine has an upper limit. The first rigorous definition of the second law based on the concept of entropy came from German scientist Rudolf Clausius in the 1850s and included his statement that heat can never pass from a colder to a warmer body without some other change, connected therewith, occurring at the same time.\n",
      "-The first law of thermodynamics provides the definition of the internal energy of a thermodynamic system, and expresses its change for a closed system in terms of work and heat. It can be linked to the law of conservation of energy. Conceptually, the first law describes the fundamental principle that systems do not consume or 'use up' energy, that energy is neither created nor destroyed, but is simply converted from one form to another.\n",
      "-Thermodynamic entropy The concept of thermodynamic entropy arises from the second law of thermodynamics. This law of entropy increase quantifies the reduction in the capacity of an isolated compound thermodynamic system to do thermodynamic work on its surroundings, or indicates whether a thermodynamic process may occur. For example, whenever there is a suitable pathway, heat spontaneously flows from a hotter body to a colder one.\n",
      "-The second law can be conceptually stated as follows: Matter and energy have the tendency to reach a state of uniformity or internal and external equilibrium, a state of maximum disorder (entropy). Real non-equilibrium processes always produce entropy, causing increased disorder in the universe, while idealized reversible processes produce no entropy and no process is known to exist that destroys entropy. The tendency of a system to approach uniformity may be counteracted, and the system may become more ordered or complex, by the combination of two things, a work or exergy source and some form of instruction or intelligence. Where ‘exergy’ is the thermal, mechanical, electric or chemical work potential of an energy source or flow, and ‘instruction or intelligence’, although subjective, is in the context of the set of category IV processes.\n",
      "-The first law of thermodynamics states that, when energy passes into or out of a system (as work, heat, or matter), the system's internal energy changes in accordance with the law of conservation of energy.\n",
      "The second law of thermodynamics states that in a natural thermodynamic process, the sum of the entropies of the interacting thermodynamic systems never decreases. A common corollary of the statement is that heat does not spontaneously pass from a colder body to a warmer body.\n",
      "\n",
      "\n",
      "\n",
      "What are amorphous ferromagnetic metallic alloys, and what are their advantages?\n",
      "-Amorphous (non-crystalline) ferromagnetic metallic alloys can be made by very rapid quenching (cooling) of an alloy. These have the advantage that their properties are nearly isotropic (not aligned along a crystal axis); this results in low coercivity, low hysteresis loss, high permeability, and high electrical resistivity. One such typical material is a transition metal-metalloid alloy, made from about 80% transition metal (usually Fe, Co, or Ni) and a metalloid component (B, C, Si, P, or Al) that lowers the melting point.\n",
      "-Amorphous metal is usually an alloy rather than a pure metal. The alloys contain atoms of significantly different sizes, leading to low free volume (and therefore up to orders of magnitude higher viscosity than other metals and alloys) in molten state. The viscosity prevents the atoms moving enough to form an ordered lattice. The material structure also results in low shrinkage during cooling, and resistance to plastic deformation. The absence of grain boundaries, the weak spots of crystalline materials, leads to better resistance to wear and corrosion. Amorphous metals, while technically glasses, are also much tougher and less brittle than oxide glasses and ceramics. Amorphous metals can be grouped in two categories, as either non-ferromagnetic, if they are composed of Ln, Mg, Zr, Ti, Pd, Ca, Cu, Pt and Au, or ferromagnetic alloys, if they are composed of Fe, Co, and Ni.Thermal conductivity of amorphous materials is lower than that of crystalline metal. As formation of amorphous structure relies on fast cooling, this limits the maximum achievable thickness of amorphous structures. To achieve formation of amorphous structure even during slower cooling, the alloy has to be made of three or more components, leading to complex crystal units with higher potential energy and lower chance of formation. The atomic radius of the components has to be significantly different (over 12%), to achieve high packing density and low free volume. The combination of components should have negative heat of mixing, inhibiting crystal nucleation and prolonging the time the molten metal stays in supercooled state.\n",
      "-Electric and Magnetic Properties The amorphous material produced by melt spinning is considered a soft magnet. That is to say that their natural coercivity is less than 1000 Am-1, which means that the metal's magnetism is more responsive to outside influences and as a result can be easily switched on and off. This makes amorphous metals particularly useful in applications requiring the repeated magnetization and demagnetization of a material in order to function. Certain amorphous alloys also provide the ability to enhance and or channel flux created by electrical currents, making them useful for magnetic shielding and insulation.\n",
      "-Perhaps the most useful property of bulk amorphous alloys is that they are true glasses, which means that they soften and flow upon heating. This allows for easy processing, such as by injection molding, in much the same way as polymers. As a result, amorphous alloys have been commercialized for use in sports equipment, medical devices, and as cases for electronic equipment.Thin films of amorphous metals can be deposited via high velocity oxygen fuel technique as protective coatings.\n",
      "-For certain metallic elements the superconducting critical temperature Tc can be higher in the amorphous state (e.g. upon alloying) than in the crystalline state, and in several cases Tc increases upon increasing the structural disorder. This behavior can be understood and rationalized by considering the effect of structural disorder on the electron-phonon coupling.Amorphous metals have higher tensile yield strengths and higher elastic strain limits than polycrystalline metal alloys, but their ductilities and fatigue strengths are lower. Amorphous alloys have a variety of potentially useful properties. In particular, they tend to be stronger than crystalline alloys of similar chemical composition, and they can sustain larger reversible (\"elastic\") deformations than crystalline alloys. Amorphous metals derive their strength directly from their non-crystalline structure, which does not have any of the defects (such as dislocations) that limit the strength of crystalline alloys. One modern amorphous metal, known as Vitreloy, has a tensile strength that is almost twice that of high-grade titanium. However, metallic glasses at room temperature are not ductile and tend to fail suddenly when loaded in tension, which limits the material applicability in reliability-critical applications, as the impending failure is not evident. Therefore, there is considerable interest in producing metal matrix composites consisting of a metallic glass matrix containing dendritic particles or fibers of a ductile crystalline metal.\n",
      "\n",
      "\n",
      "\n",
      "What is the Penrose process?\n",
      "-The Penrose process (also called Penrose mechanism) is theorised by Sir Roger Penrose as a means whereby energy can be extracted from a rotating black hole. The process takes advantage of the ergosphere – a region of spacetime around the black hole dragged by its rotation faster than the speed of light, meaning that from the point of an outside observer any matter inside is forced to move in the direction of the rotation of the black hole.\n",
      "-In this way, rotational energy is extracted from the black hole, resulting in the black hole being spun down to a lower rotational speed. The maximum amount of energy (per mass of the thrown in object) is extracted if the black hole is rotating at the maximal rate, the object just grazes the event horizon and decays into forwards and backwards moving packets of light (the first escapes the black hole, the second falls inside).In an adjunct process, a black hole can be spun up (its rotational speed increased) by sending in particles that do not split up, but instead give their entire angular momentum to the black hole. However, this is not a reverse of the Penrose process, as both increase the entropy of the black hole by throwing material into it.\n",
      "-Inside the ergosphere even light cannot keep up with the rotation of the black hole, as the trajectories of stationary (from the outside perspective) objects become space-like, rather than time-like (that normal matter would have), or light-like. Mathematically, the dt2 component of the metric changes its sign inside the ergosphere. That allows matter to have negative energy inside of the ergosphere as long as it moves counter the black hole's rotation fast enough (or, from outside perspective, resists being dragged along to a sufficient degree). Penrose mechanism exploits that by diving into the ergosphere, dumping an object that was given negative energy, and returning with more energy than before.\n",
      "-In the process, a working body falls (black thick line in the figure) into the ergosphere (gray region). At its lowest point (red dot) the body fires a propellant backwards; however, to a faraway observer both seem to continue to move forward due to frame-dragging (albeit at different speeds). The propellant, being slowed, falls (thin gray line) to the event horizon of the black hole (black disk). And the remains of the body, being sped up, fly away (thin black line) with an excess of energy (that more than offsets the loss of the propellant and the energy used to shoot it).\n",
      "-The outer surface of the ergosphere is the surface at which light that moves in the direction opposite to the rotation of the black hole remains at a fixed angular coordinate, according to an external observer. Since massive particles necessarily travel slower than light, massive particles will necessarily move along with the black hole's rotation. The inner boundary of the ergosphere is the event horizon, the spatial perimeter beyond which light cannot escape.\n",
      "\n",
      "\n",
      "\n",
      "What was the aim of the Gravity Probe B (GP-B) mission?\n",
      "-Gravitomagnetism The existence of gravitomagnetism was proven by Gravity Probe B (GP-B), a satellite-based mission which launched on 20 April 2004. The spaceflight phase lasted until 2005. The mission aim was to measure spacetime curvature near Earth, with particular emphasis on gravitomagnetism.\n",
      "-The satellite was launched on 20 April 2004 on a Delta II rocket. The spaceflight phase lasted until 2005; Its aim was to measure spacetime curvature near Earth, and thereby the stress–energy tensor (which is related to the distribution and the motion of matter in space) in and near Earth. This provided a test of general relativity, gravitomagnetism and related models. The principal investigator was Francis Everitt.\n",
      "-Some preliminary results were presented at a special session during the American Physical Society meeting in April 2007. NASA initially requested a proposal for extending the GP-B data analysis phase through December 2007. The data analysis phase was further extended to September 2008 using funding from Richard Fairbank, Stanford and NASA, and beyond that point using non-NASA funding only. Final science results were reported in 2011.\n",
      "-The Stanford-based analysis group and NASA announced on 4 May 2011 that the data from GP-B indeed confirms the two predictions of Albert Einstein's general theory of relativity. The findings were published in the journal Physical Review Letters. The prospects for further experimental measurement of frame-dragging after GP-B were commented on in the journal Europhysics Letters.\n",
      "-Gravity Probe B (GP-B) was a satellite-based experiment to test two unverified predictions of general relativity: the geodetic effect and frame-dragging. This was to be accomplished by measuring, very precisely, tiny changes in the direction of spin of four gyroscopes contained in an Earth-orbiting satellite at 650 km (400 mi) of altitude, crossing directly over the poles.\n",
      "\n",
      "\n",
      "\n",
      "What was Pierre de Fermat's solution to the problem of refraction?\n",
      "-Fermat vs. the Cartesians In 1657, Pierre de Fermat received from Marin Cureau de la Chambre a copy of newly published treatise, in which La Chambre noted Hero's principle and complained that it did not work for refraction.Fermat replied that refraction might be brought into the same framework by supposing that light took the path of least resistance, and that different media offered different resistances. His eventual solution, described in a letter to La Chambre dated 1 January 1662, construed \"resistance\" as inversely proportional to speed, so that light took the path of least time. That premise yielded the ordinary law of refraction, provided that light traveled more slowly in the optically denser medium.Fermat's solution was a landmark in that it unified the then-known laws of geometrical optics under a variational principle or action principle, setting the precedent for the principle of least action in classical mechanics and the corresponding principles in other fields (see History of variational principles in physics). It was the more notable because it used the method of adequality, which may be understood in retrospect as finding the point where the slope of an infinitesimally short chord is zero, without the intermediate step of finding a general expression for the slope (the derivative).\n",
      "-It was also immediately controversial. The ordinary law of refraction was at that time attributed to René Descartes (d. 1650), who had tried to explain it by supposing that light was a force that propagated instantaneously, or that light was analogous to a tennis ball that traveled faster in the denser medium, either premise being inconsistent with Fermat's. Descartes' most prominent defender, Claude Clerselier, criticized Fermat for apparently ascribing knowledge and intent to nature, and for failing to explain why nature should prefer to economize on time rather than distance. Clerselier wrote in part: 1. The principle that you take as the basis of your demonstration, namely that nature always acts in the shortest and simplest ways, is merely a moral principle and not a physical one; it is not, and cannot be, the cause of any effect in nature.... For otherwise we would attribute knowledge to nature; but here, by \"nature\", we understand only this order and this law established in the world as it is, which acts without foresight, without choice, and by a necessary determination.\n",
      "-Laplace, Young, Fresnel, and Lorentz On 30 January 1809, Pierre-Simon Laplace, reporting on the work of his protégé Étienne-Louis Malus, claimed that the extraordinary refraction of calcite could be explained under the corpuscular theory of light with the aid of Maupertuis's principle of least action: that the integral of speed with respect to distance was a minimum. The corpuscular speed that satisfied this principle was proportional to the reciprocal of the ray speed given by the radius of Huygens' spheroid. Laplace continued: According to Huygens, the velocity of the extraordinary ray, in the crystal, is simply expressed by the radius of the spheroid; consequently his hypothesis does not agree with the principle of the least action: but it is remarkable that it agrees with the principle of Fermat, which is, that light passes, from a given point without the crystal, to a given point within it, in the least possible time; for it is easy to see that this principle coincides with that of the least action, if we invert the expression of the velocity.\n",
      "-Maupertuis was the first to publish a principle of least action, where he defined action as  ∫ v d s {\\textstyle \\int v\\,ds} , which was to be minimized over all paths connecting two specified points. However, Maupertuis applied the principle only to light, not matter (see the 1744 Maupertuis reference below). He arrived at the principle by considering Snell's law for the refraction of light, which Fermat had explained by Fermat's principle, that light follows the path of shortest time, not distance. This troubled Maupertuis, since he felt that time and distance should be on an equal footing: \"why should light prefer the path of shortest time over that of distance?\" Accordingly, Maupertuis asserts with no further justification the principle of least action as equivalent but more fundamental than Fermat's principle, and uses it to derive Snell's law. Maupertuis specifically states that light does not follow the same laws as material objects.\n",
      "-Fermat's principle, also known as the principle of least time, is the link between ray optics and wave optics. Fermat's principle states that the path taken by a ray between two given points is the path that can be traveled in the least time.  First proposed by the French mathematician Pierre de Fermat in 1662, as a means of explaining the ordinary law of refraction of light (Fig. 1), Fermat's principle was initially controversial because it seemed to ascribe knowledge and intent to nature. Not until the 19th century was it understood that nature's ability to test alternative paths is merely a fundamental property of waves. If points A and B are given, a wavefront expanding from A sweeps all possible ray paths radiating from A, whether they pass through B or not. If the wavefront reaches point B, it sweeps not only the ray path(s) from A to B, but also an infinitude of nearby paths with the same endpoints. Fermat's principle describes any ray that happens to reach point B; there is no implication that the ray \"knew\" the quickest path or \"intended\" to take that path.\n",
      "\n",
      "\n",
      "\n",
      "What is the reason behind the adoption of a logarithmic scale of 5√100 ≈ 2.512 between magnitudes in astronomy?\n",
      "-Thus in 1856 Norman Pogson of Oxford proposed that a logarithmic scale of 5√100 ≈ 2.512 be adopted between magnitudes, so five magnitude steps corresponded precisely to a factor of 100 in brightness. Every interval of one magnitude equates to a variation in brightness of 5√100 or roughly 2.512 times. Consequently, a magnitude 1 star is about 2.5 times brighter than a magnitude 2 star, about 2.52 times brighter than a magnitude 3 star, about 2.53 times brighter than a magnitude 4 star, and so on.\n",
      "-Luminosity is an intrinsic measurable property of a star independent of distance. The concept of magnitude, on the other hand, incorporates distance. The apparent magnitude is a measure of the diminishing flux of light as a result of distance according to the inverse-square law. The Pogson logarithmic scale is used to measure both apparent and absolute magnitudes, the latter corresponding to the brightness of a star or other celestial body as seen if it would be located at an interstellar distance of 10 parsecs (3.1×1017 metres). In addition to this brightness decrease from increased distance, there is an extra decrease of brightness due to extinction from intervening interstellar dust.By measuring the width of certain absorption lines in the stellar spectrum, it is often possible to assign a certain luminosity class to a star without knowing its distance. Thus a fair measure of its absolute magnitude can be determined without knowing its distance nor the interstellar extinction.\n",
      "-However, by the mid-nineteenth century astronomers had measured the distances to stars via stellar parallax, and so understood that stars are so far away as to essentially appear as point sources of light. Following advances in understanding the diffraction of light and astronomical seeing, astronomers fully understood both that the apparent sizes of stars were spurious and how those sizes depended on the intensity of light coming from a star (this is the star's apparent brightness, which can be measured in units such as watts per square metre) so that brighter stars appeared larger.\n",
      "-Tycho Brahe attempted to directly measure the \"bigness\" of the stars in terms of angular size, which in theory meant that a star's magnitude could be determined by more than just the subjective judgment described in the above quote. He concluded that first magnitude stars measured 2 arc minutes (2′) in apparent diameter (1⁄30 of a degree, or 1⁄15 the diameter of the full moon), with second through sixth magnitude stars measuring 1+1⁄2′, 1+1⁄12′, 3⁄4′, 1⁄2′, and 1⁄3′, respectively. The development of the telescope showed that these large sizes were illusory—stars appeared much smaller through the telescope. However, early telescopes produced a spurious disk-like image of a star that was larger for brighter stars and smaller for fainter ones. Astronomers from Galileo to Jaques Cassini mistook these spurious disks for the physical bodies of stars, and thus into the eighteenth century continued to think of magnitude in terms of the physical size of a star. Johannes Hevelius produced a very precise table of star sizes measured telescopically, but now the measured diameters ranged from just over six seconds of arc for first magnitude down to just under 2 seconds for sixth magnitude. By the time of William Herschel astronomers recognized that the telescopic disks of stars were spurious and a function of the telescope as well as the brightness of the stars, but still spoke in terms of a star's size more than its brightness. Even well into the nineteenth century the magnitude system continued to be described in terms of six classes determined by apparent size, in which There is no other rule for classing the stars but the estimation of the observer; and hence it is that some astronomers reckon those stars of the first magnitude which others esteem to be of the second.\n",
      "-First-magnitude stars are the brightest stars in the night sky, with apparent magnitudes lower (i.e. brighter) than +1.50. Hipparchus, in the 1st century BC, introduced the magnitude scale. He allocated the first magnitude to the 20 brightest stars and the sixth magnitude to the faintest stars visible to the naked eye.\n",
      "In the 19th century, this ancient scale of apparent magnitude was logarithmically defined, so that a star of magnitude 1.00 is exactly 100 times as bright as one of 6.00. The scale was also extended to even brighter celestial bodies such as Sirius (-1.5), Venus (-4), the full Moon (-12.7), and the Sun (-26.7).\n",
      "\n",
      "\n",
      "\n",
      "What is the spin quantum number?\n",
      "-Spin is described mathematically as a vector for some particles such as photons, and as spinors and bispinors for other particles such as electrons. Spinors and bispinors behave similarly to vectors: they have definite magnitudes and change under rotations; however, they use an unconventional \"direction\". All elementary particles of a given kind have the same magnitude of spin angular momentum, though its direction may change. These are indicated by assigning the particle a spin quantum number.: 183–184 The SI unit of spin is the same as classical angular momentum (i.e., N·m·s, J·s, or kg·m2·s−1). In practice, spin is usually given as a dimensionless spin quantum number by dividing the spin angular momentum by the reduced Planck constant ħ, which has the same dimensions as angular momentum. Often, the \"spin quantum number\" is simply called \"spin\".\n",
      "-The name \"spin\" comes from a geometrical spinning of the electron about an axis, as proposed by Uhlenbeck and Goudsmit. However, this simplistic picture was quickly realized to be physically unrealistic, because it would require the electrons to rotate faster than the speed of light. It was therefore replaced by a more abstract quantum-mechanical description.\n",
      "-In physics, the spin quantum number is a quantum number (designated s) that describes the intrinsic angular momentum (or spin angular momentum, or simply spin) of an electron or other particle. It has the same value for all particles of the same type, such as s = 1/2 for all electrons. It is an integer for all bosons, such as photons, and a half-odd-integer for all fermions, such as electrons and protons. The component of the spin along a specified axis is given by the spin magnetic quantum number, conventionally written ms. The value of ms is the component of spin angular momentum, in units of the reduced Planck constant ħ, parallel to a given direction (conventionally labelled the z–axis). It can take values ranging from +s to −s in integer increments. For an electron, ms can be either ++1/2 or −+1/2 .\n",
      "-Spin obeys the mathematical laws of angular momentum quantization. The specific properties of spin angular momenta include: Spin quantum numbers may take half-integer values.\n",
      "Although the direction of its spin can be changed, the magnitude of the spin of an elementary particle cannot be changed.\n",
      "-The spin of a charged particle is associated with a magnetic dipole moment with a g-factor that differs from 1. (In the classical context, this would imply the internal charge and mass distributions differing for a rotating object.)The conventional definition of the spin quantum number is s = n/2, where n can be any non-negative integer. Hence the allowed values of s are 0, 1/2, 1, 3/2, 2, etc. The value of s for an elementary particle depends only on the type of particle and cannot be altered in any known way (in contrast to the spin direction described below). The spin angular momentum S of any physical system is quantized. The allowed values of S are where h is the Planck constant, and  ℏ = h 2 π {\\textstyle \\hbar ={\\frac {h}{2\\pi }}} is the reduced Planck constant. In contrast, orbital angular momentum can only take on integer values of s; i.e., even-numbered values of n.\n",
      "\n",
      "\n",
      "\n",
      "What is the synapstor or synapse transistor?\n",
      "-A synaptic transistor is an electrical device that can learn in ways similar to a neural synapse. It optimizes its own properties for the functions it has carried out in the past. The device mimics the behavior of the property of neurons called spike-timing-dependent plasticity, or STDP.\n",
      "-In July 2008, Erokhin and Fontana claimed to have developed a polymeric memristor before the more recently announced titanium dioxide memristor.In 2010, Alibart, Gamrat, Vuillaume et al. introduced a new hybrid organic/nanoparticle device (the NOMFET : Nanoparticle Organic Memory Field Effect Transistor), which behaves as a memristor and which exhibits the main behavior of a biological spiking synapse. This device, also called a synapstor (synapse transistor), was used to demonstrate a neuro-inspired circuit (associative memory showing a pavlovian learning).In 2012, Crupi, Pradhan and Tozer described a proof of concept design to create neural synaptic memory circuits using organic ion-based memristors. The synapse circuit demonstrated long-term potentiation for learning as well as inactivity based forgetting. Using a grid of circuits, a pattern of light was stored and later recalled. This mimics the behavior of the V1 neurons in the primary visual cortex that act as spatiotemporal filters that process visual signals such as edges and moving lines.\n",
      "-The model for gated synapses was originally derived from the model electronic circuit, in which the gatekeeper serves as a transistor in a circuit. In a circuit, a transistor can act as a switch that turns an electrical signal on or off. In addition, a transistor can serve to amplify an existing current in a circuit. In effect, the gatekeeper neuron acts as the transistor of a gated synapse by modulating the transmission of the signal between the pre-synaptic and post-synaptic neurons.\n",
      "-NOMFET is a nanoparticle organic memory field-effect transistor. The transistor is designed to mimic the feature of the human synapse known as plasticity, or the variation of the speed and strength of the signal going from neuron to neuron. The device uses gold nano-particles of about 5—20 nm set with pentacene to emulate the change in voltages and speed within the signal. This device uses charge trapping/detrapping in an array of gold nanoparticles (NPs) at the SiO2/pentacene interface to design a SYNAPSTOR (synapse transistor) mimicking the dynamic plasticity of a biological synapse. This device (memristor-like) mimics short-term plasticity (STP) and temporal correlation plasticity (STDP, spike-timing dependent plasticity), two \"functions\" at the basis of learning processes. A compact model was developed, and these organic synapstors were used to demonstrate an associative memory, which can be trained to present a pavlovian response. A recent report showed that these organic synapse-transistors (synapstor) are working at 1 volt and with a plasticity typical response time in the range 100-200 ms. The device also works in contact with an electrolyte (EGOS : electrolyte gated organic synapstor) and can be interfaced with biologic neurons. The recent creation of this novel transistor gives prospects to better recreation of certain types of human cognitive processes, such as recognition and image processing. When the NOMFET is used in a neuromorphic circuit it is able to replicate the functionality of plasticity that previously required groups of several transistors to emulate and thus continue to decrease the size of the processor that would be attempting to utilize the computational advantages of a pseudo-synaptic operation. (See Moore's Law) \n",
      "-A synaptic transistor has a traditional immediate response whose amount of current that passes between the source and drain contacts varies with voltage applied to the gate electrode. It also produces a much slower learned response such that the conductivity of the SNO layer varies in response to the transistor's STDP history, essentially by shuttling oxygen ions between the SNO and the ionic liquid.The analog of strengthening a synapse is to increase the SNO's conductivity, which essentially increases gain. Similarly, weakening a synapse is analogous to decreasing the SNO's conductivity, lowering the gain.The input and output of the synaptic transistor are continuous analog values, rather than digital on-off signals. While the physical structure of the device has the potential to learn from history, it contains no way to bias the transistor to control the memory effect. An external supervisory circuit converts the time delay between input and output into a voltage applied to the ionic liquid that either drives ions into the SNO or removes them.A network of such devices can learn particular responses to \"sensory inputs\", with those responses being learned through experience rather than explicitly programmed.\n",
      "\n",
      "\n",
      "\n",
      "What is spontaneous symmetry breaking?\n",
      "-Spontaneous symmetry breaking is a spontaneous process of symmetry breaking, by which a physical system in a symmetric state spontaneously ends up in an asymmetric state. In particular, it can describe systems where the equations of motion or the Lagrangian obey symmetries, but the lowest-energy vacuum solutions do not exhibit that same symmetry. When the system goes to one of those vacuum solutions, the symmetry is broken for perturbations around that vacuum even though the entire Lagrangian retains that symmetry.\n",
      "-By definition, spontaneous symmetry breaking requires the existence of physical laws (e.g. quantum mechanics) which are invariant under a symmetry transformation (such as translation or rotation), so that any pair of outcomes differing only by that transformation have the same probability distribution. For example if measurements of an observable at any two different positions have the same probability distribution, the observable has translational symmetry.  Spontaneous symmetry breaking occurs when this relation breaks down, while the underlying physical laws remain symmetrical.\n",
      "-This section describes spontaneous symmetry breaking. In layman's terms, this is the idea that for a physical system, the lowest energy configuration (the vacuum state) is not the most symmetric configuration of the system. Roughly speaking there are three types of symmetry that can be broken: discrete, continuous and gauge, ordered in increasing technicality.\n",
      "-Showing that a system admits spontaneous symmetry breaking requires introducing a weak external source field that breaks the symmetry and gives rise to a preferred ground state. The system is then taken to the thermodynamic limit after which the external source field is switched off. If the vacuum expectation value of symmetry non-invariant operators is nonzero in this limit then there is spontaneous symmetry breaking. Physically it means that the system never leaves the original ground state into which it was placed by the external field. For global symmetries this occurs because the energy barrier between the various ground states is proportional to the volume, so in the thermodynamic limit this diverges, locking the system into the ground state. Local symmetries get around this construction because the energy barrier between two ground states depends only on local features so transitions to different gauge related ground states can occur locally and does not require the field to change everywhere at the same time as it does for global symmetries.\n",
      "-Such a symmetry breaking is parametrized by an order parameter. A special case of this type of symmetry breaking is dynamical symmetry breaking.\n",
      "\n",
      "\n",
      "\n",
      "What is the proper distance for a redshift of 8.2?\n",
      "-The most distant astronomical object identified (as of 2022) is a galaxy classified as HD1, with a redshift of 13.27, corresponding to a distance of about 33.4 billion light years. In 2009, a gamma ray burst, GRB 090423, was found to have a redshift of 8.2, which indicates that the collapsing star that caused it exploded when the universe was only 630 million years old. The burst happened approximately 13 billion years ago, so a distance of about 13 billion light-years was widely quoted in the media (or sometimes a more precise figure of 13.035 billion light-years) - however, this would be the \"light travel distance\" (see Distance measures (cosmology)) rather than the \"proper distance\" used in both Hubble's law and in defining the size of the observable universe, and cosmologist Ned Wright argues against using this measure. The proper distance for a redshift of 8.2 would be about 9.2 Gpc, or about 30 billion light-years.\n",
      "-Using a model of the expansion of the universe, redshift can be related to the age of an observed object, the so-called cosmic time–redshift relation. Denote a density ratio as Ω0: crit , with ρcrit the critical density demarcating a universe that eventually crunches from one that simply expands. This density is about three hydrogen atoms per cubic meter of space. At large redshifts, 1 + z > Ω0−1, one finds: t(z)≈23H0Ω01/2(1+z)3/2, where H0 is the present-day Hubble constant, and z is the redshift.There are websites for calculating light-travel distance from redshift.\n",
      "-Note that the comoving distance is recovered from the transverse comoving distance by taking the limit  Ωk→0 , such that the two distance measures are equivalent in a flat universe.\n",
      "There are websites for calculating light-travel distance from redshift.The age of the universe then becomes  lim z→∞dT(z)/c , and the time elapsed since redshift  z until now is:  t(z)=dT(z)/c.\n",
      "Etherington's distance duality The Etherington's distance-duality equation is the relationship between the luminosity distance of standard candles and the angular-diameter distance. It is expressed as follows:  dL=(1+z)2dA \n",
      "-124 Ym – redshift 7.54 – 13.1 billion light-years – Light travel distance (LTD) to the quasar ULAS J1342+0928, the most distant-known quasar as of 2017 130 Ym – redshift 1,000 – 13.8 billion light-years – Distance (LTD) to the source of the cosmic microwave background radiation; radius of the observable universe measured as a LTD 260 Ym – 27.4 billion light-years – Diameter of the observable universe (double LTD) 440 Ym – 46 billion light-years – Radius of the universe measured as a comoving distance 590 Ym – 62 billion light-years – Cosmological event horizon: the largest comoving distance from which light will ever reach us (the observer) at any time in the future 886.48 Ym – 93.7 billion light-years – The diameter of the observable universe (twice the particle horizon); however, there might be unobserved distances that are even greater.\n",
      "-When accounting for the earth's peculiar velocity, the redshift that would pertain in that case should be used but  dA should be corrected for the motion of the solar system by a factor between 0.99867 and 1.00133, depending on the direction. (If one starts to move with velocity v towards an object, at any distance, the angular diameter of that object decreases by a factor of  (1+v/c)/(1−v/c).\n",
      "\n",
      "\n",
      "\n",
      "Who was the first to determine the velocity of a star moving away from the Earth using the Doppler effect?\n",
      "-The history of the subject began with the development in the 19th century of classical wave mechanics and the exploration of phenomena associated with the Doppler effect. The effect is named after Christian Doppler, who offered the first known physical explanation for the phenomenon in 1842. The hypothesis was tested and confirmed for sound waves by the Dutch scientist Christophorus Buys Ballot in 1845. Doppler correctly predicted that the phenomenon should apply to all waves, and in particular suggested that the varying colors of stars could be attributed to their motion with respect to the Earth. Before this was verified, however, it was found that stellar colors were primarily due to a star's temperature, not motion. Only later was Doppler vindicated by verified redshift observations.The first Doppler redshift was described by French physicist Hippolyte Fizeau in 1848, who pointed to the shift in spectral lines seen in stars as being due to the Doppler effect. The effect is sometimes called the \"Doppler–Fizeau effect\". In 1868, British astronomer William Huggins was the first to determine the velocity of a star moving away from the Earth by this method. In 1871, optical redshift was confirmed when the phenomenon was observed in Fraunhofer lines using solar rotation, about 0.1 Å in the red. In 1887, Vogel and Scheiner discovered the annual Doppler effect, the yearly change in the Doppler shift of stars located near the ecliptic due to the orbital velocity of the Earth. In 1901, Aristarkh Belopolsky verified optical redshift in the laboratory using a system of rotating mirrors.Arthur Eddington used the term red shift as early as 1923. The word does not appear unhyphenated until about 1934 by Willem de Sitter.Beginning with observations in 1912, Vesto Slipher discovered that most spiral galaxies, then mostly thought to be spiral nebulae, had considerable redshifts. Slipher first reports on his measurement in the inaugural volume of the Lowell Observatory Bulletin. Three years later, he wrote a review in the journal Popular Astronomy. In it he states that \"the early discovery that the great Andromeda spiral had the quite exceptional velocity of –300 km(/s) showed the means then available, capable of investigating not only the spectra of the spirals but their velocities as well.\" Slipher reported the velocities for 15 spiral nebulae spread across the entire celestial sphere, all but three having observable \"positive\" (that is recessional) velocities. Subsequently, Edwin Hubble discovered an approximate relationship between the redshifts of such \"nebulae\" and the distances to them with the formulation of his eponymous Hubble's law. These observations corroborated Alexander Friedmann's 1922 work, in which he derived the Friedmann–Lemaître equations. In the present day they are considered strong evidence for an expanding universe and the Big Bang theory.\n",
      "-Doppler first proposed this effect in 1842 in his treatise \"Über das farbige Licht der Doppelsterne und einiger anderer Gestirne des Himmels\" (On the coloured light of the binary stars and some other stars of the heavens). The hypothesis was tested for sound waves by Buys Ballot in 1845. He confirmed that the sound's pitch was higher than the emitted frequency when the sound source approached him, and lower than the emitted frequency when the sound source receded from him. Hippolyte Fizeau discovered independently the same phenomenon on electromagnetic waves in 1848 (in France, the effect is sometimes called \"effet Doppler-Fizeau\" but that name was not adopted by the rest of the world as Fizeau's discovery was six years after Doppler's proposal). In Britain, John Scott Russell made an experimental study of the Doppler effect (1848).\n",
      "-1832–1838 – Following over 100 years of unsuccessful attempts, Thomas Henderson, Friedrich Bessel, and Otto Struve measure the parallax of a few nearby stars; these are the first measurements of any distances outside the Solar System.1842 – Christian Doppler proposes the redshift and blueshift effects, based on an analog effect found in sound. Hippolyte Fizeau discovered independently the same phenomenon on electromagnetic waves in 1848.\n",
      "-The use of the Doppler effect in astronomy depends on knowledge of precise frequencies of discrete lines in the spectra of stars.\n",
      "Among the nearby stars, the largest radial velocities with respect to the Sun are +308 km/s (BD-15°4041, also known as LHS 52, 81.7 light-years away) and −260 km/s (Woolley 9722, also known as Wolf 1106 and LHS 64, 78.2 light-years away). Positive radial speed means the star is receding from the Sun, negative that it is approaching.\n",
      "-William Huggins ventured in 1868 to estimate the radial velocity of Sirius with respect to the Sun, based on observed redshift of the star's light.\n",
      "\n",
      "\n",
      "\n",
      "What is the information loss paradox in black holes?\n",
      "-Information loss paradox Because a black hole has only a few internal parameters, most of the information about the matter that went into forming the black hole is lost. Regardless of the type of matter which goes into a black hole, it appears that only information concerning the total mass, charge, and angular momentum are conserved. As long as black holes were thought to persist forever this information loss is not that problematic, as the information can be thought of as existing inside the black hole, inaccessible from the outside, but represented on the event horizon in accordance with the holographic principle. However, black holes slowly evaporate by emitting Hawking radiation. This radiation does not appear to carry any additional information about the matter that formed the black hole, meaning that this information appears to be gone forever.The question whether information is truly lost in black holes (the black hole information paradox) has divided the theoretical physics community. In quantum mechanics, loss of information corresponds to the violation of a property called unitarity, and it has been argued that loss of unitarity would also imply violation of conservation of energy, though this has also been disputed. Over recent years evidence has been building that indeed information and unitarity are preserved in a full quantum gravitational treatment of the problem.One attempt to resolve the black hole information paradox is known as black hole complementarity. In 2012, the \"firewall paradox\" was introduced with the goal of demonstrating that black hole complementarity fails to solve the information paradox. According to quantum field theory in curved spacetime, a single emission of Hawking radiation involves two mutually entangled particles. The outgoing particle escapes and is emitted as a quantum of Hawking radiation; the infalling particle is swallowed by the black hole. Assume a black hole formed a finite time in the past and will fully evaporate away in some finite time in the future. Then, it will emit only a finite amount of information encoded within its Hawking radiation. According to research by physicists like Don Page and Leonard Susskind, there will eventually be a time by which an outgoing particle must be entangled with all the Hawking radiation the black hole has previously emitted. This seemingly creates a paradox: a principle called \"monogamy of entanglement\" requires that, like any quantum system, the outgoing particle cannot be fully entangled with two other systems at the same time; yet here the outgoing particle appears to be entangled both with the infalling particle and, independently, with past Hawking radiation. In order to resolve this contradiction, physicists may eventually be forced to give up one of three time-tested principles: Einstein's equivalence principle, unitarity, or local quantum field theory. One possible solution, which violates the equivalence principle, is that a \"firewall\" destroys incoming particles at the event horizon. In general, which—if any—of these assumptions should be abandoned remains a topic of debate.\n",
      "-The temperature of the black hole is in turn dependent on the mass, charge and angular momentum of the black hole. For a Schwarzschild black hole the temperature is given by This means that if the black hole starts out with an initial mass  M0 , it evaporates completely in a time proportional to  M03 The important aspect of the formulas above is they suggest that the final gas of radiation formed through this process depends only on the black hole's temperature and is independent of other details of the initial state. This leads to the following paradox. Consider two distinct initial states that collapse to form a Schwarzschild black hole of the same mass. Even though the states were distinct to start with, since the mass (and, hence, the temperature) of the black holes is the same, they will emit the same Hawking radiation. Once the black holes evaporate completely, in both cases, one will be left with a featureless gas of radiation. This gas cannot be used to distinguish between the two initial states, and therefore information has been lost.\n",
      "-The simplest models of black hole evaporation lead to the black hole information paradox. The information content of a black hole appears to be lost when it dissipates, as under these models the Hawking radiation is random (it has no relation to the original information). A number of solutions to this problem have been proposed, including suggestions that Hawking radiation is perturbed to contain the missing information, that the Hawking evaporation leaves some form of remnant particle containing the missing information, and that information is allowed to be lost under these conditions.\n",
      "-Information is stored in a large remnantThis idea suggests that Hawking radiation stops before the black hole reaches the Planck size. Since the black hole never evaporates, information about its initial state can remain inside the black hole and the paradox disappears. However, there is no accepted mechanism that would allow Hawking radiation to stop while the black hole remains macroscopic.\n",
      "-The information paradox appears when one considers a process in which a black hole is formed through a physical process and then evaporates away entirely through Hawking radiation. Hawking's calculation suggests that the final state of radiation would retain information only about the total mass, electric charge and angular momentum of the initial state. Since many different states can have the same mass, charge and angular momentum, this suggests that many initial physical states could evolve into the same final state. Therefore, information about the details of the initial state would be permanently lost; however, this violates a core precept of both classical and quantum physics—that, in principle, the state of a system at one point in time should determine its value at any other time. Specifically, in quantum mechanics the state of the system is encoded by its wave function. The evolution of the wave function is determined by a unitary operator, and unitarity implies that the wave function at any instant of time can be used to determine the wave function either in the past or the future. In 1993, Don Page argued that if a black hole starts in a pure quantum state and evaporates completely by a unitary process, the von Neumann entropy of the Hawking radiation initially increases and then decreases back to zero when the black hole has disappeared. This is called the Page curve.It is now generally believed that information is preserved in black-hole evaporation. For many researchers, deriving the Page curve is synonymous with solving the black hole information puzzle.: 291  However, views differ as to how, precisely, Hawking's original semi-classical calculation should be corrected. In recent years, several extensions of the original paradox have been explored. Taken together these puzzles about black hole evaporation have implications for how gravity and quantum mechanics must be combined, leading to the information paradox remaining an active field of research within quantum gravity.\n",
      "\n",
      "\n",
      "\n",
      "What is the Kutta condition?\n",
      "-The Kutta condition is a principle in steady-flow fluid dynamics, especially aerodynamics, that is applicable to solid bodies with sharp corners, such as the trailing edges of airfoils. It is named for German mathematician and aerodynamicist Martin Kutta.\n",
      "Kuethe and Schetzer state the Kutta condition as follows:: § 4.11 A body with a sharp trailing edge which is moving through a fluid will create about itself a circulation of sufficient strength to hold the rear stagnation point at the trailing edge.\n",
      "In fluid flow around a body with a sharp corner, the Kutta condition refers to the flow pattern in which fluid approaches the corner from above and below, meets at the corner, and then flows away from the body. None of the fluid flows around the sharp corner.\n",
      "The Kutta condition is significant when using the Kutta–Joukowski theorem to calculate the lift created by an airfoil with a sharp trailing edge. The value of circulation of the flow around the airfoil must be that value which would cause the Kutta condition to exist.\n",
      "-As the airfoil continues on its way, there is a stagnation point at the trailing edge. The flow over the topside conforms to the upper surface of the airfoil. The flow over both the topside and the underside join up at the trailing edge and leave the airfoil travelling parallel to one another. This is known as the Kutta condition.: § 4.8 When an airfoil is moving with an angle of attack, the starting vortex has been cast off and the Kutta condition has become established, there is a finite circulation of the air around the airfoil. The airfoil is generating lift, and the magnitude of the lift is given by the Kutta–Joukowski theorem.: § 4.5 One of the consequences of the Kutta condition is that the airflow over the topside of the airfoil travels much faster than the airflow under the underside. A parcel of air which approaches the airfoil along the stagnation streamline is cleaved in two at the stagnation point, one half traveling over the topside and the other half traveling along the underside. The flow over the topside is so much faster than the flow along the underside that these two halves never meet again. They do not even re-join in the wake long after the airfoil has passed. There is a popular fallacy called the equal transit-time fallacy that claims the two halves rejoin at the trailing edge of the airfoil. This has been understood as a fallacy since Martin Kutta's discovery.\n",
      "-Any real fluid is viscous, which implies that the fluid velocity vanishes on the airfoil. Prandtl showed that for large Reynolds number, defined as  Re =ρV∞cAμ , and small angle of attack, the flow around a thin airfoil is composed of a narrow viscous region called the boundary layer near the body and an inviscid flow region outside. In applying the Kutta-Joukowski theorem, the loop must be chosen outside this boundary layer. (For example, the circulation calculated using the loop corresponding to the surface of the airfoil would be zero for a viscous fluid.) The sharp trailing edge requirement corresponds physically to a flow in which the fluid moving along the lower and upper surfaces of the airfoil meet smoothly, with no fluid moving around the trailing edge of the airfoil. This is known as the Kutta condition.\n",
      "-Kutta condition – is a principle in steady-flow fluid dynamics, especially aerodynamics, that is applicable to solid bodies with sharp corners, such as the trailing edges of airfoils. It is named for German mathematician and aerodynamicist Martin Kutta.Kuethe and Schetzer state the Kutta condition as follows:: § 4.11 A body with a sharp trailing edge which is moving through a fluid will create about itself a circulation of sufficient strength to hold the rear stagnation point at the trailing edge.In fluid flow around a body with a sharp corner, the Kutta condition refers to the flow pattern in which fluid approaches the corner from above and below, meets at the corner, and then flows away from the body. None of the fluid flows around the sharp corner.The Kutta condition is significant when using the Kutta–Joukowski theorem to calculate the lift created by an airfoil with a sharp trailing edge. The value of circulation of the flow around the airfoil must be that value that would cause the Kutta condition to exist.Kutta–Joukowski theorem – is a fundamental theorem in aerodynamics used for the calculation of lift of an airfoil and any two-dimensional bodies including circular cylinders translating into a uniform fluid at a constant speed large enough so that the flow seen in the body-fixed frame is steady and unseparated. The theorem relates the lift generated by an airfoil to the speed of the airfoil through the fluid, the density of the fluid and the circulation around the airfoil. The circulation is defined as the line integral around a closed-loop enclosing the airfoil of the component of the velocity of the fluid tangent to the loop. It is named after Martin Kutta and Nikolai Zhukovsky (or Joukowski) who first developed its key ideas in the early 20th century. Kutta–Joukowski theorem is an inviscid theory, but it is a good approximation for real viscous flow in typical aerodynamic applications.\n",
      "-The condition can be expressed in a number of ways. One is that there cannot be an infinite change in velocity at the trailing edge. Although an inviscid fluid can have abrupt changes in velocity, in reality viscosity smooths out sharp velocity changes. If the trailing edge has a non-zero angle, the flow velocity there must be zero. At a cusped trailing edge, however, the velocity can be non-zero although it must still be identical above and below the airfoil. Another formulation is that the pressure must be continuous at the trailing edge.\n",
      "\n",
      "\n",
      "\n",
      "What is classical mechanics?\n",
      "-Classical mechanics is a model of the physics of forces acting upon bodies; includes sub-fields to describe the behaviors of solids, gases, and fluids. It is often referred to as \"Newtonian mechanics\" after Isaac Newton and his laws of motion. It also includes the classical approach as given by Hamiltonian and Lagrange methods. It deals with the motion of particles and the general system of particles.\n",
      "-Since the end of the 20th century, classical mechanics in physics has no longer been an independent theory. Instead, classical mechanics is now considered an approximate theory to the more general quantum mechanics. Emphasis has shifted to understanding the fundamental forces of nature as in the Standard Model and its more modern extensions into a unified theory of everything. Classical mechanics is a theory useful for the study of the motion of non-quantum mechanical, low-energy particles in weak gravitational fields. Also, it has been extended into the complex domain where complex classical mechanics exhibits behaviors very similar to quantum mechanics.\n",
      "-Classical mechanics is a physical theory describing the motion of macroscopic objects, from projectiles to parts of machinery and astronomical objects, such as spacecraft, planets, stars, and galaxies. For objects governed by classical mechanics, if the present state is known, it is possible to predict how it will move in the future (determinism), and how it has moved in the past (reversibility).\n",
      "-There are many branches of classical mechanics, such as: statics, dynamics, kinematics, continuum mechanics (which includes fluid mechanics), statistical mechanics, etc.\n",
      "Mechanics: A branch of physics in which we study the object and properties of an object in form of a motion under the action of the force.\n",
      "-Classical theory has at least two distinct meanings in physics. In the context of quantum mechanics, classical theory refers to theories of physics that do not use the quantisation paradigm, which includes classical mechanics and relativity. Likewise, classical field theories, such as general relativity and classical electromagnetism, are those that do not use quantum mechanics. In the context of general and special relativity, classical theories are those that obey Galilean relativity.Depending on point of view, among the branches of theory sometimes included in classical physics are variably: Classical mechanics Newton's laws of motion Classical Lagrangian and Hamiltonian formalisms Classical electrodynamics (Maxwell's Equations) Classical thermodynamics Special relativity and general relativity Classical chaos theory and nonlinear dynamics \n",
      "\n",
      "\n",
      "\n",
      "Who shared the other half of the Nobel Prize with Yoichiro Nambu for discovering the origin of the explicit breaking of CP symmetry in the weak interactions?\n",
      "-On October 7, 2008, the Royal Swedish Academy of Sciences awarded the 2008 Nobel Prize in Physics to three scientists for their work in subatomic physics symmetry breaking. Yoichiro Nambu, of the University of Chicago, won half of the prize for the discovery of the mechanism of spontaneous broken symmetry in the context of the strong interactions, specifically chiral symmetry breaking. Physicists Makoto Kobayashi and Toshihide Maskawa, of Kyoto University, shared the other half of the prize for discovering the origin of the explicit breaking of CP symmetry in the weak interactions. This origin is ultimately reliant on the Higgs mechanism, but, so far understood as a \"just so\" feature of Higgs couplings, not a spontaneously broken symmetry phenomenon.\n",
      "-Indirect CP violation In 1964, James Cronin, Val Fitch and coworkers provided clear evidence from kaon decay that CP-symmetry could be broken. This work won them the 1980 Nobel Prize. This discovery showed that weak interactions violate not only the charge-conjugation symmetry C between particles and antiparticles and the P or parity, but also their combination. The discovery shocked particle physics and opened the door to questions still at the core of particle physics and of cosmology today. The lack of an exact CP-symmetry, but also the fact that it is so close to a symmetry, introduced a great puzzle.\n",
      "-However, this theory allowed a compound symmetry CP to be conserved. CP combines parity P (switching left to right) with charge conjugation C (switching particles with antiparticles). Physicists were again surprised when in 1964, James Cronin and Val Fitch provided clear evidence in kaon decays that CP symmetry could be broken too, winning them the 1980 Nobel Prize in Physics. In 1973, Makoto Kobayashi and Toshihide Maskawa showed that CP violation in the weak interaction required more than two generations of particles, effectively predicting the existence of a then unknown third generation. This discovery earned them half of the 2008 Nobel Prize in Physics.Unlike parity violation, CP violation occurs only in rare circumstances. Despite its limited occurrence under present conditions, it is widely believed to be the reason that there is much more matter than antimatter in the universe, and thus forms one of Andrei Sakharov's three conditions for baryogenesis.\n",
      "-Chiral symmetry breaking, the spontaneous symmetry breaking of an important global symmetry of quarks, detailed below, with the result of generating masses for hadrons far above the masses of the quarks, and making pseudoscalar mesons exceptionally light. Yoichiro Nambu was awarded the 2008 Nobel Prize in Physics for elucidating the phenomenon, a dozen years before the advent of QCD. Lattice simulations have confirmed all his generic predictions.\n",
      "-In particle physics, chiral symmetry breaking is the spontaneous symmetry breaking of a chiral symmetry – usually by a gauge theory such as quantum chromodynamics, the quantum field theory of the strong interaction. Yoichiro Nambu was awarded the 2008 Nobel prize in physics for describing this phenomenon (\"for the discovery of the mechanism of spontaneous broken symmetry in subatomic physics\").\n",
      "\n",
      "\n",
      "\n",
      "What are some models that attempt to account for all observations without invoking supplemental non-baryonic matter?\n",
      "-Some widely accepted and studied theories and models in astronomy, now included in the Lambda-CDM model are the Big Bang, Cosmic inflation, dark matter, and fundamental theories of physics.\n",
      "A few examples of this process: \n",
      "-Although the Standard Model is believed to be theoretically self-consistent and has demonstrated success in providing experimental predictions, it leaves some physical phenomena unexplained and so falls short of being a complete theory of fundamental interactions. For example, it does not fully explain baryon asymmetry, incorporate the full theory of gravitation as described by general relativity, or account for the universe's accelerating expansion as possibly described by dark energy. The model does not contain any viable dark matter particle that possesses all of the required properties deduced from observational cosmology. It also does not incorporate neutrino oscillations and their non-zero masses. Accordingly, it is used as a basis for building more exotic models that incorporate hypothetical particles, extra dimensions, and elaborate symmetries (such as supersymmetry) to explain experimental results at variance with the Standard Model, such as the existence of dark matter and neutrino oscillations.\n",
      "-Although the Standard Model is believed to be theoretically self-consistent and has demonstrated some success in providing experimental predictions, it leaves some physical phenomena unexplained and so falls short of being a complete theory of fundamental interactions. For example, it does not fully explain baryon asymmetry, incorporate the full theory of gravitation as described by general relativity, or account for the universe's accelerating expansion as possibly described by dark energy. The model does not contain any viable dark matter particle that possesses all of the required properties deduced from observational cosmology. It also does not incorporate neutrino oscillations and their non-zero masses.\n",
      "-Dark matter explanation While acknowledging that Milgrom's law provides a succinct and accurate description of a range of galactic phenomena, many physicists reject the idea that classical dynamics itself needs to be modified and attempt instead to explain the law's success by reference to the behavior of dark matter. Some effort has gone towards establishing the presence of a characteristic acceleration scale as a natural consequence of the behavior of cold dark matter halos, although Milgrom has argued that such arguments explain only a small subset of MOND phenomena. An alternative proposal is to modify the properties of dark matter (e.g., to make it interact strongly with itself or baryons) in order to induce the tight coupling between the baryonic and dark matter mass that the observations point to. Finally, some researchers suggest that explaining the empirical success of Milgrom's law requires a more radical break with conventional assumptions about the nature of dark matter. One idea (dubbed \"dipolar dark matter\") is to make dark matter gravitationally polarizable by ordinary matter and have this polarization enhance the gravitational attraction between baryons.\n",
      "-Cosmological observations of phenomena such as the cosmic microwave background and the cosmic abundance of elements, together with the predictions of the Standard Model of particle physics, place constraints on the physical conditions in the early universe. The success of the Standard Model at explaining these observations support its validity under conditions beyond those which can be produced in a laboratory. Conversely, phenomena discovered through cosmological observations, such as dark matter and baryon asymmetry, suggest the presence of physics that goes beyond the Standard Model.\n",
      "\n",
      "\n",
      "\n",
      "What is the purpose of the proximity-focusing design in a RICH detector?\n",
      "-The most advanced type of a detector is the RICH, or ring-imaging Cherenkov detector, developed in the 1980s. In a RICH detector, a cone of Cherenkov light is produced when a high-speed charged particle traverses a suitable medium, often called radiator. This light cone is detected on a position sensitive planar photon detector, which allows reconstructing a ring or disc, whose radius is a measure for the Cherenkov emission angle. Both focusing and proximity-focusing detectors are in use. In a focusing RICH detector, the photons are collected by a spherical mirror and focused onto the photon detector placed at the focal plane. The result is a circle with a radius independent of the emission point along the particle track. This scheme is suitable for low refractive index radiators—i.e. gases—due to the larger radiator length needed to create enough photons. In the more compact proximity-focusing design, a thin radiator volume emits a cone of Cherenkov light which traverses a small distance—the proximity gap—and is detected on the photon detector plane. The image is a ring of light whose radius is defined by the Cherenkov emission angle and the proximity gap. The ring thickness is determined by the thickness of the radiator. An example of a proximity gap RICH detector is the High Momentum Particle Identification Detector (HMPID), a detector currently under construction for ALICE (A Large Ion Collider Experiment), one of the six experiments at the LHC (Large Hadron Collider) at CERN.\n",
      "-RICH Types Both focusing and proximity-focusing detectors are in use. In a focusing RICH detector, the photons are collected by a spherical mirror with focal length  f and focused onto the photon detector placed at the focal plane. The result is a circle with a radius  r=fθc , independent of the emission point along the particle's track ( θc≪1 ). This scheme is suitable for low refractive index radiators (i.e., gases) with their larger radiator length needed to create enough photons.\n",
      "-In the more compact proximity-focusing design a thin radiator volume emits a cone of Cherenkov light which traverses a small distance, the proximity gap, and is detected on the photon detector plane. The image is a ring of light the radius of which is defined by the Cherenkov emission angle and the proximity gap. The ring thickness is mainly determined by the thickness of the radiator. An example of a proximity gap RICH detector is the High Momentum Particle Identification (HMPID), one of the detectors of ALICE (A Large Ion Collider Experiment), which is one of the five experiments at the LHC (Large Hadron Collider) at CERN.\n",
      "-In a RICH detector the photons within this light-cone pass through an optical system and impinge upon a position sensitive photon detector. With a suitably focusing optical system this allows reconstruction of a ring, similar to that above, the radius of which gives a measure of the Cherenkov emission angle  θc . The resolving power of this method is illustrated by comparing the Cherenkov angle per photon, see the first plot above, with the mean Cherenkov angle per particle (averaged over all photons emitted by that particle) obtained by ring-imaging, shown below; the greatly enhanced separation between particle types is very clear: This ability of a RICH system to successfully resolve different hypotheses for the particle type depends on two principal factors, which in turn depend upon the listed sub-factors; The effective angular resolution per photon,  σ Chromatic dispersion in the radiator ( n varies with photon frequency) Aberrations in the optical system Position resolution of the photon detector The maximum number of detected photons in the ring-image,  Nc The length of radiator through which the particle travels Photon transmission through the radiator material Photon transmission through the optical system Quantum efficiency of the photon detectors σ is a measure of the intrinsic optical precision of the RICH detector.  Nc is a measure of the optical response of the RICH; it can be thought of as the limiting case of the number of actually detected photons produced by a particle whose velocity approaches that of light, averaged over all relevant particle trajectories in the RICH detector. The average number of Cherenkov photons detected, for a slower particle, of charge  q (normally ±1), emitting photons at angle  θc is then sin 2⁡(θc)1−1n2 and the precision with which the mean Cherenkov angle can be determined with these photons is approximately σm=σN to which the angular precision of the emitting particle's measured direction must be added in quadrature, if it is not negligible compared to  σm Given the known momentum of the emitting particle and the refractive index of the radiator, the expected Cherenkov angle for each particle type can be predicted, and its difference from the observed mean Cherenkov angle calculated. Dividing this difference by  σm then gives a measure of the 'number of sigma' deviation of the hypothesis from the observation, which can be used in computing a probability or likelihood for each possible hypothesis. The following figure shows the 'number of sigma' deviation of the kaon hypothesis from a true pion ring image (π not k) and of the pion hypothesis from a true kaon ring image (k not π), as a function of momentum, for a RICH with  n = 1.0005,  Nc = 25,  σ = 0.64 milliradians; Also shown are the average number of detected photons from pions(Ngπ) or from kaons(Ngk). One can see that the RICH's ability to separate the two particle types exceeds 4-sigma everywhere between threshold and 80 GeV/c, finally dropping below 3-sigma at about 100 GeV. It is important to note that this result is for an 'ideal' detector, with homogeneous acceptance and efficiency, normal error distributions and zero background. No such detector exists, of course, and in a real experiment much more sophisticated procedures are actually used to account for those effects; position dependent acceptance and efficiency; non-Gaussian error distributions; non negligible and variable event-dependent backgrounds.In practice, for the multi-particle final states produced in a typical collider experiment, separation of kaons from other final state hadrons, mainly pions, is the most important purpose of the RICH. In that context the two most vital RICH functions, which maximise signal and minimise combinatorial backgrounds, are its ability to correctly identify a kaon as a kaon and its ability not to misidentify a pion as a kaon. The related probabilities, which are the usual measures of signal detection and background rejection in real data, are plotted below to show their variation with momentum (simulation with 10% random background); Note that the ~30% π → k misidentification rate at 100 GeV is, for the most part, due to the presence of 10% background hits (faking photons) in the simulated detector; the 3-sigma separation in the mean Cherenkov angle (shown in the 4th plot above) would, by itself, only account for about 6% misidentification. More detailed analyses of the above type, for operational RICH detectors, can be found in the published literature.\n",
      "-Note that, because the points of emission of the photons can be at any place on the (normally straight line) trajectory of the particle through the radiator, the emerging photons fill a light-cone in space.\n",
      "\n",
      "\n",
      "\n",
      "What is a light-year?\n",
      "-A light-year, alternatively spelled light year, is a unit of length used to express astronomical distances and is equivalent to about 9.46 trillion kilometers (9.46×1012 km), or 5.88 trillion miles (5.88×1012 mi). As defined by the International Astronomical Union (IAU), a light-year is the distance that light travels in a vacuum in one Julian year (365.25 days). Because it includes the word \"year\", the term is sometimes misinterpreted as a unit of time.The light-year is most often used when expressing distances to stars and other distances on a galactic scale, especially in non-specialist contexts and popular science publications. The unit most commonly used in professional astronomy is the parsec (symbol: pc, about 3.26 light-years) which derives from astrometry; it is the distance at which one astronomical unit (au) subtends an angle of one second of arc.\n",
      "-Astronomical unit An astronomical unit (AU) is approximately the average distance between the Earth and Sun. It was redefined in 2012 as exactly 149597870700 m. Previously the AU was not based on the International System of Units but in terms of the gravitational force exerted by the Sun in the framework of classical mechanics. The current definition uses the recommended value in metres for the previous definition of the astronomical unit, which was determined by measurement. This redefinition is analogous to that of the metre and likewise has the effect of fixing the speed of light to an exact value in astronomical units per second (via the exact speed of light in metres per second).Previously, the inverse of c expressed in seconds per astronomical unit was measured by comparing the time for radio signals to reach different spacecraft in the Solar System, with their position calculated from the gravitational effects of the Sun and various planets. By combining many such measurements, a best fit value for the light time per unit distance could be obtained. For example, in 2009, the best estimate, as approved by the International Astronomical Union (IAU), was: light time for unit distance: tau = 499.004783836(10) s c = 0.00200398880410(4) AU/s = 173.144632674(3) AU/day.The relative uncertainty in these measurements is 0.02 parts per billion (2×10−11), equivalent to the uncertainty in Earth-based measurements of length by interferometry. Since the metre is defined to be the length travelled by light in a certain time interval, the measurement of the light time in terms of the previous definition of the astronomical unit can also be interpreted as measuring the length of an AU (old definition) in metres.\n",
      "-Use in unit systems Since 1983, the constant c has been defined in the International System of Units (SI) as exactly 299792458 m/s; this relationship is used to define the metre as exactly the distance that light travels in vacuum in 1⁄299792458 of a second. By using the value of c, as well as an accurate measurement of the second, one can thus establish a standard for the metre. As a dimensional physical constant, the numerical value of c is different for different unit systems. For example, in imperial units, the speed of light is approximately 186282 miles per second, or roughly 1 foot per nanosecond.In branches of physics in which c appears often, such as in relativity, it is common to use systems of natural units of measurement or the geometrized unit system where c = 1. Using these units, c does not appear explicitly because multiplication or division by 1 does not affect the result. Its unit of light-second per second is still relevant, even if omitted.\n",
      "-light-year (ly) A unit of length used to express astronomical distances that is equivalent to the distance that an object moving at the speed of light in vacuum would travel in one Julian year: approximately 9.46 trillion kilometres (9.46×1012 km) or 5.88 trillion miles (5.88×1012 mi). Though the light-year is often used to measure galactic-scale distances in non-specialist publications, the unit of length most commonly used in professional astrometry is the parsec.\n",
      "-In physics, the Planck length, denoted ℓP, is a unit of length, equal to 1.616199(97)×10−35 metres. It is a unit in the system of Planck units, developed by physicist Max Planck. The Planck length is defined in terms of three fundamental physical constants: the speed of light, the Planck constant, and the Newtonian constant of gravitation. In contrast, the largest observable thing is the observable universe. The comoving distance – the distance as would be measured at a specific time, including the present – between Earth and the edge of the observable universe is 46 billion light-years (14×10^9 pc), making the diameter of the observable universe about 91 billion light-years (28×10^9 pc).\n",
      "\n",
      "\n",
      "\n",
      "What is the main advantage of ferroelectric memristors?\n",
      "-Ferroelectric memristor The ferroelectric memristor is based on a thin ferroelectric barrier sandwiched between two metallic electrodes. Switching the polarization of the ferroelectric material by applying a positive or negative voltage across the junction can lead to a two order of magnitude resistance variation: ROFF ≫ RON (an effect called Tunnel Electro-Resistance). In general, the polarization does not switch abruptly. The reversal occurs gradually through the nucleation and growth of ferroelectric domains with opposite polarization. During this process, the resistance is neither RON or ROFF, but in between. When the voltage is cycled, the ferroelectric domain configuration evolves, allowing a fine tuning of the resistance value. The ferroelectric memristor's main advantages are that ferroelectric domain dynamics can be tuned, offering a way to engineer the memristor response, and that the resistance variations are due to purely electronic phenomena, aiding device reliability, as no deep change to the material structure is involved.\n",
      "-Carbon nanotube memristor In 2013, Ageev, Blinov et al. reported observing memristor effect in structure based on vertically aligned carbon nanotubes studying bundles of CNT by scanning tunneling microscope.\n",
      "Later it was found that CNT memristive switching is observed when a nanotube has a non-uniform elastic strain ΔL0. It was shown that the memristive switching mechanism of strained СNT is based on the formation and subsequent redistribution of non-uniform elastic strain and piezoelectric field Edef in the nanotube under the influence of an external electric field E(x,t).\n",
      "-Memory storage Electronic memory designs in the past have largely relied on the formation of transistors. However, research into crossbar switch based electronics have offered an alternative using reconfigurable interconnections between vertical and horizontal wiring arrays to create ultra high density memories. Two leaders in this area are Nantero which has developed a carbon nanotube based crossbar memory called Nano-RAM and Hewlett-Packard which has proposed the use of memristor material as a future replacement of Flash memory.An example of such novel devices is based on spintronics. The dependence of the resistance of a material (due to the spin of the electrons) on an external field is called magnetoresistance. This effect can be significantly amplified (GMR - Giant Magneto-Resistance) for nanosized objects, for example when two ferromagnetic layers are separated by a nonmagnetic layer, which is several nanometers thick (e.g. Co-Cu-Co). The GMR effect has led to a strong increase in the data storage density of hard disks and made the gigabyte range possible. The so-called tunneling magnetoresistance (TMR) is very similar to GMR and based on the spin dependent tunneling of electrons through adjacent ferromagnetic layers. Both GMR and TMR effects can be used to create a non-volatile main memory for computers, such as the so-called magnetic random access memory or MRAM.\n",
      "-One advantage of memristive networks is that they can be implemented using relatively simple and inexpensive hardware, making them an attractive option for developing low-cost artificial intelligence systems. They also have the potential to be more energy efficient than traditional artificial neural networks, as they can store and process information using less power. However, the field of memristive networks is still in the early stages of development, and more research is needed to fully understand their capabilities and limitations.\n",
      "-Polymeric memristor In 2004, Krieger and Spitzer described dynamic doping of polymer and inorganic dielectric-like materials that improved the switching characteristics and retention required to create functioning nonvolatile memory cells. They used a passive layer between electrode and active thin films, which enhanced the extraction of ions from the electrode. It is possible to use fast ion conductor as this passive layer, which allows a significant reduction of the ionic extraction field.\n",
      "\n",
      "\n",
      "\n",
      "What is the term used to describe the conduction that occurs in non-crystalline semiconductors by charges quantum tunnelling from one localised site to another?\n",
      "-Extrinsic (doped) semiconductors have a far more complicated temperature profile. As temperature increases starting from absolute zero they first decrease steeply in resistance as the carriers leave the donors or acceptors. After most of the donors or acceptors have lost their carriers, the resistance starts to increase again slightly due to the reducing mobility of carriers (much as in a metal). At higher temperatures, they behave like intrinsic semiconductors as the carriers from the donors/acceptors become insignificant compared to the thermally generated carriers.In non-crystalline semiconductors, conduction can occur by charges quantum tunnelling from one localised site to another. This is known as variable range hopping and has the characteristic form of where n = 2, 3, 4, depending on the dimensionality of the system.\n",
      "-Crystalline solids and molecular solids are two opposite extreme cases of materials that exhibit substantially different transport mechanisms. While in atomic solids transport is intra-molecular, also known as band transport, in molecular solids the transport is inter-molecular, also known as hopping transport. The two different mechanisms result in different charge mobilities.\n",
      "In disordered solids, disordered potentials result in weak localization effects (traps), which reduce the mean free path, and hence the mobility, of mobile charges. Carrier recombination also decreases mobility.\n",
      "-In electronics/spintronics, a tunnel junction is a barrier, such as a thin insulating layer or electric potential, between two electrically conducting materials. Electrons (or quasiparticles) pass through the barrier by the process of quantum tunnelling. Classically, the electron has zero probability of passing through the barrier. However, according to quantum mechanics, the electron has a non-zero wave amplitude in the barrier, and hence it has some probability of passing through the barrier. Tunnel junctions serve a variety of different purposes.\n",
      "-The conductivity of intrinsic semiconductors is strongly dependent on the band gap. The only available charge carriers for conduction are the electrons that have enough thermal energy to be excited across the band gap and the electron holes that are left off when such an excitation occurs.\n",
      "-The distinction between semiconductors and insulators is a matter of convention. One approach is to think of semiconductors as a type of insulator with a narrow band gap. Insulators with a larger band gap, usually greater than 4 eV, are not considered semiconductors and generally do not exhibit semiconductive behaviour under practical conditions. Electron mobility also plays a role in determining a material's informal classification.\n",
      "\n",
      "\n",
      "\n",
      "What is resistivity?\n",
      "-Electrical resistivity (also called volume resistivity or specific electrical resistance) is a fundamental specific property of a material that measures its electrical resistance or how strongly it resists electric current. A low resistivity indicates a material that readily allows electric current. Resistivity is commonly represented by the Greek letter ρ (rho). The SI unit of electrical resistivity is the ohm-metre (Ω⋅m). For example, if a 1 m3 solid cube of material has sheet contacts on two opposite faces, and the resistance between these contacts is 1 Ω, then the resistivity of the material is 1 Ω⋅m.\n",
      "-In a hydraulic analogy, passing current through a high-resistivity material is like pushing water through a pipe full of sand - while passing current through a low-resistivity material is like pushing water through an empty pipe. If the pipes are the same size and shape, the pipe full of sand has higher resistance to flow. Resistance, however, is not solely determined by the presence or absence of sand. It also depends on the length and width of the pipe: short or wide pipes have lower resistance than narrow or long pipes.\n",
      "-Both resistance and resistivity describe how difficult it is to make electrical current flow through a material, but unlike resistance, resistivity is an intrinsic property and doesn't depend on geometric properties of a material. This means that all pure copper (Cu) wires (which have not been subjected to distortion of their crystalline structure etc.), irrespective of their shape and size, have the same resistivity, but a long, thin copper wire has a much larger resistance than a thick, short copper wire. Every material has its own characteristic resistivity. For example, rubber has a far larger resistivity than copper.\n",
      "-As shown below, this expression simplifies to a single number when the electric field and current density are constant in the material.\n",
      "-Conductivity is the inverse (reciprocal) of resistivity. Here, it is given by: For example, rubber is a material with large ρ and small σ — because even a very large electric field in rubber makes almost no current flow through it. On the other hand, copper is a material with small ρ and large σ — because even a small electric field pulls a lot of current through it.\n",
      "\n",
      "\n",
      "\n",
      "What did Newton adopt after his correspondence with Hooke in 1679-1680?\n",
      "-In regard to evidence that still survives of the earlier history, manuscripts written by Newton in the 1660s show that Newton himself had, by 1669, arrived at proofs that in a circular case of planetary motion, \"endeavour to recede\" (what was later called centrifugal force) had an inverse-square relation with distance from the center. After his 1679–1680 correspondence with Hooke, Newton adopted the language of inward or centripetal force. According to Newton scholar J. Bruce Brackenridge, although much has been made of the change in language and difference of point of view, as between centrifugal or centripetal forces, the actual computations and proofs remained the same either way. They also involved the combination of tangential and radial displacements, which Newton was making in the 1660s. The lesson offered by Hooke to Newton here, although significant, was one of perspective and did not change the analysis. This background shows there was basis for Newton to deny deriving the inverse square law from Hooke.\n",
      "-Newton's acknowledgment On the other hand, Newton did accept and acknowledge, in all editions of the Principia, that Hooke (but not exclusively Hooke) had separately appreciated the inverse square law in the solar system. Newton acknowledged Wren, Hooke, and Halley in this connection in the Scholium to Proposition 4 in Book 1. Newton also acknowledged to Halley that his correspondence with Hooke in 1679–80 had reawakened his dormant interest in astronomical matters, but that did not mean, according to Newton, that Hooke had told Newton anything new or original: \"yet am I not beholden to him for any light into that business but only for the diversion he gave me from my other studies to think on these things & for his dogmaticalness in writing as if he had found the motion in the Ellipsis, which inclined me to try it ...\" Modern priority controversy Since the time of Newton and Hooke, scholarly discussion has also touched on the question of whether Hooke's 1679 mention of 'compounding the motions' provided Newton with something new and valuable, even though that was not a claim actually voiced by Hooke at the time. As described above, Newton's manuscripts of the 1660s do show him actually combining tangential motion with the effects of radially directed force or endeavour, for example in his derivation of the inverse square relation for the circular case. They also show Newton clearly expressing the concept of linear inertia—for which he was indebted to Descartes' work, published in 1644 (as Hooke probably was). These matters do not appear to have been learned by Newton from Hooke.\n",
      "-Newton's early work on motion In the 1660s Newton studied the motion of colliding bodies and deduced that the centre of mass of two colliding bodies remains in uniform motion. Surviving manuscripts of the 1660s also show Newton's interest in planetary motion and that by 1669 he had shown, for a circular case of planetary motion, that the force he called \"endeavour to recede\" (now called centrifugal force) had an inverse-square relation with distance from the center. After his 1679–1680 correspondence with Hooke, described below, Newton adopted the language of inward or centripetal force. According to Newton scholar J. Bruce Brackenridge, although much has been made of the change in language and difference of point of view, as between centrifugal or centripetal forces, the actual computations and proofs remained the same either way. They also involved the combination of tangential and radial displacements, which Newton was making in the 1660s. The difference between the centrifugal and centripetal points of view, though a significant change of perspective, did not change the analysis. Newton also clearly expressed the concept of linear inertia in the 1660s: for this Newton was indebted to Descartes' work published 1644.\n",
      "-Robert Hooke had published his concept of gravitational forces in 1674, stating that all celestial bodies have an attraction or gravitating power towards their own centers, and also attract all the other celestial bodies that are within the sphere of their activity. He further stated that gravitational attraction increases by how much nearer the body wrought upon is to its own center. In correspondence with Isaac Newton from 1679 and 1680, Hooke conjectured that gravitational forces might decrease according to the double of the distance between the two bodies. Hooke urged Newton, who was a pioneer in the development of calculus, to work through the mathematical details of Keplerian orbits to determine if Hooke's hypothesis was correct. Newton's own investigations verified that Hooke was correct, but due to personal differences between the two men, Newton chose not to reveal this to Hooke. Isaac Newton kept quiet about his discoveries until 1684, at which time he told a friend, Edmond Halley, that he had solved the problem of gravitational orbits, but had misplaced the solution in his office. After being encouraged by Halley, Newton decided to develop his ideas about gravity and publish all of his findings. In November 1684, Isaac Newton sent a document to Edmund Halley, now lost but presumed to have been titled De motu corporum in gyrum (Latin for \"On the motion of bodies in an orbit\"). Halley presented Newton's findings to the Royal Society of London, with a promise that a fuller presentation would follow. Newton later recorded his ideas in a three-book set, entitled Philosophiæ Naturalis Principia Mathematica (Latin: Mathematical Principles of Natural Philosophy). The first was received by the Royal Society on 28 April 1685–86; the second on 2 March 1686–87; and the third on 6 April 1686–87. The Royal Society published Newton's entire collection at their own expense in May 1686–87.: 31 Isaac Newton had bridged the gap between Kepler's gravitational mass and Galileo's gravitational acceleration, resulting in the discovery of the following relationship which governed both of these: g=−μR^|R|2 where g is the apparent acceleration of a body as it passes through a region of space where gravitational fields exist, μ is the gravitational mass (standard gravitational parameter) of the body causing gravitational fields, and R is the radial coordinate (the distance between the centers of the two bodies).\n",
      "-Hooke's statements up to 1674 made no mention, however, that an inverse square law applies or might apply to these attractions. Hooke's gravitation was also not yet universal, though it approached universality more closely than previous hypotheses. He also did not provide accompanying evidence or mathematical demonstration. On the latter two aspects, Hooke himself stated in 1674: \"Now what these several degrees [of attraction] are I have not yet experimentally verified\"; and as to his whole proposal: \"This I only hint at present\", \"having my self many other things in hand which I would first compleat, and therefore cannot so well attend it\" (i.e. \"prosecuting this Inquiry\"). It was later on, in writing on 6 January 1679|80 to Newton, that Hooke communicated his \"supposition ... that the Attraction always is in a duplicate proportion to the Distance from the Center Reciprocall, and Consequently that the Velocity will be in a subduplicate proportion to the Attraction and Consequently as Kepler Supposes Reciprocall to the Distance.\" (The inference about the velocity was incorrect.)Hooke's correspondence with Newton during 1679–1680 not only mentioned this inverse square supposition for the decline of attraction with increasing distance, but also, in Hooke's opening letter to Newton, of 24 November 1679, an approach of \"compounding the celestial motions of the planets of a direct motion by the tangent & an attractive motion towards the central body\".\n",
      "\n",
      "\n",
      "\n",
      "What is the metallicity of Kapteyn's star estimated to be?\n",
      "-In 2014, the first planets around a halo star were announced around Kapteyn's star, the nearest halo star to Earth, around 13 light years away. However, later research suggests that Kapteyn b is just an artefact of stellar activity and that Kapteyn c needs more study to be confirmed. The metallicity of Kapteyn's star is estimated to be about 8 times less than the Sun.Different types of galaxies have different histories of star formation and hence planet formation. Planet formation is affected by the ages, metallicities, and orbits of stellar populations within a galaxy. Distribution of stellar populations within a galaxy varies between the different types of galaxies.\n",
      "-Hence, iron can be used as a chronological indicator of nucleosynthesis. Iron is relatively easy to measure with spectral observations in the star's spectrum given the large number of iron lines in the star's spectra (even though oxygen is the most abundant heavy element – see metallicities in HII regions below). The abundance ratio is the common logarithm of the ratio of a star's iron abundance compared to that of the Sun and is calculated thus: log 10 log 10 ⁡(NFeNH)⊙, where  NFe and  NH are the number of iron and hydrogen atoms per unit of volume respectively,  ⊙ is the standard symbol for the Sun, and  ⋆ for a star (often omitted below). The unit often used for metallicity is the dex, contraction of \"decimal exponent\". By this formulation, stars with a higher metallicity than the Sun have a positive common logarithm, whereas those more dominated by hydrogen have a corresponding negative value. For example, stars with a  [FeH]⋆ value of +1 have 10 times the metallicity of the Sun (10+1); conversely, those with a  [FeH]⋆ value of −1 have 1/10, while those with a  [FeH]⋆ value of 0 have the same metallicity as the Sun, and so on.Young population I stars have significantly higher iron-to-hydrogen ratios than older population II stars. Primordial population III stars are estimated to have metallicity less than −6, a millionth of the abundance of iron in the Sun.\n",
      "-Relationship between stellar metallicity and planets A star's metallicity measurement is one parameter that helps determine whether a star may have a giant planet, as there is a direct correlation between metallicity and the presence of a giant planet. Measurements have demonstrated the connection between a star's metallicity and gas giant planets, like Jupiter and Saturn. The more metals in a star and thus its planetary system and protoplanetary disk, the more likely the system may have gas giant planets. Current models show that the metallicity along with the correct planetary system temperature and distance from the star are key to planet and planetesimal formation. For two stars that have equal age and mass but different metallicity, the less metallic star is bluer. Among stars of the same color, less metallic stars emit more ultraviolet radiation. The Sun, with eight planets and five known dwarf planets, is used as the reference, with a  [FeH] of 0.00.\n",
      "-The remainder of the elements are collectively referred to as \"metals\", and the metallicity – the mass fraction of elements heavier than helium – is calculated as Z=∑e>HemeM=1−X−Y.\n",
      "For the surface of the Sun (symbol  ⊙ ), these parameters are measured to have the following values: Due to the effects of stellar evolution, neither the initial composition nor the present day bulk composition of the Sun is the same as its present-day surface composition.\n",
      "Chemical abundance ratios The overall stellar metallicity is conventionally defined using the total hydrogen content, since its abundance is considered to be relatively constant in the Universe, or the iron content of the star, which has an abundance that is generally linearly increasing in time in the Universe.\n",
      "-Observation of stellar spectra has revealed that stars older than the Sun have fewer heavy elements compared with the Sun. This immediately suggests that metallicity has evolved through the generations of stars by the process of stellar nucleosynthesis.\n",
      "\n",
      "\n",
      "\n",
      "What is the SI base unit of time and how is it defined?\n",
      "-A unit of time is any particular time interval, used as a standard way of measuring or expressing duration. The base unit of time in the International System of Units (SI), and by extension most of the Western world, is the second, defined as about 9 billion oscillations of the caesium atom. The exact modern SI definition is \"[The second] is defined by taking the fixed numerical value of the cesium frequency, ΔνCs, the unperturbed ground-state hyperfine transition frequency of the cesium 133 atom, to be 9 192 631 770 when expressed in the unit Hz, which is equal to s−1.\"Historically, many units of time were defined by the movements of astronomical objects.\n",
      "-In the International System of Units (SI), the unit of time is the second (symbol:  s ). It is a SI base unit, and has been defined since 1967 as \"the duration of 9,192,631,770 [cycles] of the radiation corresponding to the transition between the two hyperfine levels of the ground state of the caesium 133 atom\". This definition is based on the operation of a caesium atomic clock. These clocks became practical for use as primary reference standards after about 1955, and have been in use ever since.\n",
      "-International System of Units definition Since 1968, the SI has defined the second as the duration of 9192631770 cycles of radiation corresponding to the transition between two energy levels of the ground state of the caesium-133 atom. In 1997, the International Committee for Weights and Measures (CIPM) added that the preceding definition refers to a caesium atom at rest at a temperature of absolute zero.: 113 This definition makes the caesium oscillator the primary standard for time and frequency measurements, called the caesium standard. Following the 2019 redefinition of the SI base units, the definition of every base unit except the mole and almost every derived unit relies on the definition of the second.\n",
      "-The SI base units are the standard units of measurement defined by the International System of Units (SI) for the seven base quantities of what is now known as the International System of Quantities: they are notably a basic set from which all other SI units can be derived. The units and their physical quantities are the second for time, the metre (sometimes spelled meter) for length or distance, the kilogram for mass, the ampere for electric current, the kelvin for thermodynamic temperature, the mole for amount of substance, and the candela for luminous intensity. The SI base units are a fundamental part of modern metrology, and thus part of the foundation of modern science and technology.\n",
      "-An atomic clock is a clock that measures time by monitoring the resonant frequency of atoms. It is based on atoms having different energy levels. Electron states in an atom are associated with different energy levels, and in transitions between such states they interact with a very specific frequency of electromagnetic radiation. This phenomenon serves as the basis for the International System of Units' (SI) definition of a second:The second, symbol s, is the SI unit of time. It is defined by taking the fixed numerical value of the caesium frequency,  ΔνCs , the unperturbed ground-state hyperfine transition frequency of the caesium 133 atom, to be 9192631770 when expressed in the unit Hz, which is equal to s−1. This definition is the basis for the system of International Atomic Time (TAI), which is maintained by an ensemble of atomic clocks around the world. The system of Coordinated Universal Time (UTC) that is the basis of civil time implements leap seconds to allow clock time to track changes in Earth's rotation to within one second while being based on clocks that are based on the definition of the second.\n",
      "\n",
      "\n",
      "\n",
      "What is a planetary system?\n",
      "-A planetary system is a set of gravitationally bound non-stellar objects in or out of orbit around a star or star system. Generally speaking, systems with one or more planets constitute a planetary system, although such systems may also consist of bodies such as dwarf planets, asteroids, natural satellites, meteoroids, comets, planetesimals and circumstellar disks. The Sun together with the planetary system revolving around it, including Earth, forms the Solar System. The term exoplanetary system is sometimes used in reference to other planetary systems.\n",
      "-At present, few systems have been found to be analogous to the Solar System with terrestrial planets close to the parent star. More commonly, systems consisting of multiple Super-Earths have been detected.\n",
      "-Classification Planetary system architectures may be partitioned into four classes based on how the mass of the planets is distributed around the host star: Similar: The masses of all planets in a system are similar to each other. This architecture class is the most commonly-observed in our galaxy. Examples include Trappist-1. The planets in these systems are said to be like 'peas in a pod'.\n",
      "-The Solar System consists of an inner region of small rocky planets and outer region of large gas giants. However, other planetary systems can have quite different architectures. Studies suggest that architectures of planetary systems are dependent on the conditions of their initial formation. Many systems with a hot Jupiter gas giant very close to the star have been found. Theories, such as planetary migration or scattering, have been proposed for the formation of large planets close to their parent stars.\n",
      "-Known satellite systems of the Solar System consisting of multiple objects or around planetary mass objects, in order of perihelion: Planetary mass Small Solar System body\n",
      "\n",
      "\n",
      "\n",
      "What is the result of the collapse of a cavitation bubble?\n",
      "-Inertial cavitation Inertial cavitation was first observed in the late 19th century, considering the collapse of a spherical void within a liquid. When a volume of liquid is subjected to a sufficiently low pressure, it may rupture and form a cavity. This phenomenon is coined cavitation inception and may occur behind the blade of a rapidly rotating propeller or on any surface vibrating in the liquid with sufficient amplitude and acceleration. A fast-flowing river can cause cavitation on rock surfaces, particularly when there is a drop-off, such as on a waterfall.Other ways of generating cavitation voids involve the local deposition of energy, such as an intense focused laser pulse (optic cavitation) or with an electrical discharge through a spark. Vapor gases evaporate into the cavity from the surrounding medium; thus, the cavity is not a vacuum at all, but rather a low-pressure vapor (gas) bubble. Once the conditions which caused the bubble to form are no longer present, such as when the bubble moves downstream, the surrounding liquid begins to implode due its higher pressure, building up inertia as it moves inward. As the bubble finally collapses, the inward inertia of the surrounding liquid causes a sharp increase of pressure and temperature of the vapor within. The bubble eventually collapses to a minute fraction of its original size, at which point the gas within dissipates into the surrounding liquid via a rather violent mechanism which releases a significant amount of energy in the form of an acoustic shock wave and as visible light. At the point of total collapse, the temperature of the vapor within the bubble may be several thousand kelvin, and the pressure several hundred atmospheres.Inertial cavitation can also occur in the presence of an acoustic field. Microscopic gas bubbles that are generally present in a liquid will be forced to oscillate due to an applied acoustic field. If the acoustic intensity is sufficiently high, the bubbles will first grow in size and then rapidly collapse. Hence, inertial cavitation can occur even if the rarefaction in the liquid is insufficient for a Rayleigh-like void to occur. High-power ultrasonics usually utilize the inertial cavitation of microscopic vacuum bubbles for treatment of surfaces, liquids, and slurries.\n",
      "-Hydrodynamic cavitation Hydrodynamic cavitation is the process of vaporisation, bubble generation and bubble implosion which occurs in a flowing liquid as a result of a decrease and subsequent increase in local pressure. Cavitation will only occur if the local pressure declines to some point below the saturated vapor pressure of the liquid and subsequent recovery above the vapor pressure. If the recovery pressure is not above the vapor pressure then flashing is said to have occurred. In pipe systems, cavitation typically occurs either as the result of an increase in the kinetic energy (through an area constriction) or an increase in the pipe elevation.\n",
      "-The physical process of cavitation inception is similar to boiling. The major difference between the two is the thermodynamic paths that precede the formation of the vapor. Boiling occurs when the local temperature of the liquid reaches the saturation temperature, and further heat is supplied to allow the liquid to sufficiently phase change into a gas. Cavitation inception occurs when the local pressure falls sufficiently far below the saturated vapor pressure, a value given by the tensile strength of the liquid at a certain temperature.In order for cavitation inception to occur, the cavitation \"bubbles\" generally need a surface on which they can nucleate. This surface can be provided by the sides of a container, by impurities in the liquid, or by small undissolved microbubbles within the liquid. It is generally accepted that hydrophobic surfaces stabilize small bubbles. These pre-existing bubbles start to grow unbounded when they are exposed to a pressure below the threshold pressure, termed Blake's threshold. The presence of an incompressible core inside a cavitation nucleus substantially lowers the cavitation threshold below the Blake threshold.The vapor pressure here differs from the meteorological definition of vapor pressure, which describes the partial pressure of water in the atmosphere at some value less than 100% saturation. Vapor pressure as relating to cavitation refers to the vapor pressure in equilibrium conditions and can therefore be more accurately defined as the equilibrium (or saturated) vapor pressure.\n",
      "-As an impeller's (in a pump) or propeller's (as in the case of a ship or submarine) blades move through a fluid, low-pressure areas are formed as the fluid accelerates around and moves past the blades. The faster the blade moves, the lower the pressure can become around it. As it reaches vapor pressure, the fluid vaporizes and forms small bubbles of gas. This is cavitation. When the bubbles collapse later, they typically cause very strong local shock waves in the fluid, which may be audible and may even damage the blades.\n",
      "-Cavitation is the formation of vapour bubbles in liquid caused by flow around an object. Bubbles form when water accelerates around sharp corners and the pressure drops below the vapour pressure. Pressure increases upon deceleration, and the water generally reabsorbs the vapour; however, vapour bubbles can implode and apply small concentrated impulses that may damage surfaces like ship propellers and pump impellers.\n",
      "\n",
      "\n",
      "\n",
      "Who was Giordano Bruno?\n",
      "-Speculation on extrasolar planetary systems In the 16th century the Italian philosopher Giordano Bruno, an early supporter of the Copernican theory that Earth and other planets orbit the Sun, put forward the view that the fixed stars are similar to the Sun and are likewise accompanied by planets. He was burned at the stake for his ideas by the Roman Inquisition.In the 18th century the same possibility was mentioned by Sir Isaac Newton in the \"General Scholium\" that concludes his Principia. Making a comparison to the Sun's planets, he wrote \"And if the fixed stars are the centres of similar systems, they will all be constructed according to a similar design and subject to the dominion of One.\"His theories gained traction through the 19th and 20th centuries despite a lack of supporting evidence. Long before their confirmation by astronomers, conjecture on the nature of planetary systems had been a focus of the search for extraterrestrial intelligence and has been a prevalent theme in fiction, particularly science fiction.\n",
      "-Early modern period There was a dramatic paradigm shift in thinking initiated by the invention of the telescope and the Copernican assault on geocentric cosmology. The geocentric model was replaced by heliocentrism, which placed the Sun at the center of the universe instead. Eventually, the sun was considered only the center of the Solar System. Under this understanding, the notion of extraterrestrial life became feasible: if Earth is but just a planet orbiting around a star, there may be planets similar to Earth elsewhere. The astronomical study of distant bodies also proved that physical laws are the same elsewhere in the universe as on Earth, with nothing making the planet truly special.The best-known early-modern proponent of such ideas was the Italian philosopher Giordano Bruno, who argued in the 16th century for an infinite universe in which every star is surrounded by its own planetary system. Bruno wrote that other worlds \"have no less virtue nor a nature different to that of our earth\" and, like Earth, \"contain animals and inhabitants\". Bruno's belief in the plurality of worlds was one of the charges leveled against him by the Venetian Holy Inquisition, which trialed and executed him.In the early 17th century, the Czech astronomer Anton Maria Schyrleus of Rheita mused that \"if Jupiter has (...) inhabitants (...) they must be larger and more beautiful than the inhabitants of Earth, in proportion to the [characteristics] of the two spheres\".In Baroque literature such as The Other World: The Societies and Governments of the Moon by Cyrano de Bergerac, extraterrestrial societies are presented as humoristic or ironic parodies of earthly society. The didactic poet Henry More took up the classical theme of the Greek Democritus in \"Democritus Platonissans, or an Essay Upon the Infinity of Worlds\" (1647). In \"The Creation: a Philosophical Poem in Seven Books\" (1712), Sir Richard Blackmore observed: \"We may pronounce each orb sustains a race / Of living things adapted to the place\". With the new relative viewpoint that the Copernican revolution had wrought, he suggested \"our world's sunne / Becomes a starre elsewhere\". Fontanelle's \"Conversations on the Plurality of Worlds\" (translated into English in 1686) offered similar excursions on the possibility of extraterrestrial life, expanding, rather than denying, the creative sphere of a Maker.The possibility of extraterrestrials remained a widespread speculation as scientific discovery accelerated. William Herschel, the discoverer of Uranus, was one of many 18th–19th-century astronomers who believed that the Solar System is populated by alien life. Other scholars of the period who championed \"cosmic pluralism\" included Immanuel Kant and Benjamin Franklin. At the height of the Enlightenment, even the Sun and Moon were considered candidates for extraterrestrial inhabitants.\n",
      "-Copernican heliocentrism did away with the planetary spheres, but it did not necessarily preclude the existence of a sphere for the fixed stars. The first astronomer of the European Renaissance to suggest that the stars were distant suns was Giordano Bruno in his De l'infinito universo et mondi (1584). This idea was among the charges, albeit not in a prominent position, brought against him by the Inquisition.\n",
      "-1584 – Giordano Bruno published two important philosophical dialogues (La Cena de le Ceneri and De l'infinito universo et mondi) in which he argued against the planetary spheres and affirmed the Copernican principle. Bruno's infinite universe was filled with a substance—a \"pure air\", aether, or spiritus—that offered no resistance to the heavenly bodies which, in Bruno's view, rather than being fixed, moved under their own impetus (momentum). Most dramatically, he completely abandoned the idea of a hierarchical universe. Bruno's cosmology distinguishes between \"suns\" which produce their own light and heat, and have other bodies moving around them; and \"earths\" which move around suns and receive light and heat from them. Bruno suggested that some, if not all, of the objects classically known as fixed stars are in fact suns, so he was arguably the first person to grasp that \"stars are other suns with their own planets.\" Bruno wrote that other worlds \"have no less virtue nor a nature different from that of our Earth\" and, like Earth, \"contain animals and inhabitants\".\n",
      "-Early speculations This space we declare to be infinite... In it are an infinity of worlds of the same kind as our own.\n",
      "In the sixteenth century, the Italian philosopher Giordano Bruno, an early supporter of the Copernican theory that Earth and other planets orbit the Sun (heliocentrism), put forward the view that the fixed stars are similar to the Sun and are likewise accompanied by planets.\n",
      "\n",
      "\n",
      "\n",
      "What are the Navier-Stokes equations?\n",
      "-The Navier–Stokes equations are strictly a statement of the balance of momentum. To fully describe fluid flow, more information is needed, how much depending on the assumptions made. This additional information may include boundary data (no-slip, capillary surface, etc.), conservation of mass, balance of energy, and/or an equation of state.\n",
      "-The Navier–Stokes equations mathematically express momentum balance and conservation of mass for Newtonian fluids. They are sometimes accompanied by an equation of state relating pressure, temperature and density. They arise from applying Isaac Newton's second law to fluid motion, together with the assumption that the stress in the fluid is the sum of a diffusing viscous term (proportional to the gradient of velocity) and a pressure term—hence describing viscous flow. The difference between them and the closely related Euler equations is that Navier–Stokes equations take viscosity into account while the Euler equations model only inviscid flow. As a result, the Navier–Stokes are a parabolic equation and therefore have better analytic properties, at the expense of having less mathematical structure (e.g. they are never completely integrable).\n",
      "-Navier–Stokes equations – In physics, the Navier–Stokes equations() are certain partial differential equations which describe the motion of viscous fluid substances, named after French engineer and physicist Claude-Louis Navier and Anglo-Irish physicist and mathematician George Gabriel Stokes. They were developed over several decades of progressively building the theories, from 1822 (Navier) to 1842–1850 (Stokes).The Navier–Stokes equations mathematically express conservation of momentum and conservation of mass for Newtonian fluids. They are sometimes accompanied by an equation of state relating pressure, temperature and density. They arise from applying Isaac Newton's second law to fluid motion, together with the assumption that the stress in the fluid is the sum of a diffusing viscous term (proportional to the gradient of velocity) and a pressure term—hence describing viscous flow. The difference between them and the closely related Euler equations is that Navier–Stokes equations take viscosity into account while the Euler equations model only inviscid flow. As a result, the Navier–Stokes are a parabolic equation and therefore have better analytic properties, at the expense of having less mathematical structure (e.g. they are never completely integrable).Newton (unit) – The newton (symbol: N) is the International System of Units (SI) derived unit of force. It is named after Isaac Newton in recognition of his work on classical mechanics, specifically Newton's second law of motion.A newton is defined as 1 kg⋅m/s2, which is the force which gives a mass of 1 kilogram an acceleration of 1 metre per second, per second.Newton's law of universal gravitation – is usually stated as that every particle attracts every other particle in the universe with a force that is directly proportional to the product of their masses and inversely proportional to the square of the distance between their centers. The publication of the theory has become known as the \"first great unification\", as it marked the unification of the previously described phenomena of gravity on Earth with known astronomical behaviors.This is a general physical law derived from empirical observations by what Isaac Newton called inductive reasoning. It is a part of classical mechanics and was formulated in Newton's work Philosophiæ Naturalis Principia Mathematica (\"the Principia\"), first published on 5 July 1687. When Newton presented Book 1 of the unpublished text in April 1686 to the Royal Society, Robert Hooke made a claim that Newton had obtained the inverse square law from him.In today's language, the law states that every point mass attracts every other point mass by a force acting along the line intersecting the two points. The force is proportional to the product of the two masses, and inversely proportional to the square of the distance between them.The equation for universal gravitation thus takes the form: F=Gm1m2r2, where F is the gravitational force acting between two objects, m1 and m2 are the masses of the objects, r is the distance between the centers of their masses, and G is the gravitational constant.Newton's laws of motion – are three laws of classical mechanics that describe the relationship between the motion of an object and the forces acting on it. These laws can be paraphrased as follows:Law 1. A body continues in its state of rest, or in uniform motion in a straight line, unless acted upon by a force.Law 2. A body acted upon by a force moves in such a manner that the time rate of change of momentum equals the force.Law 3. If two bodies exert forces on each other, these forces are equal in magnitude and opposite in direction.The three laws of motion were first stated by Isaac Newton in his Philosophiæ Naturalis Principia Mathematica (Mathematical Principles of Natural Philosophy), first published in 1687. Newton used them to explain and investigate the motion of many physical objects and systems, which laid the foundation for Newtonian mechanics.Nose cone design – Given the problem of the aerodynamic design of the nose cone section of any vehicle or body meant to travel through a compressible fluid medium (such as a rocket or aircraft, missile or bullet), an important problem is the determination of the nose cone geometrical shape for optimum performance. For many applications, such a task requires the definition of a solid of revolution shape that experiences minimal resistance to rapid motion through such a fluid medium.\n",
      "-The Navier–Stokes equations (named after Claude-Louis Navier and George Gabriel Stokes) are differential equations that describe the force balance at a given point within a fluid. For an incompressible fluid with vector velocity field  u , the Navier–Stokes equations are ∂u∂t+(u⋅∇)u=−1ρ∇p+ν∇2u .These differential equations are the analogues for deformable materials to Newton's equations of motion for particles – the Navier–Stokes equations describe changes in momentum (force) in response to pressure  p and viscosity, parameterized by the kinematic viscosity  ν . Occasionally, body forces, such as the gravitational force or Lorentz force are added to the equations.\n",
      "-Applicability Together with supplemental equations (for example, conservation of mass) and well-formulated boundary conditions, the Navier–Stokes equations seem to model fluid motion accurately; even turbulent flows seem (on average) to agree with real world observations.\n",
      "\n",
      "\n",
      "\n",
      "What is the revised view of the atmosphere's nature based on the time-varying multistability that is associated with the modulation of large-scale processes and aggregated feedback of small-scale processes?\n",
      "-Model for the nature of chaos and order in the atmosphere The scientific community accepts that the chaotic features found in low-dimensional Lorenz models could represent features of the Earth's atmosphere (), yielding the statement of “weather is chaotic.” By comparison, based on the concept of attractor coexistence within the generalized Lorenz model and the original Lorenz model (), Shen and his co-authors proposed a revised view that “weather possesses both chaos and order with distinct predictability”. The revised view, which is a build-up of the conventional view, is used to suggest that “the chaotic and regular features found in theoretical Lorenz models could better represent features of the Earth's atmosphere”.\n",
      "-The dual nature with distinct predictability Over 50 years since Lorenz’s 1963 study and a follow-up presentation in 1972, the statement “weather is chaotic” has been well accepted. Such a view turns our attention from regularity associated with Laplace’s view of determinism to irregularity associated with chaos. In contrast to single-type chaotic solutions, recent studies using a generalized Lorenz model have focused on the coexistence of chaotic and regular solutions that appear within the same model using the same modeling configurations but different initial conditions. The results, with attractor coexistence, suggest that the entirety of weather possesses a dual nature of chaos and order with distinct predictability.Using a slowly varying, periodic heating parameter within a generalized Lorenz model, Shen and his co-authors suggested a revised view: “The atmosphere possesses chaos and order; it includes, as examples, emerging organized systems (such as tornadoes) and time varying forcing from recurrent seasons”.\n",
      "-Atmospheric circulation is the large-scale movement of air and together with ocean circulation is the means by which thermal energy is redistributed on the surface of the Earth. The Earth's atmospheric circulation varies from year to year, but the large-scale structure of its circulation remains fairly constant. The smaller-scale weather systems – mid-latitude depressions, or tropical convective cells – occur chaotically, and long-range weather predictions of those cannot be made beyond ten days in practice, or a month in theory (see chaos theory and the butterfly effect).\n",
      "-Weather is a chaotic system that is readily modified by small changes to the environment, so accurate weather forecasting is limited to only a few days. Overall, two things are happening worldwide: (1) temperature is increasing on the average; and (2) regional climates have been undergoing noticeable changes.\n",
      "-Atmospheric dynamics is the study of motion systems of meteorological importance, integrating observations at multiple locations and times and theories. Common topics studied include diverse phenomena such as thunderstorms, tornadoes, gravity waves, tropical cyclones, extratropical cyclones, jet streams, and global-scale circulations. The goal of dynamical studies is to explain the observed circulations on the basis of fundamental principles from physics. The objectives of such studies incorporate improving weather forecasting, developing methods for predicting seasonal and interannual climate fluctuations, and understanding the implications of human-induced perturbations (e.g., increased carbon dioxide concentrations or depletion of the ozone layer) on the global climate.\n",
      "\n",
      "\n",
      "\n",
      "What is the reason that it is nearly impossible to see light emitted at the Lyman-alpha transition wavelength from a star farther than a few hundred light years from Earth?\n",
      "-Extinction provides one of the best ways of mapping the three-dimensional structure of the ISM, especially since the advent of accurate distances to millions of stars from the Gaia mission. The total amount of dust in front of each star is determined from its reddening, and the dust is then located along the line of sight by comparing the dust column density in front of stars projected close together on the sky, but at different distances. By 2022 it was possible to generate a map of ISM structures within 3 kpc (10,000 light years) of the Sun.Far ultraviolet light is absorbed effectively by the neutral hydrogen gas the ISM. Specifically, atomic hydrogen absorbs very strongly at about 121.5 nanometers, the Lyman-alpha transition, and also at the other Lyman series lines. Therefore, it is nearly impossible to see light emitted at those wavelengths from a star farther than a few hundred light years from Earth, because most of it is absorbed during the trip to Earth by intervening neutral hydrogen. All photons with wavelength < 91.6 nm, the Lyman limit, can ionize hydrogen and are also very strongly absorbed. The absorption gradually decreases with increasing photon energy, and the ISM begins to become transparent again in soft X-rays, with wavelengths shorter than about 1 nm.\n",
      "-For a neutral hydrogen atom, spectral lines are formed when an electron transitions between energy levels. The Lyman series of spectral lines are produced by electrons transitioning between the ground state and higher energy levels (excited states). The Lyman-alpha transition corresponds to an electron transitioning between the ground state (n = 1) and the first excited state (n = 2). The Lyman-alpha spectral line has a laboratory wavelength (or rest wavelength) of 1216 Å, which is in the ultraviolet portion of the electromagnetic spectrum.The Lyman-alpha absorption lines in the quasar spectra result from intergalactic gas through which the galaxy or quasar's light has traveled. Since neutral hydrogen clouds in the intergalactic medium are at different degrees of redshift (due to their varying distance from Earth), their absorption lines are observed at a range of wavelengths. Each individual cloud leaves its fingerprint as an absorption line at a different position in the observed spectrum.\n",
      "-The Lyman limit is at the wavelength of 91.2 nm (912 Å), corresponding to a frequency of 3.29 million GHz and a photon energy of 13.6 eV. LyC energies are mostly in the ultraviolet C portion of the electromagnetic spectrum (see Lyman series). Although X-rays and gamma-rays will also ionize a hydrogen atom, there are far fewer of them emitted from a star's photosphere—LyC are predominantly UV-C. The photon absorption process leading to the ionization of atomic hydrogen can occur in reverse: an electron and a proton can collide and form atomic hydrogen. If the two particles were traveling slowly (so that kinetic energy can be ignored), then the photon the atom emits upon its creation will theoretically be 13.6 eV (in reality, the energy will be less if the atom is formed in an excited state). At faster speeds, the excess (kinetic) energy is radiated (but momentum must be conserved) as photons of lower wavelength (higher energy). Therefore, photons with energies above 13.6 eV are emitted by the combination of energetic protons and electrons forming atomic hydrogen, and emission from photoionized hydrogen.\n",
      "-Lyman continuum photons (abbrev. LyC), shortened to Ly continuum photons or Lyc photons, are the photons emitted from stars at photon energies above the Lyman limit. Hydrogen is ionized by absorbing LyC. Working from Victor Schumann's discovery of ultraviolet light, from 1906 to 1914, Theodore Lyman observed that atomic hydrogen absorbs light only at specific frequencies (or wavelengths) and the Lyman series is thus named after him. All the wavelengths in the Lyman series are in the ultraviolet band. This quantized absorption behavior occurs only up to an energy limit, known as the ionization energy. In the case of neutral atomic hydrogen, the minimum ionization energy is equal to the Lyman limit, where the photon has enough energy to completely ionize the atom, resulting in a free proton and a free electron. Above this energy (below this wavelength), all wavelengths of light may be absorbed. This forms a continuum in the energy spectrum; the spectrum is continuous rather than composed of many discrete lines, which are seen at lower energies.\n",
      "-Lyman-alpha emitters have many unknown properties. The Lyman-alpha photon escape fraction varies greatly in these galaxies. This is what portion of the light emitted at the Lyman-alpha line wavelength inside the galaxy actually escapes and will be visible to distant observers. There is much evidence that the dust content of these galaxies could be significant and therefore is obscuring the brightness of these galaxies. It is also possible that anisotropic distribution of hydrogen density and velocity play a significant role in the varying escape fraction due to the photons' continued interaction with the hydrogen gas (radiative transfer). Evidence now shows strong evolution in the Lyman-alpha escape fraction with redshift, most likely associated with the buildup of dust in the ISM. Dust is shown to be the main parameter setting the escape of Lyman-alpha photons. Additionally the metallicity, outflows, and detailed evolution with redshift is unknown.\n",
      "\n",
      "\n",
      "\n",
      "What is a Schwarzschild black hole?\n",
      "-According to Birkhoff's theorem, the Schwarzschild metric is the most general spherically symmetric, vacuum solution of the Einstein field equations. A Schwarzschild black hole or static black hole is a black hole that has no charge or angular momentum. A Schwarzschild black hole is described by the Schwarzschild metric, and cannot be distinguished from any other Schwarzschild black hole except by its mass.\n",
      "-Physical properties The simplest static black holes have mass but neither electric charge nor angular momentum. These black holes are often referred to as Schwarzschild black holes after Karl Schwarzschild who discovered this solution in 1916. According to Birkhoff's theorem, it is the only vacuum solution that is spherically symmetric. This means there is no observable difference at a distance between the gravitational field of such a black hole and that of any other spherical object of the same mass. The popular notion of a black hole \"sucking in everything\" in its surroundings is therefore correct only near a black hole's horizon; far away, the external gravitational field is identical to that of any other body of the same mass.Solutions describing more general black holes also exist. Non-rotating charged black holes are described by the Reissner–Nordström metric, while the Kerr metric describes a non-charged rotating black hole. The most general stationary black hole solution known is the Kerr–Newman metric, which describes a black hole with both charge and angular momentum.While the mass of a black hole can take any positive value, the charge and angular momentum are constrained by the mass. The total electric charge Q and the total angular momentum J are expected to satisfy the inequality Q24πϵ0+c2J2GM2≤GM2 for a black hole of mass M. Black holes with the minimum possible mass satisfying this inequality are called extremal. Solutions of Einstein's equations that violate this inequality exist, but they do not possess an event horizon. These solutions have so-called naked singularities that can be observed from the outside, and hence are deemed unphysical. The cosmic censorship hypothesis rules out the formation of such singularities, when they are created through the gravitational collapse of realistic matter. This is supported by numerical simulations.Due to the relatively large strength of the electromagnetic force, black holes forming from the collapse of stars are expected to retain the nearly neutral charge of the star. Rotation, however, is expected to be a universal feature of compact astrophysical objects. The black-hole candidate binary X-ray source GRS 1915+105 appears to have an angular momentum near the maximum allowed value. That uncharged limit is J≤GM2c, allowing definition of a dimensionless spin parameter such that 1.\n",
      "-In Einstein's theory of general relativity, the Schwarzschild metric (also known as the Schwarzschild solution) is an  exact solution to the Einstein field equations that describes the gravitational field outside a spherical mass, on the assumption that the electric charge of the mass, angular momentum of the mass, and universal cosmological constant are all zero. The solution is a useful approximation for describing slowly rotating astronomical objects such as many stars and planets, including Earth and the Sun. It was found by Karl Schwarzschild in 1916, and around the same time independently by Johannes Droste, who published his more complete and modern-looking discussion four months after Schwarzschild.According to Birkhoff's theorem, the Schwarzschild metric is the most general spherically symmetric vacuum solution of the Einstein field equations. A Schwarzschild black hole or static black hole is a black hole that has neither electric charge nor angular momentum. A Schwarzschild black hole is described by the Schwarzschild metric, and cannot be distinguished from any other Schwarzschild black hole except by its mass.\n",
      "-A mathematically-oriented article describes that the Reissner–Nordström metric for a charged, non-rotating black hole. A similarly technical article on the Kerr–Newman black hole gives an overview of the most general known solution for a black hole, which has both angular momentum and charge (all the other solutions are simplified special cases of the Kerr–Newman black hole).\n",
      "-A charged black hole is a black hole that possesses electric charge. Since the electromagnetic repulsion in compressing an electrically charged mass is dramatically greater than the gravitational attraction (by about 40 orders of magnitude), it is not expected that black holes with a significant electric charge will be formed in nature.\n",
      "\n",
      "\n",
      "\n",
      "What is the definition of Atomristor?\n",
      "-Atomristor Atomristor is defined as the electrical devices showing memristive behavior in atomically thin nanomaterials or atomic sheets. In 2018, Ge and Wu et al. in the Akinwande group at the University of Texas, first reported a universal memristive effect in single-layer TMD (MX2, M = Mo, W; and X = S, Se) atomic sheets based on vertical metal-insulator-metal (MIM) device structure. The work was later extended to monolayer hexagonal boron nitride, which is the thinnest memory material of around 0.33 nm. These atomristors offer forming-free switching and both unipolar and bipolar operation. The switching behavior is found in single-crystalline and poly-crystalline films, with various conducting electrodes (gold, silver and graphene). Atomically thin TMD sheets are prepared via CVD/MOCVD, enabling low-cost fabrication. Afterwards, taking advantage of the low \"on\" resistance and large on/off ratio, a high-performance zero-power RF switch is proved based on MoS2 or h-BN atomristors, indicating a new application of memristors for 5G, 6G and THz communication and connectivity systems. In 2020, atomistic understanding of the conductive virtual point mechanism was elucidated in an article in nature nanotechnology.\n",
      "-Titanium dioxide memristor Interest in the memristor revived when an experimental solid-state version was reported by R. Stanley Williams of Hewlett Packard in 2007. The article was the first to demonstrate that a solid-state device could have the characteristics of a memristor based on the behavior of nanoscale thin films. The device neither uses magnetic flux as the theoretical memristor suggested, nor stores charge as a capacitor does, but instead achieves a resistance dependent on the history of current.\n",
      "-Layered memristor In 2014, Bessonov et al. reported a flexible memristive device comprising a MoOx/MoS2 heterostructure sandwiched between silver electrodes on a plastic foil. The fabrication method is entirely based on printing and solution-processing technologies using two-dimensional layered transition metal dichalcogenides (TMDs). The memristors are mechanically flexible, optically transparent and produced at low cost. The memristive behaviour of switches was found to be accompanied by a prominent memcapacitive effect. High switching performance, demonstrated synaptic plasticity and sustainability to mechanical deformations promise to emulate the appealing characteristics of biological neural systems in novel computing technologies.\n",
      "-Silicon dioxide memristor It seems that memristance has been registered in nanoscale thin films realized with silicon dioxide since 60s .However, hysteretic conductance in silicon has been associated to memristive effect in 2009 only,  while Tony Kenyon and his group has clearly demonstrated that the resistive switching in silicon oxide thin films is due to silicon nanoinclusions in highly nonstoichiometric suboxide phases .\n",
      "-The identification of memristive properties in electronic devices has attracted controversy. Experimentally, the ideal memristor has yet to be demonstrated.\n",
      "\n",
      "\n",
      "\n",
      "Who published the first theory that was able to encompass previously separate field theories to provide a unifying theory of electromagnetism?\n",
      "-Classic theory The first successful classical unified field theory was developed by James Clerk Maxwell. In 1820, Hans Christian Ørsted discovered that electric currents exerted forces on magnets, while in 1831, Michael Faraday made the observation that time-varying magnetic fields could induce electric currents. Until then, electricity and magnetism had been thought of as unrelated phenomena. In 1864, Maxwell published his famous paper on a dynamical theory of the electromagnetic field. This was the first example of a theory that was able to encompass previously separate field theories (namely electricity and magnetism) to provide a unifying theory of electromagnetism. By 1905, Albert Einstein had used the constancy of the speed of light in Maxwell's theory to unify our notions of space and time into an entity we now call spacetime and in 1915 he expanded this theory of special relativity to a description of gravity, general relativity, using a field to describe the curving geometry of four-dimensional spacetime.\n",
      "-Field theory had its origins in the 18th century in a mathematical formulation of Newtonian mechanics, but it was seen as deficient as it implied action at a distance. In 1852, Michael Faraday treated the magnetic field as a physical object, reasoning about lines of force. James Clerk Maxwell used Faraday's conceptualisation to help formulate his unification of electricity and magnetism in his electromagnetic theory.\n",
      "-Attempts to create a unified field theory based on classical physics are classical unified field theories. During the years between the two World Wars, the idea of unification of gravity with electromagnetism was actively pursued by several mathematicians and physicists like Albert Einstein, Theodor Kaluza, Hermann Weyl, Arthur Eddington, Gustav Mie and Ernst Reichenbacher.Early attempts to create such theory were based on incorporation of electromagnetic fields into the geometry of general relativity. In 1918, the case for the first geometrization of the electromagnetic field was proposed in 1918 by Hermann Weyl.\n",
      "-The physical phenomena that electromagnetism describes have been studied as separate fields since antiquity. For example, there were many advances in the field of optics centuries before light was understood to be an electromagnetic wave. However, the theory of electromagnetism, as it is currently understood, grew out of Michael Faraday's experiments suggesting the existence of an electromagnetic field and James Clerk Maxwell's use of differential equations to describe it in his A Treatise on Electricity and Magnetism (1873). The development of electromagnetism in Europe included the development of methods to measure voltage, current, capacitance, and resistance. Detailed historical accounts are given by Wolfgang Pauli, E. T. Whittaker, Abraham Pais, and Bruce J. Hunt.\n",
      "-Some of the simplest physical fields are vector force fields. Historically, the first time that fields were taken seriously was with Faraday's lines of force when describing the electric field. The gravitational field was then similarly described.\n",
      "Newtonian gravitation The first field theory of gravity was Newton's theory of gravitation in which the mutual interaction between two masses obeys an inverse square law. This was very useful for predicting the motion of planets around the Sun.\n",
      "\n",
      "\n",
      "\n",
      "What is the relevant type of coherence for the Young's double-slit interferometer?\n",
      "-In some systems, such as water waves or optics, wave-like states can extend over one or two dimensions. Spatial coherence describes the ability for two spatial points x1 and x2 in the extent of a wave to interfere when averaged over time. More precisely, the spatial coherence is the cross-correlation between two points in a wave for all times. If a wave has only 1 value of amplitude over an infinite length, it is perfectly spatially coherent. The range of separation between the two points over which there is significant interference defines the diameter of the coherence area, Ac (Coherence length, often a feature of a source, is usually an industrial term related to the coherence time of the source, not the coherence area in the medium.) Ac is the relevant type of coherence for the Young's double-slit interferometer. It is also used in optical imaging systems and particularly in various types of astronomy telescopes. Sometimes people also use \"spatial coherence\" to refer to the visibility when a wave-like state is combined with a spatially shifted copy of itself.\n",
      "-Coherence controls the visibility or contrast of interference patterns. For example visibility of the double slit experiment pattern requires that both slits be illuminated by a coherent wave as illustrated in the figure. Large sources without collimation or sources that mix many different frequencies will have lower visibility.: 264 Coherence contains several distinct concepts. Spatial coherence describes the correlation (or predictable relationship) between waves at different points in space, either lateral or longitudinal. Temporal coherence describes the correlation between waves observed at different moments in time. Both are observed in the Michelson–Morley experiment and Young's interference experiment. Once the fringes are obtained in the Michelson interferometer, when one of the mirrors is moved away gradually from the beam-splitter, the time for the beam to travel increases and the fringes become dull and finally disappear, showing temporal coherence. Similarly, in a double-slit experiment, if the space between the two slits is increased, the coherence dies gradually and finally the fringes disappear, showing spatial coherence. In both cases, the fringe amplitude slowly disappears, as the path difference increases past the coherence length.\n",
      "-Young double slit experiment In Young's double slit experiment, light from a light source is allowed to pass through two pinholes separated by some distance, and a screen is placed some distance away from the pinholes where the interference between the light waves is observed (Figure. 1). Young's double slit experiment demonstrates the dependence of interference on coherence, specifically on the first-order correlation. This experiment is equivalent to the Mach–Zehnder interferometer with the caveat that Young's double slit experiment is concerned with spatial coherence, while the Mach–Zehnder interferometer relies on temporal coherence.The intensity measured at the position  r at time  t is ⟨I⟩=⟨|E+(r,t)|2⟩=⟨I⟩=I1+I2+2I1I2|γ(1)(x1,x2)|cosϕ(x1,x2) .Light field has highest degree of coherence when the corresponding interference pattern has the maximum contrast on the screen. The fringe contrast is defined as  V=Imax−IminImax+Imin Classically,  Iminmax=I1+I2±2I1I2|γ(1)(x1,x2)| and hence  V=2I1I2|γ(1)(x1,x2)|I1+I2 . As coherence is the ability to interfere visibility and coherence are linked: |γ(1)(x1,x2)|=1 means highest contrast, complete coherence 0<|γ(1)(x1,x2)|<1 means partial fringe visibility, partial coherence |γ(1)(x1,x2)|=0 means no contrast, complete incoherence.\n",
      "-Coherence was originally conceived in connection with Thomas Young's double-slit experiment in optics but is now used in any field that involves waves, such as acoustics, electrical engineering, neuroscience, and quantum mechanics. The property of coherence is the basis for commercial applications such as holography, the Sagnac gyroscope, radio antenna arrays, optical coherence tomography and telescope interferometers (astronomical optical interferometers and radio telescopes).\n",
      "-Applications In optical interferometers such as the Michelson interferometer, Mach–Zehnder interferometer, or Sagnac interferometer, one splits an electric field into two components, introduces a time delay to one of the components, and then recombines them. The intensity of resulting field is measured as a function of the time delay. In this specific case involving two equal input intensities, the visibility of the resulting interference pattern is given by: ν=|g(1)(τ)|ν=|g(1)(r1,t1;r2,t2)| where the second expression involves combining two space-time points from a field.\n",
      "\n",
      "\n",
      "\n",
      "What is the Peierls bracket in canonical quantization?\n",
      "-In theoretical physics, the Peierls bracket is an equivalent description of the Poisson bracket. It can be defined directly from the action and does not require the canonical coordinates and their canonical momenta to be defined in advance.The bracket [A,B] is defined as DA(B)−DB(A) ,as the difference between some kind of action of one quantity on the other, minus the flipped term.\n",
      "-In quantum mechanics, the Peierls bracket becomes a commutator i.e. a Lie bracket.\n",
      "-The method does not apply to all possible actions (for instance, actions with a noncausal structure or actions with gauge \"flows\"). It starts with the classical algebra of all (smooth) functionals over the configuration space. This algebra is quotiented over by the ideal generated by the Euler–Lagrange equations. Then, this quotient algebra is converted into a Poisson algebra by introducing a Poisson bracket derivable from the action, called the Peierls bracket. This Poisson algebra is then ℏ -deformed in the same way as in canonical quantization.\n",
      "-Deformation quantization The classical theory is described using a spacelike foliation of spacetime with the state at each slice being described by an element of a symplectic manifold with the time evolution given by the symplectomorphism generated by a Hamiltonian function over the symplectic manifold. The quantum algebra of \"operators\" is an ħ-deformation of the algebra of smooth functions over the symplectic space such that the leading term in the Taylor expansion over ħ of the commutator [A, B] expressed in the phase space formulation is iħ{A, B} . (Here, the curly braces denote the Poisson bracket. The subleading terms are all encoded in the Moyal bracket, the suitable quantum deformation of the Poisson bracket.) In general, for the quantities (observables) involved, and providing the arguments of such brackets, ħ-deformations are highly nonunique—quantization is an \"art\", and is specified by the physical context.\n",
      "-The Dirac bracket is a generalization of the Poisson bracket developed by Paul Dirac to treat classical systems with second class constraints in Hamiltonian mechanics, and to thus allow them to undergo canonical quantization. It is an important part of Dirac's development of Hamiltonian mechanics to elegantly handle more general Lagrangians; specifically, when constraints are at hand, so that the number of apparent variables exceeds that of dynamical ones. More abstractly, the two-form implied from the Dirac bracket is the restriction of the symplectic form to the constraint surface in phase space.This article assumes familiarity with the standard Lagrangian and Hamiltonian formalisms, and their connection to canonical quantization. Details of Dirac's modified Hamiltonian formalism are also summarized to put the Dirac bracket in context.\n",
      "\n",
      "\n",
      "\n",
      "What is the isophotal diameter used for in measuring a galaxy's size?\n",
      "-Examples of isophotal diameter measurements: Large Magellanic Cloud - 9.86 kiloparsecs (32,200 light-years) at the 25.0 B-mag/arcsec2 isophote.\n",
      "Milky Way - has a diameter at the 25.0 B-mag/arcsec2 isophote of 26.8 ± 1.1 kiloparsecs (87,400 ± 3,590 light-years).\n",
      "Messier 87 - has a has a diameter at the 25.0 B-mag/arcsec2 isophote of 40.55 kiloparsecs (132,000 light-years).\n",
      "Andromeda Galaxy - has a has a diameter at the 25.0 B-mag/arcsec2 isophote of 46.56 kiloparsecs (152,000 light-years).\n",
      "-Isophotal diameter The isophotal diameter is introduced as a conventional way of measuring a galaxy's size based on its apparent surface brightness. Isophotes are curves in a diagram - such as a picture of a galaxy - that adjoins points of equal brightnesses, and are useful in defining the extent of the galaxy. The apparent brightness flux of a galaxy is measured in units of magnitudes per square arcsecond (mag/arcsec2; sometimes expressed as mag arcsec−2), which defines the brightness depth of the isophote. To illustrate how this unit works, a typical galaxy has a brightness flux of 18 mag/arcsec2 at its central region. This brightness is equivalent to the light of an 18th magnitude hypothetical point object (like a star) being spread out evenly in a one square arcsecond area of the sky. For the purposes of objectivity, the spectrum of light being used is sometimes also given in figures. As an example, the Milky Way has an average surface brightness of 22.1 B-mag/arcsec−2, where B-mag refers to the brightness at the B-band (445 nm wavelength of light, in the blue part of the visible spectrum).\n",
      "-A critique of an earlier version of this method has been issued by IPAC, with the method causing a magnitude of error (upwards to 10%) of the values than using isophotal diameter. The use of Petrosian magnitudes also have the disadvantage of missing most of the light outside the Petrosian aperture, which is defined relative to the galaxy's overall brightness profile, especially for elliptical galaxies, with higher signal-to-noise ratios on higher distances and redshifts. A correction for this method has been issued by Graham et al. in 2005, based on the assumption that galaxies follow Sersic's law.\n",
      "-This conventional standard, however, is not universally agreed upon. Erik Holmberg in 1958 measured the diameters of at least 300 galaxies at the isophote of about 26.5 mag/arcsec2 (originally defined as where the photographic brightness density with respect to plate background is 0.5%). Various other surveys such that of the ESO in 1989 use isophotes as faint as 27.0 mag/arcsec2. Nevertheless, corrections of these diameters were introduced by both the Second and Third Reference Catalogue of Galaxies (RC2 and RC3), at least to those galaxies being covered by the two catalogues.\n",
      "-Galaxies do not have a definite boundary by their nature, and are characterized by a gradually decreasing stellar density as a function of increasing distance from their center, making measurements of their true extents difficult. Nevertheless, astronomers over the past few decades have made several criteria in defining the sizes of galaxies. As early as the time of Edwin Hubble in 1936, there have been attempts to characterize the diameters of galaxies. With the advent of large sky surveys in the second half of the 20th century, the need for a standard for accurate determination of galaxy sizes has been in greater demand due to its enormous implications in astrophysics, such as the accurate determination of the Hubble constant. Various standards have been adapted over the decades, some more preferred than others. Below are some of these examples.\n",
      "\n",
      "\n",
      "\n",
      "What is the Maxwell's Demon thought experiment?\n",
      "-Maxwell's demon James Clerk Maxwell imagined one container divided into two parts, A and B. Both parts are filled with the same gas at equal temperatures and placed next to each other, separated by a wall. Observing the molecules on both sides, an imaginary demon guards a microscopic trapdoor in the wall. When a faster-than-average molecule from A flies towards the trapdoor, the demon opens it, and the molecule will fly from A to B. The average speed of the molecules in B will have increased while in A they will have slowed down on average. Since average molecular speed corresponds to temperature, the temperature decreases in A and increases in B, contrary to the second law of thermodynamics.One response to this question was suggested in 1929 by Leó Szilárd and later by Léon Brillouin. Szilárd pointed out that a real-life Maxwell's demon would need to have some means of measuring molecular speed, and that the act of acquiring information would require an expenditure of energy. Likewise, Brillouin demonstrated that the decrease in entropy caused by the demon would be less than the entropy produced by choosing molecules based on their speed.Maxwell's 'demon' repeatedly alters the permeability of the wall between A and B. It is therefore performing thermodynamic operations on a microscopic scale, not just observing ordinary spontaneous or natural macroscopic thermodynamic processes.\n",
      "-Maxwell's demon is a thought experiment that would hypothetically violate the second law of thermodynamics. It was proposed by the physicist James Clerk Maxwell in 1867. In his first letter, Maxwell referred to the entity as a \"finite being\" or a \"being who can play a game of skill with the molecules\". Lord Kelvin would later call it a \"demon\".In the thought experiment, a demon controls a small massless door between two chambers of gas. As individual gas molecules (or atoms) approach the door, the demon quickly opens and closes the door to allow only fast-moving molecules to pass through in one direction, and only slow-moving molecules to pass through in the other. Because the kinetic temperature of a gas depends on the velocities of its constituent molecules, the demon's actions cause one chamber to warm up and the other to cool down. This would decrease the total entropy of the system, without applying any work, thereby violating the second law of thermodynamics.\n",
      "-The second law of thermodynamics ensures (through statistical probability) that two bodies of different temperature, when brought into contact with each other and isolated from the rest of the Universe, will evolve to a thermodynamic equilibrium in which both bodies have approximately the same temperature. The second law is also expressed as the assertion that in an isolated system, entropy never decreases.Maxwell conceived a thought experiment as a way of furthering the understanding of the second law. His description of the experiment is as follows: ... if we conceive of a being whose faculties are so sharpened that he can follow every molecule in its course, such a being, whose attributes are as essentially finite as our own, would be able to do what is impossible to us. For we have seen that molecules in a vessel full of air at uniform temperature are moving with velocities by no means uniform, though the mean velocity of any great number of them, arbitrarily selected, is almost exactly uniform. Now let us suppose that such a vessel is divided into two portions, A and B, by a division in which there is a small hole, and that a being, who can see the individual molecules, opens and closes this hole, so as to allow only the swifter molecules to pass from A to B, and only the slower molecules to pass from B to A. He will thus, without expenditure of work, raise the temperature of B and lower that of A, in contradiction to the second law of thermodynamics.In other words, Maxwell imagines one container divided into two parts, A and B. Both parts are filled with the same gas at equal temperatures and placed next to each other. Observing the molecules on both sides, an imaginary demon guards a trapdoor between the two parts. When a faster-than-average molecule from A flies towards the trapdoor, the demon opens it, and the molecule will fly from A to B. Likewise, when a slower-than-average molecule from B flies towards the trapdoor, the demon will let it pass from B to A. The average speed of the molecules in B will have increased while in A they will have slowed down on average. Since average molecular speed corresponds to temperature, the temperature decreases in A and increases in B, contrary to the second law of thermodynamics. A heat engine operating between the thermal reservoirs A and B could extract useful work from this temperature difference.\n",
      "-Maxwell's demon can distinguish between fast and slow moving molecules. If this demon only let fast moving molecules through a trapdoor to a container, the temperature inside the container would increase without any work being applied. Such a scenario violates the second law of thermodynamics. Leo Szilard's refinement of Maxwell's demon in the context of information theory is sometimes referred to as Szilard's demon. The biological equivalent of Maxwell's \"finite being\" is a Molecular demon.\n",
      "-The demon must allow molecules to pass in both directions in order to produce only a temperature difference; one-way passage only of faster-than-average molecules from A to B will cause higher temperature and pressure to develop on the B side.\n",
      "\n",
      "\n",
      "\n",
      "What is the application of Memristor?\n",
      "-One advantage of memristive networks is that they can be implemented using relatively simple and inexpensive hardware, making them an attractive option for developing low-cost artificial intelligence systems. They also have the potential to be more energy efficient than traditional artificial neural networks, as they can store and process information using less power. However, the field of memristive networks is still in the early stages of development, and more research is needed to fully understand their capabilities and limitations.\n",
      "-In 2013 Leon Chua published a tutorial underlining the broad span of complex phenomena and applications that memristors span and how they can be used as non-volatile analog memories and can mimic classic habituation and learning phenomena.\n",
      "-A memristive network is a type of artificial neural network that is based on memristive devices, which are electronic components that exhibit the property of memristance.  In a memristive network, the memristive devices are used to simulate the behavior of neurons and synapses in the human brain. The network consists of layers of memristive devices, each of which is connected to other layers through a set of weights. These weights are adjusted during the training process, allowing the network to learn and adapt to new input data.\n",
      "-The identification of memristive properties in electronic devices has attracted controversy. Experimentally, the ideal memristor has yet to be demonstrated.\n",
      "-A memristor (; a portmanteau of memory resistor) is a non-linear two-terminal electrical component relating electric charge and magnetic flux linkage. It was described and named in 1971 by Leon Chua, completing a theoretical quartet of fundamental electrical components which comprises also the resistor, capacitor and inductor.Chua and Kang later generalized the concept to memristive systems. Such a system comprises a circuit, of multiple conventional components, which mimics key properties of the ideal memristor component and is also commonly referred to as a memristor. Several such memristor system technologies have been developed, notably ReRAM.\n",
      "\n",
      "\n",
      "\n",
      "What is the effect generated by a spinning superconductor?\n",
      "-London moment Conversely, a spinning superconductor generates a magnetic field, precisely aligned with the spin axis. The effect, the London moment, was put to good use in Gravity Probe B. This experiment measured the magnetic fields of four superconducting gyroscopes to determine their spin axes. This was critical to the experiment since it is one of the few ways to accurately determine the spin axis of an otherwise featureless sphere.\n",
      "-London moment A London moment gyroscope relies on the quantum-mechanical phenomenon, whereby a spinning superconductor generates a magnetic field whose axis lines up exactly with the spin axis of the gyroscopic rotor. A magnetometer determines the orientation of the generated field, which is interpolated to determine the axis of rotation. Gyroscopes of this type can be extremely accurate and stable. For example, those used in the Gravity Probe B experiment measured changes in gyroscope spin axis orientation to better than 0.5 milliarcseconds (1.4×10−7 degrees, or about 2.4×10−9 radians) over a one-year period. This is equivalent to an angular separation the width of a human hair viewed from 32 kilometers (20 mi) away.The GP-B gyro consists of a nearly-perfect spherical rotating mass made of fused quartz, which provides a dielectric support for a thin layer of niobium superconducting material. To eliminate friction found in conventional bearings, the rotor assembly is centered by the electric field from six electrodes. After the initial spin-up by a jet of helium which brings the rotor to 4,000 RPM, the polished gyroscope housing is evacuated to an ultra-high vacuum to further reduce drag on the rotor. Provided the suspension electronics remain powered, the extreme rotational symmetry, lack of friction, and low drag will allow the angular momentum of the rotor to keep it spinning for about 15,000 years.A sensitive DC SQUID that can discriminate changes as small as one quantum, or about 2 ×10−15 Wb, is used to monitor the gyroscope. A precession, or tilt, in the orientation of the rotor causes the London moment magnetic field to shift relative to the housing. The moving field passes through a superconducting pickup loop fixed to the housing, inducing a small electric current. The current produces a voltage across a shunt resistance, which is resolved to spherical coordinates by a microprocessor. The system is designed to minimize Lorentz torque on the rotor.\n",
      "-The London moment (after Fritz London) is a quantum-mechanical phenomenon whereby a spinning superconductor generates a magnetic field whose axis lines up exactly with the spin axis.\n",
      "The term may also refer to the magnetic moment of any rotation of any superconductor, caused by the electrons lagging behind the rotation of the object, although the field strength is independent of the charge carrier density in the superconductor.\n",
      "-In science fiction, artificial gravity (or cancellation of gravity) or \"paragravity\" is sometimes present in spacecraft that are neither rotating nor accelerating. At present, there is no confirmed technique as such that can simulate gravity other than actual mass or acceleration. There have been many claims over the years of such a device. Eugene Podkletnov, a Russian engineer, has claimed since the early 1990s to have made such a device consisting of a spinning superconductor producing a powerful \"gravitomagnetic field\", but there has been no verification or even negative results from third parties. In 2006, a research group funded by ESA claimed to have created a similar device that demonstrated positive results for the production of gravitomagnetism, although it produced only 0.0001 g. This result has not been replicated.\n",
      "-Increasing the field adds vortices to the system. Eventually the density of vortices becomes so large that they overlap. The core of the vortex contains normal electrons (i.e. the amplitude of the superconducting order parameter is zero), so when they overlap, superconductivity is killed by destroying the amplitude of the order parameter. Increasing the field further leads to a very interesting possibility - in two-dimensions where the fluctuations are enhanced - that the vortices may condense into a Bose-condensate, which localizes the superconducting pairs.\n",
      "\n",
      "\n",
      "\n",
      "What is the main focus of cryogenic and noble liquid detectors in dark matter experiments?\n",
      "-These experiments mostly use either cryogenic or noble liquid detector technologies. Cryogenic detectors operating at temperatures below 100 mK, detect the heat produced when a particle hits an atom in a crystal absorber such as germanium. Noble liquid detectors detect scintillation produced by a particle collision in liquid xenon or argon. Cryogenic detector experiments include: CDMS, CRESST, EDELWEISS, EURECA. Noble liquid experiments include LZ, XENON, DEAP, ArDM, WARP, DarkSide, PandaX, and LUX, the Large Underground Xenon experiment. Both of these techniques focus strongly on their ability to distinguish background particles (which predominantly scatter off electrons) from dark matter particles (that scatter off nuclei). Other experiments include SIMPLE and PICASSO.\n",
      "-Noble gas scintillators Noble gas scintillators use the property of certain materials to scintillate, which is when a material absorbs energy from a particle and remits the same amount of energy as light. Of particular interest for dark matter detection is the use of noble gases, even more specifically liquid xenon.  The XENON series of experiments, also located at the Gran Sasso National Lab, is a forefront user of liquid xenon scintillators. Common across all generations of the experiment, the detector consists of a tank of liquid xenon with a gaseous layer on top. At the top and bottom of the detector is a layer of photomultiplier tubes (PMTs). When a dark matter particle collides with the liquid xenon, it rapidly releases a photon which is detected by the PMTs. To cross reference this data point an electric field is applied which is sufficiently large to prevent complete recombination of the electrons knocked loose by the interaction. These drift to the top of the detector and are also detected, creating two separate detections for each event. Measuring the time delay between these allows for a complete 3-D reconstruction of the interaction. The detector is also able to discriminate between electronic recoils and nuclear recoils, as both types of events would produce differing ratios of the photon energy and the released electron energy.  The most recently completed version of the XENON experiment is XENON1T, which used 3.2 tons of liquid xenon. This experiment produced a then record limit for the cross section of WIMP dark matter of 4.1×10−47 cm2 at a mass of 30 GeV/c2. The most recent iteration of the XENON succession is XENONnT, which is currently running with 8 tones of liquid xenon. This experiment is projected to be able to probe WIMP-nucleon cross sections of 1.4×10−48 cm2 for a 50 GeV/c2 WIMP mass. At this ultra-low cross section, interference from the background neutrino flux is predicted to be problematic.\n",
      "-Noble gas scintillators – Another way of detecting atoms \"knocked about\" by a WIMP is to use scintillating material, so that light pulses are generated by the moving atom and detected, often with PMTs. Experiments such as DEAP at SNOLAB and DarkSide at the LNGS instrument a very large target mass of liquid argon for sensitive WIMP searches. ZEPLIN, and XENON used xenon to exclude WIMPs at higher sensitivity, with the most stringent limits to date provided by the XENON1T detector, utilizing 3.5 tons of liquid xenon. Even larger multi-ton liquid xenon detectors have been approved for construction from the XENON, LUX-ZEPLIN and PandaX collaborations.\n",
      "-Cryogenic dark matter experiments use particle detectors operating at millikelvin temperatures to search for the elastic scattering of WIMPs of an atomic nuclei. A particle interaction inside an absorber crystal will create a large number of phonons, these thermalise inside a thermometer on the crystal surface, which records the rise in temperature. Such cryogenic detectors are used as they combine a high sensitivity with a low energy threshold and excellent resolution.\n",
      "-Liquid xenon is used in calorimeters to measure gamma rays, and as a detector of hypothetical weakly interacting massive particles, or WIMPs. When a WIMP collides with a xenon nucleus, theory predicts it will impart enough energy to cause ionization and scintillation. Liquid xenon is useful for these experiments because its density makes dark matter interaction more likely and it permits a quiet detector through self-shielding.\n",
      "\n",
      "\n",
      "\n",
      "What is a pycnometer?\n",
      "-A gas pycnometer is a laboratory device used for measuring the density—or, more accurately, the volume—of solids, be they regularly shaped, porous or non-porous, monolithic, powdered, granular or in some way comminuted, employing some method of gas displacement and the volume:pressure relationship known as Boyle's Law. A gas pycnometer is also sometimes referred to as a helium pycnometer.\n",
      "-Pycnometer is the preferred spelling in modern American English usage. Pyknometer is to be found in older texts, and is used interchangeably with pycnometer in British English. The term has its origins in the Greek word πυκνός, meaning \"dense\".\n",
      "The density calculated from a volume measured using a gas pycnometer is often referred to as skeletal density, true density or helium density.\n",
      "For non-porous solids a pycnometer can be used to measure particle density.\n",
      "An extreme example of the gas displacement principle for volume measurement is described in U.S. Patent 5,231,873 (Lindberg, 1993) wherein a chamber large enough to hold a flatbed truck is used to measure the volume of a load of timber.\n",
      "-Gas expansion pycnometer Gas expansion pycnometer is also known as constant volume gas pycnometer. The simplest type of gas pycnometer (due to its relative lack of moving parts) consists of two chambers, one (with a removable gas-tight lid) to hold the sample and a second chamber of fixed, known (via calibration) internal volume – referred to as the reference volume or added volume. The device additionally comprises a valve to admit a gas under pressure to one of the chambers, a pressure measuring device – usually a transducer – connected to the first chamber, a valved pathway connecting the two chambers, and a valved vent from the second of the chambers. In practice the sample may occupy either chamber, that is gas pycnometers can be constructed such that the sample chamber is pressurized first, or such that it is the reference chamber that starts at the higher pressure. Various design parameters have been analyzed by Tamari. The working equation of a gas pycnometer wherein the sample chamber is pressurized first is as follows: Vs=Vc+Vr1−P1P2 where Vs is the sample volume, Vc is the volume of the empty sample chamber (known from a prior calibration step), Vr is the volume of the reference volume (again known from a prior calibration step), P1 is the first pressure (i.e. in the sample chamber only) and P2 is the second (lower) pressure after expansion of the gas into the combined volumes of sample chamber and reference chamber.\n",
      "-A gas pycnometer, the gas-based manifestation of a pycnometer, compares the change in pressure caused by a measured change in a closed volume containing a reference (usually a steel sphere of known volume) with the change in pressure caused by the sample under the same conditions. The difference in change of pressure represents the volume of the sample as compared to the reference sphere, and is usually used for solid particulates that may dissolve in the liquid medium of the pycnometer design described above, or for porous materials into which the liquid would not fully penetrate.\n",
      "-Pycnometer A pycnometer (from Ancient Greek: πυκνός, romanized: puknos, lit. 'dense'), also called pyknometer or specific gravity bottle, is a device used to determine the density of a liquid. A pycnometer is usually made of glass, with a close-fitting ground glass stopper with a capillary tube through it, so that air bubbles may escape from the apparatus. This device enables a liquid's density to be measured accurately by reference to an appropriate working fluid, such as water or mercury, using an analytical balance.If the flask is weighed empty, full of water, and full of a liquid whose relative density is desired, the relative density of the liquid can easily be calculated. The particle density of a powder, to which the usual method of weighing cannot be applied, can also be determined with a pycnometer. The powder is added to the pycnometer, which is then weighed, giving the weight of the powder sample. The pycnometer is then filled with a liquid of known density, in which the powder is completely insoluble. The weight of the displaced liquid can then be determined, and hence the relative density of the powder.\n",
      "\n",
      "\n",
      "\n",
      "What is the estimated redshift of CEERS-93316, a candidate high-redshift galaxy observed by the James Webb Space Telescope?\n",
      "-While some scientists have claimed other objects (such as Abell 1835 IR1916) have higher redshifts (and therefore are seen in an earlier stage of the universe's evolution), IOK-1's age and composition have been more reliably established. In December 2012, astronomers reported that UDFj-39546284 is the most distant object known and has a redshift value of 11.9. The object, estimated to have existed around 380 million years after the Big Bang (which was about 13.8 billion years ago), is about 13.42 billion light travel distance years away. The existence of galaxies so soon after the Big Bang suggests that protogalaxies must have grown in the so-called \"dark ages\". As of May 5, 2015, the galaxy EGS-zs8-1 is the most distant and earliest galaxy measured, forming 670 million years after the Big Bang. The light from EGS-zs8-1 has taken 13 billion years to reach Earth, and is now 30 billion light-years away, because of the expansion of the universe during 13 billion years. On 17 August 2022, NASA released a large mosaic image of 690 individual frames taken by the Near Infrared Camera (NIRCam) on the James Webb Space Telescope (JWST) of numerous very early galaxies.In May 2023, a study in the journal Nature identified an ultra-faint galaxy named JD1. Galaxy JD1 was observed by the JWST using the near-infrared spectrograph instrument NIRSpec and was found to have a distance value of redshift z=9.79. This means that JD1 was observed at 480 million years after the Big Bang when the universe was only about 4% of its present age. Observations of this ultra-faint galaxy were aided by the effect of a gravitational lens in the galaxy cluster Abell 2744 which helped make the image of JD1 larger and 13 times brighter than it otherwise would be. This effect and the use of the JWST's NIRCam showed JD1's structure to be three starforming clumps of dust and gas. One of the authors of the study Tommaso Treu said: \"The combination of JWST and the magnifying power of gravitational lensing is a revolution. We are rewriting the book on how galaxies formed and evolved in the immediate aftermath of the Big Bang.\" The detailed process by which the earliest galaxies formed is an open question in astrophysics. Theories can be divided into two categories: top-down and bottom-up. In top-down correlations (such as the Eggen–Lynden-Bell–Sandage [ELS] model), protogalaxies form in a large-scale simultaneous collapse lasting about one hundred million years. In bottom-up theories (such as the Searle-Zinn [SZ] model), small structures such as globular clusters form first, and then a number of such bodies accrete to form a larger galaxy.\n",
      "-Redshift (z) can be expressed by the following equations: In these equations, frequency is denoted by  f and wavelength by  λ . The larger the value of z, the more redshifted the light and the farther away the object is from the Earth. As of January 2013, the largest galaxy redshift of z~12 was found using the Hubble Ultra-Deep Field, corresponding to an age of over 13 billion years (the universe is approximately 13.82 billion years old).The Doppler effect and Hubble's law can be combined to form the equation Hubble c where c is the speed of light.\n",
      "-2022 — James Webb Space Telescope (JWST) releases the Webb's First Deep Field.\n",
      "2022 — JWST detects CEERS-93316, a candidate high-redshift galaxy, with an estimated redshift of approximately z = 16.7, corresponding to 235.8 million years after the Big Bang. If confirmed, it is one of the earliest and most distant known galaxies observed.\n",
      "-Highest redshifts Currently, the objects with the highest known redshifts are galaxies and the objects producing gamma ray bursts. The most reliable redshifts are from spectroscopic data, and the highest-confirmed spectroscopic redshift of a galaxy is that of GN-z11, with a redshift of z = 11.1, corresponding to 400 million years after the Big Bang. The previous record was held by UDFy-38135539 at a redshift of z = 8.6, corresponding to 600 million years after the Big Bang. Slightly less reliable are Lyman-break redshifts, the highest of which is the lensed galaxy A1689-zD1 at a redshift z = 7.5 and the next highest being z = 7.0. The most distant-observed gamma-ray burst with a spectroscopic redshift measurement was GRB 090423, which had a redshift of z = 8.2. The most distant-known quasar, ULAS J1342+0928, is at z = 7.54. The highest-known redshift radio galaxy (TGSS1530) is at a redshift z = 5.72 and the highest-known redshift molecular material is the detection of emission from the CO molecule from the quasar SDSS J1148+5251 at z = 6.42.Extremely red objects (EROs) are astronomical sources of radiation that radiate energy in the red and near infrared part of the electromagnetic spectrum. These may be starburst galaxies that have a high redshift accompanied by reddening from intervening dust, or they could be highly redshifted elliptical galaxies with an older (and therefore redder) stellar population. Objects that are even redder than EROs are termed hyper extremely red objects (HEROs).The cosmic microwave background has a redshift of z = 1089, corresponding to an age of approximately 379,000 years after the Big Bang and a proper distance of more than 46 billion light-years. The yet-to-be-observed first light from the oldest Population III stars, not long after atoms first formed and the CMB ceased to be absorbed almost completely, may have redshifts in the range of 20 < z < 100. Other high-redshift events predicted by physics but not presently observable are the cosmic neutrino background from about two seconds after the Big Bang (and a redshift in excess of z > 1010) and the cosmic gravitational wave background emitted directly from inflation at a redshift in excess of z > 1025.In June 2015, astronomers reported evidence for Population III stars in the Cosmos Redshift 7 galaxy at z = 6.60. Such stars are likely to have existed in the very early universe (i.e., at high redshift), and may have started the production of chemical elements heavier than hydrogen that are needed for the later formation of planets and life as we know it.\n",
      "-MACS0647-JD is a galaxy with a redshift of about z = 10.7, equivalent to a light travel distance of 13.26 billion light-years (4 billion parsecs). If the distance estimate is correct, it formed about 427 million years after the Big Bang.\n",
      "\n",
      "\n",
      "\n",
      "What is bollard pull primarily used for measuring?\n",
      "-Bollard pull is a conventional measure of the pulling (or towing) power of a watercraft. It is defined as the force (usually in tonnes-force or kilonewtons (kN)) exerted by a vessel under full power, on a shore-mounted bollard through a tow-line, commonly measured in a practical test (but sometimes simulated) under test conditions that include calm water, no tide, level trim, and sufficient depth and side clearance for a free propeller stream. Like the horsepower or mileage rating of a car, it is a convenient but idealized number that must be adjusted for operating conditions that differ from the test. The bollard pull of a vessel may be reported as two numbers, the static or maximum bollard pull – the highest force measured – and the steady or continuous bollard pull, the average of measurements over an interval of, for example, 10 minutes. An equivalent measurement on land is known as drawbar pull, or tractive force, which is used to measure the total horizontal force generated by a locomotive, a piece of heavy machinery such as a tractor, or a truck, (specifically a ballast tractor), which is utilized to move a load.\n",
      "-Values for bollard pull can be determined in two ways.\n",
      "-Practical bollard pull tests under simplified conditions are conducted for human powered vehicles. There, bollard pull is often a category in competitions and gives an indication of the power train efficiency. Although conditions for such measurements are inaccurate in absolute terms, they are the same for all competitors. Hence, they can still be valid for comparing several craft.\n",
      "-Conditions must be static. The engine power, the heading of the ship, the conditions of the propeller discharge race and the tension in the towing line must have settled to a constant or near-constant value for a reliable measurement.\n",
      "-Bollard pull values are stated in tonnes-force (written as t or tonnef) or kilonewtons (kN).\n",
      "Estimated horsepower is equal to total resistance times velocity divided by 550. In the English system units,  550 \n",
      "\n",
      "\n",
      "\n",
      "What is the piezoelectric strain coefficient for AT-cut quartz crystals?\n",
      "-Amplitude of motion The amplitude of lateral displacement rarely exceeds a nanometer. More specifically one has u0=4(nπ)2dQUel with u0 the amplitude of lateral displacement, n the overtone order, d the piezoelectric strain coefficient, Q the quality factor, and Uel the amplitude of electrical driving. The piezoelectric strain coefficient is given as d = 3.1·10‑12 m/V for AT-cut quartz crystals. Due to the small amplitude, stress and strain usually are proportional to each other. The QCM operates in the range of linear acoustics.\n",
      "-The strain-charge for a material of the 4mm (C4v) crystal class (such as a poled piezoelectric ceramic such as tetragonal PZT or BaTiO3) as well as the 6mm crystal class may also be written as (ANSI IEEE 176): 11 12 13 21 22 23 31 32 33 44 55 66 11 12 31 32 33 24 15 15 24 31 32 33 11 22 33 ][E1E2E3] where the first equation represents the relationship for the converse piezoelectric effect and the latter for the direct piezoelectric effect.Although the above equations are the most used form in literature, some comments about the notation are necessary. Generally, D and E are vectors, that is, Cartesian tensors of rank 1; and permittivity ε is a Cartesian tensor of rank 2. Strain and stress are, in principle, also rank-2 tensors. But conventionally, because strain and stress are all symmetric tensors, the subscript of strain and stress can be relabeled in the following fashion: 11 → 1; 22 → 2; 33 → 3; 23 → 4; 13 → 5; 12 → 6. (Different conventions may be used by different authors in literature. For example, some use 12 → 4; 23 → 5; 31 → 6 instead.) That is why S and T appear to have the \"vector form\" of six components. Consequently, s appears to be a 6-by-6 matrix instead of a rank-3 tensor. Such a relabeled notation is often called Voigt notation. Whether the shear strain components S4, S5, S6 are tensor components or engineering strains is another question. In the equation above, they must be engineering strains for the 6,6 coefficient of the compliance matrix to be written as shown, i.e., 2(sE11 − sE12). Engineering shear strains are double the value of the corresponding tensor shear, such as S6 = 2S12 and so on. This also means that s66 = 1/G12, where G12 is the shear modulus.\n",
      "-Assumptions For a number of experimental configurations, there are explicit expressions relating the shifts of frequency and bandwidth to the sample properties. The assumptions underlying the equations are the following: The resonator and all cover layers are laterally homogeneous and infinite.\n",
      "The distortion of the crystal is given by a transverse plane wave with the wave-vector perpendicular to the surface normal (thickness-shear mode). There are neither compressional waves nor flexural contributions to the displacement pattern. There are no nodal lines in the plane of the resonator.\n",
      "All stresses are proportional to strain. Linear viscoelasticity holds.\n",
      "Piezoelectric stiffening may be ignored.\n",
      "Semi-infinite viscoelastic medium For a semi-infinite medium, one has Δf∗ff=iπZqσu˙=iπZqZac=iπZqρiωη =1πZq−1+i2ρω(η′−iη′′)=iπZqρ(G′+iG′′) η’ and η’’ are the real and the imaginary part of the viscosity, respectively. Zac = ρc =(G ρ)1/2 is the acoustic impedance of the medium. ρ is the density, c, the speed of sound, and G = i ωη is the shear modulus.\n",
      "-Converse piezoelectric effect The converse piezoelectric effect (CPE) describes how an applied electric field will create a resultant strain which in turn leads to a physical deformation of the material. This effect can be described through the constitutive equations. The CPE can be written as Xi=dkiEk where Xi is the strain tensor, dki is the piezoelectric tensor, and Ek is the electric field. If the piezoelectric tensor is considered to be that of the tetragonal crystal system (that of BaTiO3) then it is 31 32 33 15 15 00000][E1E2E3] such that the equation will lead to the strain components for an applied field. If the field is applied exclusively in one direction i.e. E3 for example, then the resulting strain components are: d31E3, d32E3, d33E3 Thus for an electric field applied along the c-axis of BaTiO3 i.e. E3, then the resulting deformation of the crystal will be an elongation along the c-axis and an axially symmetric contraction along the other orthogonal directions. PFM uses the effect of this deformation to detect domains and also to determine their orientation.\n",
      "-Equivalent circuits build on the electromechanical analogy. In the same way as the current through a network of resistors can be predicted from their arrangement and the applied voltage, the displacement of a network of mechanical elements can be predicted from the topology of the network and the applied force. The electro-mechanical analogy maps forces onto voltages and speeds onto currents. The ratio of force and speed is termed \"mechanical impedance\". Note: Here, speed means the time derivative of a displacement, not the speed of sound. There also is an electro-acoustic analogy, within which stresses (rather than forces) are mapped onto voltages. In acoustics, forces are normalized to area. The ratio of stress and speed should not be called \"acoustic impedance\" (in analogy to the mechanical impedance) because this term is already in use for the material property Zac = ρc with ρ the density and c the speed of sound). The ratio of stress and speed at the crystal surface is called load impedance, ZL. Synonymous terms are \"surface impedance\" and \"acoustic load.\" The load impedance is in general not equal to the material constant Zac = ρc = (Gρ)1/2. Only for propagating plane waves are the values of ZL and Zac the same.\n",
      "\n",
      "\n",
      "\n",
      "What is the difference between probability mass function (PMF) and probability density function (PDF)?\n",
      "-The terms probability distribution function and probability function have also sometimes been used to denote the probability density function. However, this use is not standard among probabilists and statisticians. In other sources, \"probability distribution function\" may be used when the probability distribution is defined as a function over general sets of values or it may refer to the cumulative distribution function, or it may be a probability mass function (PMF) rather than the density. \"Density function\" itself is also used for the probability mass function, leading to further confusion. In general though, the PMF is used in the context of discrete random variables (random variables that take values on a countable set), while the PDF is used in the context of continuous random variables.\n",
      "-A probability mass function differs from a probability density function (PDF) in that the latter is associated with continuous rather than discrete random variables. A PDF must be integrated over an interval to yield a probability.The value of the random variable having the largest probability mass is called the mode.\n",
      "-In probability and statistics, a probability mass function is a function that gives the probability that a discrete random variable is exactly equal to some value. Sometimes it is also known as the discrete probability density function. The probability mass function is often the primary means of defining a discrete probability distribution, and such functions exist for either scalar or multivariate random variables whose domain is discrete.\n",
      "-Probability mass function is the probability distribution of a discrete random variable, and provides the possible values and their associated probabilities. It is the function  p:R→[0,1] defined by for  −∞<x<∞ , where  P is a probability measure.  pX(x) can also be simplified as  p(x) .The probabilities associated with all (hypothetical) values must be non-negative and sum up to 1, and  Thinking of probability as mass helps to avoid mistakes since the physical mass is conserved as is the total probability for all hypothetical outcomes  x \n",
      "-Some key concepts and terms, widely used in the literature on the topic of probability distributions, are listed below.\n",
      "Basic terms Random variable: takes values from a sample space; probabilities describe which values and set of values are taken more likely.\n",
      "Event: set of possible values (outcomes) of a random variable that occurs with a certain probability.\n",
      "Probability function or probability measure: describes the probability  P(X∈E) that the event  E, occurs.\n",
      "Cumulative distribution function: function evaluating the probability that  X will take a value less than or equal to  x for a random variable (only for real-valued random variables).\n",
      "Quantile function: the inverse of the cumulative distribution function. Gives  x such that, with probability  q ,  X will not exceed  x Discrete probability distributions Discrete probability distribution: for many random variables with finitely or countably infinitely many values.\n",
      "Probability mass function (pmf): function that gives the probability that a discrete random variable is equal to some value.\n",
      "Frequency distribution: a table that displays the frequency of various outcomes in a sample.\n",
      "Relative frequency distribution: a frequency distribution where each value has been divided (normalized) by a number of outcomes in a sample (i.e. sample size).\n",
      "Categorical distribution: for discrete random variables with a finite set of values.\n",
      "Absolutely continuous probability distributions Absolutely continuous probability distribution: for many random variables with uncountably many values.\n",
      "Probability density function (pdf) or probability density: function whose value at any given sample (or point) in the sample space (the set of possible values taken by the random variable) can be interpreted as providing a relative likelihood that the value of the random variable would equal that sample.\n",
      "\n",
      "\n",
      "\n",
      "How do the Lunar Laser Ranging Experiment, radar astronomy, and the Deep Space Network determine distances to the Moon, planets, and spacecraft?\n",
      "-Distance measurement Radar systems measure the distance to a target by the time it takes a radio-wave pulse to return to the radar antenna after being reflected by the target: the distance to the target is half the round-trip transit time multiplied by the speed of light. A Global Positioning System (GPS) receiver measures its distance to GPS satellites based on how long it takes for a radio signal to arrive from each satellite, and from these distances calculates the receiver's position. Because light travels about 300000 kilometres (186000 mi) in one second, these measurements of small fractions of a second must be very precise. The Lunar Laser Ranging experiment, radar astronomy and the Deep Space Network determine distances to the Moon, planets and spacecraft, respectively, by measuring round-trip transit times.\n",
      "-As of 2009, the distance to the Moon can be measured with millimeter precision. In a relative sense, this is one of the most precise distance measurements ever made, and is equivalent in accuracy to determining the distance between Los Angeles and New York to within the width of a human hair.\n",
      "-Lunar Laser Ranging (LLR) is the practice of measuring the distance between the surfaces of the Earth and the Moon using laser ranging. The distance can be calculated from the round-trip time of laser light pulses travelling at the speed of light, which are reflected back to Earth by the Moon's surface or by one of five retroreflectors installed on the Moon. Three were installed during the Apollo program (11, 14, and 15) and two on the Lunokhod 1 and 2 missions.Although it is possible to reflect light or radio waves directly from the Moon's surface (a process known as EME), a much more precise range measurement can be made using retroreflectors, since because of their small size, the temporal spread in the reflected signal is much smaller.\n",
      "-Laser ranging An experiment which measured the round-trip time of flight of laser pulses reflected directly off the surface of the Moon was performed in 1962, by a team from Massachusetts Institute of Technology, and a Soviet team at the Crimean Astrophysical Observatory.During the Apollo missions in 1969, astronauts placed retroreflectors on the surface of the Moon for the purpose of refining the accuracy and precision of this technique. The measurements are ongoing and involve multiple laser facilities. The instantaneous precision of the Lunar Laser Ranging experiments can achieve small millimeter resolution, and is the most reliable method of determining the lunar distance. The semi-major axis is determined to be 384,399.0 km.\n",
      "-Millimeter-precision measurements of the lunar distance are made by measuring the time taken for laser beam light to travel between stations on Earth and retroreflectors placed on the Moon. The Moon is spiraling away from Earth at an average rate of 3.8 cm (1.5 in) per year, as detected by the Lunar Laser Ranging experiment.\n",
      "\n",
      "\n",
      "\n",
      "What is the Ozma Problem?\n",
      "-The last several chapters deal with a conundrum called the Ozma Problem, which examines whether there is any fundamental asymmetry to the universe. This discussion concerns various aspects of atomic and subatomic physics and how they relate to mirror asymmetry and the related concepts of chirality, antimatter, magnetic and electrical polarity, parity, charge and spin. Time invariance (and reversal) is discussed. Implications for particle physics, theoretical physics and cosmology are covered and brought up to date (in later editions of the book) with regard to Grand Unified Theories, theories of everything, superstring theory and M-theory.\n",
      "-The problem was first implied in Immanuel Kant's discussion of a hand isolated in space, which would have no meaning as left or right by itself; Gardner posits that Kant would today explain his problem using the reversibility of objects through a higher dimension. A three-dimensional hand can be reversed in a mirror or a hypothetical fourth dimension. In more easily visualizable terms, an outline of a hand in Flatland could be flipped over; the meaning of left or right would not apply until a being missing a corresponding hand came along. Charles Howard Hinton expressed the essential problem in 1888, as did William James in his The Principles of Psychology (1890). Gardner follows the thread of several false leads on the road to the solution of the problem, such as the magnetic poles of astronomical bodies and the chirality of life molecules, which could be arbitrary based on how life locally originated.The solution to the Ozma Problem was finally realized in the famous Wu experiment, conducted in 1956 by Chinese-American physicist Chien-Shiung Wu (1912–1997), involving the beta decay of cobalt-60. At a conference earlier that year, Richard Feynman had asked (on behalf of Martin M. Block) whether parity was sometimes violated, leading Tsung-Dao Lee and Chen-Ning Yang to propose Wu's experiment, for which Lee and Yang were awarded the 1957 Nobel Prize in Physics. It was the first experiment to disprove the conservation of parity, and according to Gardner, one could use it to convey the meaning of left and right to remote extraterrestrials. An earlier example of asymmetry had actually been detected as early as 1928 in the decay of a radionuclide of radium, but its significance was not then realized.\n",
      "-The Ozma Problem The 18th chapter, \"The Ozma Problem\", poses a problem that Gardner claims would arise if Earth should ever enter into communication with life on another planet through Project Ozma. This is the problem of how to communicate the meaning of left and right, where the two communicants are conditionally not allowed to view any one object in common.\n",
      "-The book begins with the subject of mirror reflection, and from there passes through symmetry in geometry, poetry, art, music, galaxies, stars, planets and living organisms. It then moves down into the molecular scale and looks at how symmetry and asymmetry have evolved from the beginning of life on Earth. There is a chapter on carbon and its versatility and on chirality in biochemistry.\n",
      "-Particle physics Symmetry is one of the most powerful tools in particle physics, because it has become evident that practically all laws of nature originate in symmetries. Violations of symmetry therefore present theoretical and experimental puzzles that lead to a deeper understanding of nature. Asymmetries in experimental measurements also provide powerful handles that are often relatively free from background or systematic uncertainties.\n",
      "\n",
      "\n",
      "\n",
      "What is a Hilbert space in quantum mechanics?\n",
      "-Quantum mechanics In the mathematically rigorous formulation of quantum mechanics, developed by John von Neumann, the possible states (more precisely, the pure states) of a quantum mechanical system are represented by unit vectors (called state vectors) residing in a complex separable Hilbert space, known as the state space, well defined up to a complex number of norm 1 (the phase factor). In other words, the possible states are points in the projectivization of a Hilbert space, usually called the complex projective space. The exact nature of this Hilbert space is dependent on the system; for example, the position and momentum states for a single non-relativistic spin zero particle is the space of all square-integrable functions, while the states for the spin of a single proton are unit elements of the two-dimensional complex Hilbert space of spinors. Each observable is represented by a self-adjoint linear operator acting on the state space. Each eigenstate of an observable corresponds to an eigenvector of the operator, and the associated eigenvalue corresponds to the value of the observable in that eigenstate.The inner product between two state vectors is a complex number known as a probability amplitude. During an ideal measurement of a quantum mechanical system, the probability that a system collapses from a given initial state to a particular eigenstate is given by the square of the absolute value of the probability amplitudes between the initial and final states. The possible results of a measurement are the eigenvalues of the operator—which explains the choice of self-adjoint operators, for all the eigenvalues must be real. The probability distribution of an observable in a given state can be found by computing the spectral decomposition of the corresponding operator.For a general system, states are typically not pure, but instead are represented as statistical mixtures of pure states, or mixed states, given by density matrices: self-adjoint operators of trace one on a Hilbert space. Moreover, for general quantum mechanical systems, the effects of a single measurement can influence other parts of a system in a manner that is described instead by a positive operator valued measure. Thus the structure both of the states and observables in the general theory is considerably more complicated than the idealization for pure states.\n",
      "-In the mathematically rigorous formulation of quantum mechanics, the state of a quantum mechanical system is a vector  ψ belonging to a (separable) complex Hilbert space  H . This vector is postulated to be normalized under the Hilbert space inner product, that is, it obeys  ⟨ψ,ψ⟩=1 , and it is well-defined up to a complex number of modulus 1 (the global phase), that is,  ψ and  eiαψ represent the same physical system. In other words, the possible states are points in the projective space of a Hilbert space, usually called the complex projective space. The exact nature of this Hilbert space is dependent on the system – for example, for describing position and momentum the Hilbert space is the space of complex square-integrable functions  L2(C) , while the Hilbert space for the spin of a single proton is simply the space of two-dimensional complex vectors  C2 with the usual inner product.\n",
      "-Description of the state of a system Each isolated physical system is associated with a (topologically) separable complex Hilbert space H with inner product ⟨φ|ψ⟩. Rays (that is, subspaces of complex dimension 1) in H are associated with quantum states of the system.\n",
      "-Specifically, in quantum mechanics a state space is a complex Hilbert space in which each unit vector represents a different state that could come out of a measurement. Each unit vector specifies a different dimension, so the numbers of dimensions in this Hilbert space depends on the system we choose to describe. Any state vector in this space can be written as a linear combination of unit vectors. Having an nonzero component along multiple dimensions is called a superposition. These state vectors, using Dirac's bra–ket notation, can often be treated like coordinate vectors and operated on using the rules of linear algebra. This Dirac formalism of quantum mechanics can replace calculation of complicated integrals with simpler vector operations.\n",
      "-In mathematics, Hilbert spaces (named after David Hilbert) allow the methods of linear algebra and calculus to be generalized from (finite-dimensional) Euclidean vector spaces to spaces that may be infinite-dimensional. Hilbert spaces arise naturally and frequently in mathematics and physics, typically as function spaces. Formally, a Hilbert space is a vector space equipped with an inner product that induces a distance function for which the space is a complete metric space.  The earliest Hilbert spaces were studied from this point of view in the first decade of the 20th century by David Hilbert, Erhard Schmidt, and Frigyes Riesz. They are indispensable tools in the theories of partial differential equations, quantum mechanics, Fourier analysis (which includes applications to signal processing and heat transfer), and ergodic theory (which forms the mathematical underpinning of thermodynamics). John von Neumann coined the term Hilbert space for the abstract concept that underlies many of these diverse applications. The success of Hilbert space methods ushered in a very fruitful era for functional analysis. Apart from the classical Euclidean vector spaces, examples of Hilbert spaces include spaces of square-integrable functions, spaces of sequences, Sobolev spaces consisting of generalized functions, and Hardy spaces of holomorphic functions.\n",
      "\n",
      "\n",
      "\n",
      "What is the significance of the speed of light in vacuum?\n",
      "-The speed at which light waves propagate in vacuum is independent both of the motion of the wave source and of the inertial frame of reference of the observer. This invariance of the speed of light was postulated by Einstein in 1905, after being motivated by Maxwell's theory of electromagnetism and the lack of evidence for motion against the luminiferous aether; it has since been consistently confirmed by many experiments. It is only possible to verify experimentally that the two-way speed of light (for example, from a source to a mirror and back again) is frame-independent, because it is impossible to measure the one-way speed of light (for example, from a source to a distant detector) without some convention as to how clocks at the source and at the detector should be synchronized. However, by adopting Einstein synchronization for the clocks, the one-way speed of light becomes equal to the two-way speed of light by definition. The special theory of relativity explores the consequences of this invariance of c with the assumption that the laws of physics are the same in all inertial frames of reference. One consequence is that c is the speed at which all massless particles and waves, including light, must travel in vacuum.\n",
      "-In addition, the speed of light is an invariant quantity: it has the same value, irrespective of the position or speed of the observer. This property makes the speed of light c a natural measurement unit for speed and a fundamental constant of nature.\n",
      "-Casimir vacuum and quantum tunnelling Special relativity postulates that the speed of light in vacuum is invariant in inertial frames. That is, it will be the same from any frame of reference moving at a constant speed. The equations do not specify any particular value for the speed of light, which is an experimentally determined quantity for a fixed unit of length. Since 1983, the SI unit of length (the meter) has been defined using the speed of light.\n",
      "-The speed of light in vacuum is defined to be exactly 299 792 458 m/s (approx. 186,282 miles per second). The fixed value of the speed of light in SI units results from the fact that the metre is now defined in terms of the speed of light. All forms of electromagnetic radiation move at exactly this same speed in vacuum.\n",
      "-The simplest picture of light given by classical physics is of a wave or disturbance in the electromagnetic field. In a vacuum, Maxwell's equations predict that these disturbances will travel at a specific speed, denoted by the symbol c. This well-known physical constant is commonly referred to as the speed of light. The postulate of the constancy of the speed of light in all inertial reference frames lies at the heart of special relativity and has given rise to a popular notion that the \"speed of light is always the same\". However, in many situations light is more than a disturbance in the electromagnetic field.\n",
      "\n",
      "\n",
      "\n",
      "What is the term used to describe the proportionality factor to the Stefan-Boltzmann law that is utilized in subsequent evaluations of the radiative behavior of grey bodies?\n",
      "-Matter that does not absorb all incident radiation emits less total energy than a black body. Emissions are reduced by a factor  ε , where the emissivity,  ε , is a material property which, for most matter, satisfies  0≤ε≤1 . Emissivity can in general depend on wavelength, direction, and polarization. However, the emissivity which appears in the non-directional form of the Stefan–Boltzmann law is the hemispherical total emissivity, which reflects emissions as totaled over all wavelengths, directions, and polarizations.: 60 The form of the Stefan–Boltzmann law that includes emissivity is applicable to all matter, provided that matter is in a state of local thermodynamic equilibrium (LTE) so that its temperature is well-defined.: 66n, 541  (This is a trivial conclusion, since the emissivity,  ε , is defined to be the quantity that makes this equation valid. What is non-trivial is the proposition that  ε≤1 , which is a consequence of Kirchhoff's law of thermal radiation.: 385 ) A so-called grey body is a body for which the spectral emissivity is independent of wavelength, so that the total emissivity,  ε , is a constant.: 71  In the more general (and realistic) case, the spectral emissivity depends on wavelength. The total emissivity, as applicable to the Stefan–Boltzmann law, may be calculated as a weighted average of the spectral emissivity, with the blackbody emission spectrum serving as the weighting function. It follows that if the spectral emissivity depends on wavelength then the total emissivity depends on the temperature, i.e.,  ε=ε(T) .: 60  However, if the dependence on wavelength is small, then the dependence on temperature will be small as well.\n",
      "-The constant of proportionality,  σ , is called the Stefan–Boltzmann constant. It has a value σ= 5.670374419...×10−8 W m−2 K−4 .In the general case, the Stefan–Boltzmann law for radiant exitance takes the form: M=εM∘=εσT4 where  ε is the emissivity of the matter doing the emitting. The emissivity is generally between zero and one, although some exotic materials may have an emissivity greater than one. An emissivity of one corresponds to a black body.\n",
      "-Matter that does not absorb all incident radiation emits less total energy than a black body. Emissions are reduced by a factor  ε , where the emissivity,  ε , is a material property which, for most matter, satisfies  0≤ε≤1 . Emissivity can in general depend on wavelength, direction, and polarization. However, the emissivity which appears in the non-directional form of the Stefan–Boltzmann law is the hemispherical total emissivity, which reflects emissions as totaled over all wavelengths, directions, and polarizations.: 60 The form of the Stefan–Boltzmann law that includes emissivity is applicable to all matter, provided that matter is in a state of local thermodynamic equilibrium (LTE) so that its temperature is well-defined.: 66n, 541  (This is a trivial conclusion, since the emissivity,  ε , is defined to be the quantity that makes this equation valid. What is non-trivial is the proposition that  ε≤1 , which is a consequence of Kirchhoff's law of thermal radiation.: 385 ) A so-called grey body is a body for which the spectral emissivity is independent of wavelength, so that the total emissivity,  ε , is a constant.: 71  In the more general (and realistic) case, the spectral emissivity depends on wavelength. The total emissivity, as applicable to the Stefan–Boltzmann law, may be calculated as a weighted average of the spectral emissivity, with the blackbody emission spectrum serving as the weighting function. It follows that if the spectral emissivity depends on wavelength then the total emissivity depends on the temperature, i.e.,  ε=ε(T) .: 60  However, if the dependence on wavelength is small, then the dependence on temperature will be small as well.\n",
      "-The Stefan–Boltzmann law, also known as Stefan's law, describes the intensity of the thermal radiation emitted by matter in terms of that matter's temperature. It is named for Josef Stefan, who empirically derived the relationship, and Ludwig Boltzmann who derived the law theoretically.\n",
      "For an ideal absorber/emitter or black body, the Stefan–Boltzmann law states that the total energy radiated per unit surface area per unit time (also known as the radiant exitance) is directly proportional to the fourth power of the black body's temperature, T: M∘=σT4.\n",
      "-The Stefan–Boltzmann law, also known as Stefan's law, describes the intensity of the thermal radiation emitted by matter in terms of that matter's temperature. It is named for Josef Stefan, who empirically derived the relationship, and Ludwig Boltzmann who derived the law theoretically.\n",
      "For an ideal absorber/emitter or black body, the Stefan–Boltzmann law states that the total energy radiated per unit surface area per unit time (also known as the radiant exitance) is directly proportional to the fourth power of the black body's temperature, T: M∘=σT4.\n",
      "\n",
      "\n",
      "\n",
      "What is the reason for the formation of stars exclusively within molecular clouds?\n",
      "-Star formation The formation of stars occurs exclusively within molecular clouds. This is a natural consequence of their low temperatures and high densities, because the gravitational force acting to collapse the cloud must exceed the internal pressures that are acting \"outward\" to prevent a collapse. There is observed evidence that the large, star-forming clouds are confined to a large degree by their own gravity (like stars, planets, and galaxies) rather than by external pressure. The evidence comes from the fact that the \"turbulent\" velocities inferred from CO linewidth scale in the same manner as the orbital velocity (a virial relation).\n",
      "-In the dense nebulae where stars are produced, much of the hydrogen is in the molecular (H2) form, so these nebulae are called molecular clouds. The Herschel Space Observatory has revealed that filaments, or elongated dense gas structures, are truly ubiquitous in molecular clouds and central to the star formation process. They fragment into gravitationally bound cores, most of which will evolve into stars. Continuous accretion of gas, geometrical bending, and magnetic fields may control the detailed manner in which the filaments are fragmented. Observations of supercritical filaments have revealed quasi-periodic chains of dense cores with spacing comparable to the filament inner width, and embedded protostars with outflows.Observations indicate that the coldest clouds tend to form low-mass stars, which are first observed via the infrared light they emit inside the clouds, and then as visible light when the clouds dissipate. Giant molecular clouds, which are generally warmer, produce stars of all masses. These giant molecular clouds have typical densities of 100 particles per cm3, diameters of 100 light-years (9.5×1014 km), masses of up to 6 million solar masses (M☉), or six million times the mass of Earth's sun. The average interior temperature is 10 K (−441.7 °F).\n",
      "-The formation of an open cluster begins with the collapse of part of a giant molecular cloud, a cold dense cloud of gas and dust containing up to many thousands of times the mass of the Sun. These clouds have densities that vary from 102 to 106 molecules of neutral hydrogen per cm3, with star formation occurring in regions with densities above 104 molecules per cm3. Typically, only 1–10% of the cloud by volume is above the latter density. Prior to collapse, these clouds maintain their mechanical equilibrium through magnetic fields, turbulence, and rotation.Many factors may disrupt the equilibrium of a giant molecular cloud, triggering a collapse and initiating the burst of star formation that can result in an open cluster. These include shock waves from a nearby supernova, collisions with other clouds, or gravitational interactions. Even without external triggers, regions of the cloud can reach conditions where they become unstable against collapse. The collapsing cloud region will undergo hierarchical fragmentation into ever smaller clumps, including a particularly dense form known as infrared dark clouds, eventually leading to the formation of up to several thousand stars. This star formation begins enshrouded in the collapsing cloud, blocking the protostars from sight but allowing infrared observation. In the Milky Way galaxy, the formation rate of open clusters is estimated to be one every few thousand years.\n",
      "-A molecular cloud, sometimes called a stellar nursery (if star formation is occurring within), is a type of interstellar cloud, the density and size of which permit absorption nebulae, the formation of molecules (most commonly molecular hydrogen, H2), and the formation of H II regions. This is in contrast to other areas of the interstellar medium that contain predominantly ionized gas.\n",
      "-Physics The physics of molecular clouds is poorly understood and much debated. Their internal motions are governed by turbulence in a cold, magnetized gas, for which the turbulent motions are highly supersonic but comparable to the speeds of magnetic disturbances. This state is thought to lose energy rapidly, requiring either an overall collapse or a steady reinjection of energy. At the same time, the clouds are known to be disrupted by some process—most likely the effects of massive stars—before a significant fraction of their mass has become stars.\n",
      "\n",
      "\n",
      "\n",
      "What is the identity operation in symmetry groups?\n",
      "-Identity Operation The identity operation corresponds to doing nothing to the object. Because every molecule is indistinguishable from itself if nothing is done to it, every object possesses at least the identity operation. The identity operation is denoted by E or I. In the identity operation, no change can be observed for the molecule. Even the most asymmetric molecule possesses the identity operation. The need for such an identity operation arises from the mathematical requirements of group theory.\n",
      "-Basic point group symmetry operations The five basic symmetry operations mentioned above are: Identity Operation E (from the German 'Einheit' meaning unity): The identity operation leaves the molecule unchanged. It forms the identity element in the symmetry group. Though its inclusion seems to be trivial, it is important also because even for the most asymmetric molecule, this symmetry is present. The corresponding symmetry element is the entire molecule itself.\n",
      "-In group theory, geometry, representation theory and molecular geometry, a symmetry operation is a geometric transformation of an object that leaves the object looking the same after it has been carried out. For example, as transformations of an object in space, rotations, reflections and inversions are all symmetry operations. Such symmetry operations are performed with respect to symmetry elements (for example, a point, line or plane). In the context of molecular symmetry, a symmetry operation is a permutation of atoms such that the molecule or crystal is transformed into a state indistinguishable from the starting state.\n",
      "-Groups The symmetry operations of a molecule (or other object) form a group. In mathematics, a group is a set with a binary operation that satisfies the four properties listed below.  In a symmetry group, the group elements are the symmetry operations (not the symmetry elements), and the binary combination consists of applying first one symmetry operation and then the other. An example is the sequence of a C4 rotation about the z-axis and a reflection in the xy-plane, denoted σ(xy)C4. By convention the order of operations is from right to left.\n",
      "-Application of symmetry transformations are associative.\n",
      "There is always a trivial transformation, where nothing is done to the original co-ordinates. This is the identity element of the group.\n",
      "\n",
      "\n",
      "\n",
      "What is a regular polytope?\n",
      "-At the start of the 20th century, the definition of a regular polytope was as follows.\n",
      "A regular polygon is a polygon whose edges are all equal and whose angles are all equal.\n",
      "A regular polyhedron is a polyhedron whose faces are all congruent regular polygons, and whose vertex figures are all congruent and regular.\n",
      "-And so on, a regular n-polytope is an n-dimensional polytope whose (n − 1)-dimensional faces are all regular and congruent, and whose vertex figures are all regular and congruent.This is a \"recursive\" definition. It defines regularity of higher dimensional figures in terms of regular figures of a lower dimension. There is an equivalent (non-recursive) definition, which states that a polytope is regular if it has a sufficient degree of symmetry.\n",
      "-Regular polytopes are the generalized analog in any number of dimensions of regular polygons (for example, the square or the regular pentagon) and regular polyhedra (for example, the cube). The strong symmetry of the regular polytopes gives them an aesthetic quality that interests both non-mathematicians and mathematicians.\n",
      "Classically, a regular polytope in n dimensions may be defined as having regular facets ([n–1]-faces) and regular vertex figures. These two conditions are sufficient to ensure that all faces are alike and all vertices are alike. Note, however, that this definition does not work for abstract polytopes.\n",
      "A regular polytope can be represented by a Schläfli symbol of the form {a, b, c, ..., y, z}, with regular facets as {a, b, c, ..., y}, and regular vertex figures as {b, c, ..., y, z}.\n",
      "-In mathematics, a regular polytope is a polytope whose symmetry group acts transitively on its flags, thus giving it the highest degree of symmetry. All its elements or j-faces (for all 0 ≤ j ≤ n, where n is the dimension of the polytope) — cells, faces and so on — are also transitive on the symmetries of the polytope, and are regular polytopes of dimension ≤ n.\n",
      "-A regular polyhedron is a polyhedron whose symmetry group acts transitively on its flags. A regular polyhedron is highly symmetrical, being all of edge-transitive, vertex-transitive and face-transitive. In classical contexts, many different equivalent definitions are used; a common one is that the faces are congruent regular polygons which are assembled in the same way around each vertex.\n",
      "\n",
      "\n",
      "\n",
      "What is the reason behind the largest externally observed electrical effects when two conductors are separated by the smallest distance without touching?\n",
      "-Contact electrification If two conducting surfaces are moved relative to each other, and there is potential difference in the space between them, then an electric current will be driven. This is because the surface charge on a conductor depends on the magnitude of the electric field, which in turn depends on the distance between the surfaces. The externally observed electrical effects are largest when the conductors are separated by the smallest distance without touching (once brought into contact, the charge will instead flow internally through the junction between the conductors). Since two conductors in equilibrium can have a built-in potential difference due to work function differences, this means that bringing dissimilar conductors into contact, or pulling them apart, will drive electric currents. These contact currents can damage sensitive microelectronic circuitry and occur even when the conductors would be grounded in the absence of motion.\n",
      "-The proximity effect can significantly increase the AC resistance of adjacent conductors when compared to its resistance to a DC current. The effect increases with frequency. At higher frequencies, the AC resistance of a conductor can easily exceed ten times its DC resistance.\n",
      "-In electromagnetics, proximity effect is a redistribution of electric current occurring in nearby parallel electrical conductors carrying alternating current (AC), caused by magnetic effects. In adjacent conductors carrying AC current in the same direction, it causes the current in the conductor to concentrate on the side away from the nearby conductor. In conductors carrying AC current in opposite directions, it causes the current in the conductor to concentrate on the side adjacent to the nearby conductor. Proximity effect is caused by eddy currents induced within a conductor by the time-varying magnetic field of the other conductor, by electromagnetic induction. For example, in a coil of wire carrying alternating current with multiple turns of wire lying next to each other, the current in each wire will be concentrated in a strip on each side of the wire facing away from the adjacent wires. This \"current crowding\" effect causes the current to occupy a smaller effective cross-sectional area of the conductor, increasing current density and AC electrical resistance of the conductor. The concentration of current on the side of the conductor gets larger with increasing frequency, so proximity effect causes adjacent wires carrying the same current to have more resistance at higher frequencies.\n",
      "-A changing magnetic field will influence the distribution of an electric current flowing within an electrical conductor, by electromagnetic induction. When an alternating current (AC) flows through a conductor, it creates an associated alternating magnetic field around it. The alternating magnetic field induces eddy currents in adjacent conductors, altering the overall distribution of current flowing through them. The result is that the current is concentrated in the areas of the conductor farthest away from nearby conductors carrying current in the same direction.\n",
      "-Proximity effect In a conductor (ACSR and other types) carrying AC current, if currents are flowing through one or more other nearby conductors the distribution of current within each conductor will be constrained to smaller regions. The resulting current crowding is termed as the proximity effect. This crowding gives an increase in the effective AC resistance of the circuit, with the effect at 60 Hertz being greater than at 50 Hertz. Geometry, conductivity, and frequency are factors in determining the amount of proximity effect.\n",
      "\n",
      "\n",
      "\n",
      "What is the formalism that angular momentum is associated with in rotational invariance?\n",
      "-In modern (20th century) theoretical physics, angular momentum (not including any intrinsic angular momentum – see below) is described using a different formalism, instead of a classical pseudovector. In this formalism, angular momentum is the 2-form Noether charge associated with rotational invariance. As a result, angular momentum is not conserved for general curved spacetimes, unless it happens to be asymptotically rotationally invariant.In classical mechanics, the angular momentum of a particle can be reinterpreted as a plane element: in which the exterior product (∧) replaces the cross product (×) (these products have similar characteristics but are nonequivalent). This has the advantage of a clearer geometric interpretation as a plane element, defined using the vectors x and p, and the expression is true in any number of dimensions. In Cartesian coordinates: or more compactly in index notation: The angular velocity can also be defined as an anti-symmetric second order tensor, with components ωij. The relation between the two anti-symmetric tensors is given by the moment of inertia which must now be a fourth order tensor: Again, this equation in L and ω as tensors is true in any number of dimensions. This equation also appears in the geometric algebra formalism, in which L and ω are bivectors, and the moment of inertia is a mapping between them.\n",
      "-When describing the motion of a charged particle in an electromagnetic field, the canonical momentum P (derived from the Lagrangian for this system) is not gauge invariant. As a consequence, the canonical angular momentum L = r × P is not gauge invariant either. Instead, the momentum that is physical, the so-called kinetic momentum (used throughout this article), is (in SI units) where e is the electric charge of the particle and A the magnetic vector potential of the electromagnetic field. The gauge-invariant angular momentum, that is kinetic angular momentum, is given by The interplay with quantum mechanics is discussed further in the article on canonical commutation relations.\n",
      "-The relationship between the angular momentum operator and the rotation operators is the same as the relationship between Lie algebras and Lie groups in mathematics. The close relationship between angular momentum and rotations is reflected in Noether's theorem that proves that angular momentum is conserved whenever the laws of physics are rotationally invariant.\n",
      "-III. Rotational invariance The conservation of the angular momentum L = r × p is analogous to its linear momentum counterpart. It is assumed that the symmetry of the Lagrangian is rotational, i.e., that the Lagrangian does not depend on the absolute orientation of the physical system in space. For concreteness, assume that the Lagrangian does not change under small rotations of an angle δθ about an axis n; such a rotation transforms the Cartesian coordinates by the equation r→r+δθn×r.\n",
      "-Noether's theorem states that every conservation law is associated with a symmetry (invariant) of the underlying physics. The symmetry associated with conservation of angular momentum is rotational invariance. The fact that the physics of a system is unchanged if it is rotated by any angle about an axis implies that angular momentum is conserved.\n",
      "\n",
      "\n",
      "\n",
      "Which hand should be used to apply the right-hand rule when tightening or loosening nuts, screws, bolts, bottle caps, and jar lids?\n",
      "-To apply the right-hand rule, place one's loosely clenched right hand above the object with the thumb pointing in the direction one wants the screw, nut, bolt, or cap ultimately to move, and the curl of the fingers, from the palm to the tips, will indicate in which way one needs to turn the screw, nut, bolt or cap to achieve the desired result. Almost all threaded objects obey this rule except for a few left-handed exceptions described below.\n",
      "-Shop-work Typical nuts, screws, bolts, bottle caps, and jar lids are tightened (moved away from the observer) clockwise and loosened (moved towards the observer) counterclockwise in accordance with the right-hand rule.\n",
      "-Also, unlike the Circle change, when throwing the screwball the middle finger applies the most pressure to the baseball, while the ring and pinky exert no pressure at all. For left-handed pitchers, as the middle finger presses hard down on the ball, their hand pronates (turns) inwardly in a clockwise manner near the end of the pitching motion, until much of the hand is beneath the ball. Conversely, right-handed pitchers turn their hand counter-clockwise.\n",
      "-The reason for the clockwise standard for most screws and bolts is that supination of the arm, which is used by a right-handed person to tighten a screw clockwise, is generally stronger than pronation used to loosen.  Sometimes the opposite (left-handed, counterclockwise, reverse) sense of threading is used for a special reason. A thread might need to be left-handed to prevent operational stresses from loosening it. For example, some older cars and trucks had right-handed lug nuts on the right wheels and left-handed lug nuts on the left wheels, so that, as the vehicle moved forward, the lug nuts tended to tighten rather than loosen. For bicycle pedals, the one on the left must be reverse-threaded to prevent it unscrewing during use. Similarly, the flyer whorl of a spinning wheel uses a left-hand thread to keep it from loosening. A turnbuckle has right-handed threads on one end and left-handed threads on the other. Some gas fittings are left-handed to prevent disastrous misconnections: oxygen fittings are right-handed, but acetylene, propane, and other flammable gases are unmistakably distinguished by left-handed fittings.\n",
      "-The threads of a screw are helical and therefore screws can be right- or left-handed. To properly fasten or unfasten a screw, one applies the above rules: if a screw is right-handed, pointing one's right thumb in the direction of the hole and turning in the direction of the right hand's curled fingers (i.e. clockwise) will fasten the screw, while pointing away from the hole and turning in the new direction (i.e. counterclockwise) will unfasten the screw.\n",
      "\n",
      "\n",
      "\n",
      "What is the Minkowski diagram used for?\n",
      "-Overview The term Minkowski diagram refers to a specific form of spacetime diagram frequently used in special relativity. A Minkowski diagram is a two-dimensional graphical depiction of a portion of Minkowski space, usually where space has been curtailed to a single dimension. The units of measurement in these diagrams are taken such that the light cone at an event consists of the lines of slope plus or minus one through that event. The horizontal lines correspond to the usual notion of simultaneous events for a stationary observer at the origin.\n",
      "-The most well-known class of spacetime diagrams are known as Minkowski diagrams, developed by Hermann Minkowski in 1908. Minkowski diagrams are two-dimensional graphs that depict events as happening in a universe consisting of one space dimension and one time dimension. Unlike a regular distance-time graph, the distance is displayed on the horizontal axis and time on the vertical axis. Additionally, the time and space units of measurement are chosen in such a way that an object moving at the speed of light is depicted as following a 45° angle to the diagram's axes.\n",
      "-The geometry of Minkowski space can be depicted using Minkowski diagrams, which are useful also in understanding many of the thought experiments in special relativity.\n",
      "Physics in spacetime Transformations of physical quantities between reference frames Above, the Lorentz transformation for the time coordinate and three space coordinates illustrates that they are intertwined. This is true more generally: certain pairs of \"timelike\" and \"spacelike\" quantities naturally combine on equal footing under the same Lorentz transformation.\n",
      "-Minkowski's principal tool is the Minkowski diagram, and he uses it to define concepts and demonstrate properties of Lorentz transformations (e.g. proper time and length contraction) and to provide geometrical interpretation to the generalization of Newtonian mechanics to relativistic mechanics. For these special topics, see the referenced articles, as the presentation below will be principally confined to the mathematical structure (Minkowski metric and from it derived quantities and the Poincaré group as symmetry group of spacetime) following from the invariance of the spacetime interval on the spacetime manifold as consequences of the postulates of special relativity, not to specific application or derivation of the invariance of the spacetime interval. This structure provides the background setting of all present relativistic theories, barring general relativity for which flat Minkowski spacetime still provides a springboard as curved spacetime is locally Lorentzian.\n",
      "-History Albert Einstein discovered special relativity in 1905, with Hermann Minkowski providing his graphical representation in 1908.In Minkowski's 1908 paper there were three diagrams, first to illustrate the Lorentz transformation, then the partition of the plane by the light-cone, and finally illustration of worldlines. The first diagram used a branch of the unit hyperbola  t2−x2=1 to show the locus of a unit of proper time depending on velocity, thus illustrating time dilation. The second diagram showed the conjugate hyperbola to calibrate space, where a similar stretching leaves the impression of FitzGerald contraction. In 1914 Ludwik Silberstein included a diagram of \"Minkowski's representation of the Lorentz transformation\". This diagram included the unit hyperbola, its conjugate, and a pair of conjugate diameters. Since the 1960s a version of this more complete configuration has been referred to as The Minkowski Diagram, and used as a standard illustration of the transformation geometry of special relativity. E. T. Whittaker has pointed out that the principle of relativity is tantamount to the arbitrariness of what hyperbola radius is selected for time in the Minkowski diagram. In 1912 Gilbert N. Lewis and Edwin B. Wilson applied the methods of synthetic geometry to develop the properties of the non-Euclidean plane that has Minkowski diagrams.When Taylor and Wheeler composed Spacetime Physics (1966), they did not use the term Minkowski diagram for their spacetime geometry. Instead they included an acknowledgement of Minkowski's contribution to philosophy by the totality of his innovation of 1908.\n",
      "\n",
      "\n",
      "\n",
      "What are the two main interpretations for the disparity between the presence of matter and antimatter in the observable universe?\n",
      "-There is considerable speculation both in science and science fiction as to why the observable universe is apparently almost entirely matter (in the sense of quarks and leptons but not antiquarks or antileptons), and whether other places are almost entirely antimatter (antiquarks and antileptons) instead. In the early universe, it is thought that matter and antimatter were equally represented, and the disappearance of antimatter requires an asymmetry in physical laws called CP (charge-parity) symmetry violation, which can be obtained from the Standard Model, but at this time the apparent asymmetry of matter and antimatter in the visible universe is one of the great unsolved problems in physics. Possible processes by which it came about are explored in more detail under baryogenesis.\n",
      "-Matter–antimatter asymmetry. The universe is made out of mostly matter. However, the standard model predicts that matter and antimatter should have been created in (almost) equal amounts if the initial conditions of the universe did not involve disproportionate matter relative to antimatter. Yet, there is no mechanism in the Standard Model to sufficiently explain this asymmetry.\n",
      "-There is strong evidence that the observable universe is composed almost entirely of ordinary matter, as opposed to an equal mixture of matter and antimatter. This asymmetry of matter and antimatter in the visible universe is one of the great unsolved problems in physics. The process by which this inequality between matter and antimatter particles developed is called baryogenesis.\n",
      "-Of the four fundamental interactions, gravitation is the dominant at astronomical length scales. Gravity's effects are cumulative; by contrast, the effects of positive and negative charges tend to cancel one another, making electromagnetism relatively insignificant on astronomical length scales. The remaining two interactions, the weak and strong nuclear forces, decline very rapidly with distance; their effects are confined mainly to sub-atomic length scales.: 1470 The universe appears to have much more matter than antimatter, an asymmetry possibly related to the CP violation. This imbalance between matter and antimatter is partially responsible for the existence of all matter existing today, since matter and antimatter, if equally produced at the Big Bang, would have completely annihilated each other and left only photons as a result of their interaction. The universe also appears to have neither net momentum nor angular momentum, which follows accepted physical laws if the universe is finite. These laws are Gauss's law and the non-divergence of the stress–energy–momentum pseudotensor.\n",
      "-Mirror anti-universe The state of the universe, as it is, does not violate the CPT symmetry, because the Big Bang could be considered as a double sided event, both classically and quantum mechanically, consisting of a universe-antiuniverse pair. This means that this universe is the charge (C), parity (P) and time (T) image of the anti-universe. This pair emerged from the Big Bang epochs not directly into a hot, radiation-dominated era. The antiuniverse would flow back in time from the Big Bang, becoming bigger as it does so, and would be also dominated by antimatter. Its spatial properties are inverted if compared to those in our universe, a situation analogous to creating electron–positron pairs in a vacuum. This model, devised by physicists from the Perimeter Institute for Theoretical Physics in Canada, proposes that temperature fluctuations in the cosmic microwave background (CMB) are due to the quantum-mechanical nature of space-time near the Big Bang singularity. This means that a point in the future of our universe and a point in the distant past of the antiuniverse would provide fixed classical points, while all possible quantum-based permutations would exist in between. Quantum uncertainty causes the universe and antiuniverse to not be exact mirror images of each other.This model has not shown if it can reproduce certain observations regarding the inflation scenario, such as explaining the uniformity of the cosmos on large scales. However, it provides a natural and straightforward explanation for dark matter. Such a universe-antiuniverse pair would produce large numbers of superheavy neutrinos, also known as sterile neutrinos. These neutrinos might also be the source of recently observed bursts of high-energy cosmic rays.\n",
      "\n",
      "\n",
      "\n",
      "What is the Ramsauer-Townsend effect?\n",
      "-The Ramsauer–Townsend effect, also sometimes called the Ramsauer effect or the Townsend effect, is a physical phenomenon involving the scattering of low-energy electrons by atoms of a noble gas. The effect can not be explained by classical mechanics, but requires the wave theory of quantum mechanics.\n",
      "-Because noble gas atoms have a relatively high first ionization energy and the electrons do not carry enough energy to cause excited electronic states, ionization and excitation of the atom are unlikely, and the probability of elastic scattering over all angles is approximately equal to the probability of collision.\n",
      "-When an electron moves through a gas, its interactions with the gas atoms cause scattering to occur. These interactions are classified as inelastic if they cause excitation or ionization of the atom to occur and elastic if they do not.\n",
      "-No good explanation for the phenomenon existed until the introduction of quantum mechanics, which explains that the effect results from the wave-like properties of the electron. A simple model of the collision that makes use of wave theory can predict the existence of the Ramsauer–Townsend minimum. Bohr presents one such model that considers the atom as a finite square potential well.\n",
      "-The effect is named for Carl Ramsauer (1879-1955) and John Sealy Townsend (1868-1957), who each independently studied the collisions between atoms and low-energy electrons in the early 1920s.\n",
      "\n",
      "\n",
      "\n",
      "What is Minkowski space?\n",
      "-In mathematical physics, Minkowski space (or Minkowski spacetime) () combines inertial space and time manifolds (x,y) with a non-inertial reference frame of space and time (x',t') into a four-dimensional model relating a position (inertial frame of reference) to the field (physics). A four-vector (x,y,z,t) consists of a coordinate axes such as a Euclidean space plus time. This may be used with the non-inertial frame to illustrate specifics of motion, but should not be confused with the spacetime model generally.  The model helps show how a spacetime interval between any two events is independent of the inertial frame of reference in which they are recorded. Mathematician Hermann Minkowski developed it from the work of Hendrik Lorentz, Henri Poincaré, and others, and said it \"was grown on experimental physical grounds.\"  Minkowski space is closely associated with Einstein's theories of special relativity and general relativity and is the most common mathematical structure by which special relativity is formalized. While the individual components in Euclidean space and time might differ due to length contraction and time dilation, in Minkowski spacetime, all frames of reference will agree on the total interval in spacetime between events. Minkowski space differs from four-dimensional Euclidean space insofar as it treats time differently than the three spatial dimensions.\n",
      "-Minkowski space (or Minkowski spacetime) is a mathematical setting in which special relativity is conveniently formulated. Minkowski space is named for the German mathematician Hermann Minkowski, who around 1907 realized that the theory of special relativity (previously developed by Poincaré and Einstein) could be elegantly described using a four-dimensional spacetime, which combines the dimension of time with the three dimensions of space.\n",
      "-Curvature As a flat spacetime, the three spatial components of Minkowski spacetime always obey the Pythagorean Theorem. Minkowski space is a suitable basis for special relativity, a good description of physical systems over finite distances in systems without significant gravitation. However, in order to take gravity into account, physicists use the theory of general relativity, which is formulated in the mathematics of a non-Euclidean geometry. When this geometry is used as a model of physical space, it is known as curved space.\n",
      "-Space is one of the few fundamental quantities in physics, meaning that it cannot be defined via other quantities because nothing more fundamental is known at the present. On the other hand, it can be related to other fundamental quantities. Thus, similar to other fundamental quantities (like time and mass), space can be explored via measurement and experiment.\n",
      "Today, our three-dimensional space is viewed as embedded in a four-dimensional spacetime, called Minkowski space (see special relativity). The idea behind spacetime is that time is hyperbolic-orthogonal to each of the three spatial dimensions.\n",
      "-It is assumed below that spacetime is endowed with a coordinate system corresponding to an inertial frame. This provides an origin, which is necessary for spacetime to be modeled as a vector space. This addition is not required and more complex treatments analogous to an affine space can remove the extra structure. However this is not the introductory convention and is not covered here.\n",
      "\n",
      "\n",
      "\n",
      "What is the Optical Signal-to-Noise Ratio (OSNR)?\n",
      "-Optical signals have a carrier frequency (about 200 THz and more) that is much higher than the modulation frequency. This way the noise covers a bandwidth that is much wider than the signal itself. The resulting signal influence relies mainly on the filtering of the noise. To describe the signal quality without taking the receiver into account, the optical SNR (OSNR) is used. The OSNR is the ratio between the signal power and the noise power in a given bandwidth. Most commonly a reference bandwidth of 0.1 nm is used. This bandwidth is independent of the modulation format, the frequency and the receiver. For instance an OSNR of 20 dB/0.1 nm could be given, even the signal of 40 GBit DPSK would not fit in this bandwidth. OSNR is measured with an optical spectrum analyzer.\n",
      "-Signal-to-noise ratio is defined as the ratio of the power of a signal (meaningful input) to the power of background noise (meaningless or unwanted input): SNR=PsignalPnoise, where P is average power. Both signal and noise power must be measured at the same or equivalent points in a system, and within the same system bandwidth.\n",
      "-Industry standards define sensitivity in terms of the ISO film speed equivalent, using SNR thresholds (at average scene luminance) of 40:1 for \"excellent\" image quality and 10:1 for \"acceptable\" image quality.SNR is sometimes quantified in decibels (dB) of signal power relative to noise power, though in the imaging field the concept of \"power\" is sometimes taken to be the power of a voltage signal proportional to optical power; so a 20 dB SNR may mean either 10:1 or 100:1 optical power, depending on which definition is in use.\n",
      "-Signal-to-noise ratio (SNR or S/N) is a measure used in science and engineering that compares the level of a desired signal to the level of background noise. SNR is defined as the ratio of signal power to noise power, often expressed in decibels. A ratio higher than 1:1 (greater than 0 dB) indicates more signal than noise.\n",
      "-SNR is usually taken to indicate an average signal-to-noise ratio, as it is possible that instantaneous signal-to-noise ratios will be considerably different. The concept can be understood as normalizing the noise level to 1 (0 dB) and measuring how far the signal 'stands out'.\n",
      "\n",
      "\n",
      "\n",
      "What is the interpretation of supersymmetry in stochastic supersymmetric theory?\n",
      "-Spontaneous breakdown of a topological supersymmetry Kinematic dynamo can be also viewed as the phenomenon of the spontaneous breakdown of the topological supersymmetry of the associated stochastic differential equation related to the flow of the background matter. Within stochastic supersymmetric theory, this supersymmetry is an intrinsic property of all stochastic differential equations, its interpretation is that the model’s phase space preserves continuity via continuous time flows. When the continuity of that flow spontaneously breaks down, the system is in the stochastic state of deterministic chaos. In other words, kinematic dynamo arises because of chaotic flow in the underlying background matter.\n",
      "-In supersymmetric theory of SDEs, stochastic dynamics is defined via stochastic evolution operator acting on the differential forms on the phase space of the model. In this exact formulation of stochastic dynamics, all SDEs possess topological supersymmetry which represents the preservation of the continuity of the phase space by continuous time flow. The spontaneous breakdown of this supersymmetry is the mathematical essence of the ubiquitous dynamical phenomenon known across disciplines as chaos, turbulence, self-organized criticality etc. and the Goldstone theorem explains the associated long-range dynamical behavior, i.e., the butterfly effect, 1/f and crackling noises, and scale-free statistics of earthquakes, neuroavalanches, solar flares etc.\n",
      "-The main idea of the theory is to study, instead of trajectories, the SDE-defined temporal evolution of differential forms. This evolution has an intrinsic BRST or topological supersymmetry representing the preservation of topology and/or the concept of proximity in the phase space by continuous time dynamics. The theory identifies a model as chaotic, in the generalized, stochastic sense, if its ground state is not supersymmetric, i.e., if the supersymmetry is broken spontaneously. Accordingly, the emergent long-range behavior that always accompanies dynamical chaos and its derivatives such as turbulence and self-organized criticality can be understood as a consequence of the Goldstone theorem.\n",
      "-Supersymmetry in dynamical systems All stochastic (partial) differential equations, the models for all types of continuous time dynamical systems, possess topological supersymmetry. In the operator representation of stochastic evolution, the topological supersymmetry is the exterior derivative which is commutative with the stochastic evolution operator defined as the stochastically averaged pullback induced on differential forms by SDE-defined diffeomorphisms of the phase space. The topological sector of the so-emerging supersymmetric theory of stochastic dynamics can be recognized as the Witten-type topological field theory.\n",
      "-Classification of stochastic dynamics STS provides classification for stochastic models depending on whether TS is broken and integrability of flow vector field. In can be exemplified as a part of the general phase diagram at the border of chaos (see figure on the right). The phase diagram has the following properties:  For physical models, TS gets restored eventually with the increase of noise intensity.\n",
      "\n",
      "\n",
      "\n",
      "What is the purpose of expressing a map's scale as a ratio, such as 1:10,000?\n",
      "-The first way is the ratio of the size of the generating globe to the size of the Earth. The generating globe is a conceptual model to which the Earth is shrunk and from which the map is projected. The ratio of the Earth's size to the generating globe's size is called the nominal scale (= principal scale = representative fraction). Many maps state the nominal scale and may even display a bar scale (sometimes merely called a 'scale') to represent it.\n",
      "-Actual printed maps are produced from the projection map by a constant scaling denoted by a ratio such as 1:100M (for whole world maps) or 1:10000 (for such as town plans). To avoid confusion in the use of the word 'scale' this constant scale fraction is called the representative fraction (RF) of the printed map and it is to be identified with the ratio printed on the map. The actual printed map coordinates for the equirectangular cylindrical projection are printed map:  x=(RF)aλ y=(RF)aφ This convention allows a clear distinction of the intrinsic projection scaling and the reduction scaling.\n",
      "-A lexical scale in a language known to the user may be easier to visualise than a ratio: if the scale is an inch to two miles and the map user can see two villages that are about two inches apart on the map, then it is easy to work out that the villages are about four miles apart on the ground.\n",
      "-In the English language, the word large-scale is often used to mean \"extensive\". However, as explained above, cartographers use the term \"large scale\" to refer to less extensive maps – those that show a smaller area. Maps that show an extensive area are \"small scale\" maps. This can be a cause of confusion.\n",
      "Scale variation Mapping large areas causes noticeable distortions because it significantly flattens the curved surface of the earth. How distortion gets distributed depends on the map projection. Scale varies across the map, and the stated map scale is only an approximation. This is discussed in detail below.\n",
      "-Map scales require careful discussion. A town plan may be constructed as an exact scale drawing, but for larger areas a map projection is necessary and no projection can represent the Earth's surface at a uniform scale. In general the scale of a projection depends on position and direction. The variation of scale may be considerable in small scale maps which may cover the globe. In large scale maps of small areas the variation of scale may be insignificant for most purposes but it is always present. The scale of a map projection must be interpreted as a nominal scale. (The usage large and small in relation to map scales relates to their expressions as fractions. The fraction 1/10,000 used for a local map is much larger than the1/100,000,000 used for a global map. There is no fixed dividing line between small and large scales.) A scale model is a representation or copy of an object that is larger or smaller than the actual size of the object being represented. Very often the scale model is smaller than the original and used as a guide to making the object in full size.\n",
      "\n",
      "\n",
      "\n",
      "What is the main sequence in astronomy?\n",
      "-main sequence A category of stars which form a continuous and distinctive band on plots of stellar temperature versus luminosity, in particular the Hertzsprung–Russell diagram. These stars are characterized by being in hydrostatic equilibrium and undergoing nuclear fusion of hydrogen-1 in their core region. The Sun is a main-sequence star.\n",
      "major axis See semi-major axis.\n",
      "March equinox Also the Northward equinox.\n",
      "-In astronomy, the main sequence is a continuous and distinctive band of stars that appears on plots of stellar color versus brightness. These color-magnitude plots are known as Hertzsprung–Russell diagrams after their co-developers, Ejnar Hertzsprung and Henry Norris Russell. Stars on this band are known as main-sequence stars or dwarf stars. These are the most numerous true stars in the universe and include the Sun.\n",
      "-The more massive a star is, the shorter its lifespan on the main sequence. After the hydrogen fuel at the core has been consumed, the star evolves away from the main sequence on the HR diagram, into a supergiant, red giant, or directly to a white dwarf.\n",
      "-An F-type main-sequence star (F V) is a main-sequence, hydrogen-fusing star of spectral type F and luminosity class V. These stars have from 1.0 to 1.4 times the mass of the Sun and surface temperatures between 6,000 and 7,600 K.Tables VII and VIII. This temperature range gives the F-type stars a whitish hue when observed by the atmosphere. Because a main-sequence star is referred to as a dwarf star, this class of star may also be termed a yellow-white dwarf (not to be confused with white dwarfs, remnant stars that are a possible final stage of stellar evolution). Notable examples include Procyon A, Gamma Virginis A and B, and KIC 8462852.\n",
      "-For main sequence stars—those stars that are generating energy through the thermonuclear fusion of hydrogen at the core, the presence and location of radiative regions depends on the star's mass. Main sequence stars below about 0.3 solar masses are entirely convective, meaning they do not have a radiative zone. From 0.3 to 1.2 solar masses, the region around the stellar core is a radiation zone, separated from the overlying convection zone by the tachocline. The radius of the radiative zone increases monotonically with mass, with stars around 1.2 solar masses being almost entirely radiative. Above 1.2 solar masses, the core region becomes a convection zone and the overlying region is a radiation zone, with the amount of mass within the convective zone increasing with the mass of the star.\n",
      "\n",
      "\n",
      "\n",
      "Who proposed the concept of \"maximal acceleration\"?\n",
      "-Concerning the historical development, relativistic equations containing accelerations can already be found in the early years of relativity, as summarized in early textbooks by Max von Laue (1911, 1921) or Wolfgang Pauli (1921). For instance, equations of motion and acceleration transformations were developed in the papers of Hendrik Antoon Lorentz (1899, 1904), Henri Poincaré (1905), Albert Einstein (1905), Max Planck (1906), and four-acceleration, proper acceleration, hyperbolic motion, accelerating reference frames, Born rigidity, have been analyzed by Einstein (1907), Hermann Minkowski (1907, 1908), Max Born (1909), Gustav Herglotz (1909), Arnold Sommerfeld (1910), von Laue (1911), Friedrich Kottler (1912, 1914), see section on history.\n",
      "-When on January 11, 1911, the Kaiser Wilhelm Society, the preceding organisation of the Max Planck Society, was founded in Berlin, Prandtl acted again. Only one month later he appealed to the new organisation in a memorandum. In it he asked for an institute for fundamental research in fluid dynamics to be built in Göttingen. As reasons for his choice of location he listed his own previous research and the high esteem of mathematics at the University of Göttingen.\n",
      "-The Max Planck Institute for the History of Science (German: Max-Planck-Institut für Wissenschaftsgeschichte) is a scientific research institute founded in March 1994. It is dedicated to addressing fundamental questions of the history of knowledge from the Neolithic era to the present day, and its researchers pursue a historical epistemology in their study of how new categories of thought, proof, and experience have emerged in interactions between the sciences and their ambient cultures.\n",
      "-An article by Helge Kragh published in Physics World gives an account of this history.\n",
      "-The postulate was introduced by Max Planck in his derivation of his law of black body radiation in 1900. This assumption allowed Planck to derive a formula for the entire spectrum of the radiation emitted by a black body. Planck was unable to justify this assumption based on classical physics; he considered quantization as being purely a mathematical trick, rather than (as is now known) a fundamental change in the understanding of the world. In other words, Planck then contemplated virtual oscillators.\n",
      "\n",
      "\n",
      "\n",
      "What is indirect photophoresis?\n",
      "-Direct photophoresis is caused by the transfer of photon momentum to a particle by refraction and reflection. Movement of particles in the forward direction occurs when the particle is transparent and has an index of refraction larger compared to its surrounding medium. Indirect photophoresis occurs as a result of an increase in the kinetic energy of molecules when particles absorb incident light only on the irradiated side, thus creating a temperature gradient within the particle. In this situation the surrounding gas layer reaches temperature equilibrium with the surface of the particle. Molecules with higher kinetic energy in the region of higher gas temperature impinge on the particle with greater momenta than molecules in the cold region; this causes a migration of particles in a direction opposite to the surface temperature gradient. The component of the photophoretic force responsible for this phenomenon is called the radiometric force. This comes as a result of uneven distribution of radiant energy (source function within a particle).\n",
      "-Photophoresis denotes the phenomenon that small particles suspended in gas (aerosols) or liquids (hydrocolloids) start to migrate when illuminated by a sufficiently intense beam of light. The existence of this phenomenon is owed to a non-uniform distribution of temperature of an illuminated particle in a fluid medium. Separately from photophoresis, in a fluid mixture of different kinds of particles, the migration of some kinds of particles may be due to differences in their absorptions of thermal radiation and other thermal effects collectively known as thermophoresis. In laser photophoresis, particles migrate once they have a refractive index different from their surrounding medium. The migration of particles is usually possible when the laser is slightly or not focused. A particle with a higher refractive index compared to its surrounding molecule moves away from the light source due to momentum transfer from absorbed and scattered light photons. This is referred to as a radiation pressure force. This force depends on light intensity and particle size but has nothing to do with the surrounding medium. Just like in Crookes radiometer, light can heat up one side and gas molecules bounce from that surface with greater velocity, hence push the particle to the other side. Under certain conditions, with particles of diameter comparable to the wavelength of light, the phenomenon of a negative indirect photophoresis occurs, due to the unequal heat generation on the laser irradiation between the back and front sides of particles, this produces a temperature gradient in the medium around the particle such that molecules at the far side of the particle from the light source may get to heat up more, causing the particle to move towards the light source.If the suspended particle is rotating, it will also experience the Yarkovsky effect.\n",
      "-Indirect photophoretic force depends on the physical properties of the particle and the surrounding medium.\n",
      "-In many photo-productive systems this charge separation is kinetically isolated by delivery of the electron to a lower energy conductor attached to the p/n junction or into an electron transport chain. In this case some of the energy can be captured to do work. If the electron is not kinetically isolated thermodynamics will take over and the products will react with each other to regenerate the ground state starting material. This process is called recombination and the photon's energy is released as heat.\n",
      "-Uplighting is less common, often used to bounce indirect light off the ceiling and back down. It is commonly used in lighting applications that require minimal glare and uniform general illuminance levels. Uplighting (indirect) uses a diffuse surface to reflect light in a space and can minimize disabling glare on computer displays and other dark glossy surfaces. It gives a more uniform presentation of the light output in operation. However indirect lighting is completely reliant upon the reflectance value of the surface. While indirect lighting can create a diffused and shadow free light effect it can be regarded as an uneconomical lighting principle.\n",
      "\n",
      "\n",
      "\n",
      "What does Earnshaw's theorem state?\n",
      "-Earnshaw's theorem states that a collection of point charges cannot be maintained in a stable stationary equilibrium configuration solely by the electrostatic interaction of the charges. This was first proven by British mathematician Samuel Earnshaw in 1842.\n",
      "It is usually cited in reference to magnetic fields, but was first applied to electrostatic fields.\n",
      "Earnshaw's theorem applies to classical inverse-square law forces (electric and gravitational) and also to the magnetic forces of permanent magnets, if the magnets are hard (the magnets do not vary in strength with external fields). Earnshaw's theorem forbids magnetic levitation in many common situations.\n",
      "If the materials are not hard, Braunbeck's extension shows that materials with relative magnetic permeability greater than one (paramagnetism) are further destabilising, but materials with a permeability less than one (diamagnetic materials) permit stable configurations.\n",
      "-It is also possible to prove this theorem directly from the force/energy equations for static magnetic dipoles (below). Intuitively, though, it is plausible that if the theorem holds for a single point charge then it would also hold for two opposite point charges connected together. In particular, it would hold in the limit where the distance between the charges is decreased to zero while maintaining the dipole moment – that is, it would hold for an electric dipole. But if the theorem holds for an electric dipole, then it will also hold for a magnetic dipole, since the (static) force/energy equations take the same form for both electric and magnetic dipoles.\n",
      "-Informally, the case of a point charge in an arbitrary static electric field is a simple consequence of Gauss's law. For a particle to be in a stable equilibrium, small perturbations (\"pushes\") on the particle in any direction should not break the equilibrium; the particle should \"fall back\" to its previous position. This means that the force field lines around the particle's equilibrium position should all point inward, toward that position. If all of the surrounding field lines point toward the equilibrium point, then the divergence of the field at that point must be negative (i.e. that point acts as a sink). However, Gauss's law says that the divergence of any possible electric force field is zero in free space. In mathematical notation, an electrical force F(r) deriving from a potential U(r) will always be divergenceless (satisfy Laplace's equation): Therefore, there are no local minima or maxima of the field potential in free space, only saddle points. A stable equilibrium of the particle cannot exist and there must be an instability in some direction. This argument may not be sufficient if all the second derivatives of U are null.To be completely rigorous, strictly speaking, the existence of a stable point does not require that all neighbouring force vectors point exactly toward the stable point; the force vectors could spiral in toward the stable point, for example. One method for dealing with this invokes the fact that, in addition to the divergence, the curl of any electric field in free space is also zero (in the absence of any magnetic currents).\n",
      "-Detailed proofs Earnshaw's theorem was originally formulated for electrostatics (point charges) to show that there is no stable configuration of a collection of point charges. The proofs presented here for individual dipoles should be generalizable to collections of magnetic dipoles because they are formulated in terms of energy, which is additive. A rigorous treatment of this topic is, however, currently beyond the scope of this article.\n",
      "-The idea of particle instability in an electrostatic field originated with Samuel Earnshaw in 1839 and was formalized by James Clerk Maxwell in 1874 who gave it the title \"Earnshaw's theorem\" and proved it with the Laplace equation. Earnshaw's theorem explains why a system of electrons is not stable and was invoked by Niels Bohr in his atom model of 1913 when criticizing J. J. Thomson's atom.  Earnshaw's theorem holds that a charged particle suspended in an electrostatic field is unstable, because the forces of attraction and repulsion vary at an equal rate that is proportional to the inverse square law and remain in balance wherever a particle moves. Since the forces remain in balance, there is no inequality to provide a restoring force; and the particle remains unstable and can freely move without restriction.\n",
      "\n",
      "\n",
      "\n",
      "What is radiosity in radiometry?\n",
      "-In radiometry, radiosity is the radiant flux leaving (emitted, reflected and transmitted by) a surface per unit area, and spectral radiosity is the radiosity of a surface per unit frequency or wavelength, depending on whether the spectrum is taken as a function of frequency or of wavelength. The SI unit of radiosity is the watt per square metre (W/m2), while that of spectral radiosity in frequency is the watt per square metre per hertz (W·m−2·Hz−1) and that of spectral radiosity in wavelength is the watt per square metre per metre (W·m−3)—commonly the watt per square metre per nanometre (W·m−2·nm−1). The CGS unit erg per square centimeter per second (erg·cm−2·s−1) is often used in astronomy. Radiosity is often called intensity in branches of physics other than radiometry, but in radiometry this usage leads to confusion with radiant intensity.\n",
      "-Radiosity Radiosity of a surface, denoted Je (\"e\" for \"energetic\", to avoid confusion with photometric quantities), is defined as Je=∂Φe∂A=Je,em+Je,r+Je,tr, where ∂ is the partial derivative symbol Φe is the radiant flux leaving (emitted, reflected and transmitted) A is the area Je,em=Me is the emitted component of the radiosity of the surface, that is to say its exitance Je,r is the reflected component of the radiosity of the surface Je,tr is the transmitted component of the radiosity of the surfaceFor an opaque surface, the transmitted component of radiosity Je,tr vanishes and only two components remain: Je=Me+Je,r.\n",
      "-In heat transfer, combining these two factors into one radiosity term helps in determining the net energy exchange between multiple surfaces.\n",
      "Spectral radiosity Spectral radiosity in frequency of a surface, denoted Je,ν, is defined as Je,ν=∂Je∂ν, where ν is the frequency.\n",
      "Spectral radiosity in wavelength of a surface, denoted Je,λ, is defined as Je,λ=∂Je∂λ, where λ is the wavelength.\n",
      "-The fundamental quantity that describes a field of radiation is called spectral radiance in radiometric terms (in other fields it is often called specific intensity). For a very small area element in the radiation field, there can be electromagnetic radiation passing in both senses in every spatial direction through it. In radiometric terms, the passage can be completely characterized by the amount of energy radiated in each of the two senses in each spatial direction, per unit time, per unit area of surface of sourcing passage, per unit solid angle of reception at a distance, per unit wavelength interval being considered (polarization will be ignored for the moment).\n",
      "-This is essentially the same distribution that a path-tracing program would sample in tracing back one diffuse reflection step; or that a bidirectional ray-tracing program would sample to achieve one forward diffuse reflection step when light source mapping forwards. The sampling approach therefore to some extent represents a convergence between the two techniques, the key difference remaining that the radiosity technique aims to build up a sufficiently accurate map of the radiance of all the surfaces in the scene, rather than just a representation of the current view.\n",
      "\n",
      "\n",
      "\n",
      "What is a virtual particle?\n",
      "-The term is somewhat loose and vaguely defined, in that it refers to the view that the world is made up of \"real particles\". \"Real particles\" are better understood to be excitations of the underlying quantum fields. Virtual particles are also excitations of the underlying fields, but are \"temporary\" in the sense that they appear in calculations of interactions, but never as asymptotic states or indices to the scattering matrix. The accuracy and use of virtual particles in calculations is firmly established, but as they cannot be detected in experiments, deciding how to precisely describe them is a topic of debate. Although widely used, they are by no means a necessary feature of QFT, but rather are mathematical conveniences - as demonstrated by lattice field theory, which avoids using the concept altogether.\n",
      "-There are two principal ways in which the notion of virtual particles appears in modern physics. They appear as intermediate terms in Feynman diagrams; that is, as terms in a perturbative calculation. They also appear as an infinite set of states to be summed or integrated over in the calculation of a semi-non-perturbative effect. In the latter case, it is sometimes said that virtual particles contribute to a mechanism that mediates the effect, or that the effect occurs through the virtual particles.: 118 \n",
      "-A virtual particle is a theoretical transient particle that exhibits some of the characteristics of an ordinary particle, while having its existence limited by the uncertainty principle. The concept of virtual particles arises in the perturbation theory of quantum field theory where interactions between ordinary particles are described in terms of exchanges of virtual particles. A process involving virtual particles can be described by a schematic representation known as a Feynman diagram, in which virtual particles are represented by internal lines.Virtual particles do not necessarily carry the same mass as the corresponding real particle, although they always conserve energy and momentum. The closer its characteristics come to those of ordinary particles, the longer the virtual particle exists. They are important in the physics of many processes, including particle scattering and Casimir forces. In quantum field theory, forces—such as the electromagnetic repulsion or attraction between two charges—can be thought of as due to the exchange of virtual photons between the charges. Virtual photons are the exchange particle for the electromagnetic interaction.\n",
      "-There are many observable physical phenomena that arise in interactions involving virtual particles. For bosonic particles that exhibit rest mass when they are free and actual, virtual interactions are characterized by the relatively short range of the force interaction produced by particle exchange. Confinement can lead to a short range, too. Examples of such short-range interactions are the strong and weak forces, and their associated field bosons.\n",
      "-Quantum tunnelling may be considered a manifestation of virtual particle exchanges.: 235  The range of forces carried by virtual particles is limited by the uncertainty principle, which regards energy and time as conjugate variables; thus, virtual particles of larger mass have more limited range.Written in the usual mathematical notations, in the equations of physics, there is no mark of the distinction between virtual and actual particles. The amplitudes of processes with a virtual particle interfere with the amplitudes of processes without it, whereas for an actual particle the cases of existence and non-existence cease to be coherent with each other and do not interfere any more. In the quantum field theory view, actual particles are viewed as being detectable excitations of underlying quantum fields. Virtual particles are also viewed as excitations of the underlying fields, but appear only as forces, not as detectable particles. They are \"temporary\" in the sense that they appear in some calculations, but are not detected as single particles. Thus, in mathematical terms, they never appear as indices to the scattering matrix, which is to say, they never appear as the observable inputs and outputs of the physical process being modelled.\n",
      "\n",
      "\n",
      "\n",
      "Who proposed the principle of \"complexity from noise\" and when was it first introduced?\n",
      "-According to Danish noise and music theorist Torben Sangild, one single definition of noise in music is not possible. Sangild instead provides three basic definitions of noise: a musical acoustics definition, a second communicative definition based on distortion or disturbance of a communicative signal, and a third definition based in subjectivity (what is noise to one person can be meaningful to another; what was considered unpleasant sound yesterday is not today).According to Murray Schafer there are four types of noise: unwanted noise, unmusical sound, any loud sound, and a disturbance in any signaling system (such as static on a telephone). Definitions regarding what is considered noise, relative to music, have changed over time. Ben Watson, in his article Noise as Permanent Revolution, points out that Ludwig van Beethoven's Grosse Fuge (1825) \"sounded like noise\" to his audience at the time. Indeed, Beethoven's publishers persuaded him to remove it from its original setting as the last movement of a string quartet. He did so, replacing it with a sparkling Allegro. They subsequently published it separately.In attempting to define noise music and its value, Paul Hegarty (2007) cites the work of noted cultural critics Jean Baudrillard, Georges Bataille and Theodor Adorno and through their work traces the history of \"noise\". He defines noise at different times as \"intrusive, unwanted\", \"lacking skill, not being appropriate\" and \"a threatening emptiness\". He traces these trends starting with 18th-century concert hall music. Hegarty contends that it is John Cage's composition 4'33\", in which an audience sits through four and a half minutes of \"silence\" (Cage 1973), that represents the beginning of noise music proper. For Hegarty, \"noise music\", as with 4'33\", is that music made up of incidental sounds that represent perfectly the tension between \"desirable\" sound (properly played musical notes) and undesirable \"noise\" that make up all noise music from Erik Satie to NON to Glenn Branca. Writing about Japanese noise music, Hegarty suggests that \"it is not a genre, but it is also a genre that is multiple, and characterized by this very multiplicity ... Japanese noise music can come in all styles, referring to all other genres ... but crucially asks the question of genre—what does it mean to be categorized, categorizable, definable?\" (Hegarty 2007:133).\n",
      "-This type of noise was discovered and first measured by John B. Johnson at Bell Labs in 1926. He described his findings to Harry Nyquist, also at Bell Labs, who was able to explain the results.\n",
      "-The cybernetician William Ross Ashby formulated the original principle of self-organization in 1947. It states that any deterministic dynamic system automatically evolves towards a state of equilibrium that can be described in terms of an attractor in a basin of surrounding states. Once there, the further evolution of the system is constrained to remain in the attractor. This constraint implies a form of mutual dependency or coordination between its constituent components or subsystems. In Ashby's terms, each subsystem has adapted to the environment formed by all other subsystems.The cybernetician Heinz von Foerster formulated the principle of \"order from noise\" in 1960. It notes that self-organization is facilitated by random perturbations (\"noise\") that let the system explore a variety of states in its state space. This increases the chance that the system will arrive into the basin of a \"strong\" or \"deep\" attractor, from which it then quickly enters the attractor itself. The biophysicist Henri Atlan developed this concept by proposing the principle of \"complexity from noise\" (French: le principe de complexité par le bruit) first in the 1972 book L'organisation biologique et la théorie de l'information and then in the 1979 book Entre le cristal et la fumée. The physicist and chemist Ilya Prigogine formulated a similar principle as \"order through fluctuations\" or \"order out of chaos\". It is applied in the method of simulated annealing for problem solving and machine learning.\n",
      "-Complexity and chaos theory Complex systems theory is rooted in chaos theory, which in turn has its origins more than a century ago in the work of the French mathematician Henri Poincaré. Chaos is sometimes viewed as extremely complicated information, rather than as an absence of order. Chaotic systems remain deterministic, though their long-term behavior can be difficult to predict with any accuracy. With perfect knowledge of the initial conditions and the relevant equations describing the chaotic system's behavior, one can theoretically make perfectly accurate predictions of the system, though in practice this is impossible to do with arbitrary accuracy. Ilya Prigogine argued that complexity is non-deterministic and gives no way whatsoever to precisely predict the future.The emergence of complex systems theory shows a domain between deterministic order and randomness which is complex. This is referred to as the \"edge of chaos\".\n",
      "-The beginning of systematic studies in computational complexity is attributed to the seminal 1965 paper \"On the Computational Complexity of Algorithms\" by Juris Hartmanis and Richard E. Stearns, which laid out the definitions of time complexity and space complexity, and proved the hierarchy theorems. In addition, in 1965 Edmonds suggested to consider a \"good\" algorithm to be one with running time bounded by a polynomial of the input size.Earlier papers studying problems solvable by Turing machines with specific bounded resources include John Myhill's definition of linear bounded automata (Myhill 1960), Raymond Smullyan's study of rudimentary sets (1961), as well as Hisao Yamada's paper on real-time computations (1962). Somewhat earlier, Boris Trakhtenbrot (1956), a pioneer in the field from the USSR, studied another specific complexity measure. As he remembers: However, [my] initial interest [in automata theory] was increasingly set aside in favor of computational complexity, an exciting fusion of combinatorial methods, inherited from switching theory, with the conceptual arsenal of the theory of algorithms. These ideas had occurred to me earlier in 1955 when I coined the term \"signalizing function\", which is nowadays commonly known as \"complexity measure\".\n",
      "\n",
      "\n",
      "\n",
      "What is the order parameter that breaks the electromagnetic gauge symmetry in superconductors?\n",
      "-In superconductors, there is a condensed-matter collective field ψ, which acts as the order parameter breaking the electromagnetic gauge symmetry.\n",
      "-Symmetry breaking is when some variable that previously didn't affect the measured results (it was originally a \"symmetry\") now does affect the measured results (it's now \"broken\" and no longer a symmetry). In 1962 physicist Philip Anderson, an expert in condensed matter physics, observed that symmetry breaking played a role in superconductivity, and suggested it could also be part of the answer to the problem of gauge invariance in particle physics.\n",
      "-Higgs mechanism The strong, weak, and electromagnetic forces can all be understood as arising from gauge symmetries, which is a redundancy in the description of the symmetry. The Higgs mechanism, the spontaneous symmetry breaking of gauge symmetries, is an important component in understanding the superconductivity of metals and the origin of particle masses in the standard model of particle physics.  The term \"spontaneous symmetry breaking\" is a misnomer here as Elitzur's theorem states that local gauge symmetries can never be spontaneously broken. Rather, after gauge fixing, the global symmetry (or redundancy) can be broken in a manner formally resembling spontaneous symmetry breaking.  One important consequence of the distinction between true symmetries and gauge symmetries, is that the massless Nambu–Goldstone resulting from spontaneous breaking of a gauge symmetry are absorbed in the description of the gauge vector field, providing massive vector field modes, like the plasma mode in a superconductor, or the Higgs mode observed in particle physics.\n",
      "-In two dimensions, the subject of superconductivity becomes very interesting because the existence of true long-range order is not possible. In the 1970's, J. Michael Kosterlitz and David J. Thouless (along with Vadim Berezinski) showed that a different kind of long-range order could exist - topological order - which showed power law correlations (meaning that by measuring the two-point correlation function  ⟨Ψ(0)Ψ(r)⟩∝r−γ it decays algebraically).\n",
      "-Symmetry breaking of the electroweak interaction Below an extremely high temperature, electroweak symmetry breaking causes the electroweak interaction to manifest in part as the short-ranged weak force, which is carried by massive gauge bosons. In the history of the universe, electroweak symmetry breaking is believed to have happened at about 1 picosecond (10−12 s) after the Big Bang, when the universe was at a temperature 159.5±1.5 GeV/kB. This symmetry breaking is required for atoms and other structures to form, as well as for nuclear reactions in stars, such as the Sun. The Higgs field is responsible for this symmetry breaking.\n",
      "\n",
      "\n",
      "\n",
      "What is the reason for the sun appearing slightly yellowish when viewed from Earth?\n",
      "-Sunlight and neutrinos The Sun emits light across the visible spectrum, so its color is white, with a CIE color-space index near (0.3, 0.3), when viewed from space or when the Sun is high in the sky. The Solar radiance per wavelength peaks in the green portion of the spectrum when viewed from space. When the Sun is very low in the sky, atmospheric scattering renders the Sun yellow, red, orange, or magenta, and in rare occasions even green or blue. Despite its typical whiteness (white sunrays, white ambient light, white illumination of the Moon, etc.), some cultures mentally picture the Sun as yellow and some even red; the reasons for this are cultural and exact ones are the subject of debate.\n",
      "-A portion of the beam of light coming from the sun scatters off molecules of gas and other small particles in the atmosphere. Here, Rayleigh scattering primarily occurs through sunlight's interaction with randomly located air molecules. It is this scattered light that gives the surrounding sky its brightness and its color. As previously stated, Rayleigh scattering is inversely proportional to the fourth power of wavelength, so that shorter wavelength violet and blue light will scatter more than the longer wavelengths (yellow and especially red light). However, the Sun, like any star, has its own spectrum and so I0 in the scattering formula above is not constant but falls away in the violet. In addition the oxygen in the Earth's atmosphere absorbs wavelengths at the edge of the ultra-violet region of the spectrum. The resulting color, which appears like a pale blue, actually is a mixture of all the scattered colors, mainly blue and green. Conversely, glancing toward the sun, the colors that were not scattered away—the longer wavelengths such as red and yellow light—are directly visible, giving the sun itself a slightly yellowish hue. Viewed from space, however, the sky is black and the sun is white.\n",
      "-Colors Air molecules and airborne particles scatter white sunlight as it passes through the Earth's atmosphere. This is done by a combination of Rayleigh scattering and Mie scattering.As a ray of white sunlight travels through the atmosphere to an observer, some of the colors are scattered out of the beam by air molecules and airborne particles, changing the final color of the beam the viewer sees. Because the shorter wavelength components, such as blue and green, scatter more strongly, these colors are preferentially removed from the beam.At sunrise and sunset, when the path through the atmosphere is longer, the blue and green components are removed almost completely, leaving the longer-wavelength orange and red hues seen at those times. The remaining reddened sunlight can then be scattered by cloud droplets and other relatively large particles to light up the horizon red and orange. The removal of the shorter wavelengths of light is due to Rayleigh scattering by air molecules and particles much smaller than the wavelength of visible light (less than 50 nm in diameter). The scattering by cloud droplets and other particles with diameters comparable to or larger than the sunlight's wavelengths (more than 600 nm) is due to Mie scattering and is not strongly wavelength-dependent. Mie scattering is responsible for the light scattered by clouds, and also for the daytime halo of white light around the Sun (forward scattering of white light).Sunset colors are typically more brilliant than sunrise colors, because the evening air contains more particles than morning air. Ash from volcanic eruptions, trapped within the troposphere, tends to mute sunset and sunrise colors, while volcanic ejecta that is instead lofted into the stratosphere (as thin clouds of tiny sulfuric acid droplets), can yield beautiful post-sunset colors called afterglows and pre-sunrise glows. A number of eruptions, including those of Mount Pinatubo in 1991 and Krakatoa in 1883, have produced sufficiently high stratospheric sulfuric acid clouds to yield remarkable sunset afterglows (and pre-sunrise glows) around the world. The high altitude clouds serve to reflect strongly reddened sunlight still striking the stratosphere after sunset, down to the surface.\n",
      "-Daily at any global venue experiencing sunrise or sunset, most of the solar beam of visible sunlight arrives nearly tangentially to Earth's surface. Here, the path of sunlight through the atmosphere is elongated such that much of the blue or green light is scattered away from the line of perceivable visible light. This phenomenon leaves the Sun's rays, and the clouds they illuminate, abundantly orange-to-red in colors, which one sees when looking at a sunset or sunrise.\n",
      "-The blue colour of the sky results from Rayleigh scattering, as the size of the gas particles in the atmosphere is much smaller than the wavelength of visible light. Rayleigh scattering is much greater for blue light than for other colours due to its shorter wavelength. As sunlight passes through the atmosphere, its blue component is Rayleigh scattered strongly by atmospheric gases but the longer wavelength (e.g. red/yellow) components are not. The sunlight arriving directly from the Sun therefore appears to be slightly yellow, while the light scattered through rest of the sky appears blue. During sunrises and sunsets, the effect of Rayleigh scattering on the spectrum of the transmitted light is much greater due to the greater distance the light rays have to travel through the high-density air near the Earth's surface.\n",
      "\n",
      "\n",
      "\n",
      "What is the Landau-Lifshitz-Gilbert equation used for in physics?\n",
      "-In physics, the Landau–Lifshitz–Gilbert equation, named for Lev Landau, Evgeny Lifshitz, and T. L. Gilbert, is a name used for a differential equation describing the precessional motion of magnetization M in a solid. It is a modification by Gilbert of the original equation of Landau and Lifshitz.\n",
      "-The various forms of the equation are commonly used in micromagnetics to model the effects of a magnetic field on ferromagnetic materials. In particular it can be used to model the time domain behavior of magnetic elements due to a magnetic field. An additional term was added to the equation to describe the effect of spin polarized current on magnets.\n",
      "-In a ferromagnet, the magnitude of the magnetization M at each point equals the saturation magnetization Ms (although it can be smaller when averaged over a chunk of volume). The Landau–Lifshitz–Gilbert equation predicts the rotation of the magnetization in response to torques. An earlier, but equivalent, equation (the Landau–Lifshitz equation) was introduced by Landau & Lifshitz (1935): where γ is the electron gyromagnetic ratio and λ is a phenomenological damping parameter, often replaced by λ=αγMs, where α is a dimensionless constant called the damping factor. The effective field Heff is a combination of the external magnetic field, the demagnetizing field (magnetic field due to the magnetization), and some quantum mechanical effects. To solve this equation, additional equations for the demagnetizing field must be included.\n",
      "-In 1955 Gilbert replaced the damping term in the Landau–Lifshitz (LL) equation by one that depends on the time derivative of the magnetization: This is the Landau–Lifshitz–Gilbert (LLG) equation, where η is the damping parameter, which is characteristic of the material. It can be transformed into the Landau–Lifshitz equation: where and λ=γ2η1+γ2η2Ms2.\n",
      "In this form of the LL equation, the precessional term γ' depends on the damping term. This better represents the behavior of real ferromagnets when the damping is large.\n",
      "-In physics, the Landau–Lifshitz equation (LLE), named for Lev Landau and Evgeny Lifshitz, is a name used for several different differential equations  For the Landau–Lifshitz aeroacoustic equation see aeroacoustics.\n",
      "For the Landau–Lifshitz equation describing the precessional motion of magnetization M in a solid with an effective magnetic field H and with damping, see Landau–Lifshitz–Gilbert equation.\n",
      "For the Landau–Lifshitz equation describing a magnetic field in 1 + n dimensions see Landau–Lifshitz model.\n",
      "For the Landau-Lifshitz equation approximating the Abraham-Lorentz force.\n",
      "\n",
      "\n",
      "\n",
      "What is spatial dispersion?\n",
      "-In the physics of continuous media, spatial dispersion is a phenomenon where material parameters such as permittivity or conductivity have dependence on wavevector. Normally, such a dependence is assumed to be absent for simplicity, however spatial dispersion exists to varying degrees in all materials.\n",
      "-Spatial dispersion can be compared to temporal dispersion, the latter often just called dispersion. Temporal dispersion represents memory effects in systems, commonly seen in optics and electronics. Spatial dispersion on the other hand represents spreading effects and is usually significant only at microscopic length scales. Spatial dispersion contributes relatively small perturbations to optics, giving weak effects such as optical activity. Spatial dispersion and temporal dispersion may occur in the same system.\n",
      "-In optics and in wave propagation in general, dispersion is the phenomenon in which the phase velocity of a wave depends on its frequency; sometimes the term chromatic dispersion is used for specificity to optics in particular.\n",
      "A medium having this common property may be termed a dispersive medium (plural dispersive media).\n",
      "-Although the term is used in the field of optics to describe light and other electromagnetic waves, dispersion in the same sense can apply to any sort of wave motion such as acoustic dispersion in the case of sound and seismic waves, and in gravity waves (ocean waves). Within optics, dispersion is a property of telecommunication signals along transmission lines (such as microwaves in coaxial cable) or the pulses of light in optical fiber.\n",
      "-In electromagnetics and optics, the term dispersion generally refers to aforementioned temporal or frequency dispersion. Spatial dispersion refers to the non-local response of the medium to the space; this can be reworded as the wavevector dependence of the permittivity. For an exemplary anisotropic medium, the spatial relation between electric and electric displacement field can be expressed as a convolution: Di(t,r)=Ei(t,r)+∫0∞∫fik(τ;r,r′)Ek(t−τ,r′)dV′dτ, where the kernel  fik is dielectric response (susceptibility); its indices make it in general a tensor to account for the anisotropy of the medium. Spatial dispersion is negligible in most macroscopic cases, where the scale of variation of  Ek(t−τ,r′) is much larger than atomic dimensions, because the dielectric kernel dies out at macroscopic distances. Nevertheless, it can result in non-negligible macroscopic effects, particularly in conducting media such as metals, electrolytes and plasmas. Spatial dispersion also plays role in optical activity and Doppler broadening, as well as in the theory of metamaterials.\n",
      "\n",
      "\n",
      "\n",
      "What are the constituents of cold dark matter?\n",
      "-Cold dark matter Cold dark matter offers the simplest explanation for most cosmological observations. It is dark matter composed of constituents with an FSL much smaller than a protogalaxy. This is the focus for dark matter research, as hot dark matter does not seem capable of supporting galaxy or galaxy cluster formation, and most particle candidates slowed early.\n",
      "-Dark matter is detected through its gravitational interactions with ordinary matter and radiation. As such, it is very difficult to determine what the constituents of cold dark matter are. The candidates fall roughly into three categories: Axions, very light particles with a specific type of self-interaction that makes them a suitable CDM candidate. In recent years, axions have become one of the most promising candidates for dark matter. Axions have the theoretical advantage that their existence solves the strong CP problem in quantum chromodynamics, but axion particles have only been theorized and never detected. Axions are an example of a more general category of particle called a WISP (weakly interacting \"slender\" or \"slim\" particle), which are the low-mass counterparts of WIMPs.Massive compact halo objects (MACHOs), large, condensed objects such as black holes, neutron stars, white dwarfs, very faint stars, or non-luminous objects like planets. The search for these objects consists of using gravitational lensing to detect the effects of these objects on background galaxies. Most experts believe that the constraints from those searches rule out MACHOs as a viable dark matter candidate.Weakly interacting massive particles (WIMPs). There is no currently known particle with the required properties, but many extensions of the standard model of particle physics predict such particles. The search for WIMPs involves attempts at direct detection by highly sensitive detectors, as well as attempts at production of WIMPs by particle accelerators. Historically, WIMPs were regarded as one of the most promising candidates for the composition of dark matter, but in recent years WIMPs have since been supplanted by axions with the non-detection of WIMPs in experiments. The DAMA/NaI experiment and its successor DAMA/LIBRA have claimed to have directly detected dark matter particles passing through the Earth, but many scientists remain skeptical because no results from similar experiments seem compatible with the DAMA results.\n",
      "-In astrophysics and cosmology, dark matter is matter of unknown composition that does not emit or reflect enough electromagnetic radiation to be observed directly, but whose presence can be inferred from gravitational effects on visible matter. Observational evidence of the early universe and the Big Bang theory require that this matter have energy and mass, but not be composed of ordinary baryons (protons and neutrons). The commonly accepted view is that most of the dark matter is non-baryonic in nature. As such, it is composed of particles as yet unobserved in the laboratory. Perhaps they are supersymmetric particles, which are not Standard Model particles but relics formed at very high energies in the early phase of the universe and still floating about.\n",
      "-The three known neutrino flavors are the only candidates for dark matter that are experimentally established elementary particles – specifically, they would be hot dark matter. However, the currently known neutrino types seem to be essentially ruled out as a substantial proportion of dark matter, based on observations of the cosmic microwave background. It still seems plausible that heavier, sterile neutrinos might compose warm dark matter, if they exist.\n",
      "-Composition There are various hypotheses about what dark matter could consist of, as set out in the table below.\n",
      "Dark matter can refer to any substance which interacts predominantly via gravity with visible matter (e.g., stars and planets). Hence in principle it need not be composed of a new type of fundamental particle but could, at least in part, be made up of standard baryonic matter, such as protons or neutrons.\n",
      "\n",
      "\n",
      "\n",
      "What is the mechanism of FTIR?\n",
      "-FTIR is a method of measuring infrared absorption and emission spectra. For a discussion of why people measure infrared absorption and emission spectra, i.e. why and how substances absorb and emit infrared light, see the article: Infrared spectroscopy.\n",
      "-FTIR Fourier transform infrared (FTIR) spectroscopy is a measurement technique that allows one to record infrared spectra. Infrared light is guided through an interferometer and then through the sample (or vice versa). A moving mirror inside the apparatus alters the distribution of infrared light that passes through the interferometer. The signal directly recorded, called an \"interferogram\", represents light output as a function of mirror position. A data-processing technique called Fourier transform turns this raw data into the desired result (the sample's spectrum): light output as a function of infrared wavelength (or equivalently, wavenumber). As described above, the sample's spectrum is always compared to a reference.\n",
      "-Nano-FTIR (nanoscale Fourier transform infrared spectroscopy) is a scanning probe technique that utilizes as a combination of two techniques: Fourier transform infrared spectroscopy (FTIR) and scattering-type scanning near-field optical microscopy (s-SNOM). As s-SNOM, nano-FTIR is based on atomic-force microscopy (AFM), where a sharp tip is illuminated by an external light source and the tip-scattered light (typically back-scattered) is detected as a function of tip position. A typical nano-FTIR setup thus consists of an atomic force microscope, a broadband infrared light source used for tip illumination, and a Michelson interferometer acting as Fourier-transform spectrometer. In nano-FTIR, the sample stage is placed in one of the interferometer arms, which allows for recording both amplitude and phase of the detected light (unlike conventional FTIR that normally does not yield phase information). Scanning the tip allows for performing hyperspectral imaging (i.e. complete spectrum at every pixel of the scanned area) with nanoscale spatial resolution determined by the tip apex size. The use of broadband infrared sources enables the acquisition of continuous spectra, which is a distinctive feature of nano-FTIR compared to s-SNOM.\n",
      "-Fourier-transform infrared spectroscopy (FTIR) is a technique used to obtain an infrared spectrum of absorption or emission of a solid, liquid, or gas. An FTIR spectrometer simultaneously collects high-resolution spectral data over a wide spectral range. This confers a significant advantage over a dispersive spectrometer, which measures intensity over a narrow range of wavelengths at a time.\n",
      "The term Fourier-transform infrared spectroscopy originates from the fact that a Fourier transform (a mathematical process) is required to convert the raw data into the actual spectrum.\n",
      "-Nano-FTIR detects the tip-scattered light interferometrically. The sample stage is placed into one arm of a conventional Michelson interferometer, while a mirror on a piezo stage is placed into another, reference arm. Recording the backscattered signal while translating the reference mirror yields an interferogram. The subsequent Fourier transform of this interferogram returns the near-field spectra of the sample.  Placement of the sample stage into one of the interferometer's arms (instead of outside of the interferometer as typically implemented in conventional FTIR) is a key element of nano-FTIR. It boosts the weak near-field signal due to interference with the strong reference field, helps to eliminate the background caused by parasitic scattering off everything that falls into large diffraction-limited beam focus, and most importantly, allows for recording of both amplitude s and phase φ spectra of the tip-scattered radiation. With the detection of phase, nano-FTIR provides complete information about near fields, which is essential for quantitative studies and many other applications. For example, for soft matter samples (organics, polymers, biomaterials, etc.), φ directly relates to the absorption in the sample material. This permits a direct comparison of nano-FTIR spectra with conventional absorption spectra of the sample material, thus allowing for simple spectroscopic identification according to standard FTIR databases.\n",
      "\n",
      "\n",
      "\n",
      "What is the origin of the permanent moment in paramagnetism?\n",
      "-Constituent atoms or molecules of paramagnetic materials have permanent magnetic moments (dipoles), even in the absence of an applied field. The permanent moment generally is due to the spin of unpaired electrons in atomic or molecular electron orbitals (see Magnetic moment). In pure paramagnetism, the dipoles do not interact with one another and are randomly oriented in the absence of an external field due to thermal agitation, resulting in zero net magnetic moment. When a magnetic field is applied, the dipoles will tend to align with the applied field, resulting in a net magnetic moment in the direction of the applied field. In the classical description, this alignment can be understood to occur due to a torque being provided on the magnetic moments by an applied field, which tries to align the dipoles parallel to the applied field. However, the true origins of the alignment can only be understood via the quantum-mechanical properties of spin and angular momentum.\n",
      "-These unpaired dipoles (often called simply \"spins\", even though they also generally include orbital angular momentum) tend to align in parallel to an external magnetic field – leading to a macroscopic effect called paramagnetism. In ferromagnetism, however, the magnetic interaction between neighboring atoms' magnetic dipoles is strong enough that they align with each other regardless of any applied field, resulting in the spontaneous magnetization of so-called domains. This results in the large observed magnetic permeability of ferromagnetics, and the ability of \"hard\" magnetic materials to form permanent magnets.\n",
      "-The preferred classical explanation of a magnetic moment has changed over time. Before the 1930s, textbooks explained the moment using hypothetical magnetic point charges. Since then, most have defined it in terms of Ampèrian currents. In magnetic materials, the cause of the magnetic moment are the spin and orbital angular momentum states of the electrons, and varies depending on whether atoms in one region are aligned with atoms in another.\n",
      "-Magnetic moment of an electron Electrons and many elementary particles also have intrinsic magnetic moments, an explanation of which requires a quantum mechanical treatment and relates to the intrinsic angular momentum of the particles as discussed in the article Electron magnetic moment. It is these intrinsic magnetic moments that give rise to the macroscopic effects of magnetism, and other phenomena, such as electron paramagnetic resonance.\n",
      "-Origin of atomic magnetism One of the fundamental properties of an electron (besides that it carries charge) is that it has a magnetic dipole moment, i.e., it behaves like a tiny magnet, producing a magnetic field. This dipole moment comes from the more fundamental property of the electron that it has quantum mechanical spin. Due to its quantum nature, the spin of the electron can be in one of only two states; with the magnetic field either pointing \"up\" or \"down\" (for any choice of up and down). The spin of the electrons in atoms is the main source of ferromagnetism, although there is also a contribution from the orbital angular momentum of the electron about the nucleus. When these magnetic dipoles in a piece of matter are aligned, (point in the same direction) their individually tiny magnetic fields add together to create a much larger macroscopic field.\n",
      "\n",
      "\n",
      "\n",
      "What is the reason that Newton's second law cannot be used to calculate the development of a physical system in quantum mechanics?\n",
      "-Relation to Newton's second law of motion While angular momentum total conservation can be understood separately from Newton's laws of motion as stemming from Noether's theorem in systems symmetric under rotations, it can also be understood simply as an efficient method of calculation of results that can also be otherwise arrived at directly from Newton's second law, together with laws governing the forces of nature (such as Newton's third law, Maxwell's equations and Lorentz force). Indeed, given initial conditions of position and velocity for every point, and the forces at such a condition, one may use Newton's second law to calculate the second derivative of position, and solving for this gives full information on the development of the physical system with time. Note, however, that this is no longer true in quantum mechanics, due to the existence of particle spin, which is angular momentum that cannot be described by the cumulative effect of point-like motions in space.\n",
      "-Newton founded his principles of natural philosophy on three proposed laws of motion: the law of inertia, his second law of acceleration (mentioned above), and the law of action and reaction; and hence laid the foundations for classical mechanics. Both Newton's second and third laws were given the proper scientific and mathematical treatment in Newton's Philosophiæ Naturalis Principia Mathematica. Here they are distinguished from earlier attempts at explaining similar phenomena, which were either incomplete, incorrect, or given little accurate mathematical expression. Newton also enunciated the principles of conservation of momentum and angular momentum. In mechanics, Newton was also the first to provide the first correct scientific and mathematical formulation of gravity in Newton's law of universal gravitation. The combination of Newton's laws of motion and gravitation provides the fullest and most accurate description of classical mechanics. He demonstrated that these laws apply to everyday objects as well as to celestial objects. In particular, he obtained a theoretical explanation of Kepler's laws of motion of the planets.\n",
      "-If two bodies exert forces on each other, these forces have the same magnitude but opposite directions.The three laws of motion were first stated by Isaac Newton in his Philosophiæ Naturalis Principia Mathematica (Mathematical Principles of Natural Philosophy), originally published in 1687. Newton used them to investigate and explain the motion of many physical objects and systems, which laid the foundation for classical mechanics. In the time since Newton, the conceptual content of classical physics has been reformulated in alternative ways, involving different mathematical approaches that have yielded insights which were obscured in the original, Newtonian formulation. Limitations to Newton's laws have also been discovered; new theories are necessary when objects move at very high speeds (special relativity), are very massive (general relativity), or are very small (quantum mechanics).\n",
      "-Particle physics, the motion, structure, and reactions of particles Nuclear physics, the motion, structure, and reactions of nuclei Condensed matter physics, quantum gases, solids, liquids, etc.Historically, classical mechanics had been around for nearly a quarter millennium before quantum mechanics developed. Classical mechanics originated with Isaac Newton's laws of motion in Philosophiæ Naturalis Principia Mathematica, developed over the seventeenth century. Quantum mechanics developed later, over the nineteenth century, precipitated by Planck's postulate and Albert Einstein's explanation of the photoelectric effect. Both fields are commonly held to constitute the most certain knowledge that exists about physical nature.\n",
      "-Although spin and the Pauli principle can only be derived from relativistic generalizations of quantum mechanics, the properties mentioned in the last two paragraphs belong to the basic postulates already in the non-relativistic limit. Especially, many important properties in natural science, e.g. the periodic system of chemistry, are consequences of the two properties.\n",
      "\n",
      "\n",
      "\n",
      "What is the butterfly effect, as defined by Lorenz in his book \"The Essence of Chaos\"?\n",
      "-In chaos theory, the butterfly effect is the sensitive dependence on initial conditions in which a small change in one state of a deterministic nonlinear system can result in large differences in a later state.\n",
      "-The Lorenz system is a system of ordinary differential equations first studied by mathematician and meteorologist Edward Lorenz. It is notable for having chaotic solutions for certain parameter values and initial conditions. In particular, the Lorenz attractor is a set of chaotic solutions of the Lorenz system. In popular media the \"butterfly effect\" stems from the real-world implications of the Lorenz attractor, namely that several different initial chaotic conditions evolve in phase space in a way that never repeats, so all chaos is unpredictable. This underscores that chaotic systems can be completely deterministic and yet still be inherently unpredictable over long periods of time. Because chaos continually increases in systems, we cannot predict the future of systems well. E.g., even the small flap of a butterfly’s wings could set the world on a vastly different trajectory, such as by causing a hurricane. The shape of the Lorenz attractor itself, when plotted in phase space, may also be seen to resemble a butterfly.\n",
      "-Chaos theory Chaos theory describes the behavior of certain dynamical systems – that is, systems whose state evolves with time – that may exhibit dynamics that are highly sensitive to initial conditions (popularly referred to as the butterfly effect). As a result of this sensitivity, which manifests itself as an exponential growth of perturbations in the initial conditions, the behavior of chaotic systems appears random. This happens even though these systems are deterministic, meaning that their future dynamics are fully defined by their initial conditions, with no random elements involved. This behavior is known as deterministic chaos, or simply chaos.\n",
      "-Sensitivity to initial conditions Sensitivity to initial conditions means that each point in a chaotic system is arbitrarily closely approximated by other points that have significantly different future paths or trajectories. Thus, an arbitrarily small change or perturbation of the current trajectory may lead to significantly different future behavior.Sensitivity to initial conditions is popularly known as the \"butterfly effect\", so-called because of the title of a paper given by Edward Lorenz in 1972 to the American Association for the Advancement of Science in Washington, D.C., entitled Predictability: Does the Flap of a Butterfly's Wings in Brazil set off a Tornado in Texas?. The flapping wing represents a small change in the initial condition of the system, which causes a chain of events that prevents the predictability of large-scale phenomena. Had the butterfly not flapped its wings, the trajectory of the overall system could have been vastly different.\n",
      "-A dynamical system displays sensitive dependence on initial conditions if points arbitrarily close together separate over time at an exponential rate. The definition is not topological, but essentially metrical. Lorenz defined sensitive dependence as follows: The property characterizing an orbit (i.e., a solution) if most other orbits that pass close to it at some point do not remain close to it as time advances.\n",
      "\n",
      "\n",
      "\n",
      "What is the role of CYCLOIDEA genes in the evolution of bilateral symmetry?\n",
      "-Evolution of symmetry in plants Early flowering plants had radially symmetric flowers but since then many plants have evolved bilaterally symmetrical flowers. The evolution of bilateral symmetry is due to the expression of CYCLOIDEA genes. Evidence for the role of the CYCLOIDEA gene family comes from mutations in these genes which cause a reversion to radial symmetry. The CYCLOIDEA genes encode transcription factors, proteins which control the expression of other genes. This allows their expression to influence developmental pathways relating to symmetry. For example, in Antirrhinum majus, CYCLOIDEA is expressed during early development in the dorsal domain of the flower meristem and continues to be expressed later on in the dorsal petals to control their size and shape. It is believed that the evolution of specialized pollinators may play a part in the transition of radially symmetrical flowers to bilaterally symmetrical flowers.\n",
      "-Like all the traits of organisms, symmetry (or indeed asymmetry) evolves due to an advantage to the organism – a process of natural selection. This involves changes in the frequency of symmetry-related genes throughout time.\n",
      "-Factors influencing floral diversity How is the enormous diversity in the shape, color and sizes of flowers established? There is enormous variation in the developmental program in different plants. For example, monocots possess structures like lodicules and palea, that were believed to be analogous to the dicot petals and carpels respectively. It turns out that this is true, and the variation is due to slight changes in the MADS-box genes and their expression pattern in the monocots. Another example is that of the toad-flax, Linaria vulgaris, which has two kinds of flower symmetries: radial and bilateral. These symmetries are due to changes in copy number, timing, and location of expression in CYCLOIDEA, which is related to TCP1 in Arabidopsis.\n",
      "-In addition to animals, the flowers of some plants also show bilateral symmetry. Such plants are referred to as zygomorphic and include the orchid (Orchidaceae) and pea (Fabaceae) families, and most of the figwort family (Scrophulariaceae). The leaves of plants also commonly show approximate bilateral symmetry.\n",
      "-Although asymmetry is typically associated with being unfit, some species have evolved to be asymmetrical as an important adaptation. Many members of the phylum Porifera (sponges) have no symmetry, though some are radially symmetric.\n",
      "Symmetry breaking The presence of these asymmetrical features requires a process of symmetry breaking during development, both in plants and animals. Symmetry breaking occurs at several different levels in order to generate the anatomical asymmetry which we observe. These levels include asymmetric gene expression, protein expression, and activity of cells.\n",
      "\n",
      "\n",
      "\n",
      "What is the required excess quark per billion quark-antiquark pairs in the early universe in order to provide all the observed matter in the universe?\n",
      "-The majority of ordinary matter in the universe is found in atomic nuclei, which are made of neutrons and protons. These nucleons are made up of smaller particles called quarks, and antimatter equivalents for each are predicted to exist by the Dirac equation in 1928. Since then, each kind of antiquark has been experimentally verified. Hypotheses investigating the first few instants of the universe predict a composition with an almost equal number of quarks and antiquarks. Once the universe expanded and cooled to a critical temperature of approximately 2×1012 K, quarks combined into normal matter and antimatter and proceeded to annihilate up to the small initial asymmetry of about one part in five billion, leaving the matter around us. Free and separate individual quarks and antiquarks have never been observed in experiments—quarks and antiquarks are always found in groups of three (baryons), or bound in quark–antiquark pairs (mesons). Likewise, there is no experimental evidence that there are any significant concentrations of antimatter in the observable universe.\n",
      "-Hadrons The word hadron comes from Greek and was introduced in 1962 by Lev Okun. Nearly all composite particles contain multiple quarks (and/or antiquarks) bound together by gluons (with a few exceptions with no quarks, such as positronium and muonium). Those containing few (≤ 5) quarks (including antiquarks) are called hadrons.  Due to a property known as color confinement, quarks are never found singly but always occur in hadrons containing multiple quarks. The hadrons are divided by number of quarks (including antiquarks) into the baryons containing an odd number of quarks (almost always 3), of which the proton and neutron (the two nucleons) are by far the best known; and the mesons containing an even number of quarks (almost always 2, one quark and one antiquark), of which the pions and kaons are the best known.\n",
      "-One of CDF's most famous discoveries is the observation of the top quark in February 1995. The existence of the top quark was hypothesized after the observation of the Upsilon at Fermilab in 1977, which was found to consist of a bottom quark and an anti-bottom quark. The Standard Model, which today is the most widely accepted theory describing the particles and interactions, predicted the existence of three generations of quarks. The first generation quarks are the up and down quarks, second generation quarks are strange and charm, and third generation are top and bottom. The existence of the bottom quark solidified physicists’ conviction that the top quark existed. The top quark was the very last quark to be observed, mostly due to its comparatively high mass. Whereas the masses of the other quarks range from .005 GeV (up quark) to 4.7GeV (bottom quark), the top quark has a mass of 175 GeV. Only Fermilab's Tevatron had the energy capability to produce and detect top anti-top pairs. The large mass of the top quark caused the top quark to decay almost instantaneously, within the order of 10−25 seconds, making it extremely difficult to observe. The Standard Model predicts that the top quark may decay leptonically into a bottom quark and a W boson. This W boson may then decay into a lepton and neutrino (t→Wb→ѵlb). Therefore, CDF worked to reconstruct top events, looking specifically for evidence of bottom quarks, W bosons neutrinos. Finally in February 1995, CDF had enough evidence to say that they had \"discovered\" the top quark. On February 24, CDF and DØ experimenters simultaneously submitted papers to Physical Review Letters describing the observation of the top quark. The two collaborations announced the discovery publicly at a seminar at Fermilab on March 2 and the papers were published on April 3.In 2019, the European Physical Society awarded the 2019 European Physical Society High Energy and Particle Physics Prize to the CDF and DØ collaborations \"for the discovery of the top quark and the detailed measurement of its properties.\" \n",
      "-Apart from ordinary quark matter and strange quark matter, other types of quark-gluon plasma might theoretically occur or be formed inside neutron stars and quark stars. This includes the following, some of which has been observed and studied in laboratories: Robert L. Jaffe 1977, suggested a four-quark state with strangeness (qsqs).\n",
      "Robert L. Jaffe 1977 suggested the H dibaryon, a six-quark state with equal numbers of up-, down-, and strange quarks (represented as uuddss or udsuds).\n",
      "Bound multi-quark systems with heavy quarks (QQqq).\n",
      "In 1987, a pentaquark state was first proposed with a charm anti-quark (qqqsc).\n",
      "Pentaquark state with an antistrange quark and four light quarks consisting of up- and down-quarks only (qqqqs).\n",
      "Light pentaquarks are grouped within an antidecuplet, the lightest candidate, Θ+, which can also be described by the diquark model of Robert L. Jaffe and Wilczek (QCD).\n",
      "Θ++ and antiparticle Θ−−.\n",
      "Doubly strange pentaquark (ssddu), member of the light pentaquark antidecuplet.\n",
      "Charmed pentaquark Θc(3100) (uuddc) state was detected by the H1 collaboration.\n",
      "Tetraquark particles might form inside neutron stars and under other extreme conditions. In 2008, 2013 and 2014 the tetraquark particle of Z(4430), was discovered and investigated in laboratories on Earth.\n",
      "-The Standard Model can incorporate baryogenesis, though the amount of net baryons (and leptons) thus created may not be sufficient to account for the present baryon asymmetry. There is a required one excess quark per billion quark-antiquark pairs in the early universe in order to provide all the observed matter in the universe. This insufficiency has not yet been explained, theoretically or otherwise.\n",
      "\n",
      "\n",
      "\n",
      "What is the meaning of the term \"horror vacui\"?\n",
      "-In visual art, horror vacui (Latin for 'fear of empty space'; UK: ; US: ), or kenophobia (Greek for 'fear of the empty'), is a phenomenon in which the entire surface of a space or an artwork is filled with detail and content, leaving as little perceived emptiness as possible. It relates to the antiquated physical idea, horror vacui, proposed by Aristotle who held that \"nature abhors an empty space\".\n",
      "-Plenism means \"fullness\", from Latin plēnum, English \"plenty\", cognate via Proto-Indo-European to \"full\". In Ancient Greek, the term for the void is τὸ κενόν (to kenón).\n",
      "-In physics, horror vacui, or plenism (), commonly stated as \"nature abhors a vacuum\", is a postulate attributed to Aristotle, who articulated a belief, later criticized by the atomism of Epicurus and Lucretius, that nature contains no vacuums because the denser surrounding material continuum would immediately fill the rarity of an incipient void. He also argued against the void in a more abstract sense (as \"separable\"), for example, that by definition a void (equivocally?) itself, is nothing, and following Plato, nothing cannot rightly be said to exist. Furthermore, insofar as it would be featureless, it could neither be encountered by the senses, nor could its supposition lend additional explanatory power. Hero of Alexandria challenged the theory in the first century AD, but his attempts to create an artificial vacuum failed. The theory was debated in the context of 17th-century fluid mechanics, by Thomas Hobbes and Robert Boyle, among others, and through the early 18th century by Sir Isaac Newton and Gottfried Leibniz.\n",
      "-Contemporary architecture critic Herbert Muschamp argues that \"horror vacui\" (which is Latin for \"fear of emptiness\") is a key principle of design. He claims that it has become an obsessive quality that is the \"driving force in contemporary American taste\". Muschamp states that \"along with the commercial interests that exploit this interest, it is the major factor now shaping attitudes toward public spaces, urban spaces, and even suburban sprawl.\"Films that depict nothingness, shadows and vagueness, either in a visual sense or a moral sense are appreciated in genres such as film noir. As well, travellers and artists are often intrigued by and attracted to vast empty spaces, such as open deserts, barren wastelands or salt flats, and the open sea.\n",
      "-Italian art critic and scholar Mario Praz used this term to describe the excessive use of ornament in design during the Victorian age. Other examples of horror vacui can be seen in the densely decorated carpet pages of Insular illuminated manuscripts, where intricate patterns and interwoven symbols may have served \"apotropaic as well as decorative functions.\" The interest in meticulously filling empty spaces is also reflected in Arabesque decoration in Islamic art from ancient times to present. The art historian Ernst Gombrich theorized that such highly ornamented patterns can function like a picture frame for sacred images and spaces. \"The richer the elements of the frame,\" Gombrich wrote, \"the more the centre will gain in dignity.\"Another example comes from ancient Greece during the Geometric Age (1100–900 BCE), when horror vacui was considered a stylistic element of all art. The mature work of the French Renaissance engraver Jean Duvet consistently exhibits horror vacui.\n",
      "\n",
      "\n",
      "\n",
      "What is the Droste effect?\n",
      "-The Droste effect (Dutch pronunciation: [ˈdrɔstə]), known in art as an example of mise en abyme, is the effect of a picture recursively appearing within itself, in a place where a similar picture would realistically be expected to appear. This produces a loop which in theory could go on forever, but in practice only continues as far as the image's resolution allows.\n",
      "-The effect is named after a Dutch brand of cocoa, with an image designed by Jan Misset in 1904. It has since been used in the packaging of a variety of products. The effect is seen in the Dutch artist M. C. Escher's 1956 lithograph Print Gallery, which portrays a gallery that depicts itself. Apart from advertising, the Droste effect is displayed in the model village at Bourton-on-the-Water: this contains a model of itself, with two further iterations. The effect has been a motif, too, for the cover of many comic books, where it was especially popular in the 1940s.\n",
      "-Origins The Droste effect is named after the image on the tins and boxes of Droste cocoa powder which displayed a nurse carrying a serving tray with a cup of hot chocolate and a box with the same image, designed by Jan Misset. This familiar image was introduced in 1904 and maintained for decades with slight variations from 1912 by artists including Adolphe Mouron. The poet and columnist Nico Scheepmaker introduced wider usage of the term in the late 1970s.\n",
      "-M. C. Escher The Dutch artist M. C. Escher made use of the Droste effect in his 1956 lithograph Print Gallery, which portrays a gallery containing a print which depicts the gallery, each time both reduced and rotated, but with a void at the centre of the image. The work has attracted the attention of mathematicians including Hendrik Lenstra. They devised a method of filling in the artwork's central void in an additional application of the Droste effect by successively rotating and shrinking an image of the artwork.\n",
      "-Mathematics The appearance is recursive: the smaller version contains an even smaller version of the picture, and so on. Only in theory could this go on forever, as fractals do; practically, it continues only as long as the resolution of the picture allows, which is relatively short, since each iteration geometrically reduces the picture's size.\n",
      "\n",
      "\n",
      "\n",
      "What is water hammer?\n",
      "-Water hammer: Water hammer (or more generally, fluid hammer) is a pressure surge or wave caused when a fluid (usually a liquid but sometimes also a gas) in motion is forced to stop or change direction suddenly (momentum change). Water hammer commonly occurs when a valve closes suddenly at an end of a pipeline system, and a pressure wave propagates in the pipe. It's also called hydraulic shock.\n",
      "-Hydraulic shock (colloquial: water hammer; fluid hammer) is a pressure surge or wave caused when a fluid in motion, usually a liquid but sometimes also a gas is forced to stop or change direction suddenly; a momentum change. This phenomenon commonly occurs when a valve closes suddenly at an end of a pipeline system, and a pressure wave propagates in the pipe.\n",
      "-Water flowing through a pipe has momentum. If the moving water is suddenly stopped - such as by closing a valve downstream of the flowing water, the pressure can rise suddenly with a resulting shock wave. In domestic plumbing this shock wave is experienced as a loud banging resembling a hammering noise. Water hammer can cause pipelines to break if the pressure is sufficiently high. Air traps or stand pipes (open at the top) are sometimes added as dampers to water systems to absorb the potentially damaging forces caused by the moving water.\n",
      "-Related phenomena Steam hammer can occur in steam systems when some of the steam condenses into water in a horizontal section of the piping. The steam forcing the liquid water along the pipe forms a \"slug\" which impacts a valve of pipe fitting, creating a loud hammering noise and high pressure. Vacuum caused by condensation from thermal shock can also cause a steam hammer.\n",
      "-This pressure wave can cause major problems, from noise and vibration to pipe rupture or collapse. It is possible to reduce the effects of the water hammer pulses with accumulators, expansion tanks, surge tanks, blowoff valves, and other features. The effects can be avoided by ensuring that no valves will close too quickly with significant flow, but there are many situations that can cause the effect.\n",
      "\n",
      "\n",
      "\n",
      "What is the reason for the stochastic nature of all observed resistance-switching processes?\n",
      "-A \"resistance switching\" event can simply be enforced by setting the external bias to a value above a certain threshold value. This is the trivial case, i.e., the free-energy barrier for the transition {i} → {j} is reduced to zero. In case one applies biases below the threshold value, there is still a finite probability that the device will switch in course of time (triggered by a random thermal fluctuation), but – as one is dealing with probabilistic processes – it is impossible to predict when the switching event will occur. That is the basic reason for the stochastic nature of all observed resistance-switching (ReRAM) processes. If the free-energy barriers are not high enough, the memory device can even switch without having to do anything.\n",
      "-The above-mentioned thermodynamic principle furthermore implies that the operation of two-terminal non-volatile memory devices (e.g. \"resistance-switching\" memory devices (ReRAM)) cannot be associated with the memristor concept, i.e., such devices cannot by itself remember their current or voltage history. Transitions between distinct internal memory or resistance states are of probabilistic nature. The probability for a transition from state {i} to state {j} depends on the height of the free-energy barrier between both states. The transition probability can thus be influenced by suitably driving the memory device, i.e., by \"lowering\" the free-energy barrier for the transition {i} → {j} by means of, for example, an externally applied bias.\n",
      "-Within this context, Meuffels and Soni pointed to a fundamental thermodynamic principle: Non-volatile information storage requires the existence of free-energy barriers that separate the distinct internal memory states of a system from each other; otherwise, one would be faced with an \"indifferent\" situation, and the system would arbitrarily fluctuate from one memory state to another just under the influence of thermal fluctuations. When unprotected against thermal fluctuations, the internal memory states exhibit some diffusive dynamics, which causes state degradation. The free-energy barriers must therefore be high enough to ensure a low bit-error probability of bit operation. Consequently, there is always a lower limit of energy requirement – depending on the required bit-error probability – for intentionally changing a bit value in any memory device.In the general concept of memristive system the defining equations are (see Theory): y(t)=g(x,u,t)u(t),x˙=f(x,u,t), where u(t) is an input signal, and y(t) is an output signal. The vector x represents a set of n state variables describing the different internal memory states of the device. ẋ is the time-dependent rate of change of the state vector x with time.\n",
      "-The basic idea is that a dielectric, which is normally insulating, can be made to conduct through a filament or conduction path formed after application of a sufficiently high voltage. The conduction path can arise from different mechanisms, including vacancy or metal defect migration. Once the filament is formed, it may be reset (broken, resulting in high resistance) or set (re-formed, resulting in lower resistance) by another voltage. Many current paths, rather than a single filament, are possibly involved. The presence of these current paths in the dielectric can be in situ demonstrated via conductive atomic force microscopy.The low-resistance path can be either localized (filamentary) or homogeneous. Both effects can occur either throughout the entire distance between the electrodes or only in proximity to one of the electrodes. Filamentary and homogenous switching effects can be distinguished by measuring the area dependence of the low-resistance state.Under certain conditions, the forming operation may be bypassed. It is expected that under these conditions, the initial current is already quite high compared to insulating oxide layers.\n",
      "-An electronic flip-flop, or \"latch\", or \"bistable multivibrator\", is a circuit that due to high positive feedback is not stable in a balanced or intermediate state. Such a bistable circuit is the basis of one bit of electronic memory. The flip-flop uses a pair of amplifiers, transistors, or logic gates connected to each other so that positive feedback maintains the state of the circuit in one of two unbalanced stable states after the input signal has been removed, until a suitable alternative signal is applied to change the state. Computer random access memory (RAM) can be made in this way, with one latching circuit for each bit of memory.Thermal runaway occurs in electronic systems because some aspect of a circuit is allowed to pass more current when it gets hotter, then the hotter it gets, the more current it passes, which heats it some more and so it passes yet more current. The effects are usually catastrophic for the device in question. If devices have to be used near to their maximum power-handling capacity, and thermal runaway is possible or likely under certain conditions, improvements can usually be achieved by careful design.\n",
      "\n",
      "\n",
      "\n",
      "What is the Einstein@Home project?\n",
      "-The Einstein@Home project is a distributed computing project similar to SETI@home intended to detect this type of gravitational wave. By taking data from LIGO and GEO, and sending it out in little pieces to thousands of volunteers for parallel analysis on their home computers, Einstein@Home can sift through the data far more quickly than would be possible otherwise.\n",
      "-The Einstein@Home project is a distributed computing project similar to SETI@home intended to detect this type of simple gravitational wave. By taking data from LIGO and GEO, and sending it out in little pieces to thousands of volunteers for parallel analysis on their home computers, Einstein@Home can sift through the data far more quickly than would be possible otherwise.\n",
      "-Einstein@Home uses the power of volunteer computing in solving the computationally intensive problem of analyzing a large volume of data. Such an approach was pioneered by the SETI@home project, which is designed to look for signs of extraterrestrial life by analyzing radio wave data. Einstein@Home runs through the same software platform as SETI@home, the Berkeley Open Infrastructure for Network Computing (BOINC). As of July 2022, more than 487,000 volunteers in 226 countries had participated in the project, making it the third-most-popular active BOINC application. Users regularly contribute about 12.7 petaFLOPS of computational power, which would rank Einstein@Home among the top 45 on the TOP500 list of supercomputers.\n",
      "-Einstein@Home's first analysis used data from the \"third science run\" (S3) of LIGO. Processing of the S3 data set was conducted between 22 February 2005 and 2 August 2005. This analysis employed 60 segments from the LIGO Hanford 4-km detector, totaling ten hours of data each. Each 10-hour segment was analyzed for CW signals by the volunteers' computers using a matched-filtering technique. When all matched-filtering results were returned, the results from different segments were then combined in a \"post-processing step\" on Einstein@Home servers via a coincidence scheme to further enhance search sensitivity. Results were published on the Einstein@Home webpages.Work on the S4 data set (LIGO's fourth science run) was started via interlacing with the S3 calculations and finished in July 2006. This analysis used 10 segments of 30 hours each from the LIGO Hanford 4-km detector and 7 segments of 30 hours each from the LIGO Livingston 4-km detector. Besides the S4 data being more sensitive, a more sensitive coincidence combination scheme was also applied in the post-processing. The results of this search have led to the first scientific publication of Einstein@Home in Physical Review D.Einstein@Home gained considerable attention in the international volunteer computing community when an optimized application for the S4 data set analysis was developed and released in March 2006 by project volunteer Akos Fekete, a Hungarian programmer. Fekete improved the official S4 application and introduced SSE, 3DNow! and SSE3 optimizations into the code improving performance by up to 800%. Fekete was recognized for his efforts and was afterward officially involved with the Einstein@Home team in the development of the new S5 application. As of late July 2006, this new official application had become widely distributed among Einstein@Home users. The app created a large surge in the project's total performance and productivity, as measured by floating point speed (or FLOPS), which over time has increased by approximately 50% compared to non-optimized S4 applications.The first Einstein@Home analysis of the early LIGO S5 data set, where the instruments initially reached their design sensitivity, began on 15 June 2006. This search used 22 segments of 30 hours each from the LIGO Hanford 4-km detector and six segments of 30 hours from the LIGO Livingston 4-km detector. This analysis run (code name \"S5R1\"), employing the search methodology as Einstein@Home, was very similar to the previous S4 analysis. However, the search results were more sensitive due to the use of more data of better quality compared to S4. Over large parts of the search parameter space, these results, which also appeared in Physical Review D, are the most exhaustive published to date.The second Einstein@Home search of LIGO S5 data (code name \"S5R3\") constituted a further major improvement regarding search sensitivity. As opposed to previous searches, the ensuing results were already combined on the volunteers' computers via a Hough transform technique. This method matched-filtered results from 84 data segments of 25 hours each, parameters from which came from both 4-km LIGO Hanford and Livingston instruments.\n",
      "-MilkyWay@home is a volunteer computing project in the astrophysics category, running on the Berkeley Open Infrastructure for Network Computing (BOINC) platform. Using spare computing power from over 38,000 computers run by over 27,000 active volunteers as of November 2011, the MilkyWay@home project aims to generate accurate three-dimensional dynamic models of stellar streams in the immediate vicinity of the Milky Way. With SETI@home and Einstein@home, it is the third computing project of this type that has the investigation of phenomena in interstellar space as its primary purpose. Its secondary objective is to develop and optimize algorithms for volunteer computing.\n",
      "\n",
      "\n",
      "\n",
      "What happens to an initially inhomogeneous physical system that is isolated by a thermodynamic operation?\n",
      "-If an initially isolated physical system, without internal walls that establish adiabatically isolated subsystems, is left long enough, it will usually reach a state of thermal equilibrium in itself, in which its temperature will be uniform throughout, but not necessarily a state of thermodynamic equilibrium, if there is some structural barrier that can prevent some possible processes in the system from reaching equilibrium; glass is an example. Classical thermodynamics in general considers idealized systems that have reached internal equilibrium, and idealized transfers of matter and energy between them.\n",
      "-Such changes in isolated systems are irreversible in the sense that while such a change will occur spontaneously whenever the system is prepared in the same way, the reverse change will practically never occur spontaneously within the isolated system; this is a large part of the content of the second law of thermodynamics. Truly perfectly isolated systems do not occur in nature, and always are artificially prepared.\n",
      "-It is, however, the fruit of experience that some physical systems, including isolated ones, do seem to reach their own states of internal thermodynamic equilibrium. Classical thermodynamics postulates the existence of systems in their own states of internal thermodynamic equilibrium. This postulate is a very useful idealization.\n",
      "-An isolated system is more restrictive than a closed system as it does not interact with its surroundings in any way. Mass and energy remains constant within the system, and no energy or mass transfer takes place across the boundary. As time passes in an isolated system, internal differences in the system tend to even out and pressures and temperatures tend to equalize, as do density differences. A system in which all equalizing processes have gone practically to completion is in a state of thermodynamic equilibrium.\n",
      "-An isolated physical system may be inhomogeneous, or may be composed of several subsystems separated from each other by walls. If an initially inhomogeneous physical system, without internal walls, is isolated by a thermodynamic operation, it will in general over time change its internal state. Or if it is composed of several subsystems separated from each other by walls, it may change its state after a thermodynamic operation that changes its walls. Such changes may include change of temperature or spatial distribution of temperature, by changing the state of constituent materials. A rod of iron, initially prepared to be hot at one end and cold at the other, when isolated, will change so that its temperature becomes uniform all along its length; during the process, the rod is not in thermal equilibrium until its temperature is uniform. In a system prepared as a block of ice floating in a bath of hot water, and then isolated, the ice can melt; during the melting, the system is not in thermal equilibrium; but eventually, its temperature will become uniform; the block of ice will not re-form. A system prepared as a mixture of petrol vapour and air can be ignited by a spark and produce carbon dioxide and water; if this happens in an isolated system, it will increase the temperature of the system, and during the increase, the system is not in thermal equilibrium; but eventually, the system will settle to a uniform temperature.\n",
      "\n",
      "\n",
      "\n",
      "What is the concept of simultaneity in Einstein's book, Relativity?\n",
      "-In physics, the relativity of simultaneity is the concept that distant simultaneity – whether two spatially separated events occur at the same time – is not absolute, but depends on the observer's reference frame. This possibility was raised by mathematician Henri Poincaré in 1900, and thereafter became a central idea in the special theory of relativity.\n",
      "-Einstein (The Meaning of Relativity): \"Two events taking place at the points A and B of a system K are simultaneous if they appear at the same instant when observed from the middle point, M, of the interval AB. Time is then defined as the ensemble of the indications of similar clocks, at rest relative to K, which register the same simultaneously.\" Einstein wrote in his book, Relativity, that simultaneity is also relative, i.e., two events that appear simultaneous to an observer in a particular inertial reference frame need not be judged as simultaneous by a second observer in a different inertial frame of reference.\n",
      "-According to the special theory of relativity introduced by Albert Einstein, it is impossible to say in an absolute sense that two distinct events occur at the same time if those events are separated in space. If one reference frame assigns precisely the same time to two events that are at different points in space, a reference frame that is moving relative to the first will generally assign different times to the two events (the only exception being when motion is exactly perpendicular to the line connecting the locations of both events).\n",
      "-The relativity of simultaneity can be demonstrated using the Lorentz transformation, which relates the coordinates used by one observer to coordinates used by another in uniform relative motion with respect to the first.\n",
      "-Absolute simultaneity refers to the concurrence of events in time at different locations in space in a manner agreed upon in all frames of reference. The theory of relativity does not have a concept of absolute time because there is a relativity of simultaneity. An event that is simultaneous with another event in one frame of reference may be in the past or future of that event in a different frame of reference,: 59  which negates absolute simultaneity.\n",
      "\n",
      "\n",
      "\n",
      "What is the Josephson effect?\n",
      "-SQUIDs, or superconducting quantum interference devices, are very sensitive magnetometers that operate via the Josephson effect. They are widely used in science and engineering.\n",
      "In precision metrology, the Josephson effect provides an exactly reproducible conversion between frequency and voltage. Since the frequency is already defined precisely and practically by the caesium standard, the Josephson effect is used, for most practical purposes, to give the standard representation of a volt, the Josephson voltage standard.\n",
      "-In 1962, Brian Josephson made the important theoretical prediction that a supercurrent can flow between two pieces of superconductor separated by a thin layer of insulator. This phenomenon, now called the Josephson effect, is exploited by superconducting devices such as SQUIDs. It is used in the most accurate available measurements of the magnetic flux quantum h/2e, and thus (coupled with the quantum Hall resistivity) for Planck's constant h. Josephson was awarded the Nobel Prize in Physics for this work in 1973.\n",
      "-In physics, the Josephson effect is a phenomenon that occurs when two superconductors are placed in proximity, with some barrier or restriction between them. It is an example of a macroscopic quantum phenomenon, where the effects of quantum mechanics are observable at ordinary, rather than atomic, scale. The Josephson effect has many practical applications because it exhibits a precise relationship between different physical measures, such as voltage and frequency, facilitating highly accurate measurements.\n",
      "-Quiterons and similar superconducting switching devices.\n",
      "Josephson effect has also been observed in superfluid helium quantum interference devices (SHeQUIDs), the superfluid helium analog of a dc-SQUID.\n",
      "-DC SQUID The DC SQUID was invented in 1964 by Robert Jaklevic, John J. Lambe, James Mercereau, and Arnold Silver of Ford Research Labs after Brian Josephson postulated the Josephson effect in 1962, and the first Josephson junction was made by John Rowell and Philip Anderson at Bell Labs in 1963. It has two Josephson junctions in parallel in a superconducting loop. It is based on the DC Josephson effect. In the absence of any external magnetic field, the input current  I splits into the two branches equally. If a small external magnetic field is applied to the superconducting loop, a screening current,  Is , begins to circulate the loop that generates the magnetic field canceling the applied external flux, and creates an additional Josephson phase which is proportional to this external magnetic flux. The induced current is in the same direction as  I in one of the branches of the superconducting loop, and is opposite to  I in the other branch; the total current becomes  I/2+Is in one branch and  I/2−Is in the other. As soon as the current in either branch exceeds the critical current,  Ic , of the Josephson junction, a voltage appears across the junction.\n",
      "\n",
      "\n",
      "\n",
      "What is the SI unit of the physical quantity m/Q?\n",
      "-The pascal can be expressed using SI derived units, or alternatively solely SI base units, as: 1Pa=1Nm2=1kgm⋅s2=1Jm3 where N is the newton, m is the metre, kg is the kilogram, s is the second, and J is the joule.One pascal is the pressure exerted by a force of magnitude one newton perpendicularly upon an area of one square metre.\n",
      "-Combinations of base and derived units may be used to express other derived units. For example, the SI unit of force is the newton (N), the SI unit of pressure is the pascal (Pa)—and the pascal can be defined as one newton per square metre (N/m2).\n",
      "-The metre per second is the unit of both speed (a scalar quantity) and velocity (a vector quantity, which has direction and magnitude) in the International System of Units (SI), equal to the speed of a body covering a distance of one metre in a time of one second.\n",
      "The SI unit symbols are m/s, m·s−1, m s−1, or m/s. Sometimes it is abbreviated as \"mps\".\n",
      "-The stability of the IPK was crucial because the kilogram underpinned much of the SI as defined and structured until 2019. The majority of SI units with special names are derived units, meaning they are defined simply multiplying or dividing or in one case offsetting relative to other, more basic, units. For instance, the newton is defined as the force necessary to accelerate one kilogram at one metre per second squared. If the mass of the IPK were to change slightly then the newton would also change proportionally. In turn, the pascal, the SI unit of pressure, is defined in terms of the newton. This chain of dependency follows to many other SI units of measure. For instance, the joule, the SI unit of energy, is defined as that expended when a force of one newton acts through one metre. Next to be affected is the SI unit of power, the watt, which is one joule per second.  N = kg m/s2 Pa = N/m2 = kg/(m s) J = N m = kg m2/s2 W = J/s = N m/s = kg m2/s3Furthermore, prior to the revision the SI base unit of electric current, the ampere (A), was defined as the current needed to produce a force of 0.2 μN between 2 parallel wires 1 m apart for every metre of length. Substituting these parameters into Ampère's force law gives: 2 kA A2/m = 0.2 μN/mor A2 = μN/10 kA,making the magnitude of the ampere proportional to the square root of the newton and hence of the mass of the IPK.  The base unit of amount of substance, mole, was defined prior to the revision as the number of atoms in 12 grams of carbon 12 and the base unit of luminous intensity, candela, was defined as that of 1/683 watts per steradian of 540 THz green light. Hence the magnitudes of the mole and candela were proportional to the mass of the IPK.\n",
      "-2019 definition: The metre, symbol m, is the SI unit of length. It is defined by taking the fixed numerical value of the speed of light in vacuum c to be 299792458 when expressed in the unit m⋅s−1, where the second is defined in terms of the caesium frequency ΔνCs.The metre may be expressed directly in terms of the defining constants: 1 m = 9192631770/299792458c/ΔνCs.\n",
      "\n",
      "\n",
      "\n",
      "How many crystallographic point groups are there in three-dimensional space?\n",
      "-Up to conjugacy the set of three-dimensional point groups consists of 7 infinite series, and 7 other individual groups. In crystallography, only those point groups are considered which preserve some crystal lattice (so their rotations may only have order 1, 2, 3, 4, or 6). This crystallographic restriction of the infinite families of general point groups results in 32 crystallographic point groups (27 individual groups from the 7 series, and 5 of the 7 other individuals).\n",
      "-§ The seven remaining point groups, which have multiple 3-or-more-fold rotation axes; these groups can also be characterized as point groups having multiple 3-fold rotation axes. The possible combinations are: Four 3-fold axes (the three tetrahedral symmetries T, Th, and Td) Four 3-fold axes and three 4-fold axes (octahedral symmetries O and Oh) Ten 3-fold axes and six 5-fold axes (icosahedral symmetries I and Ih)According to the crystallographic restriction theorem, only a limited number of point groups are compatible with discrete translational symmetry: 27 from the 7 infinite series, and 5 of the 7 others. Together, these make up the 32 so-called crystallographic point groups.\n",
      "-The following table lists several notations for point groups: Hermann–Mauguin notation (used in crystallography), Schönflies notation (used to describe molecular symmetry), orbifold notation, and Coxeter notation. The latter three are not only conveniently related to its properties, but also to the order of the group. The orbifold notation is a unified notation, also applicable for wallpaper groups and frieze groups. The crystallographic groups have n restricted to 1, 2, 3, 4, and 6; removing crystallographic restriction allows any positive integer.\n",
      "-If a group is a symmetry of a two-dimensional lattice or grid, then the crystallographic restriction theorem restricts the value of n to 1, 2, 3, 4, and 6 for both families. There are thus 10 two-dimensional crystallographic point groups: C1, C2, C3, C4, C6, D1, D2, D3, D4, D6The groups may be constructed as follows: Cn. Generated by an element also called Cn, which corresponds to a rotation by angle 2π/n. Its elements are E (the identity), Cn, Cn2, ..., Cnn−1, corresponding to rotation angles 0, 2π/n, 4π/n, ..., 2(n − 1)π/n.\n",
      "-Other C2n,h of order 4n is of abstract group type Z2n × Z2. For n = 1 we get Dih2, already covered above, so n ≥ 2.\n",
      "Thus we have, with bolding of the 2 cyclic crystallographic point groups: etc.\n",
      "Dnh of order 4n is of abstract group type Dihn × Z2. For odd n this is already covered above, so we have here D2nh of order 8n, which is of abstract group type Dih2n × Z2 (n≥1).\n",
      "Thus we have, with bolding of the 3 dihedral crystallographic point groups: etc.\n",
      "The remaining seven are, with bolding of the 5 crystallographic point groups (see also above): \n",
      "\n",
      "\n",
      "\n",
      "What is the Liouville density?\n",
      "-A classical particle has a definite position and momentum, and hence it is represented by a point in phase space. Given a collection (ensemble) of particles, the probability of finding a particle at a certain position in phase space is specified by a probability distribution, the Liouville density. This strict interpretation fails for a quantum particle, due to the uncertainty principle. Instead, the above quasiprobability Wigner distribution plays an analogous role, but does not satisfy all the properties of a conventional probability distribution; and, conversely, satisfies boundedness properties unavailable to classical distributions.\n",
      "-In physics, Liouville's theorem, named after the French mathematician Joseph Liouville, is a key theorem in classical statistical and Hamiltonian mechanics. It asserts that the phase-space distribution function is constant along the trajectories of the system—that is that the density of system points in the vicinity of a given system point traveling through phase-space is constant with time. This time-independent density is in statistical mechanics known as the classical a priori probability.There are related mathematical results in symplectic topology and ergodic theory; systems obeying Liouville's theorem are examples of incompressible dynamical systems.\n",
      "-The Liouville equation describes the time evolution of the phase space distribution function. Although the equation is usually referred to as the \"Liouville equation\", Josiah Willard Gibbs was the first to recognize the importance of this equation as the fundamental equation of statistical mechanics. It is referred to as the Liouville equation because its derivation for non-canonical systems utilises an identity first derived by Liouville in 1838.\n",
      "-In the mathematical physics of quantum mechanics, Liouville space, also known as line space, is the space of operators on Hilbert space. Liouville space is itself a Hilbert space under the Hilbert-Schmidt inner product.Abstractly, Liouville space is equivalent (isometrically isomorphic) to the tensor product of a Hilbert space with its dual. A common computational technique to organize computations in Liouville space is vectorization.Liouville space underlies the density operator formalism and is a common computation technique in the study of open quantum systems.\n",
      "-Time derivatives are denoted by dots, and are evaluated according to Hamilton's equations for the system. This equation demonstrates the conservation of density in phase space (which was Gibbs's name for the theorem). Liouville's theorem states that The distribution function is constant along any trajectory in phase space.A proof of Liouville's theorem uses the n-dimensional divergence theorem. This proof is based on the fact that the evolution of  ρ obeys an 2n-dimensional version of the continuity equation: 0.\n",
      "\n",
      "\n",
      "\n",
      "What are the four qualitative levels of crystallinity described by geologists?\n",
      "-Geologists describe four qualitative levels of crystallinity: holocrystalline rocks are completely crystalline; hypocrystalline rocks are partially crystalline, with crystals embedded in an amorphous or glassy matrix; hypohyaline rocks are partially glassy; holohyaline rocks (such as obsidian) are completely glassy.\n",
      "-Grain texture According to the texture of the grains, igneous rocks may be classified as  pegmatitic: very large crystals phaneritic: rocks contain minerals with crystals visible to the unaided eye, commonly intrusive aphanitic: rapid cooling, crystal nucleation and growth is stunted, forming a uniform, fine grained rock porphyritic: containing phenocrysts in a fine groundmass vesicular: contains voids caused by trapped gas while cooling vitreous: glassy or hyaline without crystals pyroclastic: rock formed of fragments of crystals, phenocrysts and rock fragments of a volcanic origin equigranular: rock crystals are all the same size Crystal shapes Crystal shape is also an important factor in the texture of an igneous rock. Crystals may be euhedral, subeuhedral or anhedral: Euhedral or automorphic, if the crystallographic shape is preserved.\n",
      "-Crystallinity refers to the degree of structural order in a solid. In a crystal, the atoms or molecules are arranged in a regular, periodic manner. The degree of crystallinity has a big influence on hardness, density, transparency and diffusion. In an ideal gas, the relative positions of the atoms or molecules are completely random. Amorphous materials, such as liquids and glasses, represent an intermediate case, having order over short distances (a few atomic or molecular spacings) but not over longer distances.\n",
      "-Grain texture According to the texture of the grains, igneous rocks may be classified as  pegmatitic: very large crystals phaneritic: rocks contain minerals with crystals visible to the unaided eye, commonly intrusive aphanitic: rapid cooling, crystal nucleation and growth is stunted, forming a uniform, fine grained rock porphyritic: containing phenocrysts in a fine groundmass vesicular: contains voids caused by trapped gas while cooling vitreous: glassy or hyaline without crystals pyroclastic: rock formed of fragments of crystals, phenocrysts and rock fragments of a volcanic origin equigranular: rock crystals are all the same size Crystal shapes Crystal shape is also an important factor in the texture of an igneous rock. Crystals may be euhedral, subeuhedral or anhedral: Euhedral or automorphic, if the crystallographic shape is preserved.\n",
      "-Crystallopathies can be associated with four main kinds of crystalline structures: liquid non-aggregating crystal solutions, amorphous nano-scale solid particles, crystalline micro-scale solid particles, and polycrystalline larger solid structures. They can be composed of various minerals, metabolites, proteins, and microparticles, including the following: \n",
      "\n",
      "\n",
      "\n",
      "What is an order parameter?\n",
      "-Order parameters An order parameter is a measure of the degree of order across the boundaries in a phase transition system; it normally ranges between zero in one phase (usually above the critical point) and nonzero in the other. At the critical point, the order parameter susceptibility will usually diverge.\n",
      "An example of an order parameter is the net magnetization in a ferromagnetic system undergoing a phase transition. For liquid/gas transitions, the order parameter is the difference of the densities.\n",
      "-A parameter (from Ancient Greek παρά (pará) 'beside, subsidiary', and μέτρον (métron) 'measure'), generally, is any characteristic that can help in defining or classifying a particular system (meaning an event, project, object, situation, etc.). That is, a parameter is an element of a system that is useful, or critical, when identifying the system, or when evaluating its performance, status, condition, etc.\n",
      "-From a theoretical perspective, order parameters arise from symmetry breaking. When this happens, one needs to introduce one or more extra variables to describe the state of the system. For example, in the ferromagnetic phase, one must provide the net magnetization, whose direction was spontaneously chosen when the system cooled below the Curie point. However, note that order parameters can also be defined for non-symmetry-breaking transitions.\n",
      "-Parameter has more specific meanings within various disciplines, including mathematics, computer programming, engineering, statistics, logic, linguistics, and electronic musical composition.\n",
      "In addition to its technical uses, there are also extended uses, especially in non-scientific contexts, where it is used to mean defining characteristics or boundaries, as in the phrases 'test parameters' or 'game play parameters'.\n",
      "-Essential in synergetics is the order-parameter concept which was originally introduced in the Ginzburg–Landau theory in order to describe phase transitions in thermodynamics. The order parameter concept is generalized by Haken to the \"enslaving-principle\" saying that the dynamics of fast-relaxing (stable) modes is completely determined by the 'slow' dynamics of, as a rule, only a few 'order-parameters' (unstable modes). The order parameters can be interpreted as the amplitudes of the unstable modes determining the macroscopic pattern.\n",
      "\n",
      "\n",
      "\n",
      "What is the significance of the discovery of the Crab pulsar?\n",
      "-The discovery of the Crab pulsar provided confirmation of the rotating neutron star model of pulsars. The Crab pulsar 33-millisecond pulse period was too short to be consistent with other proposed models for pulsar emission. Moreover, the Crab pulsar is so named because it is located at the center of the Crab Nebula, consistent with the 1933 prediction of Baade and Zwicky.\n",
      "-In 1968, Richard V. E. Lovelace and collaborators discovered period  33 ms of the Crab pulsar using Arecibo Observatory. After this discovery, scientists concluded that pulsars were rotating neutron stars. Before that, many scientists believed that pulsars were pulsating white dwarfs.\n",
      "-In 1965, Antony Hewish and Samuel Okoye discovered \"an unusual source of high radio brightness temperature in the Crab Nebula\". This source turned out to be the Crab Pulsar that resulted from the great supernova of 1054.\n",
      "-Neutron stars Pulsars Supernovae sometimes leave behind dense spinning neutron stars called pulsars. They emit jets of charged particles which emit synchrotron radiation in the radio spectrum. Examples include the Crab Pulsar, the first pulsar to be discovered. Pulsars and quasars (dense central cores of extremely distant galaxies) were both discovered by radio astronomers. In 2003 astronomers using the Parkes radio telescope discovered two pulsars orbiting each other, the first such system known.\n",
      "-The discovery of pulsars allowed astronomers to study an object never observed before, the neutron star. This kind of object is the only place where the behavior of matter at nuclear density can be observed (though not directly). Also, millisecond pulsars have allowed a test of general relativity in conditions of an intense gravitational field.\n",
      "\n",
      "\n",
      "\n",
      "What is the De Haas-Van Alphen effect?\n",
      "-Several experimental techniques allow for the measurement of the electronic properties of a material. An important effect in metals under strong magnetic fields, is the oscillation of the differential susceptibility as function of 1/H. This behaviour is known as the De Haas–Van Alphen effect and relates the period of the susceptibility with the Fermi surface of the material.\n",
      "An analogue non-linear relation between magnetization and magnetic field happens for antiferromagnetic materials.\n",
      "-The differential magnetic susceptibility of a material is defined as χ=∂M∂H where  H is the applied external magnetic field and  M the magnetization of the material. Such that  B=μ0(H+M) , where  μ0 is the vacuum permeability. For practical purposes, the applied and the measured field are approximately the same  B≈μ0H (if the material is not ferromagnetic).\n",
      "-The De Haas–Van Alphen effect, often abbreviated to DHVA, is a quantum mechanical effect in which the magnetic susceptibility of a pure metal crystal oscillates as the intensity of the magnetic field B is increased. It can be used to determine the Fermi surface of a material. Other quantities also oscillate, such as the electrical resistivity (Shubnikov–de Haas effect), specific heat, and sound attenuation and speed. It is named after Wander Johannes de Haas and his student Pieter M. van Alphen. The DHVA effect comes from the orbital motion of itinerant electrons in the material. An equivalent phenomenon at low magnetic fields is known as Landau diamagnetism.\n",
      "-Experimentally it was discovered in 1930 by W.J. de Haas and P.M. van Alphen under careful study of the magnetization of a single crystal of bismuth. The magnetization oscillated as a function of the field. The inspiration for the experiment was the recently discovered Shubnikov–de Haas effect by Lev Shubnikov and De Haas, which showed oscillations of the electrical resistivity as function of a strong magnetic field. De Haas thought that the magnetoresistance should behave in an analogous way.The theoretical prediction of the phenomenon was formulated before the experiment, in the same year, by Lev Landau, but he discarded it as he thought that the magnetic fields necessary for its demonstration could not yet be created in a laboratory. The effect was described mathematically using Landau quantization of the electron energies in an applied magnetic field. A strong homogeneous magnetic field — typically several teslas — and a low temperature are required to cause a material to exhibit the DHVA effect. Later in life, in private discussion, David Shoenberg asked Landau why he thought that an experimental demonstration was not possible. He answered by saying that Pyotr Kapitsa, Shoenberg's advisor, had convinced him that such homogeneity in the field was impractical.After the 1950s, the DHVA effect gained wider relevance after Lars Onsager (1952), and independently, Ilya Lifshitz and Arnold Kosevich (1954), pointed out that the phenomenon could be used to image the Fermi surface of a metal. In 1954, Lifshitz and Aleksei Pogorelov determined the range of applicability of the theory and described how to determine the shape of any arbitrary convex Fermi surface by measuring the extremal sections. Lifshitz and Pogorelov also found a relation between the temperature dependence of the oscillations and the cyclotron mass of an electron.By the 1970s the Fermi surface of most metallic elements had been reconstructed using De Haas–Van Alphen and Shubnikov–de Haas effects. Other techniques to study the Fermi surface have appeared since like the angle-resolved photoemission spectroscopy (ARPES).\n",
      "-The oscillations of the differential susceptibility when plotted against  1/B , have a period  P (in teslas−1) that is inversely proportional to the area  S of the extremal orbit of the Fermi surface (m−2), in the direction of the applied field, that is P(B−1)=2πeℏS ,where  ℏ is Planck constant and  e is the elementary charge. The existence of more than one extremal orbit leads to multiple periods becoming superimposed. A more precise formula, known as Lifshitz–Kosevich formula, can be obtained using semiclassical approximations.The modern formulation allows the experimental determination of the Fermi surface of a metal from measurements performed with different orientations of the magnetic field around the sample.\n",
      "\n",
      "\n",
      "\n",
      "What is a \"coffee ring\" in physics?\n",
      "-In physics, a \"coffee ring\" is a pattern left by a puddle of particle-laden liquid after it evaporates. The phenomenon is named for the characteristic ring-like deposit along the perimeter of a spill of coffee. It is also commonly seen after spilling red wine. The mechanism behind the formation of these and similar rings is known as the coffee ring effect or in some instances, the coffee stain effect, or simply ring stain.\n",
      "-The coffee ring effect is utilized in convective deposition by researchers wanting to order particles on a substrate using capillary-driven assembly, replacing a stationary droplet with an advancing meniscus drawn across the substrate. This process differs from dip-coating in that evaporation drives flow along the substrate as opposed to gravity.\n",
      "-The coffee-ring pattern originates from the capillary flow induced by the evaporation of the drop: liquid evaporating from the edge is replenished by liquid from the interior. The resulting current can carry nearly all the dispersed material to the edge. As a function of time, this process exhibits a \"rush-hour\" effect, that is, a rapid acceleration of the flow towards the edge at the final stage of the drying process.Evaporation induces a Marangoni flow inside a droplet. The flow, if strong, redistributes particles back to the center of the droplet. Thus, for particles to accumulate at the edges, the liquid must have a weak Marangoni flow, or something must occur to disrupt the flow. For example, surfactants can be added to reduce the liquid's surface tension gradient, disrupting the induced flow. Water has a weak Marangoni flow to begin with, which is then reduced significantly by natural surfactants.Interaction of the particles suspended in a droplet with the free surface of the droplet is important in creating a coffee ring. \"When the drop evaporates, the free surface collapses and traps the suspended particles ... eventually all the particles are captured by the free surface and stay there for the rest of their trip towards the edge of the drop.\" This result means that surfactants can be used to manipulate the motion of the solute particles by changing the surface tension of the drop, rather than trying to control the bulk flow inside the drop. A number of interesting morphologies of the deposited particles can result. For example, an enantiopure poly (isocyanate) derivative has been shown to form ordered arrays of squashed donut structures.\n",
      "-The lower-limit size of a coffee ring depends on the time scale competition between the liquid evaporation and the movement of suspended particles. When the liquid evaporates much faster than the particle movement near a three-phase contact line, coffee ring cannot be formed successfully. Instead, these particles will disperse uniformly on a surface upon complete liquid evaporation. For suspended particles of size 100 nm, the minimum diameter of the coffee ring structure is found to be 10 μm, or about 10 times smaller than the width of human hair. The shape of particles in the liquid is responsible for coffee ring effect. On porous substrates, the competition among infiltration, particle motion and evaporation of the solvent governs the final deposition morphology.The pH of the solution of the drop influences the final deposit pattern. The transition between these patterns is explained by considering how DLVO interactions such as the electrostatic and Van der Waals forces modify the particle deposition process.\n",
      "-The method for making coffee in a percolator had changed very little since the introduction of the electric percolator in the early part of the 20th century. However, in 1970 commercially available \"ground coffee filter rings\" were introduced to the market. The coffee filter rings were designed for use in percolators, and each ring contained a pre-measured amount of coffee grounds that were sealed in a self-contained paper filter. The sealed rings resembled the shape of a doughnut, and the small hole in the middle of the ring enabled the coffee filter ring to be placed in the metal percolator basket around the protruding convection (percolator) tube.\n",
      "\n",
      "\n",
      "\n",
      "What is the significance of probability amplitudes in quantum mechanics?\n",
      "-In quantum mechanics, a probability amplitude is a complex number used for describing the behaviour of systems. The modulus squared of this quantity represents a probability density.\n",
      "-If the standard measure μ on X is non-atomic, such as the Lebesgue measure on the real line, or on three-dimensional space, or similar measures on manifolds, then a real-valued function |ψ(x)|2 is called a probability density; see details below. If the standard measure on X consists of atoms only (we shall call such sets X discrete), and specifies the measure of any x ∈ X equal to 1, then an integral over X is simply a sum and |ψ(x)|2 defines the value of the probability measure on the set {x}, in other words, the probability that the quantum system is in the state x. How amplitudes and the vector are related can be understood with the standard basis of L2(X), elements of which will be denoted by |x⟩ or ⟨x| (see bra–ket notation for the angle bracket notation). In this basis specifies the coordinate presentation of an abstract vector |Ψ⟩.\n",
      "-Generally, it is the case when the motion of a particle is described in the position space, where the corresponding probability amplitude function ψ is the wave function.\n",
      "-Probability amplitudes provide a relationship between the quantum state vector of a system and the results of observations of that system, a link was first proposed by Max Born, in 1926. Interpretation of values of a wave function as the probability amplitude is a pillar of the Copenhagen interpretation of quantum mechanics. In fact, the properties of the space of wave functions were being used to make physical predictions (such as emissions from atoms being at certain discrete energies) before any physical interpretation of a particular function was offered. Born was awarded half of the 1954 Nobel Prize in Physics for this understanding, and the probability thus calculated is sometimes called the \"Born probability\". These probabilistic concepts, namely the probability density and quantum measurements, were vigorously contested at the time by the original physicists working on the theory, such as Schrödinger and Einstein. It is the source of the mysterious consequences and philosophical difficulties in the interpretations of quantum mechanics—topics that continue to be debated even today.\n",
      "-Physical Neglecting some technical complexities, the problem of quantum measurement is the behaviour of a quantum state, for which the value of the observable Q to be measured is uncertain. Such a state is thought to be a coherent superposition of the observable's eigenstates, states on which the value of the observable is uniquely defined, for different possible values of the observable.\n",
      "\n",
      "\n",
      "\n",
      "What is the relationship between the amplitude of a sound wave and its loudness?\n",
      "-The amplitude of sound waves and audio signals (which relates to the volume) conventionally refers to the amplitude of the air pressure in the wave, but sometimes the amplitude of the displacement (movements of the air or the diaphragm of a speaker) is described. The logarithm of the amplitude squared is usually quoted in dB, so a null amplitude corresponds to −∞ dB. Loudness is related to amplitude and intensity and is one of the most salient qualities of a sound, although in general sounds it can be recognized independently of amplitude. The square of the amplitude is proportional to the intensity of the wave.\n",
      "-Sound intensity level is a logarithmic expression of sound intensity relative to a reference intensity.\n",
      "-Sound intensity, denoted I, is defined by where p is the sound pressure; v is the particle velocity.Both I and v are vectors, which means that both have a direction as well as a magnitude. The direction of sound intensity is the average direction in which energy is flowing.\n",
      "The average sound intensity during time T is given by For a plane wave, Where, ν is frequency of sound, δ is the amplitude of the sound wave particle displacement, ρ is density of medium in which sound is traveling, and c is speed of sound.\n",
      "-Sound intensity, also known as acoustic intensity, is defined as the power carried by sound waves per unit area in a direction perpendicular to that area. The SI unit of intensity, which includes sound intensity, is the watt per square meter (W/m2). One application is the noise measurement of sound intensity in the air at a listener's location as a sound energy quantity.Sound intensity is not the same physical quantity as sound pressure. Human hearing is sensitive to sound pressure which is related to sound intensity. In consumer audio electronics, the level differences are called \"intensity\" differences, but sound intensity is a specifically defined quantity and cannot be sensed by a simple microphone.\n",
      "-The sound field is described by physical quantities, while auditory events are described by quantities of psychoacoustical perception.  Below you can find a list with physical sound field quantities and the related psychoacoustical quantities of corresponding auditory events.\n",
      "Mostly there is no simple or proportional relationship between sound field characteristics and auditory events.  For example, the auditory event property loudness depends not only on the physical quantity sound pressure but also on the spectral characteristics of the sound and on the sound history.\n",
      "\n",
      "\n",
      "\n",
      "What are coherent turbulent structures?\n",
      "-Turbulent flows are complex multi-scale and chaotic motions that need to be classified into more elementary components, referred to coherent turbulent structures. Such a structure must have temporal coherence, i.e. it must persist in its form for long enough periods that the methods of time-averaged statistics can be applied. Coherent structures are typically studied on very large scales, but can be broken down into more elementary structures with coherent properties of their own, such examples include hairpin vortices. Hairpins and coherent structures have been studied and noticed in data since the 1930s, and have been since cited in thousands of scientific papers and reviews.\n",
      "-A turbulent flow is a flow regime in fluid dynamics where fluid velocity varies significantly and irregularly in both position and time. Furthermore, a coherent structure is defined as a turbulent flow whose vorticity expression, which is usually stochastic, contains orderly components that can be described as being instantaneously coherent over the spatial extent of the flow structure. In other words, underlying the three-dimensional chaotic vorticity expressions typical of turbulent flows, there is an organized component of that vorticity which is phase-correlated over the entire space of the structure. The instantaneously space and phase correlated vorticity found within the coherent structure expressions can be defined as coherent vorticity, hence making coherent vorticity the main characteristic identifier for coherent structures. Another characteristic inherent in turbulent flows is their intermittency, but intermittency is a very poor identifier of the boundaries of a coherent structure, hence it is generally accepted that the best way to characterize the boundary of a structure is by identifying and defining the boundary of the coherent vorticity.By defining and identifying coherent structure in this manner, turbulent flows can be decomposed into coherent structures and incoherent structures depending on their coherence, particularly their correlations with their vorticity. Hence, similarly organized events in an ensemble average of organized events can be defined as a coherent structure, and whatever events not identified as similar or phase and space aligned in the ensemble average is an incoherent turbulent structure.  Other attempts at defining a coherent structure can be done through examining the correlation between their momenta or pressure and their turbulent flows. However, it often leads to false indications of turbulence, since pressure and velocity fluctuations over a fluid could be well correlated in the absence of any turbulence or vorticity. Some coherent structures, such as vortex rings, etc. can be large-scale motions comparable to the extent of the shear flow. There are also coherent motions at much smaller scales such as hairpin vortices and typical eddies, which are typically known as coherent substructures, as in coherent structures which can be broken up into smaller more elementary substructures.\n",
      "-Coherent structures form due to some sort of instability, e.g. the Kelvin–Helmholtz instability. Identifying an instability, and hence the initial formation of a coherent structure, requires the knowledge of initial conditions of the flow structure. Hence, documentation of the initial condition is essential for capturing the evolution and interactions of coherent structures, since initial conditions are quite variable. Overlooking the initial conditions was common in early studies due to researchers overlooking their significance. Initial conditions include the mean velocity profile, thickness, shape, the probability densities of velocity and momentum, the spectrum of Reynolds stress values, etc. These measures of initial flow conditions can be organized and grouped into three broad categories: laminar, highly disturbed, and fully turbulent.Out of the three categories, coherent structures typically arise from instabilities in laminar or turbulent states. After an initial triggering, their growth is determined by evolutionary changes due to non-linear interactions with other coherent structures, or their decay onto incoherent turbulent structures. Observed rapid changes lead to the belief that there must be a regenerative cycle that takes place during decay. For example, after a structure decays, the result may be that the flow is now turbulent and becomes susceptible to a new instability determined by the new flow state, leading to a new coherent structure being formed. It is also possible that structures do not decay and instead distort by splitting into substructures or interacting with other coherent structures.\n",
      "-Although a coherent structure is by definition characterized by high levels of coherent vorticity, Reynolds stress, production, and heat and mass transportation, it does not necessarily require a high level of kinetic energy. In fact, one of the main roles of coherent structures is the large-scale transport of mass, heat, and momentum without requiring the high amounts of energy normally needed. Consequently, this implies that coherent structures are not the main production and cause of Reynolds stress, and incoherent turbulence can be similarly significant.Coherent structures cannot superimpose, i.e. they cannot overlap and each coherent structure has its own independent domain and boundary. Since eddies coexist as spatial superpositions, a coherent structure is not an eddy. For example, eddies dissipate energy by obtaining energy from the mean flow at large scales, and eventually dissipating it at the smallest scales. There is no such analogous exchange of energy between coherent structures, and any interaction such as tearing between coherent structures simply results in a new structure. However, two coherent structures can interact and influence each other. The mass of a structure change with time, with the typical case being that structures increase in volume via the diffusion of vorticity.  One of the most fundamental quantities of coherent structures is characterized by coherent vorticity,  Ωc . Perhaps the next most critical measures of coherent structures are the coherent vs. incoherent Reynold's stresses,  −ucνc and  −⟨urνr⟩ . These represent the transports of momentum, and their relative strength indicates how much momentum is being transported by coherent structures as compared to incoherent structures. The next most significant measures include contoured depictions of coherent strain rate and shear production. A useful property of such contours is that they are invariant under Galilean transformations, hence the contours of coherent vorticity constitute an excellent identifier to the structure's boundaries. The contours of these properties not only locate where exactly coherent structure quantities have their peaks and saddles, but also identify where the incoherent turbulent structures are when overlaid on their directional gradients. In addition, spatial contours can be drawn describe the shape, size, and strength of the coherent structures, depicting not only the mechanics but also the dynamical evolution of coherent structures. For example, in order for a structure to be evolving, and hence dominant, its coherent vorticity, coherent Reynolds stress, and production terms should be larger than the time averaged values of the flow structures.\n",
      "-Lagrangian Coherent Structures Lagrangian coherent structures (LCSs) are influential material surfaces that create clearly recognizable patterns in passive tracer distributions advected by an unsteady flow. LCSs can be classified as hyperbolic (locally maximally attracting or repelling material surfaces), elliptic (material vortex boundaries), and parabolic (material jet cores). These surfaces are generalizations of classical invariant manifolds, known in dynamical systems theory, to finite-time unsteady flow data. This Lagrangian perspective on coherence is concerned with structures formed by fluid elements, as opposed to the Eulerian notion of coherence, which considers features in the instantaneous velocity field of the fluid. Various mathematical techniques have been developed to identify LCSs in two- and three-dimenisonal data sets, and have been applied to laboratory experiments, numerical simulations and geophysical observations.\n",
      "\n",
      "\n",
      "\n",
      "What is the main factor that determines the occurrence of each type of supernova?\n",
      "-Progenitor The supernova classification type is closely tied to the type of star at the time of the collapse. The occurrence of each type of supernova depends on the progenitor star's metallicity, since this affects the strength of the stellar wind and thereby the rate at which the star loses mass.Type Ia supernovae are produced from white dwarf stars in binary star systems and occur in all galaxy types. Core collapse supernovae are only found in galaxies undergoing current or very recent star formation, since they result from short-lived massive stars. They are most commonly found in type Sc spirals, but also in the arms of other spiral galaxies and in irregular galaxies, especially starburst galaxies.Type Ib and Ic supernovae are hypothesized to have been produced by core collapse of massive stars that have lost their outer layer of hydrogen and helium, either via strong stellar winds or mass transfer to a companion. They normally occur in regions of new star formation, and are extremely rare in elliptical galaxies. The progenitors of type IIn supernovae also have high rates of mass loss in the period just prior to their explosions. Type Ic supernovae have been observed to occur in regions that are more metal-rich and have higher star-formation rates than average for their host galaxies. The table shows the progenitor for the main types of core collapse supernova, and the approximate proportions that have been observed in the local neighbourhood.\n",
      "-A sufficiently large and hot stellar core may generate gamma-rays energetic enough to initiate photodisintegration directly, which will cause a complete collapse of the core.The table below lists the known reasons for core collapse in massive stars, the types of stars in which they occur, their associated supernova type, and the remnant produced. The metallicity is the proportion of elements other than hydrogen or helium, as compared to the Sun. The initial mass is the mass of the star prior to the supernova event, given in multiples of the Sun's mass, although the mass at the time of the supernova may be much lower.Type IIn supernovae are not listed in the table. They can be produced by various types of core collapse in different progenitor stars, possibly even by type Ia white dwarf ignitions, although it seems that most will be from iron core collapse in luminous supergiants or hypergiants (including LBVs). The narrow spectral lines for which they are named occur because the supernova is expanding into a small dense cloud of circumstellar material. It appears that a significant proportion of supposed type IIn supernovae are supernova impostors, massive eruptions of LBV-like stars similar to the Great Eruption of Eta Carinae. In these events, material previously ejected from the star creates the narrow absorption lines and causes a shock wave through interaction with the newly ejected material.\n",
      "-A Type II supernova (plural: supernovae or supernovas) results from the rapid collapse and violent explosion of a massive star. A star must have at least eight times, but no more than 40 to 50 times, the mass of the Sun (M☉) to undergo this type of explosion. Type II supernovae are distinguished from other types of supernovae by the presence of hydrogen in their spectra. They are usually observed in the spiral arms of galaxies and in H II regions, but not in elliptical galaxies; those are generally composed of older, low-mass stars, with few of the young, very massive stars necessary to cause a supernova.\n",
      "-The Big Bang produced hydrogen, helium, and traces of lithium, while all heavier elements are synthesised in stars, supernovae, and collisions between neutron stars (thus being indirectly due to supernovae). Supernovae tend to enrich the surrounding interstellar medium with elements other than hydrogen and helium, which usually astronomers refer to as \"metals\". These ejected elements ultimately enrich the molecular clouds that are the sites of star formation. Thus, each stellar generation has a slightly different composition, going from an almost pure mixture of hydrogen and helium to a more metal-rich composition. Supernovae are the dominant mechanism for distributing these heavier elements, which are formed in a star during its period of nuclear fusion. The different abundances of elements in the material that forms a star have important influences on the star's life, and may influence the possibility of having planets orbiting it: more giant planets form around stars of higher metallicity.The kinetic energy of an expanding supernova remnant can trigger star formation by compressing nearby, dense molecular clouds in space. The increase in turbulent pressure can also prevent star formation if the cloud is unable to lose the excess energy.Evidence from daughter products of short-lived radioactive isotopes shows that a nearby supernova helped determine the composition of the Solar System 4.5 billion years ago, and may even have triggered the formation of this system.Fast Radio Bursts (FRBs) are intense, transient pulses of radio waves that typically last no more than milliseconds. Many explanations for these events have been proposed; magnetars produced by core-collapse supernovae are leading candidates.\n",
      "-Unlike the other types of supernovae, Type Ia supernovae generally occur in all types of galaxies, including ellipticals. They show no preference for regions of current stellar formation. As white dwarf stars form at the end of a star's main sequence evolutionary period, such a long-lived star system may have wandered far from the region where it originally formed. Thereafter a close binary system may spend another million years in the mass transfer stage (possibly forming persistent nova outbursts) before the conditions are ripe for a Type Ia supernova to occur.A long-standing problem in astronomy has been the identification of supernova progenitors. Direct observation of a progenitor would provide useful constraints on supernova models. As of 2006, the search for such a progenitor had been ongoing for longer than a century. Observation of the supernova SN 2011fe has provided useful constraints. Previous observations with the Hubble Space Telescope did not show a star at the position of the event, thereby excluding a red giant as the source. The expanding plasma from the explosion was found to contain carbon and oxygen, making it likely the progenitor was a white dwarf primarily composed of these elements.\n",
      "\n",
      "\n",
      "\n",
      "What is the Erlangen program?\n",
      "-In mathematics, the Erlangen program is a method of characterizing geometries based on group theory and projective geometry. It was published by Felix Klein in 1872 as Vergleichende Betrachtungen über neuere geometrische Forschungen. It is named after the University Erlangen-Nürnberg, where Klein worked.\n",
      "-In the seminal paper which introduced categories, Saunders Mac Lane and Samuel Eilenberg stated: \"This may be regarded as a continuation of the Klein Erlanger Program, in the sense that a geometrical space with its group of transformations is generalized to a category with its algebra of mappings.\"Relations of the Erlangen program with work of Charles Ehresmann on groupoids in geometry is considered in the article below by Pradines.In mathematical logic, the Erlangen program also served as an inspiration for Alfred Tarski in his analysis of logical notions.\n",
      "-When topology is routinely described in terms of properties invariant under homeomorphism, one can see the underlying idea in operation. The groups involved will be infinite-dimensional in almost all cases – and not Lie groups – but the philosophy is the same. Of course this mostly speaks to the pedagogical influence of Klein. Books such as those by H.S.M. Coxeter routinely used the Erlangen program approach to help 'place' geometries. In pedagogic terms, the program became transformation geometry, a mixed blessing in the sense that it builds on stronger intuitions than the style of Euclid, but is less easily converted into a logical system.\n",
      "-Quite often, it appears there are two or more distinct geometries with isomorphic automorphism groups. There arises the question of reading the Erlangen program from the abstract group, to the geometry.\n",
      "-The long-term effects of the Erlangen program can be seen all over pure mathematics (see tacit use at congruence (geometry), for example); and the idea of transformations and of synthesis using groups of symmetry has become standard in physics.\n",
      "\n",
      "\n",
      "\n",
      "What is emissivity?\n",
      "-Emissivity is the value given to materials based on the ratio of heat emitted compared to a perfect black body, on a scale from zero to one. A black body would have an emissivity of 1 and a perfect reflector would have a value of 0.\n",
      "-Emissivity is a term that is often misunderstood and misused. It represents a material's ability to emit thermal radiation and is an optical property of matter.\n",
      "-Emittance Emittance (or emissive power) is the total amount of thermal energy emitted per unit area per unit time for all possible wavelengths. Emissivity of a body at a given temperature is the ratio of the total emissive power of a body to the total emissive power of a perfectly black body at that temperature. Following Planck's law, the total energy radiated increases with temperature while the peak of the emission spectrum shifts to shorter wavelengths. The energy emitted at shorter wavelengths increases more rapidly with temperature. For example, an ideal blackbody in thermal equilibrium at 1,273 K (1,000 °C; 1,832 °F), will emit 97% of its energy at wavelengths below 14 μm.The term emissivity is generally used to describe a simple, homogeneous surface such as silver. Similar terms, emittance and thermal emittance, are used to describe thermal radiation measurements on complex surfaces such as insulation products.\n",
      "-The thermal emissivity of various surfaces is listed in the following table.\n",
      "-In its most general form, emissivity can specified for a particular wavelength, direction, and polarization.  However, the form of emissivity that most commonly used is the hemispherical total emissivity, which considers emissions as totaled over all wavelengths, directions, and polarizations, given a particular temperature.: 60 Some specific forms of emissivity are detailed below.\n",
      "Hemispherical emissivity Hemispherical emissivity of a surface, denoted ε, is defined as ε=MeMe∘, where Me is the radiant exitance of that surface; Me° is the radiant exitance of a black body at the same temperature as that surface.\n",
      "\n",
      "\n",
      "\n",
      "Who was the first person to describe the pulmonary circulation system?\n",
      "-The pulmonary circulation is archaically known as the \"lesser circulation\" which is still used in non-English literature.The discovery of the pulmonary circulation has been attributed to many scientists with credit distributed in varying ratios by varying sources. In much of modern medical literature, the discovery is credited to English physician William Harvey (1578 – 1657 CE) based on the comprehensive completeness and correctness of his model, despite its relative recency. Other sources credit Greek philosopher Hippocrates (460 – 370 BCE), Spanish physician Michael Servetus (c. 1509 – 1553 CE), Arab physician Ibn al-Nafis (1213 – 1288 CE), and Syrian physician Qusta ibn Luqa. Several figures such as Hippocrates and al-Nafis receive credit for accurately predicting or developing specific elements of the modern model of pulmonary circulation: Hippocrates for being the first to describe pulmonary circulation as a discrete system separable from systemic circulation as a whole and al-Nafis for making great strides over the understanding of those before him and towards a rigorous model. There is a great deal of subjectivity involved in deciding at which point a complex system is \"discovered\", as it is typically elucidated in piecemeal form so that the very first description, most complete or accurate description, and the most significant forward leaps in understanding are all considered acts of discovery of varying significance.Primitive descriptions of the cardiovascular system are found throughout several ancient cultures. The earliest known description of the role of air in circulation was produced in Egypt in 3500 BCE. At the time, the Egyptians believed that the heart was the origin of many channels that connected different parts of the body to each other and transported air – as well as urine, blood, and the soul – between them. The Edwin Smith Papyrus (1700 BCE), named for American Egyptologist Edwin Smith (1822 – 1906 CE) who purchased the scroll in 1862, provided evidence that Egyptians believed that the heartbeat created a pulse that transported the above substances throughout the body. A second scroll, the Ebers Papyrus (c. 1550 BCE), also emphasized the importance of the heart and its connection to vessels throughout the body and described methods to detect cardiac disease through pulse abnormalities. Although they had knowledge of the heartbeat, vessels, and pulse, the Egyptians attributed the movement of substances through the vessels to air that resided in these channels, rather than to the heart's exertion of pressure. The Egyptians knew that air played an important role in circulation but did not yet have a conception of the role of the lungs.\n",
      "-The next addition to the historical understanding of pulmonary circulation arrived with the Ancient Greeks. Physician Alcmaeon (520 – 450 BCE) proposed that the brain, not the heart, was the connection point for all of the vessels in the body. He believed that the function of these vessels was to bring the \"spirit\" (\"pneuma\") and air to the brain. Empedocles (492 – 432 BCE), a philosopher, proposed a series of pipes, impermeable to blood but continuous with blood vessels, that carried the pneuma throughout the body. He proposed that this spirit was internalized by pulmonary respiration.Hippocrates was the first to describe pulmonary circulation as a discrete system, separable from systemic circulation, in his Corpus Hippocraticum, which is often regarded as the foundational text of modern medicine. Hippocrates developed the view that the liver and spleen produced blood, and that this traveled to the heart to be cooled by the lungs that surrounded it. He described the heart as having two ventricles connected by an interventricular septum, and depicted the heart as the nexus point of all of the vessels of the body. He proposed that some vessels carried only blood and that others carried only air. He hypothesized that these air-carrying vessels were divisible into the pulmonary veins, which carried in air to the left ventricle, and the pulmonary artery, which carried in air to the right ventricle and blood to the lungs. He also proposed the existence of two atria of the heart functioning to capture air. He was one of the first to begin to accurately describe the anatomy of the heart and to describe the involvement of the lungs in circulation. His descriptions built substantially on previous and contemporaneous efforts but, by modern standards, his conceptions of pulmonary circulation and of the functions of the parts of the heart were still largely inaccurate.Greek philosopher and scientist Aristotle (384 – 322 BCE) followed Hippocrates and proposed that the heart had three ventricles, rather than two, that all connected to the lungs. Greek physician Erasistratus (315 – 240 BCE) agreed with Hippocrates and Aristotle that the heart was the origin of all of the vessels in the body but proposed a system in which air was drawn into the lungs and traveled to the left ventricle via pulmonary veins. It was transformed there into the pneuma and distributed throughout the body by arteries, which contained only air. In this system, veins distributed blood throughout the body, and thus blood did not circulate, but rather was consumed by the organs.The Greek physician Galen (129 – c. 210 CE) provided the next insights into pulmonary circulation. Though many of his theories, like those of his predecessors, were marginally or completely incorrect, his theory of pulmonary circulation dominated the medical community's understanding for hundreds of years after his death. Galen contradicted Erasistratus before him by proposing that arteries carried both air and blood, rather than air alone (which was essentially correct, leaving aside that blood vessels carry constituents of air and not air itself). He proposed that the liver was the originating point of all blood vessels. He also theorized that the heart was not a pumping muscle but rather an organ through which blood passed. Galen's theory included a new description of pulmonary circulation: air was inhaled into the lungs where it became the pneuma. Pulmonary veins transmitted this pneuma to the left ventricle of the heart to cool the blood simultaneously arriving there. This mixture of pneuma, blood, and cooling produced the vital spirits that could then be transported throughout the body via arteries. Galen further proposed that the heat of the blood arriving in the heart produced noxious vapors that were expelled through the same pulmonary veins that first brought the pneuma. He wrote that the right ventricle played a different role to the left: it transported blood to the lungs where the impurities were vented out so that clean blood could be distributed throughout the body. Though Galen's description of the anatomy of the heart was more complete than those of his predecessors, it included several mistakes. Most notably, Galen believed that blood flowed between the two ventricles of the heart through small, invisible pores in the interventricular septum.The next significant developments in the understanding of pulmonary circulation did not arrive until centuries later. Persian polymath Avicenna (c. 980 – 1037 CE) wrote a medical encyclopedia entitled The Canon of Medicine. In it, he translated and compiled contemporary medical knowledge and added some new information of his own. However, Avicenna's description of pulmonary circulation reflected the incorrect views of Galen.The Arab physician, Ibn al-Nafis, wrote the Commentary on Anatomy in Avicenna's Canon in 1242 in which he provided possibly the first known description of the system that remains substantially congruent with modern understandings, in spite of its flaws. Ibn al-Nafis made two key improvements on Galen's ideas. First, he disproved the existence of the pores in the interventricular septum that Galen had believed allowed blood to flow between the left and right ventricles. Second, he surmised that the only way for blood to get from the right to the left ventricle in the absence of interventricular pores was a system like pulmonary circulation. He also described the anatomy of the lungs in clear and basically correct detail, which his predecessors had not. However, like Aristotle and Galen, al-Nafis still believed in the quasi-mythical concept of vital spirit and that it was formed in the left ventricle from a mixture of blood and air. Despite the enormity of Ibn al-Nafis's improvements on the theories that preceded him, his commentary on The Canon was not widely known to Western scholars until the manuscript was discovered in Berlin, Germany, in 1924. As a result, the ongoing debate among Western scholars as to how credit for the discovery should be apportioned failed to include Ibn al-Nafis until, at earliest, the mid-20th century (shortly after which he came to enjoy a share of this credit). In 2021, several researchers described a text predating the work of al-Nafis, fargh- beyn-roh va nafs, in which there is a comparable report on pulmonary circulation. The researchers argue that its author, Qusta ibn Luqa, is the best candidate for the discoverer of pulmonary circulation on a similar basis to arguments in favour of al-Nafis generally.It took centuries for other scientists and physicians to reach conclusions that were similar to and then more accurate than those of al-Nafis and ibn Luqa. This later progress, constituting the gap between medieval and modern understanding, occurred throughout Europe. Italian polymath Leonardo da Vinci (1452 – 1519 CE) was one of the first to propose that the heart was just a muscle, rather than a vessel of spirits and air, but he still subscribed to Galen's ideas of circulation and defended the existence of interventricular pores. The Flemish physician Andreas Vesalius (1514 – 1564 CE) published corrections to Galen's view of circulatory anatomy, questioning the existence of interventricular pores, in his book De humani corporis fabrica libri septem in 1543. Spanish Michael Servetus, after him, was the first European physician to accurately describe pulmonary circulation. His assertions largely matched those of al-Nafis. In subsequent centuries, he has frequently been credited with the discovery, but some historians have propounded the idea that he potentially had access to Ibn al-Nafis's work while writing his own texts. Servetus published his findings in Christianismi Restituto (1553): a theological work that was considered heretical by Catholics and Calvinists alike. As a result, both book and author were burned at the stake and only a few copies of his work survived. Italian physician Realdo Colombo (c. 1515 – 1559 CE) published a book, De re anatomica libri XV, in 1559 that accurately described pulmonary circulation. It is still a matter of debate among historians as to whether Colombo reached his conclusions alone or based them to an unknown degree on the works of al-Nafis and Servetus. Finally, in 1628, the influential British physician William Harvey (1578 – 1657 AD) provided at the time the most complete and accurate description of pulmonary circulation of any scholar worldwide in his treatise Exercitatio Anatomica de Motu Cordis et Sanguinis in Animalibus. At the macroscopic level, his model is still recognizable in and reconcilable with modern understandings of pulmonary circulation.\n",
      "-In addition, Ibn al-Nafis had an insight into what would become a larger theory of the capillary circulation. He stated that \"there must be small communications or pores (manafidh in Arabic) between the pulmonary artery and vein,\" a prediction that preceded the discovery of the capillary system by more than 400 years. Ibn al-Nafis' theory, however, was confined to blood transit in the lungs and did not extend to the entire body.\n",
      "-In 1025, The Canon of Medicine by the Persian physician, Avicenna, \"erroneously accepted the Greek notion regarding the existence of a hole in the ventricular septum by which the blood traveled between the ventricles.\" Despite this, Avicenna \"correctly wrote on the cardiac cycles and valvular function\", and \"had a vision of blood circulation\" in his Treatise on Pulse. While also refining Galen's erroneous theory of the pulse, Avicenna provided the first correct explanation of pulsation: \"Every beat of the pulse comprises two movements and two pauses. Thus, expansion : pause : contraction : pause. [...] The pulse is a movement in the heart and arteries ... which takes the form of alternate expansion and contraction.\"In 1242, the Arabian physician, Ibn al-Nafis described the process of pulmonary circulation in greater, more accurate detail than his predecessors, though he believed, as they did, in the notion of vital spirit (pneuma), which he believed was formed in the left ventricle. Ibn al-Nafis stated in his Commentary on Anatomy in Avicenna's Canon: ...the blood from the right chamber of the heart must arrive at the left chamber but there is no direct pathway between them. The thick septum of the heart is not perforated and does not have visible pores as some people thought or invisible pores as Galen thought. The blood from the right chamber must flow through the vena arteriosa (pulmonary artery) to the lungs, spread through its substances, be mingled there with air, pass through the arteria venosa (pulmonary vein) to reach the left chamber of the heart and there form the vital spirit...\n",
      "-Pre-modern The earliest descriptions of the coronary and pulmonary circulation systems can be found in the Commentary on Anatomy in Avicenna's Canon, published in 1242 by Ibn al-Nafis. In his manuscript, al-Nafis wrote that blood passes through the pulmonary circulation instead of moving from the right to the left ventricle as previously believed by Galen. His work was later translated into Latin by Andrea Alpago.In Europe, the teachings of Galen continued to dominate the academic community and his doctrines were adopted as the official canon of the Church. Andreas Vesalius questioned some of Galen's beliefs of the heart in De humani corporis fabrica (1543), but his magnum opus was interpreted as a challenge to the authorities and he was subjected to a number of attacks. Michael Servetus wrote in Christianismi Restitutio (1553) that blood flows from one side of the heart to the other via the lungs.\n",
      "\n",
      "\n",
      "\n",
      "What is the fate of a carbocation formed in crystalline naphthalene?\n",
      "-Radical formation In a condensed phase, the carbocation can also gain an electron from surrounding molecules, thus becoming an electrically neutral radical. For example, in crystalline naphthalene, a molecule with tritium substituted for hydrogen in the 1 (or 2) position will be turned by decay into a cation with a positive charge at that position. That charge will however be quickly neutralized by an electron transported through the lattice, turning the molecule into the 1-naphthyl (or 2-naphthyl) radical; which are stable, trapped in the solid, below 170 K (−103 °C).\n",
      "-Naphthalene Naphthalene, an organic compound commonly found in pesticides such as mothballs, sublimes easily because it is made of non-polar molecules that are held together only by van der Waals intermolecular forces. Naphthalene is a solid that sublimes at standard atmospheric temperature with the sublimation point at around 80 °C or 176 °F. At low temperature, its vapour pressure is high enough, 1 mmHg at 53 °C, to make the solid form of naphthalene evaporate into gas. On cool surfaces, the naphthalene vapours will solidify to form needle-like crystals.\n",
      "-The alkali metal naphthalene salts are prepared by stirring the metal with naphthalene in an ethereal solvent, usually as tetrahydrofuran or dimethoxyethane. The resulting salt is dark green. The anion is a radical, giving a strong EPR signal near g = 2.0. Its deep green color arises from absorptions centered at 463 and 735 nm.\n",
      "Several solvates of sodium naphthalenide have been characterized by X-ray crystallography. The effects are subtle, the outer pair of CH−CH bonds contract by 3 pm and the other nine C−C bonds elongate by 2–3 pm. The net effect is that reduction weakens the bonding.\n",
      "-Quaternary carbon centers promote fragmentation by stabilizing carbocation formation through hyperconjugation. As shown in the above picture, the \"stable\" carbocation is formed, which then loses a hydrogen to give a site of unsaturation. Oxygen and nitrogen atoms also promote fragmentation through the formation of ketones and imines respectively.Sulfur is also capable of promoting fragmentation, albeit at a longer range than oxygen or nitrogen.\n",
      "-Anti-naphthalene minor carburettor This device injected a fine mist of naphtha into the outgoing gas so as to avoid the crystallization of naphthalene in the mains, and their consequent blockage. Naphtha was found to be a rather effective solvent for these purposes, even in small concentrations. Where troubles with naphthalene developed, as it occasionally did even after the introduction of this minor carburettor, a team of workers was sent out to blow steam into the main and dissolve the blockage; still, prior to its introduction, naphthalene was a very major annoyance for the gasworks.\n",
      "\n",
      "\n",
      "\n",
      "What is the main focus of the Environmental Science Center at Qatar University?\n",
      "-Environmental science is an interdisciplinary academic field that integrates physics, biology, and geography (including ecology, chemistry, plant science, zoology, mineralogy, oceanography, limnology, soil science, geology and physical geography, and atmospheric science) to the study of the environment, and the solution of environmental problems. Environmental science emerged from the fields of natural history and medicine during the Enlightenment. Today it provides an integrated, quantitative, and interdisciplinary approach to the study of environmental systems.Environmental studies incorporates more of the social sciences for understanding human relationships, perceptions and policies towards the environment. Environmental engineering focuses on design and technology for improving environmental quality in every aspect.\n",
      "-Environmental scientists seek to understand the earth's physical, chemical, biological, and geological processes, and to use that knowledge to understand how issues such as alternative energy systems, pollution control and mitigation, natural resource management, and the effects of global warming and climate change influence and affect the natural systems and processes of earth.\n",
      "Environmental issues almost always include an interaction of physical, chemical, and biological processes. Environmental scientists bring a systems approach to the analysis of environmental problems. Key elements of an effective environmental scientist include the ability to relate space, and time relationships as well as quantitative analysis.\n",
      "-This is a glossary of environmental science.\n",
      "Environmental science is the study of interactions among physical, chemical, and biological components of the environment. Environmental science provides an integrated, quantitative, and interdisciplinary approach to the study of environmental systems.\n",
      "-Environmental Earth Sciences is an international multidisciplinary scientific journal published 24 times a year by Springer. Its self-stated focus is on \"all aspects of interaction between humans, natural resources, ecosystems, special climates or unique geographic zones, and the earth\". Its subject areas include water and soil contamination caused by waste management; environmental problems associated with transportation by land, air, and water; and geological processes that may impact biosystems or humans.\n",
      "-Courses aimed at developing graduates with some specific skills in environmental systems or environmental technology are becoming more common and fall into three broads classes: Environmental Engineering or Environmental Systems courses oriented towards a civil engineering approach in which structures and the landscape are constructed to blend with or protect the environment; Environmental chemistry, sustainable chemistry or environmental chemical engineering courses oriented towards understanding the effects (good and bad) of chemicals in the environment. Such awards can focus on mining processes, pollutants and commonly also cover biochemical processes; Environmental technology courses oriented towards producing electronic, electrical or electrotechnology graduates capable of developing devices and artefacts able to monitor, measure, model and control environmental impact, including monitoring and managing energy generation from renewable sources, and developing novel energy generation technologies.\n",
      "\n",
      "\n",
      "\n",
      "What is the purpose of obtaining surgical resection specimens?\n",
      "-Surgical resection specimens are obtained by the therapeutic surgical removal of an entire diseased area or organ (and occasionally multiple organs). These procedures are often intended as definitive surgical treatment of a disease in which the diagnosis is already known or strongly suspected. However, pathological analysis of these specimens is critically important in confirming the previous diagnosis, staging the extent of malignant disease, establishing whether or not the entire diseased area was removed (a process called \"determination of the surgical margin\", often using frozen section), identifying the presence of unsuspected concurrent diseases, and providing information for postoperative treatment, such as adjuvant chemotherapy in the case of cancer.\n",
      "-Surgical pathology Surgical pathology is one of the primary areas of practice for most anatomical pathologists. Surgical pathology involves the gross and microscopic examination of surgical specimens, as well as biopsies submitted by surgeons and non-surgeons such as general internists, medical subspecialists, dermatologists, and interventional radiologists. Often an excised tissue sample is the best and most definitive evidence of disease (or lack thereof) in cases where tissue is surgically removed from a patient. These determinations are usually accomplished by a combination of gross (i.e., macroscopic) and histologic (i.e., microscopic) examination of the tissue, and may involve evaluations of molecular properties of the tissue by immunohistochemistry or other laboratory tests.There are two major types of specimens submitted for surgical pathology analysis: biopsies and surgical resections. A biopsy is a small piece of tissue removed primarily for surgical pathology analysis, most often in order to render a definitive diagnosis. Types of biopsies include core biopsies, which are obtained through the use of large-bore needles, sometimes under the guidance of radiological techniques such as ultrasound, CT scan, or magnetic resonance imaging. Incisional biopsies are obtained through diagnostic surgical procedures that remove part of a suspicious lesion, whereas excisional biopsies remove the entire lesion, and are similar to therapeutic surgical resections. Excisional biopsies of skin lesions and gastrointestinal polyps are very common. The pathologist's interpretation of a biopsy is critical to establishing the diagnosis of a benign or malignant tumor, and can differentiate between different types and grades of cancer, as well as determining the activity of specific molecular pathways in the tumor. Surgical resection specimens are obtained by the therapeutic surgical removal of an entire diseased area or organ (and occasionally multiple organs). These procedures are often intended as definitive surgical treatment of a disease in which the diagnosis is already known or strongly suspected, but pathological analysis of these specimens remains important in confirming the previous diagnosis.\n",
      "-After the biopsy is performed, the sample of tissue that was removed from the patient is sent to the pathology laboratory. A pathologist specializes in diagnosing diseases (such as cancer) by examining tissue under a microscope. When the laboratory (see Histology) receives the biopsy sample, the tissue is processed and an extremely thin slice of tissue is removed from the sample and attached to a glass slide. Any remaining tissue is saved for use in later studies, if required.The slide with the tissue attached is treated with dyes that stain the tissue, which allows the individual cells in the tissue to be seen more clearly. The slide is then given to the pathologist, who examines the tissue under a microscope, looking for any abnormal findings. The pathologist then prepares a report that lists any abnormal or important findings from the biopsy. This report is sent to the surgeon who originally performed the biopsy on the patient.\n",
      "-Gross examination of surgical specimens is typically performed by a pathologist, or by a pathologists' assistant working within a pathology practice. Individuals trained in these fields are often able to gather diagnostically critical information in this stage of processing, including the stage and margin status of surgically removed tumors.\n",
      "-== Need == In medicine, a laboratory specimen is a biological specimen of a medical patient's tissue, fluids, or other material used for laboratory analysis to assist in differential diagnosis or staging of a disease process. For example, to detect breast cancer, the breast tissue is biopsied, and the extracted specimen is sent to a lab for analysis and testing. This method of testing often yields extremely high levels of accuracy, with a reported 1-2% of cases having incorrect biopsy resultsGeneral areas for cellular tissue extraction:  Bone marrow aspiration Cardiac Core Endometrial biopsy Endoscopic biopsy Excisional and incisional Fine-needle aspiration Lymph node \n",
      "\n",
      "\n",
      "\n",
      "What is the function of mammary glands in mammals?\n",
      "-A mammary gland is an exocrine gland in humans and other mammals that produces milk to feed young offspring. Mammals get their name from the Latin word mamma, \"breast\". The mammary glands are arranged in organs such as the breasts in primates (for example, humans and chimpanzees), the udder in ruminants (for example, cows, goats, sheep, and deer), and the dugs of other animals (for example, dogs and cats). Lactorrhea, the occasional production of milk by the glands, can occur in any mammal, but in most mammals, lactation, the production of enough milk for nursing, occurs only in phenotypic females who have gestated in recent months or years. It is directed by hormonal guidance from sex steroids. In a few mammalian species, male lactation can occur. With humans, male lactation can occur only under specific circumstances.\n",
      "-Mammals are divided into 3 groups: prototherians, metatherians, and eutherians. In the case of prototherians, both males and females have functional mammary glands, but their mammary glands are without nipples. These mammary glands are modified sebaceous glands. Concerning metatherians and eutherians, only females have functional mammary glands. Their mammary glands can be termed as breasts or udders. In the case of breasts, each mammary gland has its own nipple (e.g., human mammary glands). In the case of udders, pairs of mammary glands comprise a single mass, with more than one nipple (or teat) hanging from it. For instance, cows and buffalo each have one udder with four teats, whereas sheep and goats each have two teats protruding from the udder. These mammary glands are modified sweat glands.\n",
      "-General The breasts of female humans vary from most other mammals that tend to have less conspicuous mammary glands. The number and positioning of mammary glands varies widely in different mammals. The protruding teats and accompanying glands can be located anywhere along the two milk lines. In general most mammals develop mammary glands in pairs along these lines, with a number approximating the number of young typically birthed at a time. The number of teats varies from 2 (in most primates) to 18 (in pigs). The Virginia opossum has 13, one of the few mammals with an odd number. The following table lists the number and position of teats and glands found in a range of mammals: Male mammals typically have rudimentary mammary glands and nipples, with a few exceptions: male mice do not have nipples, male marsupials do not have mammary glands, and male horses lack nipples and mammary glands. The male Dayak fruit bat has lactating mammary glands. Male lactation occurs infrequently in some species.Mammary glands are true protein factories, and several labs have constructed transgenic animals, mainly goats and cows, to produce proteins for pharmaceutical use. Complex glycoproteins such as monoclonal antibodies or antithrombin cannot be produced by genetically engineered bacteria, and the production in live mammals is much cheaper than the use of mammalian cell cultures.\n",
      "-Mammal anatomy Mammals are a diverse class of animals, mostly terrestrial but some are aquatic and others have evolved flapping or gliding flight. They mostly have four limbs, but some aquatic mammals have no limbs or limbs modified into fins, and the forelimbs of bats are modified into wings. The legs of most mammals are situated below the trunk, which is held well clear of the ground. The bones of mammals are well ossified and their teeth, which are usually differentiated, are coated in a layer of prismatic enamel. The teeth are shed once (milk teeth) during the animal's lifetime or not at all, as is the case in cetaceans. Mammals have three bones in the middle ear and a cochlea in the inner ear. They are clothed in hair and their skin contains glands which secrete sweat. Some of these glands are specialized as mammary glands, producing milk to feed the young. Mammals breathe with lungs and have a muscular diaphragm separating the thorax from the abdomen which helps them draw air into the lungs. The mammalian heart has four chambers, and oxygenated and deoxygenated blood are kept entirely separate. Nitrogenous waste is excreted primarily as urea.Mammals are amniotes, and most are viviparous, giving birth to live young. Exceptions to this are the egg-laying monotremes, the platypus and the echidnas of Australia. Most other mammals have a placenta through which the developing foetus obtains nourishment, but in marsupials, the foetal stage is very short and the immature young is born and finds its way to its mother's pouch where it latches on to a nipple and completes its development.\n",
      "-Evolution There are many theories on how mammary glands evolved. For example, it is thought that the mammary gland is a transformed sweat gland, more closely related to apocrine sweat glands. Because mammary glands do not fossilize well, supporting such theories with fossil evidence is difficult. Many of the current theories are based on comparisons between lines of living mammals—monotremes, marsupials, and eutherians. One theory proposes that mammary glands evolved from glands that were used to keep the eggs of early mammals moist and free from infection (monotremes still lay eggs). Other theories suggest that early secretions were used directly by hatched young, or that the secretions were used by young to help them orient to their mothers.Lactation is thought to have developed long before the evolution of the mammary gland and mammals; see evolution of lactation.\n",
      "\n",
      "\n",
      "\n",
      "What is the relationship between interstellar and cometary chemistry?\n",
      "-Such IR observations have determined that in dense clouds (where there are enough particles to attenuate the destructive UV radiation) thin ice layers coat the microscopic particles, permitting some low-temperature chemistry to occur. Since dihydrogen is by far the most abundant molecule in the universe, the initial chemistry of these ices is determined by the chemistry of the hydrogen. If the hydrogen is atomic, then the H atoms react with available O, C and N atoms, producing \"reduced\" species like H2O, CH4, and NH3. However, if the hydrogen is molecular and thus not reactive, this permits the heavier atoms to react or remain bonded together, producing CO, CO2, CN, etc. These mixed-molecular ices are exposed to ultraviolet radiation and cosmic rays, which results in complex radiation-driven chemistry. Lab experiments on the photochemistry of simple interstellar ices have produced amino acids. The similarity between interstellar and cometary ices (as well as comparisons of gas phase compounds) have been invoked as indicators of a connection between interstellar and cometary chemistry. This is somewhat supported by the results of the analysis of the organics from the comet samples returned by the Stardust mission but the minerals also indicated a surprising contribution from high-temperature chemistry in the solar nebula.\n",
      "-Research is progressing on the way in which interstellar and circumstellar molecules form and interact, e.g. by including non-trivial quantum mechanical phenomena for synthesis pathways on interstellar particles. This research could have a profound impact on our understanding of the suite of molecules that were present in the molecular cloud when our solar system formed, which contributed to the rich carbon chemistry of comets and asteroids and hence the meteorites and interstellar dust particles which fall to the Earth by the ton every day.\n",
      "-Chemistry in cometary comae The chemical composition of comets should reflect both the conditions in the outer solar nebula some 4.5 × 109 ayr, and the nature of the natal interstellar cloud from which the Solar System was formed. While comets retain a strong signature of their ultimate interstellar origins, significant processing must have occurred in the protosolar nebula. Early models of coma chemistry showed that reactions can occur rapidly in the inner coma, where the most important reactions are proton transfer reactions. Such reactions can potentially cycle deuterium between the different coma molecules, altering the initial D/H ratios released from the nuclear ice, and necessitating the construction of accurate models of cometary deuterium chemistry, so that gas-phase coma observations can be safely extrapolated to give nuclear D/H ratios.\n",
      "-The 1) interstellar model says that ices formed on dust grains in the dense cloud that preceded the Sun. The mix of ice and dust then aggregated into a comet without appreciable chemical modification. J. Mayo Greenberg first proposed this idea in the 1970s.In the 2) Solar System model, the ices that formed in the interstellar cloud first vaporized as part of the accretion disk of gas and dust around the protosun. The vaporized ices later resolidified and assembled into comets. So the comets in this model would have a different composition than those comets that were made directly from interstellar ice.\n",
      "-Astrochemistry Astrochemistry is the study of the abundance and reactions of molecules in the Universe, and their interaction with radiation. The discipline is an overlap of astronomy and chemistry. The word \"astrochemistry\" may be applied to both the Solar System and the interstellar medium. The study of the abundance of elements and isotope ratios in Solar System objects, such as meteorites, is also called cosmochemistry, while the study of interstellar atoms and molecules and their interaction with radiation is sometimes called molecular astrophysics. The formation, atomic and chemical composition, evolution and fate of molecular gas clouds is of special interest, because it is from these clouds that solar systems form. Studies in this field contribute to the understanding of the formation of the Solar System, Earth's origin and geology, abiogenesis, and the origin of climate and oceans.\n",
      "\n",
      "\n",
      "\n",
      "What is the reason for recycling rare metals according to the United Nations?\n",
      "-Recycling Demand for metals is closely linked to economic growth given their use in infrastructure, construction, manufacturing, and consumer goods. During the 20th century, the variety of metals used in society grew rapidly. Today, the development of major nations, such as China and India, and technological advances, are fuelling ever more demand. The result is that mining activities are expanding, and more and more of the world's metal stocks are above ground in use, rather than below ground as unused reserves. An example is the in-use stock of copper. Between 1932 and 1999, copper in use in the U.S. rose from 73 g to 238 g per person.Metals are inherently recyclable, so in principle, can be used over and over again, minimizing these negative environmental impacts and saving energy. For example, 95% of the energy used to make aluminum from bauxite ore is saved by using recycled material.Globally, metal recycling is generally low. In 2010, the International Resource Panel, hosted by the United Nations Environment Programme published reports on metal stocks that exist within society and their recycling rates. The authors of the report observed that the metal stocks in society can serve as huge mines above ground. They warned that the recycling rates of some rare metals used in applications such as mobile phones, battery packs for hybrid cars and fuel cells are so low that unless future end-of-life recycling rates are dramatically stepped up these critical metals will become unavailable for use in modern technology.\n",
      "-During the 20th century, the variety of metals used in society grew rapidly. Today, the development of major nations such as China and India and advances in technologies are fueling an ever-greater demand. The result is that metal mining activities are expanding and more and more of the world's metal stocks are above ground in use rather than below ground as unused reserves. An example is the in-use stock of copper. Between 1932 and 1999, copper in use in the US rose from 73 kilograms (161 lb) to 238 kilograms (525 lb) per person.95% of the energy used to make aluminium from bauxite ore is saved by using recycled material. However, levels of metals recycling are generally low. In 2010, the International Resource Panel, hosted by the United Nations Environment Programme (UNEP), published reports on metal stocks that exist within society and their recycling rates.The report's authors observed that the metal stocks in society can serve as huge mines above ground. However, they warned that the recycling rates of some rare metals used in applications such as mobile phones, battery packs for hybrid cars, and fuel cells are so low that unless future end-of-life recycling rates are dramatically stepped up these critical metals will become unavailable for use in modern technology.As recycling rates are low and so much metal has already been extracted, some landfills now contain higher concentrations of metal than mines themselves. This is especially true of aluminum, used in cans, and precious metals, found in discarded electronics. Furthermore, waste after 15 years has still not broken down, so less processing would be required when compared to mining ores. A study undertaken by Cranfield University has found £360 million of metals could be mined from just four landfill sites. There is also up to 20 MJ/kg of energy in waste, potentially making the re-extraction more profitable. However, although the first landfill mine opened in Tel Aviv, Israel in 1953, little work has followed due to the abundance of accessible ores.\n",
      "-Recycling can enable lower-emission second purpose to new materials like steel, aluminum, and other metals. Incorporating recycled materials into the manufacturing process of new goods is a necessary change. Recycling is standard for most materials and is found in every country and economy. Some materials that can be recycled are: AluminumAluminum offers the most savings, with cans from recycled material requiring as little as 4% of the energy needed to make the same cans from bauxite ore. Metals don't degrade as they're recycled in the same way plastics and paper do, fibers shortening every cycle, so many metals are prime candidates for recycling, especially considering their high value per ton compared to other recyclables. Aluminum is a highly desirable metal for recycling because it retains the same properties and quality, no matter how many times the aluminum can be recycled. After all, once it's melted, the structure doesn't change.\n",
      "-Recycling and reusing REEs Potential methods The rare-earth elements (REEs) are vital to modern technologies and society and are amongst the most critical elements. Despite this, typically only around 1% of REEs are recycled from end-products, with the rest deporting to waste and being removed from the materials cycle. Recycling and reusing REEs play an important role in high technology fields and manufacturing environmentally friendly products all around the world.REE recycling and reuse have been increasingly focused on in recent years. The main concerns include environmental pollution during REE recycling and increasing recycling efficiency. Literature published in 2004 suggests that, along with previously established pollution mitigation, a more circular supply chain would help mitigate some of the pollution at the extraction point. This means recycling and reusing REEs that are already in use or reaching the end of their life cycle. A study published in 2014 suggests a method to recycle REEs from waste nickel-metal hydride batteries, demonstrating a recovery rate of 95.16%. Rare-earth elements could also be recovered from industrial wastes with practical potential to reduce environmental and health impacts from mining, waste generation, and imports if known and experimental processes are scaled up. A study suggests that \"fulfillment of the circular economy approach could reduce up to 200 times the impact in the climate change category and up to 70 times the cost due to the REE mining.\" In most of the reported studies reviewed by a scientific review, \"secondary waste is subjected to chemical and or bioleaching followed by solvent extraction processes for clean separation of REEs.\"Currently, people take two essential resources into consideration for the secure supply of REEs: one is to extract REEs from primary resources like mines harboring REE-bearing ores, regolith-hosted clay deposits, ocean bed sediments, coal fly ash, etc. A work developed a green system for recovery of REEs from coal fly ash by using citrate and oxalate who are strong organic ligand and capable of complexing or precipItating with REE. The other one is from secondary resources such as electronic, industrial waste and municipal waste. E-waste contains a significant concentration of REEs, and thus is the primary option for REE recycling now. According to a study, approximately 50 million metric tons of electronic waste are dumped in landfills worldwide each year. Despite the fact that e-waste contains a significant amount of rare-earth elements (REE), only 12.5% of e-waste is currently being recycled for all metals.\n",
      "-The report's authors observed that, as metals are inherently recyclable, metal stocks in society can serve as huge above-ground mines (the term \"urban mining\" has thus been coined). However, they found that the recycling rates of many metals are low. They warned that the recycling rates of some rare metals used in applications such as mobile phones, battery packs for hybrid cars and fuel cells, are so low that unless future end-of-life recycling rates are dramatically increased, these critical metals will become unavailable for use in modern technology.\n",
      "\n",
      "\n",
      "\n",
      "What is radiometric dating?\n",
      "-Radiometric dating, radioactive dating or radioisotope dating is a technique which is used to date materials such as rocks or carbon, in which trace radioactive impurities were selectively incorporated when they were formed. The method compares the abundance of a naturally occurring radioactive isotope within the material to the abundance of its decay products, which form at a known constant rate of decay. The use of radiometric dating was first published in 1907 by Bertram Boltwood and is now the principal source of information about the absolute age of rocks and other geological features, including the age of fossilized life forms or the age of Earth itself, and can also be used to date a wide range of natural and man-made materials.\n",
      "-Together with stratigraphic principles, radiometric dating methods are used in geochronology to establish the geologic time scale. Among the best-known techniques are radiocarbon dating, potassium–argon dating and uranium–lead dating. By allowing the establishment of geological timescales, it provides a significant source of information about the ages of fossils and the deduced rates of evolutionary change. Radiometric dating is also used to date archaeological materials, including ancient artifacts.\n",
      "-Radiometric dating By measuring the amount of radioactive decay of a radioactive isotope with a known half-life, geologists can establish the absolute age of the parent material. A number of radioactive isotopes are used for this purpose, and depending on the rate of decay, are used for dating different geological periods. More slowly decaying isotopes are useful for longer periods of time, but less accurate in absolute years. With the exception of the radiocarbon method, most of these techniques are actually based on measuring an increase in the abundance of a radiogenic isotope, which is the decay-product of the radioactive parent isotope. Two or more radiometric methods can be used in concert to achieve more robust results. Most radiometric methods are suitable for geological time only, but some such as the radiocarbon method and the 40Ar/39Ar dating method can be extended into the time of early human life and into recorded history.Some of the commonly used techniques are: Radiocarbon dating. This technique measures the decay of carbon-14 in organic material and can be best applied to samples younger than about 60,000 years.\n",
      "-Radiometric dating is how geologist determine the age of a rock. In a closed system, the amount of radiogenic isotopes present in a sample is a direct function of time and the decay rate of the mineral. Therefore, to find the age of a sample, geologists find the ratio of daughter isotopes to remaining parent isotopes present in the mineral through different methods, such as mass spectrometry. From the known parent isotopes and the decay constant, we can then determine the age. Different ions can be analyzed for this and are called different dating.\n",
      "-All methods based on the radioactive decay belong to this category. The principle at the base of radiometric dating is that natural unstable isotopes, called 'parent isotopes', decay to some isotope which is instead stable, called the 'daughter isotope'.\n",
      "Under the assumptions that: (1) the initial amount of parent and daughter isotopes can be estimated, and  (2) after the geologic material formed, parent and daughter isotopes did not escape the system, the age of the material can be obtained from the measurement of isotope concentrations, through the laws of radioactive decay.\n",
      "\n",
      "\n",
      "\n",
      "What is the role of methane in Fischer-Tropsch processes?\n",
      "-The Fischer–Tropsch process is a collection of chemical reactions that converts a mixture of carbon monoxide and hydrogen, known as syngas, into liquid hydrocarbons. These reactions occur in the presence of metal catalysts, typically at temperatures of 150–300 °C (302–572 °F) and pressures of one to several tens of atmospheres. The Fischer–Tropsch process is an important reaction in both coal liquefaction and gas to liquids technology for producing liquid hydrocarbons.In the usual implementation, carbon monoxide and hydrogen, the feedstocks for FT, are produced from coal, natural gas, or biomass in a process known as gasification. The process then converts these gases into synthetic lubrication oil and synthetic fuel. This process has received intermittent attention as a source of low-sulfur diesel fuel and to address the supply or cost of petroleum-derived hydrocarbons. Fischer-Tropsch process is discussed as a step of producing carbon-neutral liquid hydrocarbon fuels from CO2 and hydrogen.The process was first developed by Franz Fischer and Hans Tropsch at the Kaiser Wilhelm Institute for Coal Research in Mülheim an der Ruhr, Germany, in 1925.\n",
      "-In order to obtain the mixture of CO and H2 required for the Fischer–Tropsch process, methane (main component of natural gas) may be subjected to partial oxidation which yields a raw synthesis gas mixture of mostly carbon dioxide, carbon monoxide, hydrogen gas (and sometimes water and nitrogen). The ratio of carbon monoxide to hydrogen in the raw synthesis gas mixture can be adjusted e.g. using the water gas shift reaction. Removing impurities, particularly nitrogen, carbon dioxide and water, from the raw synthesis gas mixture yields pure synthesis gas (syngas).\n",
      "-Industrial routes Given its cheap abundance in natural gas, there is little incentive to produce methane industrially. Methane can be produced by hydrogenating carbon dioxide through the Sabatier process. Methane is also a side product of the hydrogenation of carbon monoxide in the Fischer–Tropsch process, which is practiced on a large scale to produce longer-chain molecules than methane.\n",
      "-Carbon dioxide reuse Carbon dioxide is not a typical feedstock for FT catalysis. Hydrogen and carbon dioxide react over a cobalt-based catalyst, producing methane. With iron-based catalysts unsaturated short-chain hydrocarbons are also produced. Upon introduction to the catalyst's support, ceria functions as a reverse water-gas shift catalyst, further increasing the yield of the reaction. The short-chain hydrocarbons were upgraded to liquid fuels over solid acid catalysts, such as zeolites.\n",
      "-The pure syngas is routed into the Fischer–Tropsch process, where the syngas reacts over an iron or cobalt catalyst to produce synthetic hydrocarbons, including alcohols.\n",
      "\n",
      "\n",
      "\n",
      "What is a phageome?\n",
      "-A phageome is a community of bacteriophages and their metagenomes localized in a particular environment, similar to a microbiome. The term was first used in an article by Modi et al in 2013 and has continued to be used in scientific articles that relate to bacteriophages and their metagenomes. A bacteriophage, or phage for short, is a virus that has the ability to infect bacteria and archaea, and can replicate inside of them. Phageome is a subcategory of virome, which is all of the viruses that are associated with a host or environment. Phages make up the majority of most viromes and are currently understood as being the most abundant organism. Oftentimes scientists will look only at a phageome instead of a virome while conducting research.\n",
      "-Bacteriophages (phages), potentially the most numerous \"organisms\" on Earth, are the viruses of bacteria (more generally, of prokaryotes). Phage ecology is the study of the interaction of bacteriophages with their environments.\n",
      "-Bacteriophage (phage) are viruses of bacteria and arguably are the most numerous \"organisms\" on Earth. The history of phage study is captured, in part, in the books published on the topic. This is a list of over 100 monographs on or related to phages.\n",
      "-A bacteriophage (), also known informally as a phage (), is a duplodnaviria virus that infects and replicates within bacteria and archaea. The term was derived from \"bacteria\" and the Greek φαγεῖν (phagein), meaning \"to devour\". Bacteriophages are composed of proteins that encapsulate a DNA or RNA genome, and may have structures that are either simple or elaborate. Their genomes may encode as few as four genes (e.g. MS2) and as many as hundreds of genes. Phages replicate within the bacterium following the injection of their genome into its cytoplasm.\n",
      "-Bacteria (along with archaea) appear to be highly diverse and there possibly are millions of species. Phage-ecological interactions therefore are quantitatively vast: huge numbers of interactions. Phage-ecological interactions are also qualitatively diverse: There are huge numbers of environment types, bacterial-host types, and also individual phage types Studying phage ecology The study of phage ecology reflects established scientific disciplines in ecological studies in scope, the most obvious being general ecology. Accordingly, phage ecology is treated under the following heads— \"organismal\" ecology, population ecology, community ecology, and ecosystem ecology. Phage ecology also may be considered (though mostly less well formally explored) from perspectives of phage behavioral ecology, evolutionary ecology, functional ecology, landscape ecology, mathematical ecology, molecular ecology, physiological ecology (or ecophysiology), and spatial ecology. Phage ecology additionally draws (extensively) from microbiology, particularly in terms of environmental microbiology, but also from an enormous catalog (90 years) of study of phage and phage-bacterial interactions in terms of their physiology and, especially, their molecular biology.\n",
      "\n",
      "\n",
      "\n",
      "What is organography?\n",
      "-Organography (from Greek όργανο, organo, \"organ\"; and -γραφή, -graphy) is the scientific description of the structure and function of the organs of living things.\n",
      "-Organography as a scientific study starts with Aristotle, who considered the parts of plants as \"organs\" and began to consider the relationship between different organs and different functions. In the 17th century Joachim Jung, clearly articulated that plants are composed of different organ types such as root, stem and leaf, and he went on to define these organ types on the basis of form and position.\n",
      "-Organology (from Ancient Greek ὄργανον (organon) 'instrument' and λόγος (logos), 'the study of') is the science of musical instruments and their classifications. It embraces study of instruments' history, instruments used in different cultures, technical aspects of how instruments produce sound, and musical instrument classification. There is a degree of overlap between organology, ethnomusicology (being subsets of musicology) and the branch of the science of acoustics devoted to musical instruments.\n",
      "-In a multicellular organism, an organ is a collection of tissues joined in a structural unit to serve a common function. In the hierarchy of life, an organ lies between tissue and an organ system. Tissues are formed from same type cells to act together in a function. Tissues of different types combine to form an organ which has a specific function. The intestinal wall for example is formed by epithelial tissue and smooth muscle tissue. Two or more organs working together in the execution of a specific body function form an organ system, also called a biological system or body system.\n",
      "-An organ system is a biological system consisting of a group of organs that work together to perform one or more functions. Each organ has a specialized role in a plant or animal body, and is made up of distinct tissues.\n",
      "\n",
      "\n",
      "\n",
      "What is the definition of anatomy?\n",
      "-Anatomy (from Ancient Greek ἀνατομή (anatomḗ) 'dissection') is the branch of biology concerned with the study of the structure of organisms and their parts. Anatomy is a branch of natural science that deals with the structural organization of living things. It is an old science, having its beginnings in prehistoric times. Anatomy is inherently tied to developmental biology, embryology, comparative anatomy, evolutionary biology, and phylogeny, as these are the processes by which anatomy is generated, both over immediate and long-term timescales. Anatomy and physiology, which study the structure and function of organisms and their parts respectively, make a natural pair of related disciplines, and are often studied together. Human anatomy is one of the essential basic sciences that are applied in medicine.Anatomy is a complex and dynamic field that is constantly evolving as new discoveries are made. In recent years, there has been a significant increase in the use of advanced imaging techniques, such as MRI and CT scans, which allow for more detailed and accurate visualizations of the body's structures.\n",
      "-The branch of biology dealing with the study of the bodies and their specific structural features called morphology. Anatomy is a branch of morphology that deals with the structure of the body at a level higher than tissue. Anatomy is closely related to histology, which studies the structure of tissues, as well as cytology, which studies the structure and function of the individual cells, from which the tissues and organs of the studied macroorganism are built. Taken together, anatomy, histology, cytology and embryology represent a morphology The study of functions and mechanisms in a body is physiology.\n",
      "-The discipline of anatomy is divided into macroscopic and microscopic parts. Macroscopic anatomy, or gross anatomy, is the examination of an animal's body parts using unaided eyesight. Gross anatomy also includes the branch of superficial anatomy. Microscopic anatomy involves the use of optical instruments in the study of the tissues of various structures, known as histology, and also in the study of cells.\n",
      "-Derived from the Greek ἀνατομή anatomē \"dissection\" (from ἀνατέμνω anatémnō \"I cut up, cut open\" from ἀνά aná \"up\", and τέμνω témnō \"I cut\"), anatomy is the scientific study of the structure of organisms including their systems, organs and tissues. It includes the appearance and position of the various parts, the materials from which they are composed, and their relationships with other parts. Anatomy is quite distinct from physiology and biochemistry, which deal respectively with the functions of those parts and the chemical processes involved. For example, an anatomist is concerned with the shape, size, position, structure, blood supply and innervation of an organ such as the liver; while a physiologist is interested in the production of bile, the role of the liver in nutrition and the regulation of bodily functions.The discipline of anatomy can be subdivided into a number of branches, including gross or macroscopic anatomy and microscopic anatomy. Gross anatomy is the study of structures large enough to be seen with the naked eye, and also includes superficial anatomy or surface anatomy, the study by sight of the external body features. Microscopic anatomy is the study of structures on a microscopic scale, along with histology (the study of tissues), and embryology (the study of an organism in its immature condition). Regional anatomy is the study of the interrelationships of all of the structures in a specific body region, such as the abdomen. In contrast, systemic anatomy is the study of the structures that make up a discrete body system—that is, a group of structures that work together to perform a unique body function, such as the digestive system.Anatomy can be studied using both invasive and non-invasive methods with the goal of obtaining information about the structure and organization of organs and systems. Methods used include dissection, in which a body is opened and its organs studied, and endoscopy, in which a video camera-equipped instrument is inserted through a small incision in the body wall and used to explore the internal organs and other structures. Angiography using X-rays or magnetic resonance angiography are methods to visualize blood vessels.The term \"anatomy\" is commonly taken to refer to human anatomy. However, substantially similar structures and tissues are found throughout the rest of the animal kingdom, and the term also includes the anatomy of other animals. The term zootomy is also sometimes used to specifically refer to non-human animals. The structure and tissues of plants are of a dissimilar nature and they are studied in plant anatomy.\n",
      "-Anatomy considers the forms of macroscopic structures such as organs and organ systems. It focuses on how organs and organ systems work together in the bodies of humans and animals, in addition to how they work independently. Anatomy and cell biology are two studies that are closely related, and can be categorized under \"structural\" studies. Comparative anatomy is the study of similarities and differences in the anatomy of different groups. It is closely related to evolutionary biology and phylogeny (the evolution of species).\n",
      "\n",
      "\n",
      "\n",
      "What is a trophic level in an ecological pyramid?\n",
      "-A pyramid of energy shows how much energy is retained in the form of new biomass from each trophic level, while a pyramid of biomass shows how much biomass (the amount of living or organic matter present in an organism) is present in the organisms. There is also a pyramid of numbers representing the number of individual organisms at each trophic level. Pyramids of energy are normally upright, but other pyramids can be inverted(pyramid of biomass for marine region) or take other shapes.(spindle shaped pyramid) Ecological pyramids begin with producers on the bottom (such as plants) and proceed through the various trophic levels (such as herbivores that eat plants, then carnivores that eat flesh, then omnivores that eat both plants and flesh, and so on). The highest level is the top of the food chain.\n",
      "-When an ecosystem is healthy, this graph produces a standard ecological pyramid. This is because, in order for the ecosystem to sustain itself, there must be more energy at lower trophic levels than there is at higher trophic levels. This allows organisms on the lower levels to not only maintain a stable population, but also to transfer energy up the pyramid. The exception to this generalization is when portions of a food web are supported by inputs of resources from outside the local community. In small, forested streams, for example, the volume of higher levels is greater than could be supported by the local primary production.\n",
      "-An ecological pyramid (also trophic pyramid, Eltonian pyramid, energy pyramid, or sometimes food pyramid) is a graphical representation designed to show the biomass or bioproductivity at each trophic level in an ecosystem.\n",
      "-Trophic levels A trophic level (from Greek troph, τροφή, trophē, meaning \"food\" or \"feeding\") is \"a group of organisms acquiring a considerable majority of its energy from the lower adjacent level (according to ecological pyramids) nearer the abiotic source.\": 383  Links in food webs primarily connect feeding relations or trophism among species. Biodiversity within ecosystems can be organized into trophic pyramids, in which the vertical dimension represents feeding relations that become further removed from the base of the food chain up toward top predators, and the horizontal dimension represents the abundance or biomass at each level. When the relative abundance or biomass of each species is sorted into its respective trophic level, they naturally sort into a 'pyramid of numbers'.Species are broadly categorized as autotrophs (or primary producers), heterotrophs (or consumers), and Detritivores (or decomposers). Autotrophs are organisms that produce their own food (production is greater than respiration) by photosynthesis or chemosynthesis. Heterotrophs are organisms that must feed on others for nourishment and energy (respiration exceeds production). Heterotrophs can be further sub-divided into different functional groups, including primary consumers (strict herbivores), secondary consumers (carnivorous predators that feed exclusively on herbivores), and tertiary consumers (predators that feed on a mix of herbivores and predators). Omnivores do not fit neatly into a functional category because they eat both plant and animal tissues. It has been suggested that omnivores have a greater functional influence as predators because compared to herbivores, they are relatively inefficient at grazing.Trophic levels are part of the holistic or complex systems view of ecosystems. Each trophic level contains unrelated species that are grouped together because they share common ecological functions, giving a macroscopic view of the system. While the notion of trophic levels provides insight into energy flow and top-down control within food webs, it is troubled by the prevalence of omnivory in real ecosystems. This has led some ecologists to \"reiterate that the notion that species clearly aggregate into discrete, homogeneous trophic levels is fiction.\": 815  Nonetheless, recent studies have shown that real trophic levels do exist, but \"above the herbivore trophic level, food webs are better characterized as a tangled web of omnivores.\": 612 Keystone species A keystone species is a species that is connected to a disproportionately large number of other species in the food-web. Keystone species have lower levels of biomass in the trophic pyramid relative to the importance of their role. The many connections that a keystone species holds means that it maintains the organization and structure of entire communities. The loss of a keystone species results in a range of dramatic cascading effects (termed trophic cascades) that alters trophic dynamics, other food web connections, and can cause the extinction of other species. The term keystone species was coined by Robert Paine in 1969 and is a reference to the keystone architectural feature as the removal of a keystone species can result in a community collapse just as the removal of the keystone in an arch can result in the arch's loss of stability.Sea otters (Enhydra lutris) are commonly cited as an example of a keystone species because they limit the density of sea urchins that feed on kelp. If sea otters are removed from the system, the urchins graze until the kelp beds disappear, and this has a dramatic effect on community structure. Hunting of sea otters, for example, is thought to have led indirectly to the extinction of the Steller's sea cow (Hydrodamalis gigas). While the keystone species concept has been used extensively as a conservation tool, it has been criticized for being poorly defined from an operational stance. It is difficult to experimentally determine what species may hold a keystone role in each ecosystem. Furthermore, food web theory suggests that keystone species may not be common, so it is unclear how generally the keystone species model can be applied.\n",
      "-A pyramid of biomass shows the relationship between biomass and trophic level by quantifying the biomass present at each trophic level of an ecological community at a particular time. It is a graphical representation of biomass (total amount of living or organic matter in an ecosystem) present in unit area in different trophic levels. Typical units are grams per square meter, or calories per square meter.\n",
      "\n",
      "\n",
      "\n",
      "What is a crossover experiment?\n",
      "-In chemistry, a crossover experiment is a method used to study the mechanism of a chemical reaction. In a crossover experiment, two similar but distinguishable reactants simultaneously undergo a reaction as part of the same reaction mixture. The products formed will either correspond directly to one of the two reactants (non-crossover products) or will include components of both reactants (crossover products). The aim of a crossover experiment is to determine whether or not a reaction process involves a stage where the components of each reactant have an opportunity to exchange with each other.\n",
      "-For crossover experiments used to distinguish between intermolecular and intramolecular reactions, the absence of crossover products is less conclusive than the presence of crossover products. This is because solvent cage effects could be masking an intermolecular mechanism.\n",
      "-In medicine, a crossover study or crossover trial is a longitudinal study in which subjects receive a sequence of different treatments (or exposures). While crossover studies can be observational studies, many important crossover studies are controlled experiments, which are discussed in this article. Crossover designs are common for experiments in many scientific disciplines, for example psychology, pharmaceutical science, and medicine.\n",
      "-In designing a crossover experiment the first task is to propose possible mechanisms for the reaction being studied. Based on these possible mechanisms, the goal is to determine either a traditional crossover experiment or an isotope scrambling experiment that will enable the researcher to distinguish between the two or more possible mechanisms. Often many methods of mechanistic study will have to be employed to support or discount all of the mechanisms proposed. However, in some cases a crossover experiment alone will be able to distinguish between the main possibilities, for example in the case of intramolecular vs. intermolecular organic reaction mechanisms.\n",
      "-Crossover experiments allow for experimental study of a reaction mechanism. Mechanistic studies are of interest to theoretical and experimental chemists for a variety of reasons including prediction of stereochemical outcomes, optimization of reaction conditions for rate and selectivity, and design of improved catalysts for better turnover number, robustness, etc. Since a mechanism cannot be directly observed or determined solely based on the reactants or products, mechanisms are challenging to study experimentally. Only a handful of experimental methods are capable of providing information about the mechanism of a reaction, including crossover experiments, studies of the kinetic isotope effect, and rate variations by substituent. The crossover experiment has the advantage of being conceptually straightforward and relatively easy to design, carry out, and interpret. In modern mechanistic studies, crossover experiments and KIE studies are commonly used in conjunction with computational methods.\n",
      "\n",
      "\n",
      "\n",
      "What is the role of IL-10 in the formation of Tr1 cells and tolerogenic DCs?\n",
      "-During a tolerant state potential effector cells remain but are tightly regulated by induced antigen-specific CD4+ regulatory T cells (iTregs). Many subsets of iTregs play a part in this process, but CD4+CD25+FoxP3+ Tregs play a key role, because they have the ability to convert conventional T cells into iTregs directly by secretion of the suppressive cytokines TGF-β, IL-10 or IL-35, or indirectly via dendritic cells (DCs). Production of IL-10 induces the formation of another population of regulatory T cells called Tr1. Tr1 cells are dependent on IL-10 and TGF-β as well as Tregs, but differ from them by lacking expression of Foxp3. High IL-10 production is characteristic for Tr1 cells themselves and they also produce TGF-β. In the presence of IL-10 can be also induced tolerogenic DCs from monocytes, whose production of IL-10 is also important for Tr1 formation. These interactions lead to the production of enzymes such as IDO (indolamine 2,3-dioxygenase) that catabolize essential amino acids. This microenvironment with a lack of essential amino acids together with other signals results in mTOR (mammalian target of rapamycin) inhibition which, particularly in synergy with TGF-β, direct the induction of new FoxP3 (forkhead box protein 3) expressing Tregs.\n",
      "-The specific cell-surface markers for Tr1 cells in humans and mice are CD4+ CD49b+LAG-3+ CD226+ from which LAG-3+ and CD49b+ are indispensable. LAG-3 is a membrane protein on Tr1 cells that negatively regulates TCR-mediated signal transduction in cells. LAG-3 activates dendritic cells (DCs) and enhances the antigen-specific T-cell response which is necessary for Tr1 cells antigen specificity. CD49b belongs to the integrin family and is a receptor for many (extracellular) matrix and non-matrix molecules. CD49b provides only little contribution to the differentiation and function of Tr1 cells.They characteristically produce high levels of IL-10, IFN-γ, IL-5 and also TGF- β but neither IL-4 nor IL-2. Production of IL-10 is also much more rapid than its production by other T-helper cell types.Tr1 cells do not constitutively express FOXP3 but only transiently, upon their activation and in smaller amounts than CD25+ FOXP3+ regulatory cells. FOXP3 is not required for Tr1 induction, nor for its function. They also express repressor of GATA-3 (ROG), while CD25+ FOXP3+ regulatory cells do not. ROG then downregulates GATA-3, a characteristic transcription factor for Th2 cells.\n",
      "-IL-27, together with TGF-β induces IL-10–producing regulatory T cells with Tr1-like properties cells. IL-27 alone can induce IL-10-producing Tr1 cells, but in the absence of TGF-β, the cells produce large quantities of both IFN-γ and IL-10. IL-6 and IL-21 also plays a role in differentiation as they regulate expression of transcription factors necessary for IL-10 production, which is believed to start up the differentiation itself later on.\n",
      "-There is probably also positive regulation of thymic Treg cells development caused by recirculating Treg cells into thymus. There was found population of CD24 low FOXP3+ in thymus with increased expression of IL-1R2 (Il1r2) compared with peripheral Treg cells. High concentration of IL-1β caused by inflammation decrease de novo development of Treg cells in thymus. The presence of recirculating Treg cells in the thymus with high IL1R2 expression during inflammatory conditions helps to uptake IL-1β and reduce its concentration in the medulla microenvironment, thus they are helping to the development of de novo Treg cells. High concentration of IL-1β caused by inflammation decrease de novo development of Treg cells in thymus. Binding of IL-1β to IL1R2 on the surface of Treg cells does not cause any signal transduction because there is no present Intracellular (TIR) Toll interleukin-1 receptor domain, which is normally present in innate immune cells.\n",
      "-Mechanisms of suppression Cytokines mediatedTr1 cells secrete large amount of suppressing cytokines IL-10 and TGF-β. IL-10 directly inhibits T cells by blocking its production of IL-2, IFN-γ and GM-CSF and have tolerogenic effect on B cells and support differentiation of other regulatory T cells. IL-10 indirectly downregulates MHC II molecules and co-stimulatory molecules on antigen-presenting cells (APC) and force them to upregulate tolerogenic molecules such as ILT-3, ILT-4 and HLA-G.\n",
      "\n",
      "\n",
      "\n",
      "What is the reason behind the designation of Class L dwarfs, and what is their color and composition?\n",
      "-Spectral class L The defining characteristic of spectral class M, the coolest type in the long-standing classical stellar sequence, is an optical spectrum dominated by absorption bands of titanium(II) oxide (TiO) and vanadium(II) oxide (VO) molecules. However, GD 165B, the cool companion to the white dwarf GD 165, had none of the hallmark TiO features of M dwarfs. The subsequent identification of many objects like GD 165B ultimately led to the definition of a new spectral class, the L dwarfs, defined in the red optical region of the spectrum not by metal-oxide absorption bands (TiO, VO), but by metal hydride emission bands (FeH, CrH, MgH, CaH) and prominent atomic lines of alkali metals (Na, K, Rb, Cs). As of 2013, over 900 L dwarfs have been identified, most by wide-field surveys: the Two Micron All Sky Survey (2MASS), the Deep Near Infrared Survey of the Southern Sky (DENIS), and the Sloan Digital Sky Survey (SDSS). This spectral class contains not only the brown dwarfs, because the coolest main-sequence stars above brown dwarfs (> 80 MJ) have the spectral class L2 to L6.\n",
      "-Class L Class L dwarfs get their designation because they are cooler than M stars and L is the remaining letter alphabetically closest to M. Some of these objects have masses large enough to support hydrogen fusion and are therefore stars, but most are of substellar mass and are therefore brown dwarfs. They are a very dark red in color and brightest in infrared. Their atmosphere is cool enough to allow metal hydrides and alkali metals to be prominent in their spectra.Due to low surface gravity in giant stars, TiO- and VO-bearing condensates never form. Thus, L-type stars larger than dwarfs can never form in an isolated environment. However, it may be possible for these L-type supergiants to form through stellar collisions, an example of which is V838 Monocerotis while in the height of its luminous red nova eruption.\n",
      "-The current stellar classification system originated in the early 20th century, when stars were classified from A to Q based on the strength of the hydrogen line. It was thought that the hydrogen line strength was a simple linear function of temperature. Instead, it was more complicated: it strengthened with increasing temperature, peaked near 9000 K, and then declined at greater temperatures. The classifications were since reordered by temperature, on which the modern scheme is based.Stars are given a single-letter classification according to their spectra, ranging from type O, which are very hot, to M, which are so cool that molecules may form in their atmospheres. The main classifications in order of decreasing surface temperature are: O, B, A, F, G, K, and M. A variety of rare spectral types are given special classifications. The most common of these are types L and T, which classify the coldest low-mass stars and brown dwarfs. Each letter has 10 sub-divisions, numbered from 0 to 9, in order of decreasing temperature. However, this system breaks down at extreme high temperatures as classes O0 and O1 may not exist.In addition, stars may be classified by the luminosity effects found in their spectral lines, which correspond to their spatial size and is determined by their surface gravity. These range from 0 (hypergiants) through III (giants) to V (main sequence dwarfs); some authors add VII (white dwarfs). Main sequence stars fall along a narrow, diagonal band when graphed according to their absolute magnitude and spectral type. The Sun is a main sequence G2V yellow dwarf of intermediate temperature and ordinary size.There is additional nomenclature in the form of lower-case letters added to the end of the spectral type to indicate peculiar features of the spectrum. For example, an \"e\" can indicate the presence of emission lines; \"m\" represents unusually strong levels of metals, and \"var\" can mean variations in the spectral type.White dwarf stars have their own class that begins with the letter D. This is further sub-divided into the classes DA, DB, DC, DO, DZ, and DQ, depending on the types of prominent lines found in the spectrum. This is followed by a numerical value that indicates the temperature.\n",
      "-Spectral class T As GD 165B is the prototype of the L dwarfs, Gliese 229B is the prototype of a second new spectral class, the T dwarfs. T dwarfs are pinkish-magenta. Whereas near-infrared (NIR) spectra of L dwarfs show strong absorption bands of H2O and carbon monoxide (CO), the NIR spectrum of Gliese 229B is dominated by absorption bands from methane (CH4), features that were found only in the giant planets of the Solar System and Titan. CH4, H2O, and molecular hydrogen (H2) collision-induced absorption (CIA) give Gliese 229B blue near-infrared colors. Its steeply sloped red optical spectrum also lacks the FeH and CrH bands that characterize L dwarfs and instead is influenced by exceptionally broad absorption features from the alkali metals Na and K. These differences led J. Davy Kirkpatrick to propose the T spectral class for objects exhibiting H- and K-band CH4 absorption. As of 2013, 355 T dwarfs are known. NIR classification schemes for T dwarfs have recently been developed by Adam Burgasser and Tom Geballe. Theory suggests that L dwarfs are a mixture of very-low-mass stars and sub-stellar objects (brown dwarfs), whereas the T dwarf class is composed entirely of brown dwarfs. Because of the absorption of sodium and potassium in the green part of the spectrum of T dwarfs, the actual appearance of T dwarfs to human visual perception is estimated to be not brown, but magenta. T-class brown dwarfs, such as WISE 0316+4307, have been detected more than 100 light-years from the Sun.\n",
      "-Cool red and brown dwarf classes The new spectral types L, T, and Y were created to classify infrared spectra of cool stars. This includes both red dwarfs and brown dwarfs that are very faint in the visible spectrum.Brown dwarfs, stars that do not undergo hydrogen fusion, cool as they age and so progress to later spectral types. Brown dwarfs start their lives with M-type spectra and will cool through the L, T, and Y spectral classes, faster the less massive they are; the highest-mass brown dwarfs cannot have cooled to Y or even T dwarfs within the age of the universe. Because this leads to an unresolvable overlap between spectral types' effective temperature and luminosity for some masses and ages of different L-T-Y types, no distinct temperature or luminosity values can be given.\n",
      "\n",
      "\n",
      "\n",
      "What was Isaac Newton's explanation for rectilinear propagation of light?\n",
      "-Isaac Newton rejected the wave explanation of rectilinear propagation, believing that if light consisted of waves, it would \"bend and spread every way\" into the shadows. His corpuscular theory of light explained rectilinear propagation more simply, and it accounted for the ordinary laws of refraction and reflection, including TIR, on the hypothesis that the corpuscles of light were subject to a force acting perpendicular to the interface. In this model, for dense-to-rare incidence, the force was an attraction back towards the denser medium, and the critical angle was the angle of incidence at which the normal velocity of the approaching corpuscle was just enough to reach the far side of the force field; at more oblique incidence, the corpuscle would be turned back. Newton gave what amounts to a formula for the critical angle, albeit in words: \"as the Sines are which measure the Refraction, so is the Sine of Incidence at which the total Reflexion begins, to the Radius of the Circle\".Newton went beyond Huygens in two ways. First, not surprisingly, Newton pointed out the relationship between TIR and dispersion: when a beam of white light approaches a glass-to-air interface at increasing obliquity, the most strongly-refracted rays (violet) are the first to be \"taken out\" by \"total Reflexion\", followed by the less-refracted rays. Second, he observed that total reflection could be frustrated (as we now say) by laying together two prisms, one plane and the other slightly convex; and he explained this simply by noting that the corpuscles would be attracted not only to the first prism, but also to the second.In two other ways, however, Newton's system was less coherent. First, his explanation of partial reflection depended not only on the supposed forces of attraction between corpuscles and media, but also on the more nebulous hypothesis of \"Fits of easy Reflexion\" and \"Fits of easy Transmission\". Second, although his corpuscles could conceivably have \"sides\" or \"poles\", whose orientations could conceivably determine whether the corpuscles suffered ordinary or extraordinary refraction in \"Island-Crystal\", his geometric description of the extraordinary refraction was theoretically unsupported and empirically inaccurate.\n",
      "-Isaac Newton worked on optics throughout his research career, conducting various experiments and developing hypotheses to explain his results. He dismissed Descartes' theory of light because he rejected Descartes’ understanding of space, which derived from it. With the publication of Opticks in 1704, Newton for the first time took a clear position supporting a corpuscular interpretation, though it would fall on his followers to systemise the theory. In the book, Newton argued that the geometric nature of reflection and refraction of light could only be explained if light were made of particles because waves do not tend to travel in straight lines.  Newton's corpuscular theory was an elaboration of his view of reality as interactions of material points through forces. Note Albert Einstein's description of Newton's conception of physical reality: [Newton's] physical reality is characterised by concepts of space, time, the material point and force (interaction between material points). Physical events are to be thought of as movements according to the law of material points in space. The material point is the only representative of reality in so far as it is subject to change. The concept of the material point is obviously due to observable bodies; one conceived of the material point on the analogy of movable bodies by omitting characteristics of extension, form, spatial locality, and all their 'inner' qualities, retaining only inertia, translation, and the additional concept of force.\n",
      "-Wave theory of light Beginning in 1670 and progressing over three decades, Isaac Newton developed and championed his corpuscular theory, arguing that the perfectly straight lines of reflection demonstrated light's particle nature, as at that time no wave theory demonstrated travel in straight lines.: 19  He explained refraction by positing that particles of light accelerated laterally upon entering a denser medium. Around the same time, Newton's contemporaries Robert Hooke and Christiaan Huygens, and later Augustin-Jean Fresnel, mathematically refined the wave viewpoint, showing that if light traveled at different speeds in different media, refraction could be easily explained as the medium-dependent propagation of light waves. The resulting Huygens–Fresnel principle was extremely successful at reproducing light's behaviour and was consistent with Thomas Young's discovery of wave interference of light by his double-slit experiment in 1801. The wave view did not immediately displace the ray and particle view, but began to dominate scientific thinking about light in the mid 19th century, since it could explain polarization phenomena that the alternatives could not.James Clerk Maxwell discovered that he could apply his previously discovered Maxwell's equations, along with a slight modification to describe self-propagating waves of oscillating electric and magnetic fields. It quickly became apparent that visible light, ultraviolet light, and infrared light were all electromagnetic waves of differing frequency.: 272  This theory became a critical ingredient in the beginning of quantum mechanics.\n",
      "-The dominance of Newtonian natural philosophy in the eighteenth century was one of the decisive factors ensuring the prevalence of the corpuscular theory of light. Newtonians maintained that the corpuscles of light were projectiles that travelled from the source to the receiver with a finite speed. In this description, the propagation of light is transportation of matter. By the turn of the century, however, more evidence in the form of novel experiments on diffraction, interference, and polarization showcased issues with the theory. A wave theory based on Huygens’, Leonard Euler's, Thomas Young's, and Augustin-Jean Fresnel's work would materialise in a novel wave theory of light. To some extent, Newton's corpuscular (particle) theory of light re-emerged in the 20th century, as a light phenomenon is currently explained as particle and wave.\n",
      "-The critical angle is the smallest angle of incidence that yields total reflection, or equivalently the largest angle for which a refracted ray exists. For light waves incident from an \"internal\" medium with a single refractive index n1 ,‍ to an \"external\" medium with a single refractive index n2 ,‍ the critical angle is given by‍  arcsin ⁡(n2/n1), and is defined if‍ n2 ≤ n1.  For some other types of waves, it is more convenient to think in terms of propagation velocities rather than refractive indices. The explanation of the critical angle in terms of velocities is more general and will therefore be discussed first。 When a wavefront is refracted from one medium to another, the incident (incoming) and refracted (outgoing) portions of the wavefront meet at a common line on the refracting surface (interface). Let this line, denoted by L, move at velocity u across the surface, where u is measured normal to L‍ (Fig. 4). Let the incident and refracted wavefronts propagate with normal velocities  v1 and  v2 (respectively), and let them make the dihedral angles θ1 and θ2 (respectively) with the interface. From the geometry,‍  v1 is the component of u in the direction normal to the incident wave, so that‍  sin ⁡θ1.\n",
      "\n",
      "\n",
      "\n",
      "What is the relationship between chemical potential and quarks/antiquarks?\n",
      "-The phase diagram of quark matter is not well known, either experimentally or theoretically. A commonly conjectured form of the phase diagram is shown in the figure to the right. It is applicable to matter in a compact star, where the only relevant thermodynamic potentials are quark chemical potential μ and temperature T.  For guidance it also shows the typical values of μ and T in heavy-ion collisions and in the early universe. For readers who are not familiar with the concept of a chemical potential, it is helpful to think of μ as a measure of the imbalance between quarks and antiquarks in the system. Higher μ means a stronger bias favoring quarks over antiquarks. At low temperatures there are no antiquarks, and then higher μ generally means a higher density of quarks.\n",
      "-Sub-nuclear particles In recent years, thermal physics has applied the definition of chemical potential to systems in particle physics and its associated processes. For example, in a quark–gluon plasma or other QCD matter, at every point in space there is a chemical potential for photons, a chemical potential for electrons, a chemical potential for baryon number, electric charge, and so forth.\n",
      "-Gibbs later noted also that for the purposes of this definition, any chemical element or combination of elements in given proportions may be considered a substance, whether capable or not of existing by itself as a homogeneous body. This freedom to choose the boundary of the system allows the chemical potential to be applied to a huge range of systems. The term can be used in thermodynamics and physics for any system undergoing change. Chemical potential is also referred to as partial molar Gibbs energy (see also partial molar property). Chemical potential is measured in units of energy/particle or, equivalently, energy/mole.\n",
      "-Chemical potentials are important in many aspects of multi-phase equilibrium chemistry, including melting, boiling, evaporation, solubility, osmosis, partition coefficient, liquid-liquid extraction and chromatography. In each case the chemical potential of a given species at equilibrium is the same in all phases of the system.In electrochemistry, ions do not always tend to go from higher to lower chemical potential, but they do always go from higher to lower electrochemical potential. The electrochemical potential completely characterizes all of the influences on an ion's motion, while the chemical potential includes everything except the electric force. (See below for more on this terminology.) \n",
      "-The similar term chemical potential is used to indicate the potential of a substance to undergo a change of configuration, be it in the form of a chemical reaction, spatial transport, particle exchange with a reservoir, etc.\n",
      "\n",
      "\n",
      "\n",
      "What is the American Petroleum Institute (API) gravity?\n",
      "-API gravity is thus an inverse measure of a petroleum liquid's density relative to that of water (also known as specific gravity). It is used to compare densities of petroleum liquids. For example, if one petroleum liquid is less dense than another, it has a greater API gravity. Although API gravity is mathematically a dimensionless quantity (see the formula below), it is referred to as being in 'degrees'. API gravity is graduated in degrees on a hydrometer instrument. API gravity values of most petroleum liquids fall between 10 and 70 degrees.\n",
      "-The American Petroleum Institute gravity, or API gravity, is a measure of how heavy or light a petroleum liquid is compared to water: if its API gravity is greater than 10, it is lighter and floats on water; if less than 10, it is heavier and sinks.\n",
      "-To derive the API gravity, the specific gravity (i.e., density relative to water) is first measured using either the hydrometer, detailed in ASTM D1298 or with the oscillating U-tube method detailed in ASTM D4052.\n",
      "Density adjustments at different temperatures, corrections for soda-lime glass expansion and contraction and meniscus corrections for opaque oils are detailed in the Petroleum Measurement Tables, details of usage specified in ASTM D1250. The specific gravity is defined by the formula below.\n",
      "-The formula to calculate API gravity from specific gravity (SG) is: API gravity 141.5 SG 131.5 Conversely, the specific gravity of petroleum liquids can be derived from their API gravity value as SG at 60 141.5 API gravity 131.5 Thus, a heavy oil with a specific gravity of 1.0 (i.e., with the same density as pure water at 60 °F) has an API gravity of: 141.5 1.0 131.5 10.0 API \n",
      "-Generally speaking, oil with an API gravity between 40 and 45° commands the highest prices. Above 45°, the molecular chains become shorter and less valuable to refineries.Crude oil is classified as light, medium, or heavy according to its measured API gravity.\n",
      "\n",
      "\n",
      "\n",
      "What are the two main factors that cause resistance in a metal?\n",
      "-Most metals have electrical resistance. In simpler models (non quantum mechanical models) this can be explained by replacing electrons and the crystal lattice by a wave-like structure. When the electron wave travels through the lattice, the waves interfere, which causes resistance. The more regular the lattice is, the less disturbance happens and thus the less resistance. The amount of resistance is thus mainly caused by two factors. First, it is caused by the temperature and thus amount of vibration of the crystal lattice. Higher temperatures cause bigger vibrations, which act as irregularities in the lattice. Second, the purity of the metal is relevant as a mixture of different ions is also an irregularity. The small decrease in conductivity on melting of pure metals is due to the loss of long range crystalline order. The short range order remains and strong correlation between positions of ions results in coherence between waves diffracted by adjacent ions.\n",
      "-In addition to geometry and material, there are various other factors that influence resistance and conductance, such as temperature; see below.\n",
      "-By adding another element to a metal, differences in the size of the atoms create internal stresses in the lattice of the metallic crystals; stresses that often enhance its properties. For example, the combination of carbon with iron produces steel, which is stronger than iron, its primary element. The electrical and thermal conductivity of alloys is usually lower than that of the pure metals. The physical properties, such as density, reactivity, Young's modulus of an alloy may not differ greatly from those of its base element, but engineering properties such as tensile strength, ductility, and shear strength may be substantially different from those of the constituent materials. This is sometimes a result of the sizes of the atoms in the alloy, because larger atoms exert a compressive force on neighboring atoms, and smaller atoms exert a tensile force on their neighbors, helping the alloy resist deformation. Sometimes alloys may exhibit marked differences in behavior even when small amounts of one element are present. For example, impurities in semiconducting ferromagnetic alloys lead to different properties, as first predicted by White, Hogan, Suhl, Tian Abrie and Nakamura.Unlike pure metals, most alloys do not have a single melting point, but a melting range during which the material is a mixture of solid and liquid phases (a slush). The temperature at which melting begins is called the solidus, and the temperature when melting is just complete is called the liquidus. For many alloys there is a particular alloy proportion (in some cases more than one), called either a eutectic mixture or a peritectic composition, which gives the alloy a unique and low melting point, and no liquid/solid slush transition.\n",
      "-Resistance wire is wire intended for making electrical resistors (which are used to control the amount of current in a circuit). It is better if the alloy used has a high resistivity, since a shorter wire can then be used. In many situations, the stability of the resistor is of primary importance, and thus the alloy's temperature coefficient of resistivity and corrosion resistance play a large part in material selection.\n",
      "-Electric current consists of a flow of electrons. In metals there are many electron energy levels near the Fermi level, so there are many electrons available to move. This is what causes the high electronic conductivity of metals.\n",
      "\n",
      "\n",
      "\n",
      "What is the significance of the redshift-distance relationship in determining the expansion history of the universe?\n",
      "-Expansion of space In the earlier part of the twentieth century, Slipher, Wirtz and others made the first measurements of the redshifts and blueshifts of galaxies beyond the Milky Way. They initially interpreted these redshifts and blueshifts as being due to random motions, but later Lemaître (1927) and Hubble (1929), using previous data, discovered a roughly linear correlation between the increasing redshifts of, and distances to, galaxies. Lemaître realized that these observations could be explained by a mechanism of producing redshifts seen in Friedmann's solutions to Einstein's equations of general relativity. The correlation between redshifts and distances is required by all such models that have a metric expansion of space. As a result, the wavelength of photons propagating through the expanding space is stretched, creating the cosmological redshift.\n",
      "-For distant galaxies and AGNs observations are made of the overall shape and properties of the galaxy, as well as the groupings where they are found. Observations of certain types of variable stars and supernovae of known luminosity, called standard candles, in other galaxies allows the inference of the distance to the host galaxy. The expansion of space causes the spectra of these galaxies to be shifted, depending on the distance, and modified by the Doppler effect of the galaxy's radial velocity. Both the size of the galaxy and its redshift can be used to infer something about the distance of the galaxy. Observations of large numbers of galaxies are referred to as redshift surveys, and are used to model the evolution of galaxy forms.\n",
      "-The observational result of Hubble's Law, the proportional relationship between distance and the speed with which a galaxy is moving away from us (usually referred to as redshift) is a product of the cosmic distance ladder. Edwin Hubble observed that fainter galaxies are more redshifted. Finding the value of the Hubble constant was the result of decades of work by many astronomers, both in amassing the measurements of galaxy redshifts and in calibrating the steps of the distance ladder. Hubble's Law is the primary means we have for estimating the distances of quasars and distant galaxies in which individual distance indicators cannot be seen.\n",
      "-There is a distinction between a redshift in cosmological context as compared to that witnessed when nearby objects exhibit a local Doppler-effect redshift. Rather than cosmological redshifts being a consequence of the relative velocities that are subject to the laws of special relativity (and thus subject to the rule that no two locally separated objects can have relative velocities with respect to each other faster than the speed of light), the photons instead increase in wavelength and redshift because of a global feature of the spacetime through which they are traveling. One interpretation of this effect is the idea that space itself is expanding. Due to the expansion increasing as distances increase, the distance between two remote galaxies can increase at more than 3×108 m/s, but this does not imply that the galaxies move faster than the speed of light at their present location (which is forbidden by Lorentz covariance).\n",
      "-In astronomy, a redshift survey is a survey of a section of the sky to measure the redshift of astronomical objects: usually galaxies, but sometimes other objects such as galaxy clusters or quasars.  Using Hubble's law, the redshift can be used to estimate the distance of an object from Earth. By combining redshift with angular position data, a redshift survey maps the 3D distribution of matter within a field of the sky. These observations are used to measure detailed statistical properties of the large-scale structure of the universe. In conjunction with observations of early structure in the cosmic microwave background, these results can place strong constraints on cosmological parameters such as the average matter density and the Hubble constant.Generally the construction of a redshift survey involves two phases: first the selected area of the sky is imaged with a wide-field telescope, then galaxies brighter than a defined limit are selected from the resulting images as non-pointlike objects; optionally, colour selection may also be used to assist discrimination between stars and galaxies. Secondly, the selected galaxies are observed by spectroscopy, most commonly at visible wavelengths, to measure the wavelengths of prominent spectral lines; comparing observed and laboratory wavelengths then gives the redshift for each galaxy.\n",
      "\n",
      "\n",
      "\n",
      "What is the Evans balance?\n",
      "-An Evans balance, also known as a Johnson's balance (after a commercial producer of the Evans balance) is a device for measuring magnetic susceptibility. Magnetic susceptibility is related to the force experienced by a substance in a magnetic field. Various practical devices are available for the measurement of susceptibility, which differ in the shape of the magnetic field and the way the force is measured.The Evans balance employs a similar sample configuration but measures the force on the magnet.\n",
      "-Advantages vs alternative magnetic balances The main advantage of this system is that it is cheap to construct as it does not require a precision weighing device. Moreover, using a Evans balance is less time-consuming than using a Gouy or Faraday balances, although it is not sensitive and accurate in comparison to these last two systems. One reason that they were time-consuming is that the sample had to be suspended between the two poles of a very powerful magnet. The tube had to be suspended in the same place every time for the apparatus constant to be accurate. In the case of the Gouy balance, the static charge on the glass tube often caused the tube to stick to magnets. With the Evans balance, a reading could be taken in a matter of seconds with only small sacrifices in sensitivity and accuracy. A Johnson-Matthey balance has a range from 0.001 x 10−7 to 1.99 x 10−7 c.g.s. volume susceptibility units. The original Evans balance had an accuracy within 1% of literature values for diamagnetic solutions and within 2% of literature values of paramagnetic solids.The system allows for measurements of solid, liquid, and gaseous forms of a wide range of paramagnetic and diamagnetic materials. For each measurement, only around 250 mg of sample is required (50 mg can be used for a thin-bore sample tube).\n",
      "-Volume magnetic susceptibility is measured by the force change felt upon a substance when a magnetic field gradient is applied. Early measurements are made using the Gouy balance where a sample is hung between the poles of an electromagnet. The change in weight when the electromagnet is turned on is proportional to the susceptibility. Today, high-end measurement systems use a superconductive magnet. An alternative is to measure the force change on a strong compact magnet upon insertion of the sample. This system, widely used today, is called the Evans balance. For liquid samples, the susceptibility can be measured from the dependence of the NMR frequency of the sample on its shape or orientation.Another method using NMR techniques measures the magnetic field distortion around a sample immersed in water inside an MR scanner. This method is highly accurate for diamagnetic materials with susceptibilities similar to water.\n",
      "-The Evans balance. is a torsion balance which uses a sample in a fixed position and a variable secondary magnet to bring the magnets back to their initial position. It, too, is calibrated against HgCo(NCS)4.\n",
      "With a Faraday balance the sample is placed in a magnetic field of constant gradient, and weighed on a torsion balance. This method can yield information on magnetic anisotropy.\n",
      "SQUID is a very sensitive magnetometer.\n",
      "For substances in solution NMR may be used to measure susceptibility.\n",
      "-A Faraday balance is a device for measuring magnetic susceptibility. Magnetic susceptibility is related to the force experienced by a substance in a magnetic field. Various practical devices are available for the measurement of susceptibility, which differ in the shape of the magnetic field and the way the force is measured.In the Faraday balance the field is homogeneous. The pole pieces of the magnet are so shaped that there is a region in which the product of the field strength and field gradient in the z direction is constant. The sample is placed in this region. The force in this case is independent of the packing of the sample and depends only on the total mass of the material present. The method is sensitive and highly reproducible and can be applied to single crystals. The force is measured as a weight change, using a torsion balance.\n",
      "\n",
      "\n",
      "\n",
      "What is the definition of dimension in mathematics?\n",
      "-In mathematics, the dimension of an object is, roughly speaking, the number of degrees of freedom of a point that moves on this object. In other words, the dimension is the number of independent parameters or coordinates that are needed for defining the position of a point that is constrained to be on the object. For example, the dimension of a point is zero; the dimension of a line is one, as a point can move on a line in only one direction (or its opposite); the dimension of a plane is two etc.\n",
      "-The dimension is an intrinsic property of an object, in the sense that it is independent of the dimension of the space in which the object is or can be embedded. For example, a curve, such as a circle, is of dimension one, because the position of a point on a curve is determined by its signed distance along the curve to a fixed point on the curve. This is independent from the fact that a curve cannot be embedded in a Euclidean space of dimension lower than two, unless it is a line.\n",
      "-The rest of this section examines some of the more important mathematical definitions of dimension.\n",
      "Vector spaces The dimension of a vector space is the number of vectors in any basis for the space, i.e. the number of coordinates necessary to specify any vector. This notion of dimension (the cardinality of a basis) is often referred to as the Hamel dimension or algebraic dimension to distinguish it from other notions of dimension.\n",
      "For the non-free case, this generalizes to the notion of the length of a module.\n",
      "Manifolds The uniquely defined dimension of every connected topological manifold can be calculated. A connected topological manifold is locally homeomorphic to Euclidean n-space, in which the number n is the manifold's dimension.\n",
      "For connected differentiable manifolds, the dimension is also the dimension of the tangent vector space at any point.\n",
      "-In physics and mathematics, the dimension of a mathematical space (or object) is informally defined as the minimum number of coordinates needed to specify any point within it. Thus, a line has a dimension of one (1D) because only one coordinate is needed to specify a point on it – for example, the point at 5 on a number line. A surface, such as the boundary of a cylinder or sphere, has a dimension of two (2D) because two coordinates are needed to specify a point on it – for example, both a latitude and longitude are required to locate a point on the surface of a sphere. A two-dimensional Euclidean space is a two-dimensional space on the plane. The inside of a cube, a cylinder or a sphere is three-dimensional (3D) because three coordinates are needed to locate a point within these spaces.\n",
      "-The concept of dimension is not restricted to physical objects. High-dimensional spaces frequently occur in mathematics and the sciences. They may be Euclidean spaces or more general parameter spaces or configuration spaces such as in Lagrangian or Hamiltonian mechanics; these are abstract spaces, independent of the physical space.\n",
      "\n",
      "\n",
      "\n",
      "What is accelerator-based light-ion fusion?\n",
      "-Beam–beam or beam–target fusion Accelerator-based light-ion fusion is a technique using particle accelerators to achieve particle kinetic energies sufficient to induce light-ion fusion reactions.Accelerating light ions is relatively easy, and can be done in an efficient manner—requiring only a vacuum tube, a pair of electrodes, and a high-voltage transformer; fusion can be observed with as little as 10 kV between the electrodes. The system can be arranged to accelerate ions into a static fuel-infused target, known as beam–target fusion, or by accelerating two streams of ions towards each other, beam–beam fusion. The key problem with accelerator-based fusion (and with cold targets in general) is that fusion cross sections are many orders of magnitude lower than Coulomb interaction cross-sections. Therefore, the vast majority of ions expend their energy emitting bremsstrahlung radiation and the ionization of atoms of the target. Devices referred to as sealed-tube neutron generators are particularly relevant to this discussion. These small devices are miniature particle accelerators filled with deuterium and tritium gas in an arrangement that allows ions of those nuclei to be accelerated against hydride targets, also containing deuterium and tritium, where fusion takes place, releasing a flux of neutrons. Hundreds of neutron generators are produced annually for use in the petroleum industry where they are used in measurement equipment for locating and mapping oil reserves.A number of attempts to recirculate the ions that \"miss\" collisions have been made over the years. One of the better-known attempts in the 1970s was Migma, which used a unique particle storage ring to capture ions into circular orbits and return them to the reaction area. Theoretical calculations made during funding reviews pointed out that the system would have significant difficulty scaling up to contain enough fusion fuel to be relevant as a power source. In the 1990s, a new arrangement using a field-reverse configuration (FRC) as the storage system was proposed by Norman Rostoker and continues to be studied by TAE Technologies as of 2021. A closely related approach is to merge two FRC's rotating in opposite directions, which is being actively studied by Helion Energy. Because these approaches all have ion energies well beyond the Coulomb barrier, they often suggest the use of alternative fuel cycles like p-11B that are too difficult to attempt using conventional approaches.\n",
      "-Beam–beam or beam–target fusion Accelerator-based light-ion fusion is a technique using particle accelerators to achieve particle kinetic energies sufficient to induce light-ion fusion reactions.Accelerating light ions is relatively easy, and can be done in an efficient manner—requiring only a vacuum tube, a pair of electrodes, and a high-voltage transformer; fusion can be observed with as little as 10 kV between the electrodes. The system can be arranged to accelerate ions into a static fuel-infused target, known as beam–target fusion, or by accelerating two streams of ions towards each other, beam–beam fusion. The key problem with accelerator-based fusion (and with cold targets in general) is that fusion cross sections are many orders of magnitude lower than Coulomb interaction cross-sections. Therefore, the vast majority of ions expend their energy emitting bremsstrahlung radiation and the ionization of atoms of the target. Devices referred to as sealed-tube neutron generators are particularly relevant to this discussion. These small devices are miniature particle accelerators filled with deuterium and tritium gas in an arrangement that allows ions of those nuclei to be accelerated against hydride targets, also containing deuterium and tritium, where fusion takes place, releasing a flux of neutrons. Hundreds of neutron generators are produced annually for use in the petroleum industry where they are used in measurement equipment for locating and mapping oil reserves.A number of attempts to recirculate the ions that \"miss\" collisions have been made over the years. One of the better-known attempts in the 1970s was Migma, which used a unique particle storage ring to capture ions into circular orbits and return them to the reaction area. Theoretical calculations made during funding reviews pointed out that the system would have significant difficulty scaling up to contain enough fusion fuel to be relevant as a power source. In the 1990s, a new arrangement using a field-reverse configuration (FRC) as the storage system was proposed by Norman Rostoker and continues to be studied by TAE Technologies as of 2021. A closely related approach is to merge two FRC's rotating in opposite directions, which is being actively studied by Helion Energy. Because these approaches all have ion energies well beyond the Coulomb barrier, they often suggest the use of alternative fuel cycles like p-11B that are too difficult to attempt using conventional approaches.\n",
      "-Low-energy machines and particle therapy Everyday examples of particle accelerators are cathode ray tubes found in television sets and X-ray generators. These low-energy accelerators use a single pair of electrodes with a DC voltage of a few thousand volts between them. In an X-ray generator, the target itself is one of the electrodes. A low-energy particle accelerator called an ion implanter is used in the manufacture of integrated circuits.\n",
      "-Particle accelerators are a well-developed technology used in scientific research. They use electromagnetic fields to accelerate and direct charged particles along a predetermined path, and electrostatic \"lenses\" to focus these streams for collisions. The cathode ray tube in many twentieth-century televisions and computer monitors is a very simple type of particle accelerator. More powerful versions include synchrotrons and cyclotrons used in nuclear research. A particle-beam weapon is a weaponized version of this technology. It accelerates charged particles (in most cases electrons, positrons, protons, or ionized atoms, but very advanced versions can accelerate other particles such as mercury nuclei) to near-light speed and then directs them towards a target. The particles' kinetic energy is imparted to matter in the target, inducing near-instantaneous and catastrophic superheating at the surface, and when penetrating deeper, ionization effects that can destroy electronics. However, high-power accelerators are extremely massive (sometimes on the order of kilometers in length, like the LHC), with highly constricted construction, operation and maintenance requirements, and thus unable to be weaponized using present technologies.\n",
      "-Wakefield acceleration A plasma consists of a fluid of positive and negative charged particles, generally created by heating or photo-ionizing (direct / tunneling / multi-photon / barrier-suppression) a dilute gas. Under normal conditions the plasma will be macroscopically neutral (or quasi-neutral), an equal mix of electrons and ions in equilibrium. However, if a strong enough external electric or electromagnetic field is applied, the plasma electrons, which are very light in comparison to the background ions (by a factor of 1836), will separate spatially from the massive ions creating a charge imbalance in the perturbed region. A particle injected into such a plasma would be accelerated by the charge separation field, but since the magnitude of this separation is generally similar to that of the external field, apparently nothing is gained in comparison to a conventional system that simply applies the field directly to the particle. But, the plasma medium acts as the most efficient transformer (currently known) of the transverse field of an electromagnetic wave into longitudinal fields of a plasma wave. In existing accelerator technology various appropriately designed materials are used to convert from transverse propagating extremely intense fields into longitudinal fields that the particles can get a kick from. This process is achieved using two approaches: standing-wave structures (such as resonant cavities) or traveling-wave structures such as disc-loaded waveguides etc. But, the limitation of materials interacting with higher and higher fields is that they eventually get destroyed through ionization and breakdown. Here the plasma accelerator science provides the breakthrough to generate, sustain, and exploit the highest fields ever produced by science in the laboratory.\n",
      "\n",
      "\n",
      "\n",
      "What is the interstellar medium (ISM)?\n",
      "-In astronomy, the interstellar medium (ISM) is the matter and radiation that exist in the space between the star systems in a galaxy. This matter includes gas in ionic, atomic, and molecular form, as well as dust and cosmic rays. It fills interstellar space and blends smoothly into the surrounding intergalactic space. The energy that occupies the same volume, in the form of electromagnetic radiation, is the interstellar radiation field. Although the density of atoms in the ISM is usually far below that in the best laboratory vacuums, the mean free path between collisions is short compared to typical interstellar lengths, so on these scales the ISM behaves as a gas (more precisely, as a plasma: it is everywhere at least slightly ionized), responding to pressure forces, and not as a collection of non-interacting particles.\n",
      "-The interstellar medium is matter that occupies the space between star systems in a galaxy. 99% of this matter is gaseous – hydrogen, helium, and smaller quantities of other ionized elements such as oxygen. The other 1% is dust particles, thought to be mainly graphite, silicates, and ices. Clouds of the dust and gas are referred to as nebulae.\n",
      "-X-ray Quantum Calorimeter (XQC) project In astronomy, the interstellar medium (or ISM) is the gas and cosmic dust that pervade interstellar space: the matter that exists between the star systems within a galaxy. It fills interstellar space and blends smoothly into the surrounding intergalactic medium. The interstellar medium consists of an extremely dilute (by terrestrial standards) mixture of ions, atoms, molecules, larger dust grains, cosmic rays, and (galactic) magnetic fields. The energy that occupies the same volume, in the form of electromagnetic radiation, is the interstellar radiation field.\n",
      "-The ISM plays a crucial role in astrophysics precisely because of its intermediate role between stellar and galactic scales. Stars form within the densest regions of the ISM, which ultimately contributes to molecular clouds and replenishes the ISM with matter and energy through planetary nebulae, stellar winds, and supernovae. This interplay between stars and the ISM helps determine the rate at which a galaxy depletes its gaseous content, and therefore its lifespan of active star formation.\n",
      "-The interstellar medium is composed of multiple phases distinguished by whether matter is ionic, atomic, or molecular, and the temperature and density of the matter. The interstellar medium is composed primarily of hydrogen, followed by helium with trace amounts of carbon, oxygen, and nitrogen. The thermal pressures of these phases are in rough equilibrium with one another. Magnetic fields and turbulent motions also provide pressure in the ISM, and are typically more important, dynamically, than the thermal pressure. In the interstellar medium, matter is primarily in molecular form and reaches number densities of 1012 molecules per m3 (1 trillion molecules per m3). In hot, diffuse regions, gas is highly ionized, and the density may be as low as 100 ions per m3. Compare this with a number density of roughly 1025 molecules per m3 for air at sea level, and 1016 molecules per m3 (10 quadrillion molecules per m3) for a laboratory high-vacuum chamber. By mass, 99% of the ISM is gas in any form, and 1% is dust. Of the gas in the ISM, by number 91% of atoms are hydrogen and 8.9% are helium, with 0.1% being atoms of elements heavier than hydrogen or helium, known as \"metals\" in astronomical parlance. By mass this amounts to 70% hydrogen, 28% helium, and 1.5% heavier elements. The hydrogen and helium are primarily a result of primordial nucleosynthesis, while the heavier elements in the ISM are mostly a result of enrichment (due to stellar nucleosynthesis) in the process of stellar evolution.\n",
      "\n",
      "\n",
      "\n",
      "What is the significance of the change in slope of the pinched hysteresis curves in ReRAM and other forms of two-terminal resistance memory?\n",
      "-Pinched hysteresis One of the resulting properties of memristors and memristive systems is the existence of a pinched hysteresis effect. For a current-controlled memristive system, the input u(t) is the current i(t), the output y(t) is the voltage v(t), and the slope of the curve represents the electrical resistance. The change in slope of the pinched hysteresis curves demonstrates switching between different resistance states which is a phenomenon central to ReRAM and other forms of two-terminal resistance memory. At high frequencies, memristive theory predicts the pinched hysteresis effect will degenerate, resulting in a straight line representative of a linear resistor. It has been proven that some types of non-crossing pinched hysteresis curves (denoted Type-II) cannot be described by memristors.\n",
      "-As the frequency tends to infinity, the hysteresis loop degenerates to a straight line through the origin, whose slope depends on the amplitude and shape of the forcing signal.According to Chua all resistive switching memories including ReRAM, MRAM and phase-change memory meet these criteria and are memristors. However, the lack of data for the Lissajous curves over a range of initial conditions or over a range of frequencies complicates assessments of this claim.\n",
      "-The above-mentioned thermodynamic principle furthermore implies that the operation of two-terminal non-volatile memory devices (e.g. \"resistance-switching\" memory devices (ReRAM)) cannot be associated with the memristor concept, i.e., such devices cannot by itself remember their current or voltage history. Transitions between distinct internal memory or resistance states are of probabilistic nature. The probability for a transition from state {i} to state {j} depends on the height of the free-energy barrier between both states. The transition probability can thus be influenced by suitably driving the memory device, i.e., by \"lowering\" the free-energy barrier for the transition {i} → {j} by means of, for example, an externally applied bias.\n",
      "-Another example suggests including an offset value  a to account for an observed nanobattery effect which violates the predicted zero-crossing pinched hysteresis effect.\n",
      "y(t)=g0(x,u)(u(t)−a),x˙=f(x,u) \n",
      "-Experimental evidence shows that redox-based resistance memory (ReRAM) includes a nanobattery effect that is contrary to Chua's memristor model. This indicates that the memristor theory needs to be extended or corrected to enable accurate ReRAM modeling.\n",
      "\n",
      "\n",
      "\n",
      "What is geometric quantization in mathematical physics?\n",
      "-In mathematical physics, geometric quantization is a mathematical approach to defining a quantum theory corresponding to a given classical theory. It attempts to carry out quantization, for which there is in general no exact recipe, in such a way that certain analogies between the classical theory and the quantum theory remain manifest. For example, the similarity between the Heisenberg equation in the Heisenberg picture of quantum mechanics and the Hamilton equation in classical physics should be built in.\n",
      "-In mathematical physics, geometric quantization is a mathematical approach to defining a quantum theory corresponding to a given classical theory. It attempts to carry out quantization, for which there is in general no exact recipe, in such a way that certain analogies between the classical theory and the quantum theory remain manifest. For example, the similarity between the Heisenberg equation in the Heisenberg picture of quantum mechanics and the Hamilton equation in classical physics should be built in.\n",
      "-In theoretical physics, quantum geometry is the set of mathematical concepts generalizing the concepts of geometry whose understanding is necessary to describe the physical phenomena at distance scales comparable to the Planck length. At these distances, quantum mechanics has a profound effect on physical phenomena.\n",
      "-The idea is that in the quantum Hilbert space, the sections should be functions of only  n variables on the  2n -dimensional classical phase space.\n",
      "If  f is a function for which the associated Hamiltonian flow preserves the polarization, then  Q(f) will preserve the quantum Hilbert space.\n",
      "The assumption that the flow of  f preserve the polarization is a strong one. Typically not very many functions will satisfy this assumption.\n",
      "-The geometric quantization procedure falls into the following three steps: prequantization, polarization, and metaplectic correction. Prequantization produces a natural Hilbert space together with a quantization procedure for observables that exactly transforms Poisson brackets on the classical side into commutators on the quantum side. Nevertheless, the prequantum Hilbert space is generally understood to be \"too big\". The idea is that one should then select a Poisson-commuting set of n variables on the 2n-dimensional phase space and consider functions (or, more properly, sections) that depend only on these n variables. The n variables can be either real-valued, resulting in a position-style Hilbert space, or complex analytic, producing something like the Segal–Bargmann space.\n",
      "\n",
      "\n",
      "\n",
      "What is the definition of an improper rotation?\n",
      "-An improper rotation is the composition of a rotation about an axis, and reflection in a plane perpendicular to that axis. The order in which the rotation and reflection are performed does not matter (that is, these operations commute). Improper rotation is also defined as the composition of a rotation about an axis, and inversion about a point on the axis. These definitions are equivalent because inversion about a point is equivalent to rotation by 180° about any axis, followed by mirroring about a plane perpendicular to that axis. The symmetry elements for improper rotation are the rotation axis, and either the mirror plane, the inversion point, or both. The improper rotation group of order 2n is denoted S2n.\n",
      "-It is used as a symmetry operation in the context of geometric symmetry, molecular symmetry and crystallography, where an object that is unchanged by a combination of rotation and reflection is said to have improper rotation symmetry.\n",
      "-In geometry, an improper rotation (also called rotation-reflection, rotoreflection, rotary reflection, or rotoinversion) is an isometry in Euclidean space that is a combination of a rotation about an axis and a reflection in a plane perpendicular to that axis. Reflection and inversion are each special case of improper rotation. Any improper rotation is an affine transformation and, in cases that keep the coordinate origin fixed, a linear transformation.\n",
      "-A three-dimensional symmetry that has only one fixed point is necessarily an improper rotation.An improper rotation of an object thus produces a rotation of its mirror image. The axis is called the rotation-reflection axis. This is called an n-fold improper rotation if the angle of rotation, before or after reflexion, is 360°/n (where n must be even). There are several different systems for naming individual improper rotations: In the Schoenflies notation the symbol Sn (German, Spiegel, for mirror), where n must be even, denotes the symmetry group generated by an n-fold improper rotation. For example, the symmetry operation S6 is the combination of a rotation of (360°/6)=60° and a mirror plane reflection. (This should not to be confused with the same notation for symmetric groups).\n",
      "-In 3 dimensions, improper rotation is equivalently defined as a combination of rotation about an axis and inversion in a point on the axis. For this reason it is also called a rotoinversion or rotary inversion. The two definitions are equivalent because rotation by an angle θ followed by reflection is the same transformation as rotation by θ + 180° followed by inversion (taking the point of inversion to be in the plane of reflection). In both definitions, the operations commute.\n",
      "\n",
      "\n",
      "\n",
      "What is power density in the context of energy systems, and how does it differ between renewable and non-renewable energy sources?\n",
      "-Measured in W/m2 it describes the amount of power obtained per unit of Earth surface area used by a specific energy system, including all supporting infrastructure, manufacturing, mining of fuel (if applicable) and decommissioning., Fossil fuels and nuclear power are characterized by high power density which means large power can be drawn from power plants occupying relatively small area. Renewable energy sources have power density at least three orders of magnitude smaller and for the same energy output they need to occupy accordingly larger area, which has been already highlighted as a limiting factor of renewable energy in German Energiewende.The following table shows median surface power density of renewable and non-renewable energy sources.\n",
      "-Surface power density is an important factor in comparison of industrial energy sources. The concept was popularised by geographer Vaclav Smil. The term is usually shortened to \"power density\" in the relevant literature, which can lead to confusion with homonymous or related terms.\n",
      "-Power density is the amount of power (time rate of energy transfer) per unit volume.In energy transformers including batteries, fuel cells, motors, power supply units etc., power density refers to a volume, where it is often called volume power density, expressed as W/m3.\n",
      "In reciprocating internal combustion engines, power density (power per swept volume or brake horsepower per cubic centimeter) is an important metric, based on the internal capacity of the engine, not its external size.\n",
      "-In physics, energy density is the amount of energy stored in a given system or region of space per unit volume. It is sometimes confused with energy per unit mass which is properly called specific energy or gravimetric energy density.\n",
      "-The International Energy Agency defines renewable energy saying Renewable energy is derived from natural processes that are replenished constantly. In its various forms, it derives directly from the sun, or from heat generated deep within the earth. Included in the definition is electricity and heat generated from solar, wind, ocean, hydropower, biomass, geothermal resources, and biofuels and hydrogen derived from renewable resources. Renewable energy resources exist over wide geographical areas, in contrast to other energy sources, which are concentrated in a limited number of countries.\n",
      "\n",
      "\n",
      "\n",
      "What is Modified Newtonian Dynamics (MOND)?\n",
      "-Modified Newtonian dynamics (MOND) is a hypothesis that proposes a modification of Newton's law of universal gravitation to account for observed properties of galaxies. It is an alternative to the hypothesis of dark matter in terms of explaining why galaxies do not appear to obey the currently understood laws of physics.\n",
      "-MOND is an example of a class of theories known as modified gravity, and is an alternative to the hypothesis that the dynamics of galaxies are determined by massive, invisible dark matter halos. Since Milgrom's original proposal, proponents of MOND have claimed to successfully predict a variety of galactic phenomena that they state are difficult to understand as consequences of dark matter.Though MOND explains the anomalously great rotational velocities of galaxies at their perimeters, it does not fully explain the velocity dispersions of individual galaxies within galaxy clusters. MOND reduces the discrepancy between the velocity dispersions and clusters' observed missing baryonic mass from a factor of around 10 to a factor of about 2. However, the residual discrepancy cannot be accounted for by MOND, requiring that other explanations close the gap such as the presence of as-yet undetected missing baryonic matter.The accurate measurement of the speed of gravitational waves compared to the speed of light in 2017 ruled out a certain class of modified gravity theories but concluded that other MOND theories that dispense with the need for dark matter remained viable. Two years later, theories put forth by Constantinos Skordis and Tom Zlosnik were consistent with gravitational waves that always travel at the speed of light. Later still in 2021, Skordis and Zlosnik developed a subclass of their theory called \"RMOND\", for \"relativistic MOND\", which had \"been shown to reproduce in great detail the main observations in cosmology, including the cosmic-microwave-background power spectrum, and the matter structure power spectrum.\" \n",
      "-MOND Modified Newtonian Dynamics (MOND) is a relatively modern proposal to explain the galaxy rotation problem based on a variation of Newton's Second Law of Dynamics at low accelerations. This would produce a large-scale variation of Newton's universal theory of gravity. A modification of Newton's theory would also imply a modification of general relativistic cosmology in as much as Newtonian cosmology is the limit of Friedman cosmology. While almost all astrophysicists today reject MOND in favor of dark matter, a small number of researchers continue to enhance it, recently incorporating Brans–Dicke theories into treatments that attempt to account for cosmological observations.\n",
      "-The dependence in MOND of the internal dynamics of a system on its external environment (in principle, the rest of the universe) is strongly reminiscent of Mach's principle, and may hint towards a more fundamental structure underlying Milgrom's law. In this regard, Milgrom has commented: It has been long suspected that local dynamics is strongly influenced by the universe at large, a-la Mach's principle, but MOND seems to be the first to supply concrete evidence for such a connection. This may turn out to be the most fundamental implication of MOND, beyond its implied modification of Newtonian dynamics and general relativity, and beyond the elimination of dark matter.\n",
      "-Several observational and experimental tests have been proposed to help distinguish between MOND and dark matter-based models: The detection of particles suitable for constituting cosmological dark matter would strongly suggest that ΛCDM is correct and no modification to Newton's laws is required.\n",
      "\n",
      "\n",
      "\n",
      "What is linear frame dragging?\n",
      "-Linear frame dragging is the similarly inevitable result of the general principle of relativity, applied to linear momentum. Although it arguably has equal theoretical legitimacy to the \"rotational\" effect, the difficulty of obtaining an experimental verification of the effect means that it receives much less discussion and is often omitted from articles on frame-dragging (but see Einstein, 1921).Static mass increase is a third effect noted by Einstein in the same paper. The effect is an increase in inertia of a body when other masses are placed nearby. While not strictly a frame dragging effect (the term frame dragging is not used by Einstein), it is demonstrated by Einstein that it derives from the same equation of general relativity. It is also a tiny effect that is difficult to confirm experimentally.\n",
      "-In 2015, new general-relativistic extensions of Newtonian rotation laws were formulated to describe geometric dragging of frames which incorporates a newly discovered antidragging effect.\n",
      "-This \"mutual\" effect, and the ability of an accelerated mass to warp lightbeam geometry and lightbeam-based coordinate systems, is referred to as frame-dragging.\n",
      "Frame-dragging removes the usual distinction between accelerated frames (which show gravitational effects) and inertial frames (where the geometry is supposedly free from gravitational fields). When a forcibly-accelerated body physically \"drags\" a coordinate system, the problem becomes an exercise in warped spacetime for all observers.\n",
      "-The first frame-dragging effect was derived in 1918, in the framework of general relativity, by the Austrian physicists Josef Lense and Hans Thirring, and is also known as the Lense–Thirring effect. They predicted that the rotation of a massive object would distort the spacetime metric, making the orbit of a nearby test particle precess. This does not happen in Newtonian mechanics for which the gravitational field of a body depends only on its mass, not on its rotation. The Lense–Thirring effect is very small – about one part in a few trillion. To detect it, it is necessary to examine a very massive object, or build an instrument that is very sensitive.\n",
      "-Frame-dragging is an effect on spacetime, predicted by Albert Einstein's general theory of relativity, that is due to non-static stationary distributions of mass–energy. A stationary field is one that is in a steady state, but the masses causing that field may be non-static ⁠— rotating, for instance. More generally, the subject that deals with the effects caused by mass–energy currents is known as gravitoelectromagnetism, which is analogous to the magnetism of classical electromagnetism.\n",
      "\n",
      "\n",
      "\n",
      "What is explicit symmetry breaking in theoretical physics?\n",
      "-In theoretical physics, explicit symmetry breaking is the breaking of a symmetry of a theory by terms in its defining equations of motion (most typically, to the Lagrangian or the Hamiltonian) that do not respect the symmetry. Usually this term is used in situations where these symmetry-breaking terms are small, so that the symmetry is approximately respected by the theory. An example is the spectral line splitting in the Zeeman effect, due to a magnetic interaction perturbation in the Hamiltonian of the atoms involved.\n",
      "-This is essentially the paradigm for perturbation theory in quantum mechanics. An example of its use is in finding the fine structure of atomic spectra.\n",
      "-In explicit symmetry breaking (ESB), the equations of motion describing a system are variant under the broken symmetry. In Hamiltonian mechanics or Lagrangian Mechanics, this happens when there is at least one term in the Hamiltonian (or Lagrangian) that explicitly breaks the given symmetry.\n",
      "-This section describes spontaneous symmetry breaking. In layman's terms, this is the idea that for a physical system, the lowest energy configuration (the vacuum state) is not the most symmetric configuration of the system. Roughly speaking there are three types of symmetry that can be broken: discrete, continuous and gauge, ordered in increasing technicality.\n",
      "-Such a symmetry breaking is parametrized by an order parameter. A special case of this type of symmetry breaking is dynamical symmetry breaking.\n",
      "\n",
      "\n",
      "\n",
      "What is the role of the Higgs boson in the Standard Model?\n",
      "-The Higgs boson plays a unique role in the Standard Model, by explaining why the other elementary particles, except the photon and gluon, are massive. In particular, the Higgs boson explains why the photon has no mass, while the W and Z bosons are very heavy. Elementary-particle masses and the differences between electromagnetism (mediated by the photon) and the weak force (mediated by the W and Z bosons) are critical to many aspects of the structure of microscopic (and hence macroscopic) matter. In electroweak theory, the Higgs boson generates the masses of the leptons (electron, muon, and tau) and quarks. As the Higgs boson is massive, it must interact with itself.\n",
      "-Particle physics Validation of the Standard Model The Higgs boson validates the Standard Model through the mechanism of mass generation. As more precise measurements of its properties are made, more advanced extensions may be suggested or excluded. As experimental means to measure the field's behaviours and interactions are developed, this fundamental field may be better understood. If the Higgs field had not been discovered, the Standard Model would have needed to be modified or superseded.\n",
      "-The Higgs boson, sometimes called the Higgs particle, is an elementary particle in the Standard Model of particle physics produced by the quantum excitation of the Higgs field, one of the fields in particle physics theory. In the Standard Model, the Higgs particle is a massive scalar boson with zero spin, even (positive) parity, no electric charge, and no colour charge that couples to (interacts with) mass. It is also very unstable, decaying into other particles almost immediately upon generation.\n",
      "-The Standard Model Physicists explain the fundamental particles and forces of our universe in terms of the Standard Model – a widely accepted framework based on quantum field theory that predicts almost all known particles and forces aside from gravity with great accuracy. (A separate theory, general relativity, is used for gravity.) In the Standard Model, the particles and forces in nature (aside from gravity) arise from properties of quantum fields known as gauge invariance and symmetries. Forces in the Standard Model are transmitted by particles known as gauge bosons.\n",
      "-The Higgs boson The Higgs boson, sometimes called the Higgs particle, is an elementary particle in the Standard Model of particle physics produced by the quantum excitation of the Higgs field, one of the fields in particle physics theory. In the Standard Model, the Higgs particle is a massive scalar boson with zero spin, even (positive) parity, no electric charge, and no colour charge, that couples to (interacts with) mass. It is also very unstable, decaying into other particles almost immediately.\n",
      "\n",
      "\n",
      "\n",
      "What is Lorentz symmetry or Lorentz invariance in relativistic physics?\n",
      "-In relativistic physics, Lorentz symmetry or Lorentz invariance, named after the Dutch physicist Hendrik Lorentz, is an equivalence of observation or observational symmetry due to special relativity implying that the laws of physics stay the same for all observers that are moving with respect to one another within an inertial frame. It has also been described as \"the feature of nature that says experimental results are independent of the orientation or the boost velocity of the laboratory through space\".Lorentz covariance, a related concept, is a property of the underlying spacetime manifold. Lorentz covariance has two distinct, but closely related meanings: A physical quantity is said to be Lorentz covariant if it transforms under a given representation of the Lorentz group. According to the representation theory of the Lorentz group, these quantities are built out of scalars, four-vectors, four-tensors, and spinors. In particular, a Lorentz covariant scalar (e.g., the space-time interval) remains the same under Lorentz transformations and is said to be a Lorentz invariant (i.e., they transform under the trivial representation).\n",
      "-For example, the following laws, equations, and theories respect Lorentz symmetry: The kinematical laws of special relativity Maxwell's field equations in the theory of electromagnetism The Dirac equation in the theory of the electron The Standard Model of particle physicsThe Lorentz group expresses the fundamental symmetry of space and time of all known fundamental laws of nature. In small enough regions of spacetime where gravitational variances are negligible, physical laws are Lorentz invariant in the same manner as special relativity.\n",
      "-Lorentz invariance follows from two independent postulates: the principle of relativity and the principle of constancy of the speed of light. Dropping the latter while keeping the former leads to a new invariance, known as Fock–Lorentz symmetry or the projective Lorentz transformation. The general study of such theories began with Fock, who was motivated by the search for the general symmetry group preserving relativity without assuming the constancy of c.\n",
      "-Arguably the most important example of a symmetry in physics is that the speed of light has the same value in all frames of reference, which is described in special relativity by a group of transformations of the spacetime known as the Poincaré group. Another important example is the invariance of the form of physical laws under arbitrary differentiable coordinate transformations, which is an important idea in general relativity.\n",
      "-In physics, Lorentz transformations became known at the beginning of the 20th century, when it was discovered that they exhibit the symmetry of Maxwell's equations. Subsequently, they became fundamental to all of physics, because they formed the basis of special relativity in which they exhibit the symmetry of Minkowski spacetime, making the speed of light invariant between different inertial frames. They relate the spacetime coordinates of two arbitrary inertial frames of reference with constant relative speed v. In one frame, the position of an event is given by x,y,z and time t, while in the other frame the same event has coordinates x′,y′,z′ and t′.\n",
      "\n",
      "\n",
      "\n",
      "What is the significance of Baryon Acoustic Oscillations (BAOs) in the study of the universe?\n",
      "-In cosmology, baryon acoustic oscillations (BAO) are fluctuations in the density of the visible baryonic matter (normal matter) of the universe, caused by acoustic density waves in the primordial plasma of the early universe. In the same way that supernovae provide a \"standard candle\" for astronomical observations, BAO matter clustering provides a \"standard ruler\" for length scale in cosmology.  The length of this standard ruler is given by the maximum distance the acoustic waves could travel in the primordial plasma before the plasma cooled to the point where it became neutral atoms (the epoch of recombination), which stopped the expansion of the plasma density waves, \"freezing\" them into place. The length of this standard ruler (≈490 million light years in today's universe) can be measured by looking at the large scale structure of matter using astronomical surveys. BAO measurements help cosmologists understand more about the nature of dark energy (which causes the accelerating expansion of the universe) by constraining cosmological parameters.\n",
      "-The BAO signal is a standard ruler such that the length of the sound horizon can be measured as a function of cosmic time. This measures two cosmological distances: the Hubble parameter,  H(z) , and the angular diameter distance,  dA(z) , as a function of redshift  (z) . By measuring the subtended angle,  Δθ , of the ruler of length  Δχ , these parameters are determined as follows: Δθ=ΔχdA(z) dA(z)∝∫0zdz′H(z′) the redshift interval,  Δz , can be measured from the data and thus determining the Hubble parameter as a function of redshift: cΔz=H(z)Δχ Therefore, the BAO technique helps constrain cosmological parameters and provide further insight into the nature of dark energy.\n",
      "-The physics of the propagation of the baryon waves in the early universe is fairly simple; as a result cosmologists can predict the size of the sound horizon at the time of recombination. In addition the CMB provides a measurement of this scale to high accuracy. However, in the time between recombination and present day, the universe has been expanding. This expansion is well supported by observations and is one of the foundations of the Big Bang Model. In the late 1990s, observations of supernovae determined that not only is the universe expanding, it is expanding at an increasing rate. A better understanding of the acceleration of the universe, or dark energy, has become one of the most important questions in cosmology today. In order to understand the nature of the dark energy, it is important to have a variety of ways of measuring the acceleration. BAO can add to the body of knowledge about this acceleration by comparing observations of the sound horizon today (using clustering of galaxies) to that of the sound horizon at the time of recombination (using the CMB). Thus BAO provides a measuring stick with which to better understand the nature of the acceleration, completely independent from the supernova technique.\n",
      "-Another class of physical distance indicator is the standard ruler. In 2008, galaxy diameters have been proposed as a possible standard ruler for cosmological parameter determination. More recently the physical scale imprinted by baryon acoustic oscillations (BAO) in the early universe has been used.  In the early universe (before recombination) the baryons and photons scatter off each other, and form a tightly-coupled fluid that can support sound waves. The waves are sourced by primordial density perturbations, and travel at speed that can be predicted from the baryon density and other cosmological parameters. The total distance that these sound waves can travel before recombination determines a fixed scale, which simply expands with the universe after recombination. BAO therefore provide a standard ruler that can be measured in galaxy surveys from the effect of baryons on the clustering of galaxies. The method requires an extensive galaxy survey in order to make this scale visible, but has been measured with percent-level precision (see baryon acoustic oscillations). The scale does depend on cosmological parameters like the baryon and matter densities, and the number of neutrinos, so distances based on BAO are more dependent on cosmological model than those based on local measurements.\n",
      "-The SDSS team looked at a sample of 46,748 luminous red galaxies (LRGs), over 3,816 square-degrees of sky (approximately five billion light years in diameter) and out to a redshift of z = 0.47. They analyzed the clustering of these galaxies by calculating a two-point correlation function on the data. The correlation function (ξ) is a function of comoving galaxy separation distance (s) and describes the probability that one galaxy will be found within a given distance of another.  One would expect a high correlation of galaxies at small separation distances (due to the clumpy nature of galaxy formation) and a low correlation at large separation distances. The BAO signal would show up as a bump in the correlation function at a comoving separation equal to the sound horizon. This signal was detected by the SDSS team in 2005. SDSS confirmed the WMAP results that the sound horizon is ~150 Mpc in today's universe.\n",
      "\n",
      "\n",
      "\n",
      "What can be inferred about the electronic entropy of insulators and metals based on their densities of states at the Fermi level?\n",
      "-Insulators have zero density of states at the Fermi level due to their band gaps. Thus, the density of states-based electronic entropy is essentially zero in these systems.\n",
      "-Metals have non-zero density of states at the Fermi level. Metals with free-electron-like band structures (e.g. alkali metals, alkaline earth metals, Cu, and Al) generally exhibit relatively low density of states at the Fermi level, and therefore exhibit fairly low electronic entropies. Transition metals, wherein the flat d-bands lie close to the Fermi level, generally exhibit much larger electronic entropies than the free-electron like metals.\n",
      "-Electronic entropy is the entropy of a system attributable to electrons' probabilistic occupation of states. This entropy can take a number of forms. The first form can be termed a density of states based entropy. The Fermi–Dirac distribution implies that each eigenstate of a system, i, is occupied with a certain probability, pi. As the entropy is given by a sum over the probabilities of occupation of those states, there is an entropy associated with the occupation of the various electronic states. In most molecular systems, the energy spacing between the highest occupied molecular orbital and the lowest unoccupied molecular orbital is usually large, and thus the probabilities associated with the occupation of the excited states are small. Therefore, the electronic entropy in molecular systems can safely be neglected. Electronic entropy is thus most relevant for the thermodynamics of condensed phases, where the density of states at the Fermi level can be quite large, and the electronic entropy can thus contribute substantially to thermodynamic behavior. A second form of electronic entropy can be attributed to the configurational entropy associated with localized electrons and holes. This entropy is similar in form to the configurational entropy associated with the mixing of atoms on a lattice.\n",
      "-Useful approximation It is useful to recognize that the only states within ~±kBT of the Fermi level contribute significantly to the entropy. Other states are either fully occupied, f = 1, or completely unoccupied, f = 0. In either case, these states do not contribute to the entropy. If one assumes that the density of states is constant within ±kBT of the Fermi level, one can derive that the electron heat capacity, equal to: CV=T(∂S∂T)T,V=π23kB2Tn(EF) where n(EF) is the density of states (number of levels per unit energy) at the Fermi level. Several other approximations can be made, but they all indicate that the electronic entropy should, to first order, be proportional to the temperature and the density of states at the Fermi level. As the density of states at the Fermi level varies widely between systems, this approximation is a reasonable heuristic for inferring when it may be necessary to include electronic entropy in the thermodynamic description of a system; only systems with large densities of states at the Fermi level should exhibit non-negligible electronic entropy (where large may be approximately defined as n(EF) ≥ (k2BT)−1).\n",
      "-According to electronic band theory, solids can be classified as insulators, semiconductors, semimetals, or metals. In insulators and semiconductors the filled valence band is separated from an empty conduction band by a band gap. For insulators, the magnitude of the band gap is larger (e.g., > 4 eV) than that of a semiconductor (e.g., < 4 eV). Because of the slight overlap between the conduction and valence bands, semimetals have no band gap and a negligible density of states at the Fermi level. A metal, by contrast, has an appreciable density of states at the Fermi level because the conduction band is partially filled.\n",
      "\n",
      "\n",
      "\n",
      "What are permutation-inversion groups?\n",
      "-As discussed above in the section Point groups and permutation-inversion groups, point groups are useful for classifying the vibrational and electronic states of rigid molecules (sometimes called semi-rigid molecules) which undergo only small oscillations about a single equilibrium geometry. Longuet-Higgins introduced a more general type of symmetry group suitable not only for classifying the vibrational and electronic states of rigid molecules but also for classifying their rotational and nuclear spin states. Further, such groups can be used to classify the states of non-rigid (or fluxional) molecules that tunnel between equivalent geometries (called versions) and to allow for the distorting effects of molecular rotation. These groups are known as permutation-inversion groups, because the symmetry operations in them are energetically feasible permutations of identical nuclei, or inversion with respect to the center of mass (the parity operation), or a combination of the two.\n",
      "-Point groups and permutation-inversion groups The successive application (or composition) of one or more symmetry operations of a molecule has an effect equivalent to that of some single symmetry operation of the molecule. For example, a C2 rotation followed by a σv reflection is seen to be a σv' symmetry operation: σv*C2 = σv'. (\"Operation A followed by B to form C\" is written BA = C). Moreover, the set of all symmetry operations (including this composition operation) obeys all the properties of a group, given above. So (S,*) is a group, where S is the set of all symmetry operations of some molecule, and * denotes the composition (repeated application) of symmetry operations.\n",
      "-Technically, a permutation of a set S is defined as a bijection from S to itself. That is, it is a function from S to S for which every element occurs exactly once as an image value. This is related to the rearrangement of the elements of S in which each element s is replaced by the corresponding f(s). For example, the permutation (3, 1, 2) mentioned above is described by the function  α defined as α(1)=3,α(2)=1,α(3)=2 .The collection of all permutations of a set form a group called the symmetric group of the set. The group operation is the composition (performing two given rearrangements in succession), which results in another rearrangement. As properties of permutations do not depend on the nature of the set elements, it is often the permutations of the set  {1,2,…,n} that are considered for studying permutations.  In elementary combinatorics, the k-permutations, or partial permutations, are the ordered arrangements of k distinct elements selected from a set. When k is equal to the size of the set, these are the permutations of the set.\n",
      "-The set of permutations on n items can be given the structure of a partial order, called the weak order of permutations, which forms a lattice.\n",
      "The Hasse diagram of the inversion sets ordered by the subset relation forms the skeleton of a permutohedron.\n",
      "If a permutation is assigned to each inversion set using the place-based definition, the resulting order of permutations is that of the permutohedron, where an edge corresponds to the swapping of two elements with consecutive values. This is the weak order of permutations. The identity is its minimum, and the permutation formed by reversing the identity is its maximum.\n",
      "If a permutation were assigned to each inversion set using the element-based definition, the resulting order of permutations would be that of a Cayley graph, where an edge corresponds to the swapping of two elements on consecutive places. This Cayley graph of the symmetric group is similar to its permutohedron, but with each permutation replaced by its inverse.\n",
      "-Groups The symmetry operations of a molecule (or other object) form a group. In mathematics, a group is a set with a binary operation that satisfies the four properties listed below.  In a symmetry group, the group elements are the symmetry operations (not the symmetry elements), and the binary combination consists of applying first one symmetry operation and then the other. An example is the sequence of a C4 rotation about the z-axis and a reflection in the xy-plane, denoted σ(xy)C4. By convention the order of operations is from right to left.\n",
      "\n",
      "\n",
      "\n",
      "What is the relationship between dielectric loss and the transparency of a material?\n",
      "-Most insulators (or dielectric materials) are held together by ionic bonds. Thus, these materials do not have free conduction electrons, and the bonding electrons reflect only a small fraction of the incident wave. The remaining frequencies (or wavelengths) are free to propagate (or be transmitted). This class of materials includes all ceramics and glasses.\n",
      "If a dielectric material does not include light-absorbent additive molecules (pigments, dyes, colorants), it is usually transparent to the spectrum of visible light. Color centers (or dye molecules, or \"dopants\") in a dielectric absorb a portion of the incoming light. The remaining frequencies (or wavelengths) are free to be reflected or transmitted. This is how colored glass is produced.\n",
      "-If the object is transparent, then the light waves are passed on to neighboring atoms through the bulk of the material and re-emitted on the opposite side of the object. Such frequencies of light waves are said to be transmitted.\n",
      "Transparency in insulators An object may be not transparent either because it reflects the incoming light or because it absorbs the incoming light. Almost all solids reflect a part and absorb a part of the incoming light.\n",
      "-Classification of materials Materials can be classified according to their complex-valued permittivity ε, upon comparison of its real ε′ and imaginary ε″ components (or, equivalently, conductivity, σ, when accounted for in the latter). A perfect conductor has infinite conductivity, σ = ∞, while a perfect dielectric is a material that has no conductivity at all, σ = 0; this latter case, of real-valued permittivity (or complex-valued permittivity with zero imaginary component) is also associated with the name lossless media. Generally, when σ/ωε′ ≪ 1 we consider the material to be a low-loss dielectric (although not exactly lossless), whereas σ/ωε′ ≫ 1 is associated with a good conductor; such materials with non-negligible conductivity yield a large amount of loss that inhibit the propagation of electromagnetic waves, thus are also said to be lossy media. Those materials that do not fall under either limit are considered to be general media.\n",
      "-The loss of most transmission lines are dominated by the metal loss, which causes a frequency dependency due to finite conductivity of metals, and the skin effect inside a conductor. The skin effect causes R along the conductor to be approximately dependent on frequency according to R∝ω Losses in the dielectric depend on the loss tangent (tan δ) of the material divided by the wavelength of the signal. Thus they are directly proportional to the frequency.\n",
      "-In the field of optics, transparency (also called pellucidity or diaphaneity) is the physical property of allowing light to pass through the material without appreciable scattering of light. On a macroscopic scale (one in which the dimensions are much larger than the wavelengths of the photons in question), the photons can be said to follow Snell's law. Translucency (also called translucence or translucidity) allows light to pass through, but does not necessarily (again, on the macroscopic scale) follow Snell's law; the photons can be scattered at either of the two interfaces, or internally, where there is a change in index of refraction. In other words, a translucent material is made up of components with different indices of refraction. A transparent material is made up of components with a uniform index of refraction. Transparent materials appear clear, with the overall appearance of one color, or any combination leading up to a brilliant spectrum of every color. The opposite property of translucency is opacity. Other categories of visual appearance, related to the perception of regular or diffuse reflection and transmission of light, have been organized under the concept of cesia in an order system with three variables, including transparency, translucency and opacity among the involved aspects.\n",
      "\n",
      "\n",
      "\n",
      "What is the purpose of measuring the Larmor precession fields at about 100 microtesla with highly sensitive superconducting quantum interference devices (SQUIDs) in ultra-low field MRI?\n",
      "-Probably the most common commercial use of SQUIDs is in magnetic property measurement systems (MPMS). These are turn-key systems, made by several manufacturers, that measure the magnetic properties of a material sample. This is typically done over a temperature range from that of 300 mK to roughly 400 K. With the decreasing size of SQUID sensors since the last decade, such sensor can equip the tip of an AFM probe. Such device allows simultaneous measurement of roughness of the surface of a sample and the local magnetic flux.For example, SQUIDs are being used as detectors to perform magnetic resonance imaging (MRI). While high-field MRI uses precession fields of one to several teslas, SQUID-detected MRI uses measurement fields that lie in the microtesla range. In a conventional MRI system, the signal scales as the square of the measurement frequency (and hence precession field): one power of frequency comes from the thermal polarization of the spins at ambient temperature, while the second power of field comes from the fact that the induced voltage in the pickup coil is proportional to the frequency of the precessing magnetization. In the case of untuned SQUID detection of prepolarized spins, however, the NMR signal strength is independent of precession field, allowing MRI signal detection in extremely weak fields, on the order of Earth's magnetic field. SQUID-detected MRI has advantages over high-field MRI systems, such as the low cost required to build such a system, and its compactness. The principle has been demonstrated by imaging human extremities, and its future application may include tumor screening.Another application is the scanning SQUID microscope, which uses a SQUID immersed in liquid helium as the probe. The use of SQUIDs in oil prospecting, mineral exploration, earthquake prediction and geothermal energy surveying is becoming more widespread as superconductor technology develops; they are also used as precision movement sensors in a variety of scientific applications, such as the detection of gravitational waves.\n",
      "-Magnetoencephalography (MEG) is a functional neuroimaging technique for mapping brain activity by recording magnetic fields produced by electrical currents occurring naturally in the brain, using very sensitive magnetometers. Arrays of SQUIDs (superconducting quantum interference devices) are currently the most common magnetometer, while the SERF (spin exchange relaxation-free) magnetometer is being investigated for future machines. Applications of MEG include basic research into perceptual and cognitive brain processes, localizing regions affected by pathology before surgical removal, determining the function of various parts of the brain, and neurofeedback. This can be applied in a clinical setting to find locations of abnormalities as well as in an experimental setting to simply measure brain activity.\n",
      "-MRI requires a magnetic field that is both strong and uniform to a few parts per million across the scan volume. The field strength of the magnet is measured in teslas – and while the majority of systems operate at 1.5 T, commercial systems are available between 0.2 and 7 T. Whole-body MRI systems for research application operate in e.g. 9.4T, 10.5T, 11.7T. Even higher field whole-body MRI systems e.g. 14 T and beyond are in conceptual proposal or in engineering design. Most clinical magnets are superconducting magnets, which require liquid helium to keep them at low temperatures. Lower field strengths can be achieved with permanent magnets, which are often used in \"open\" MRI scanners for claustrophobic patients. Lower field strengths are also used in a portable MRI scanner approved by the FDA in 2020. Recently, MRI has been demonstrated also at ultra-low fields, i.e., in the microtesla-to-millitesla range, where sufficient signal quality is made possible by prepolarization (on the order of 10–100 mT) and by measuring the Larmor precession fields at about 100 microtesla with highly sensitive superconducting quantum interference devices (SQUIDs).\n",
      "-Since 2012, the use of cryogen-free magnet technology has greatly reduced infrastructure requirements and dependency on the availability of increasingly hard to obtain cryogenic coolants.Strengths: The advantage of micro-MRI is that it has good spatial resolution, up to 100 µm and even 25 µm in very high strength magnetic fields. It also has excellent contrast resolution to distinguish between normal and pathological tissue. Micro-MRI can be used in a wide variety of applications, including anatomical, functional, and molecular imaging. Furthermore, since micro-MRI's mechanism is based on a magnetic field, it is much safer compared to radiation based imaging modalities such as micro-CT and micro-PET.\n",
      "-T1 and T2 Each tissue returns to its equilibrium state after excitation by the independent relaxation processes of T1 (spin-lattice; that is, magnetization in the same direction as the static magnetic field) and T2 (spin-spin; transverse to the static magnetic field).\n",
      "To create a T1-weighted image, magnetization is allowed to recover before measuring the MR signal by changing the repetition time (TR). This image weighting is useful for assessing the cerebral cortex, identifying fatty tissue, characterizing focal liver lesions, and in general, obtaining morphological information, as well as for post-contrast imaging.\n",
      "To create a T2-weighted image, magnetization is allowed to decay before measuring the MR signal by changing the echo time (TE). This image weighting is useful for detecting edema and inflammation, revealing white matter lesions, and assessing zonal anatomy in the prostate and uterus.\n",
      "\n",
      "\n",
      "\n",
      "What is the difference between illuminance and luminance?\n",
      "-Luminance is a photometric measure of the luminous intensity per unit area of light travelling in a given direction. It describes the amount of light that passes through, is emitted from, or is reflected from a particular area, and falls within a given solid angle.  The procedure for conversion from spectral radiance to luminance is standardized by the CIE and ISO.Brightness is the term for the subjective impression of the objective luminance measurement standard (see Objectivity (science) § Objectivity in measurement for the importance of this contrast).\n",
      "-Luminance is often used to characterize emission or reflection from flat, diffuse surfaces. Luminance levels indicate how much luminous power could be detected by the human eye looking at a particular surface from a particular angle of view. Luminance is thus an indicator of how bright the surface will appear. In this case, the solid angle of interest is the solid angle subtended by the eye's pupil.  Luminance is used in the video industry to characterize the brightness of displays. A typical computer display emits between 50 and 300 cd/m2. The sun has a luminance of about 1.6×109 cd/m2 at noon.Luminance is invariant in geometric optics. This means that for an ideal optical system, the luminance at the output is the same as the input luminance.  For real, passive optical systems, the output luminance is at most equal to the input. As an example, if one uses a lens to form an image that is smaller than the source object, the luminous power is concentrated into a smaller area, meaning that the illuminance is higher at the image. The light at the image plane, however, fills a larger solid angle so the luminance comes out to be the same assuming there is no loss at the lens. The image can never be \"brighter\" than the source.\n",
      "-In photometry, illuminance is the total luminous flux incident on a surface, per unit area. It is a measure of how much the incident light illuminates the surface, wavelength-weighted by the luminosity function to correlate with human brightness perception. Similarly, luminous emittance is the luminous flux per unit area emitted from a surface. Luminous emittance is also known as luminous exitance.In SI units illuminance is measured in lux (lx), or equivalently in lumens per square metre (lm·m−2). Luminous exitance is measured in lm·m−2 only, not lux. In the CGS system, the unit of illuminance is the phot, which is equal to 10000 lux. The foot-candle is a non-metric unit of illuminance that is used in photography.Illuminance was formerly often called brightness, but this leads to confusion with other uses of the word, such as to mean luminance. \"Brightness\" should never be used for quantitative description, but only for nonquantitative references to physiological sensations and perceptions of light.\n",
      "-To help compare different orders of magnitude, the following list describes various source of lux, which is measured in lumens per square metre.\n",
      "-The luminance of a reflecting surface is related to the illuminance it receives: where the integral covers all the directions of emission ΩΣ, Mv is the surface's luminous exitance, Ev is the received illuminance, R is the reflectance.In the case of a perfectly diffuse reflector (also called a Lambertian reflector), the luminance is isotropic, per Lambert's cosine law. Then the relationship is simply \n",
      "\n",
      "\n",
      "\n",
      "What is a magnetic monopole in particle physics?\n",
      "-In particle physics, a magnetic monopole is a hypothetical elementary particle that is an isolated magnet with only one magnetic pole (a north pole without a south pole or vice versa). A magnetic monopole would have a net north or south \"magnetic charge\". Modern interest in the concept stems from particle theories, notably the grand unified and superstring theories, which predict their existence. The known elementary particles that have electric charge are electric monopoles.\n",
      "-Since around 2003, various condensed-matter physics groups have used the term \"magnetic monopole\" to describe a different and largely unrelated phenomenon.A true magnetic monopole would be a new elementary particle, and would violate Gauss's law for magnetism ∇⋅B = 0. A monopole of this kind, which would help to explain the law of charge quantization as formulated by Paul Dirac in 1931, has never been observed in experiments.The monopoles studied by condensed-matter groups have none of these properties. They are not a new elementary particle, but rather are an emergent phenomenon in systems of everyday particles (protons, neutrons, electrons, photons); in other words, they are quasi-particles. They are not sources for the B-field (i.e., they do not violate ∇⋅B = 0); instead, they are sources for other fields, for example the H-field, the \"B*-field\" (related to superfluid vorticity), or various other quantum fields. They are not directly relevant to grand unified theories or other aspects of particle physics, and do not help explain charge quantization—except insofar as studies of analogous situations can help confirm that the mathematical analyses involved are sound.There are a number of examples in condensed-matter physics where collective behavior leads to emergent phenomena that resemble magnetic monopoles in certain respects, including most prominently the spin ice materials. While these should not be confused with hypothetical elementary monopoles existing in the vacuum, they nonetheless have similar properties and can be probed using similar techniques.\n",
      "-The pole model usually treats magnetic charge as a mathematical abstraction, rather than a physical property of particles. However, a magnetic monopole is a hypothetical particle (or class of particles) that physically has only one magnetic pole (either a north pole or a south pole). In other words, it would possess a \"magnetic charge\" analogous to an electric charge. Magnetic field lines would start or end on magnetic monopoles, so if they exist, they would give exceptions to the rule that magnetic field lines neither start nor end. Some theories (such as Grand Unified Theories) have predicted the existence of magnetic monopoles, but so far, none have been observed.\n",
      "-Magnetic monopoles Since a bar magnet gets its ferromagnetism from electrons distributed evenly throughout the bar, when a bar magnet is cut in half, each of the resulting pieces is a smaller bar magnet. Even though a magnet is said to have a north pole and a south pole, these two poles cannot be separated from each other. A monopole—if such a thing exists—would be a new and fundamentally different kind of magnetic object. It would act as an isolated north pole, not attached to a south pole, or vice versa. Monopoles would carry \"magnetic charge\" analogous to electric charge. Despite systematic searches since 1931, as of 2010, they have never been observed, and could very well not exist.Nevertheless, some theoretical physics models predict the existence of these magnetic monopoles. Paul Dirac observed in 1931 that, because electricity and magnetism show a certain symmetry, just as quantum theory predicts that individual positive or negative electric charges can be observed without the opposing charge, isolated South or North magnetic poles should be observable. Using quantum theory Dirac showed that if magnetic monopoles exist, then one could explain the quantization of electric charge—that is, why the observed elementary particles carry charges that are multiples of the charge of the electron.\n",
      "-Mathematically, the magnetic field of an object is often described in terms of a multipole expansion. This is an expression of the field as the sum of component fields with specific mathematical forms. The first term in the expansion is called the monopole term, the second is called dipole, then quadrupole, then octupole, and so on. Any of these terms can be present in the multipole expansion of an electric field, for example. However, in the multipole expansion of a magnetic field, the \"monopole\" term is always exactly zero (for ordinary matter). A magnetic monopole, if it exists, would have the defining property of producing a magnetic field whose monopole term is non-zero.\n",
      "\n",
      "\n",
      "\n",
      "What is the difference between redshift due to the expansion of the universe and Doppler redshift?\n",
      "-There is a distinction between a redshift in cosmological context as compared to that witnessed when nearby objects exhibit a local Doppler-effect redshift. Rather than cosmological redshifts being a consequence of the relative velocities that are subject to the laws of special relativity (and thus subject to the rule that no two locally separated objects can have relative velocities with respect to each other faster than the speed of light), the photons instead increase in wavelength and redshift because of a global feature of the spacetime through which they are traveling. One interpretation of this effect is the idea that space itself is expanding. Due to the expansion increasing as distances increase, the distance between two remote galaxies can increase at more than 3×108 m/s, but this does not imply that the galaxies move faster than the speed of light at their present location (which is forbidden by Lorentz covariance).\n",
      "-Redshift is also used to measure the expansion of space, but this is not truly a Doppler effect. Rather, redshifting due to the expansion of space is known as cosmological redshift, which can be derived purely from the Robertson-Walker metric under the formalism of general relativity. Having said this, it also happens that there are detectable Doppler effects on cosmological scales, which, if incorrectly interpreted as cosmological in origin, lead to the observation of redshift-space distortions.\n",
      "-Distinguishing between cosmological and local effects For cosmological redshifts of z < 0.01 additional Doppler redshifts and blueshifts due to the peculiar motions of the galaxies relative to one another cause a wide scatter from the standard Hubble Law. The resulting situation can be illustrated by the Expanding Rubber Sheet Universe, a common cosmological analogy used to describe the expansion of space. If two objects are represented by ball bearings and spacetime by a stretching rubber sheet, the Doppler effect is caused by rolling the balls across the sheet to create peculiar motion. The cosmological redshift occurs when the ball bearings are stuck to the sheet and the sheet is stretched.The redshifts of galaxies include both a component related to recessional velocity from expansion of the universe, and a component related to peculiar motion (Doppler shift). The redshift due to expansion of the universe depends upon the recessional velocity in a fashion determined by the cosmological model chosen to describe the expansion of the universe, which is very different from how Doppler redshift depends upon local velocity. Describing the cosmological expansion origin of redshift, cosmologist Edward Robert Harrison said, \"Light leaves a galaxy, which is stationary in its local region of space, and is eventually received by observers who are stationary in their own local region of space. Between the galaxy and the observer, light travels through vast regions of expanding space. As a result, all wavelengths of the light are stretched by the expansion of space. It is as simple as that...\" Steven Weinberg clarified, \"The increase of wavelength from emission to absorption of light does not depend on the rate of change of a(t) [here a(t) is the Robertson–Walker scale factor] at the times of emission or absorption, but on the increase of a(t) in the whole period from emission to absorption.\"Popular literature often uses the expression \"Doppler redshift\" instead of \"cosmological redshift\" to describe the redshift of galaxies dominated by the expansion of spacetime, but the cosmological redshift is not found using the relativistic Doppler equation which is instead characterized by special relativity; thus v ≥ c is impossible while, in contrast, v ≥ c is possible for cosmological redshifts because the space which separates the objects (for example, a quasar from the Earth) can expand faster than the speed of light. More mathematically, the viewpoint that \"distant galaxies are receding\" and the viewpoint that \"the space between galaxies is expanding\" are related by changing coordinate systems. Expressing this precisely requires working with the mathematics of the Friedmann–Robertson–Walker metric.If the universe were contracting instead of expanding, we would see distant galaxies blueshifted by an amount proportional to their distance instead of redshifted.\n",
      "-Doppler effect If a source of the light is moving away from an observer, then redshift (z > 0) occurs; if the source moves towards the observer, then blueshift (z < 0) occurs. This is true for all electromagnetic waves and is explained by the Doppler effect. Consequently, this type of redshift is called the Doppler redshift. If the source moves away from the observer with velocity v, which is much less than the speed of light (v ≪ c), the redshift is given by z≈vc (since  γ≈1 )where c is the speed of light. In the classical Doppler effect, the frequency of the source is not modified, but the recessional motion causes the illusion of a lower frequency.\n",
      "-The theory requires the relation  v=HD to hold at all times, where  D is the proper distance, v is the recessional velocity, and  v ,  H , and  D vary as the universe expands (hence we write  H0 to denote the present-day Hubble \"constant\"). For distances much smaller than the size of the observable universe, the Hubble redshift can be thought of as the Doppler shift corresponding to the recession velocity  v . However, the redshift is not a true Doppler shift, but rather the result of the expansion of the universe between the time the light was emitted and the time that it was detected.An unexplained discrepancy with the determination of the Hubble constant is known as Hubble tension. Techniques based on observation of the CMB suggest a lower value of this constant compared to the quantity derived from measurements based on the cosmic distance ladder.\n",
      "\n",
      "\n",
      "\n",
      "What is the relationship between Coordinated Universal Time (UTC) and Universal Time (UT1)?\n",
      "-International Atomic Time (TAI) is the primary international time standard from which other time standards are calculated. Universal Time (UT1) is mean solar time at 0° longitude, computed from astronomical observations. It varies from TAI because of the irregularities in Earth's rotation. Coordinated Universal Time (UTC) is an atomic time scale designed to approximate Universal Time. UTC differs from TAI by an integral number of seconds. UTC is kept within 0.9 second of UT1 by the introduction of one-second steps to UTC, the \"leap second\". The Global Positioning System broadcasts a very precise time signal based on UTC time.\n",
      "-Universal Time (UT1) is the Earth Rotation Angle (ERA) linearly scaled to match historical definitions of mean solar time at 0° longitude. At high precision, Earth's rotation is irregular and is determined from the positions of distant quasars using long baseline interferometry, laser ranging of the Moon and artificial satellites, as well as GPS satellite orbits.\n",
      "Coordinated Universal Time (UTC) is an atomic time scale designed to approximate UT1. UTC differs from TAI by an integral number of seconds. UTC is kept within 0.9 second of UT1 by the introduction of one-second steps to UTC, the \"leap second\". To date these steps (and difference \"TAI-UTC\") have always been positive.\n",
      "The Global Positioning System broadcasts a very precise time signal worldwide, along with instructions for converting GPS time (GPST) to UTC. It was defined with a constant offset from TAI: GPST = TAI - 19 s. The GPS time standard is maintained independently but regularly synchronized with or from, UTC time.\n",
      "-The TAI and UT1 time scales are precisely defined, the former by atomic clocks (and thus independent of Earth's rotation) and the latter by astronomical observations (that measure actual planetary rotation and thus the solar time at the Greenwich meridian). UTC (on which civil time is usually based) is a compromise, stepping with atomic seconds but periodically reset by a leap second to match UT1.\n",
      "-Contrary to TAI, UTC is a discontinuous time scale. It is occasionally adjusted by leap seconds. Between these adjustments, it is composed of segments that are mapped to atomic time by a constant offset. From its beginning in 1961 through December 1971, the adjustments were made regularly in fractional leap seconds so that UTC approximated UT2. Afterward, these adjustments were made only in whole seconds to approximate UT1. This was a compromise arrangement in order to enable a publicly broadcast time scale. The less frequent whole-second adjustments meant that the time scale would be more stable and easier to synchronize internationally. The fact that it continues to approximate UT1 means that tasks such as navigation which require a source of Universal Time continue to be well served by the public broadcast of UTC.\n",
      "-A set of atomic clocks throughout the world keeps time by consensus: the clocks \"vote\" on the correct time, and all voting clocks are steered to agree with the consensus, which is called International Atomic Time (TAI). TAI \"ticks\" atomic seconds.: 207–218 Civil time is defined to agree with the rotation of the Earth. The international standard for timekeeping is Coordinated Universal Time (UTC). This time scale \"ticks\" the same atomic seconds as TAI, but inserts or omits leap seconds as necessary to correct for variations in the rate of rotation of the Earth.: 16–17, 207 A time scale in which the seconds are not exactly equal to atomic seconds is UT1, a form of universal time. UT1 is defined by the rotation of the Earth with respect to the Sun, and does not contain any leap seconds.: 68, 232  UT1 always differs from UTC by less than a second.\n",
      "\n",
      "\n",
      "\n",
      "What is the reason for heating metals to a temperature just above the upper critical temperature?\n",
      "-Because a smaller grain size usually enhances mechanical properties, such as toughness, shear strength and tensile strength, these metals are often heated to a temperature that is just above the upper critical temperature, in order to prevent the grains of solution from growing too large. For instance, when steel is heated above the upper critical-temperature, small grains of austenite form. These grow larger as the temperature is increased. When cooled very quickly, during a martensite transformation, the austenite grain-size directly affects the martensitic grain-size. Larger grains have large grain-boundaries, which serve as weak spots in the structure. The grain size is usually controlled to reduce the probability of breakage.The diffusion transformation is very time-dependent. Cooling a metal will usually suppress the precipitation to a much lower temperature. Austenite, for example, usually only exists above the upper critical temperature. However, if the austenite is cooled quickly enough, the transformation may be suppressed for hundreds of degrees below the lower critical temperature. Such austenite is highly unstable and, if given enough time, will precipitate into various microstructures of ferrite and cementite. The cooling rate can be used to control the rate of grain growth or can even be used to produce partially martensitic microstructures. However, the martensite transformation is time-independent. If the alloy is cooled to the martensite transformation (Ms) temperature before other microstructures can fully form, the transformation will usually occur at just under the speed of sound.When austenite is cooled slow enough that a martensite transformation does not occur, the austenite grain size will have an effect on the rate of nucleation, but it is generally temperature and the rate of cooling that controls the grain size and microstructure. When austenite is cooled extremely slowly, it will form large ferrite crystals filled with spherical inclusions of cementite. This microstructure is referred to as \"sphereoidite\". If cooled a little faster, then coarse pearlite will form. Even faster, and fine pearlite will form. If cooled even faster, bainite will form. Similarly, these microstructures will also form, if cooled to a specific temperature and then held there for a certain time.Most non-ferrous alloys are also heated in order to form a solution. Most often, these are then cooled very quickly to produce a martensite transformation, putting the solution into a supersaturated state. The alloy, being in a much softer state, may then be cold worked. This causes work hardening that increases the strength and hardness of the alloy. Moreover, the defects caused by plastic deformation tend to speed up precipitation, increasing the hardness beyond what is normal for the alloy. Even if not cold worked, the solutes in these alloys will usually precipitate, although the process may take much longer. Sometimes these metals are then heated to a temperature that is below the lower critical (A1) temperature, preventing recrystallization, in order to speed-up the precipitation.\n",
      "-Tempering is a heat treatment technique applied to ferrous alloys, such as steel or cast iron, to achieve greater toughness by decreasing the hardness of the alloy. The reduction in hardness is usually accompanied by an increase in ductility, thereby decreasing the brittleness of the metal. Tempering is usually performed after quenching, which is rapid cooling of the metal to put it in its hardest state. Tempering is accomplished by controlled heating of the quenched workpiece to a temperature below its \"lower critical temperature\". This is also called the lower transformation temperature or lower arrest (A1) temperature: the temperature at which the crystalline phases of the alloy, called ferrite and cementite, begin combining to form a single-phase solid solution referred to as austenite. Heating above this temperature is avoided, so as not to destroy the very-hard, quenched microstructure, called martensite.Precise control of time and temperature during the tempering process is crucial to achieve the desired balance of physical properties. Low tempering temperatures may only relieve the internal stresses, decreasing brittleness while maintaining a majority of the hardness. Higher tempering temperatures tend to produce a greater reduction in the hardness, sacrificing some yield strength and tensile strength for an increase in elasticity and plasticity. However, in some low alloy steels, containing other elements like chromium and molybdenum, tempering at low temperatures may produce an increase in hardness, while at higher temperatures the hardness will decrease. Many steels with high concentrations of these alloying elements behave like precipitation hardening alloys, which produces the opposite effects under the conditions found in quenching and tempering, and are referred to as maraging steels.In carbon steels, tempering alters the size and distribution of carbides in the martensite, forming a microstructure called \"tempered martensite\". Tempering is also performed on normalized steels and cast irons, to increase ductility, machinability, and impact strength. Steel is usually tempered evenly, called \"through tempering,\" producing a nearly uniform hardness, but it is sometimes heated unevenly, referred to as \"differential tempering,\" producing a variation in hardness.\n",
      "-Forging temperature is the temperature at which a metal becomes substantially more soft, but is lower than the melting temperature, such that it can be reshaped by forging. Bringing a metal to its forging temperature allows the metal's shape to be changed by applying a relatively small force, without creating cracks. For most metals, forging temperature is approximately 70% of the absolute temperature (usually measured in kelvins) of its melting point.Selecting the maximum forging temperature allows metals to be forged more easily, lowering the forging pressure and thus the wear on metal-forming dies. The temperature at which a metal is forged can affect the homogeneity in microstructure and mechanical properties of forged products, which can highly affect the performance of products used in manufacturing.\n",
      "-Temperature All of the following forging processes can be performed at various temperatures; however, they are generally classified by whether the metal temperature is above or below the recrystallization temperature. If the temperature is above the material's recrystallization temperature it is deemed hot forging; if the temperature is below the material's recrystallization temperature but above 30% of the recrystallization temperature (on an absolute scale) it is deemed warm forging; if below 30% of the recrystallization temperature (usually room temperature) then it is deemed cold forging. The main advantage of hot forging is that it can be done more quickly and precisely, and as the metal is deformed work hardening effects are negated by the recrystallization process. Cold forging typically results in work hardening of the piece.\n",
      "-Tempering quenched steel at very low temperatures, between 66 and 148 °C (151 and 298 °F), will usually not have much effect other than a slight relief of some of the internal stresses and a decrease in brittleness. Tempering at higher temperatures, from 148 to 205 °C (298 to 401 °F), will produce a slight reduction in hardness, but will primarily relieve much of the internal stresses. In some steels with low alloy content, tempering in the range of 260 and 340 °C (500 and 644 °F) causes a decrease in ductility and an increase in brittleness, and is referred to as the \"tempered martensite embrittlement\" (TME) range. Except in the case of blacksmithing, this range is usually avoided. Steel requiring more strength than toughness, such as tools, are usually not tempered above 205 °C (401 °F). Instead, a variation in hardness is usually produced by varying only the tempering time. When increased toughness is desired at the expense of strength, higher tempering temperatures, from 370 to 540 °C (698 to 1,004 °F), are used. Tempering at even higher temperatures, between 540 and 600 °C (1,004 and 1,112 °F), will produce excellent toughness, but at a serious reduction in strength and hardness. At 600 °C (1,112 °F), the steel may experience another stage of embrittlement, called \"temper embrittlement\" (TE), which occurs if the steel is held within the temperature range of temper embrittlement for too long. When heating above this temperature, the steel will usually not be held for any amount of time, and quickly cooled to avoid temper embrittlement.\n",
      "\n",
      "\n",
      "\n",
      "What is the cause of the observed change in the periods of moons orbiting a distant planet when measured from Earth?\n",
      "-Ole Christensen Rømer used an astronomical measurement to make the first quantitative estimate of the speed of light in the year 1676. When measured from Earth, the periods of moons orbiting a distant planet are shorter when the Earth is approaching the planet than when the Earth is receding from it. The distance travelled by light from the planet (or its moon) to Earth is shorter when the Earth is at the point in its orbit that is closest to its planet than when the Earth is at the farthest point in its orbit, the difference in distance being the diameter of the Earth's orbit around the Sun. The observed change in the moon's orbital period is caused by the difference in the time it takes light to traverse the shorter or longer distance. Rømer observed this effect for Jupiter's innermost major moon Io and deduced that light takes 22 minutes to cross the diameter of the Earth's orbit.\n",
      "-The instantaneous lunar distance is constantly changing. The actual distance between the Moon and Earth can change as quickly as 75 meters per second, or more than 1,000 km (620 mi) in just 6 hours, due to its non-circular orbit. There are other effects that also influence the lunar distance. Some factors include: Perturbations and eccentricity The distance to the Moon can be measured to an accuracy of 2 mm over a 1-hour sampling period, which results in an overall uncertainty of a decimeter for the semi-major axis. However, due to its elliptical orbit with varying eccentricity, the instantaneous distance varies with monthly periodicity. Furthermore, the distance is perturbed by the gravitational effects of various astronomical bodies – most significantly the Sun and less so Venus and Jupiter. Other forces responsible for minute perturbations are: gravitational attraction to other planets in the Solar System and to asteroids; tidal forces; and relativistic effects. The effect of radiation pressure from the Sun contributes an amount of ±3.6 mm to the lunar distance.Although the instantaneous uncertainty is a few millimeters, the measured lunar distance can change by more than 21,000 km (13,000 mi) from the mean value throughout a typical month. These perturbations are well understood and the lunar distance can be accurately modeled over thousands of years.\n",
      "-Considering the Earth–Moon system as a binary planet, its centre of gravity is within Earth, about 4,671 km (2,902 mi) or 73.3% of the Earth's radius from the centre of the Earth. This centre of gravity remains on the line between the centres of the Earth and Moon as the Earth completes its diurnal rotation. The path of the Earth–Moon system in its solar orbit is defined as the movement of this mutual centre of gravity around the Sun. Consequently, Earth's centre veers inside and outside the solar orbital path during each synodic month as the Moon moves in its orbit around the common centre of gravity.The Sun's gravitational effect on the Moon is more than twice that of Earth's on the Moon; consequently, the Moon's trajectory is always convex (as seen when looking Sunward at the entire Sun–Earth–Moon system from a great distance outside Earth–Moon solar orbit), and is nowhere concave (from the same perspective) or looped. That is, the region enclosed by the Moon's orbit of the Sun is a convex set.\n",
      "-The variance in the Moon's orbital distance corresponds with changes in its tangential and angular speeds, as stated in Kepler's second law. The mean angular movement relative to an imaginary observer at the Earth–Moon barycentre is 13.176° per day to the east (J2000.0 epoch).\n",
      "-Largest or named lunar inequalities Several of the largest lunar perturbations in longitude (contributions to the difference in its true ecliptic longitude relative to its mean longitude) have been named. In terms of the differential arguments, they can be expressed in the following way, with coefficients rounded to the nearest second of arc (\"): Equation of the center The Moon's equation of the center, or elliptic inequality, was known at least in approximation, to the ancients from the Babylonians and Hipparchus onwards. Knowledge of more recent date is that it corresponds to the approximate application of Kepler's law of equal areas in an elliptical orbit, and represents the speeding-up of the Moon as its distance from the Earth decreases while it moves towards its perigee, and then its slowing down as its distance from the Earth increases while it moves towards its apogee. The effect on the Moon's longitude can be approximated by a series of terms, of which the first three are  22639 sin 769 sin 36 sin ⁡(3l) Evection The evection (or its approximation) was known to Ptolemy, but its name and knowledge of its cause dates from the 17th century. Its effect on the Moon's longitude has an odd-appearing period of about 31.8 days. This can be represented in a number of ways, for example as the result of an approximate 6-monthly libration in the position of perigee, with an accompanying 6-monthly pulsation in the size of the Moon's orbital eccentricity. Its principal term is  4586 sin ⁡(2D−l) Variation The Variation, discovered by Tycho Brahe, is a speeding-up of the Moon as it approaches new-moon and full-moon, and a slowing-down as it approaches first and last quarter. Its gravitational explanation with a quantitative estimate was first given by Newton. Its principal term is  2370 sin ⁡(2D) Annual equation The annual equation, also discovered by Brahe, was qualitatively explained by Newton in terms that the Moon's orbit becomes slightly expanded in size, and longer in period, when the Earth is at perihelion closest to the Sun at the beginning of January, and the Sun's perturbing effect is strongest, and then slightly contracted in size and shorter in period when the Sun is most distant in early July, so that its perturbing effect is weaker: the modern value for the principal term due to this effect is  668 sin ⁡(l′) Parallactic inequality The parallactic inequality, first found by Newton, makes Brahe's Variation a little asymmetric as a result of the finite distance and non-zero parallax of the Sun. Its effect is that the Moon is a little behind at first quarter, and a little ahead at last quarter. Its principal term is  125 sin ⁡(D) Reduction to the ecliptic The reduction to the ecliptic represents the geometric effect of expressing the Moon's motion in terms of a longitude in the plane of the ecliptic, although its motion is really taking place in a plane that is inclined by about 5 degrees. Its principal term is  412 sin ⁡(2F) .The analysts of the mid-18th century expressed the perturbations of the Moon's position in longitude using about 25-30 trigonometrical terms. However, work in the nineteenth and twentieth century led to very different formulations of the theory so these terms are no longer current. The number of terms needed to express the Moon's position with the accuracy sought at the beginning of the twentieth century was over 1400; and the number of terms needed to emulate the accuracy of modern numerical integrations based on laser-ranging observations is in the tens of thousands: there is no limit to the increase in number of terms needed as requirements of accuracy increase.\n",
      "\n",
      "\n",
      "\n",
      "What is the origin of the radio emission observed from supernova remnants?\n",
      "-Supernova remnants A supernova occurs when a high-mass star reaches the end of its life. When nuclear fusion in the core of the star stops, the star collapses. The gas falling inward either rebounds or gets so strongly heated that it expands outwards from the core, thus causing the star to explode. The expanding shell of gas forms a supernova remnant, a special diffuse nebula. Although much of the optical and X-ray emission from supernova remnants originates from ionized gas, a great amount of the radio emission is a form of non-thermal emission called synchrotron emission. This emission originates from high-velocity electrons oscillating within magnetic fields.\n",
      "-Mixed-morphology (also called \"thermal composite\") remnants, in which central thermal X-ray emission is seen, enclosed by a radio shell. The thermal X-rays are primarily from swept-up interstellar material, rather than supernova ejecta. Examples of this class include the SNRs W28 and W44. (Confusingly, W44 additionally contains a pulsar and pulsar wind nebula; so it is simultaneously both a \"classic\" composite and a thermal composite.) Remnants which could only be created by significantly higher ejection energies than a standard supernova are called hypernova remnants, after the high-energy hypernova explosion that is assumed to have created them.\n",
      "-An SNR passes through the following stages as it expands: Free expansion of the ejecta, until they sweep up their own weight in circumstellar or interstellar medium. This can last tens to a few hundred years depending on the density of the surrounding gas.\n",
      "Sweeping up of a shell of shocked circumstellar and interstellar gas. This begins the Sedov-Taylor phase, which can be well modeled by a self-similar analytic solution (see blast wave). Strong X-ray emission traces the strong shock waves and hot shocked gas.\n",
      "Cooling of the shell, to form a thin (< 1 pc), dense (1 to 100 million atoms per cubic metre) shell surrounding the hot (few million kelvin) interior. This is the pressure-driven snowplow phase. The shell can be clearly seen in optical emission from recombining ionized hydrogen and ionized oxygen atoms.\n",
      "Cooling of the interior. The dense shell continues to expand from its own momentum. This stage is best seen in the radio emission from neutral hydrogen atoms.\n",
      "Merging with the surrounding interstellar medium. When the supernova remnant slows to the speed of the random velocities in the surrounding medium, after roughly 30,000 years, it will merge into the general turbulent flow, contributing its remaining kinetic energy to the turbulence.\n",
      "-Light curves The ejecta gases would dim quickly without some energy input to keep them hot. The source of this energy—which can maintain the optical supernova glow for months—was, at first, a puzzle. Some considered rotational energy from the central pulsar as a source. Although the energy that initially powers each type of supernovae is delivered promptly, the light curves are dominated by subsequent radioactive heating of the rapidly expanding ejecta. The intensely radioactive nature of the ejecta gases was first calculated on sound nucleosynthesis grounds in the late 1960s, and this has since been demonstrated as correct for most supernovae. It was not until SN 1987A that direct observation of gamma-ray lines unambiguously identified the major radioactive nuclei.It is now known by direct observation that much of the light curve (the graph of luminosity as a function of time) after the occurrence of a type II Supernova, such as SN 1987A, is explained by those predicted radioactive decays. Although the luminous emission consists of optical photons, it is the radioactive power absorbed by the ejected gases that keeps the remnant hot enough to radiate light. The radioactive decay of 56Ni through its daughters 56Co to 56Fe produces gamma-ray photons, primarily with energies of 847 keV and 1,238 keV, that are absorbed and dominate the heating and thus the luminosity of the ejecta at intermediate times (several weeks) to late times (several months). Energy for the peak of the light curve of SN1987A was provided by the decay of 56Ni to 56Co (half-life 6 days) while energy for the later light curve in particular fit very closely with the 77.3-day half-life of 56Co decaying to 56Fe. Later measurements by space gamma-ray telescopes of the small fraction of the 56Co and 57Co gamma rays that escaped the SN 1987A remnant without absorption confirmed earlier predictions that those two radioactive nuclei were the power sources.\n",
      "-A supernova remnant (SNR) is the structure resulting from the explosion of a star in a supernova. The supernova remnant is bounded by an expanding shock wave, and consists of ejected material expanding from the explosion, and the interstellar material it sweeps up and shocks along the way.\n",
      "\n",
      "\n",
      "\n",
      "What is the relationship between the Hamiltonians and eigenstates in supersymmetric quantum mechanics?\n",
      "-SUSY quantum mechanics involves pairs of Hamiltonians which share a particular mathematical relationship, which are called partner Hamiltonians. (The potential energy terms which occur in the Hamiltonians are then known as partner potentials.) An introductory theorem shows that for every eigenstate of one Hamiltonian, its partner Hamiltonian has a corresponding eigenstate with the same energy. This fact can be exploited to deduce many properties of the eigenstate spectrum. It is analogous to the original description of SUSY, which referred to bosons and fermions. We can imagine a \"bosonic Hamiltonian\", whose eigenstates are the various bosons of our theory. The SUSY partner of this Hamiltonian would be \"fermionic\", and its eigenstates would be the theory's fermions. Each boson would have a fermionic partner of equal energy.\n",
      "-SUSY quantum mechanics involves pairs of Hamiltonians which share a particular mathematical relationship, which are called partner Hamiltonians. (The potential energy terms which occur in the Hamiltonians are then called partner potentials.) An introductory theorem shows that for every eigenstate of one Hamiltonian, its partner Hamiltonian has a corresponding eigenstate with the same energy (except possibly for zero energy eigenstates). This fact can be exploited to deduce many properties of the eigenstate spectrum. It is analogous to the original description of SUSY, which referred to bosons and fermions. We can imagine a \"bosonic Hamiltonian\", whose eigenstates are the various bosons of our theory. The SUSY partner of this Hamiltonian would be \"fermionic\", and its eigenstates would be the theory's fermions. Each boson would have a fermionic partner of equal energy—but, in the relativistic world, energy and mass are interchangeable, so we can just as easily say that the partner particles have equal mass.\n",
      "-The Schrödinger equation for the harmonic oscillator takes the form HHOψn(x)=(−ℏ22md2dx2+mω22x2)ψn(x)=EnHOψn(x), where  ψn(x) is the  n th energy eigenstate of  HHO with energy  EnHO . We want to find an expression for  EnHO in terms of  n . We define the operators A=ℏ2mddx+W(x) and A†=−ℏ2mddx+W(x), where  W(x) , which we need to choose, is called the superpotential of  HHO . We also define the aforementioned partner Hamiltonians  H(1) and  H(2) as H(1)=A†A=−ℏ22md2dx2−ℏ2mW′(x)+W2(x) H(2)=AA†=−ℏ22md2dx2+ℏ2mW′(x)+W2(x).\n",
      "-In theoretical physics, the superpotential is a function in supersymmetric quantum mechanics. Given a superpotential, two \"partner potentials\" are derived that can each serve as a potential in the Schrödinger equation. The partner potentials have the same spectrum, apart from a possible eigenvalue of zero, meaning that the physical systems represented by the two potentials have the same characteristic energies, apart from a possible zero-energy ground state.\n",
      "-In quantum mechanics, the Hamiltonian of a system is an operator corresponding to the total energy of that system, including both kinetic energy and potential energy. Its spectrum, the system's energy spectrum or its set of energy eigenvalues, is the set of possible outcomes obtainable from a measurement of the system's total energy. Due to its close relation to the energy spectrum and time-evolution of a system, it is of fundamental importance in most formulations of quantum theory.\n",
      "\n",
      "\n",
      "\n",
      "What is the proposed name for the field that is responsible for cosmic inflation and the metric expansion of space?\n",
      "-The inflaton field is a hypothetical scalar field which is conjectured to have driven cosmic inflation in the very early universe.\n",
      "The field, originally postulated by Alan Guth, provides a mechanism by which a period of rapid expansion from 10−35 to 10−34 seconds after the initial expansion can be generated, forming a universe consistent with observed spatial isotropy and homogeneity.\n",
      "-In the approximation that the expansion is exactly exponential, the horizon is static and remains a fixed physical distance away. This patch of an inflating universe can be described by the following metric: ds2=−(1−Λr2)c2dt2+11−Λr2dr2+r2dΩ2.\n",
      "-In Guth's early proposal, it was thought that the inflaton was the Higgs field, the field that explains the mass of the elementary particles. It is now believed by some that the inflaton cannot be the Higgs field although the recent discovery of the Higgs boson has increased the number of works considering the Higgs field as inflaton.\n",
      "-Cosmic inflation Inflation is a period of accelerated expansion hypothesized to have occurred at a time of around 10-32 seconds. It would have been driven by the inflaton, a field that has a positive-energy false vacuum state. Inflation was originally proposed to explain the absence of exotic relics predicted by grand unified theories, such as magnetic monopoles, because the rapid expansion would have diluted such relics. It was subsequently realized that the accelerated expansion would also solve the horizon problem and the flatness problem. Additionally, quantum fluctuations during inflation would have created initial variations in the density of the universe, which gravity later amplified to yield the observed spectrum of matter density variations.\n",
      "-When complete, the decay of inflaton particles fills the space with hot and dense Big Bang plasma.\n",
      "\n",
      "\n",
      "\n",
      "Which of the following statements accurately describes the characteristics of gravitational waves?\n",
      "-Frequency: Usually denoted f, this is the frequency with which the wave oscillates (1 divided by the amount of time between two successive maximum stretches or squeezes) Wavelength: Usually denoted λ, this is the distance along the wave between points of maximum stretch or squeeze.\n",
      "-Speed: This is the speed at which a point on the wave (for example, a point of maximum stretch or squeeze) travels. For gravitational waves with small amplitudes, this wave speed is equal to the speed of light (c).The speed, wavelength, and frequency of a gravitational wave are related by the equation c = λf, just like the equation for a light wave. For example, the animations shown here oscillate roughly once every two seconds. This would correspond to a frequency of 0.5 Hz, and a wavelength of about 600 000 km, or 47 times the diameter of the Earth.\n",
      "-Gravitational waves Gravitational waves, a direct consequence of Einstein's theory, are distortions of geometry that propagate at the speed of light, and can be thought of as ripples in spacetime. They should not be confused with the gravity waves of fluid dynamics, which are a different concept.\n",
      "-Difficulties Gravitational waves are not easily detectable. When they reach the Earth, they have a small amplitude with strain approximately 10−21, meaning that an extremely sensitive detector is needed, and that other sources of noise can overwhelm the signal. Gravitational waves are expected to have frequencies 10−16 Hz < f < 104 Hz.\n",
      "-Gravitational waves are waves of the intensity of gravity generated by the accelerated masses of an orbital binary system that propagate as waves outward from their source at the speed of light. They were first proposed by Oliver Heaviside in 1893 and then later by Henri Poincaré in 1905 as waves similar to electromagnetic waves but the gravitational equivalent.\n",
      "\n",
      "\n",
      "\n",
      "What is the difference between the coevolution of myrmecophytes and the mutualistic symbiosis of mycorrhiza?\n",
      "-Mutualism With Mycorrhizae The relationship between plants and mycorrhizal fungi is an example of mutualism because plants provides fungi with carbohydrates and mycorrhizal fungi help plants absorb more water and nutrients. Since mycorrhizal fungi increase plants' uptake of below-ground resources, plants who form a mutualistic relationship with fungi have stimulated shoot growth and a higher shoot to root ratio.\n",
      "-Coevolution Ecological interactions can be classified broadly into a host and an associate relationship. A host is any entity that harbours another that is called the associate. Relationships between species that are mutually or reciprocally beneficial are called mutualisms. Examples of mutualism include fungus-growing ants employing agricultural symbiosis, bacteria living in the guts of insects and other organisms, the fig wasp and yucca moth pollination complex, lichens with fungi and photosynthetic algae, and corals with photosynthetic algae. If there is a physical connection between host and associate, the relationship is called symbiosis. Approximately 60% of all plants, for example, have a symbiotic relationship with arbuscular mycorrhizal fungi living in their roots forming an exchange network of carbohydrates for mineral nutrients.Indirect mutualisms occur where the organisms live apart. For example, trees living in the equatorial regions of the planet supply oxygen into the atmosphere that sustains species living in distant polar regions of the planet. This relationship is called commensalism because many others receive the benefits of clean air at no cost or harm to trees supplying the oxygen. If the associate benefits while the host suffers, the relationship is called parasitism. Although parasites impose a cost to their host (e.g., via damage to their reproductive organs or propagules, denying the services of a beneficial partner), their net effect on host fitness is not necessarily negative and, thus, becomes difficult to forecast. Co-evolution is also driven by competition among species or among members of the same species under the banner of reciprocal antagonism, such as grasses competing for growth space. The Red Queen Hypothesis, for example, posits that parasites track down and specialize on the locally common genetic defense systems of its host that drives the evolution of sexual reproduction to diversify the genetic constituency of populations responding to the antagonistic pressure.\n",
      "-Mutualism Mutualism is defined as an interaction \"between two species or individuals that is beneficial to both\". Probably the most widespread example in plants is the mutual beneficial relationship between plants and fungi, known as mycorrhizae. The plant is assisted with nutrient uptake, while the fungus receives carbohydrates. Some the earliest known fossil plants even have fossil mycorrhizae on their rhizomes.The flowering plants are a group that have evolved by using two major mutualisms. First, flowers are pollinated by insects. This relationship seems to have its origins in beetles feeding on primitive flowers, eating pollen and also acting (unwittingly) as pollinators. Second, fruits are eaten by animals, and the animals then disperse the seeds. Thus, the flowering plants actually have three major types of mutualism, since most higher plants also have mycorrhizae.Plants may also have beneficial effects upon one another, but this is less common. Examples might include \"nurse plants\" whose shade allows young cacti to establish. Most examples of mutualism, however, are largely beneficial to only one of the partners, and may not really be true mutualism. The term used for these more one-sided relationships, which are mostly beneficial to one participant, is facilitation. Facilitation among neighboring plants may act by reducing the negative impacts of a stressful environment. In general, facilitation is more likely to occur in physically stressful environments than in favorable environments, where competition may be the most important interaction among species.Commensalism is similar to facilitation, in that one plant is mostly exploiting another. A familiar example is the epiphytes which grow on branches of tropical trees, or even mosses which grow on trees in deciduous forests.\n",
      "-Mycorrhizae – Mycorrhizae are similar to rhizobia in that they interact with plants at their roots. Whereas rhizobia are bacteria that fix nitrogen, mycorrhizae are fungi that bring nutrients to the plants in return for carbon. Mycorrhizas are also capable of improving water uptake and communicating to their hosts to resist to pathogens. Three main types of mycorrhizae exist:Arbuscula: found in non-woody and tropical plants Ectomycorrhiza: found in boreal and temperate forests Ericoid: found in species of the heathland.Digestive symbiotes – Digestive symbiotes are an example of an important trophic mutualism that does not occur between an autotroph and heterotroph. Bacteria known as \"extracellular symbionts\" live within the gastrointestinal tracts of vertebrates, where they aid in the digestion of food. The bacteria benefits by extracting substrates from the eaten food, while the animal’s assimilation is increased by being able to digest certain foods that its natural system cannot. (book) In addition, these bacteria create short-chain fatty acids (SCFA), providing the vertebrate with energy totaling up to anywhere from 29%-79% of the vertebrate’s maintenance energy depending on the species.\n",
      "-Resource-resource relationships Mutualistic relationships can be thought of as a form of \"biological barter\" in mycorrhizal associations between plant roots and fungi, with the plant providing carbohydrates to the fungus in return for primarily phosphate but also nitrogenous compounds. Other examples include rhizobia bacteria that fix nitrogen for leguminous plants (family Fabaceae) in return for energy-containing carbohydrates. Metabolite exchange between multiple mutualistic species of bacteria has also been observed in a process known as cross-feeding.\n",
      "\n",
      "\n",
      "\n",
      "What is the Kelvin-Helmholtz instability and how does it affect Earth's magnetosphere?\n",
      "-Earth's magnetosphere Over Earth's equator, the magnetic field lines become almost horizontal, then return to reconnect at high latitudes. However, at high altitudes, the magnetic field is significantly distorted by the solar wind and its solar magnetic field. On the dayside of Earth, the magnetic field is significantly compressed by the solar wind to a distance of approximately 65,000 kilometers (40,000 mi). Earth's bow shock is about 17 kilometers (11 mi) thick and located about 90,000 kilometers (56,000 mi) from Earth. The magnetopause exists at a distance of several hundred kilometers above Earth's surface. Earth's magnetopause has been compared to a sieve because it allows solar wind particles to enter. Kelvin–Helmholtz instabilities occur when large swirls of plasma travel along the edge of the magnetosphere at a different velocity from the magnetosphere, causing the plasma to slip past. This results in magnetic reconnection, and as the magnetic field lines break and reconnect, solar wind particles are able to enter the magnetosphere. On Earth's nightside, the magnetic field extends in the magnetotail, which lengthwise exceeds 6,300,000 kilometers (3,900,000 mi). Earth's magnetotail is the primary source of the polar aurora. Also, NASA scientists have suggested that Earth's magnetotail might cause \"dust storms\" on the Moon by creating a potential difference between the day side and the night side.\n",
      "-The research characterised variances in formation of the interplanetary magnetic field (IMF) largely influenced by Kelvin–Helmholtz instability (which occur at the interface of two fluids) as a result of differences in thickness and numerous other characteristics of the boundary layer. Experts believe that this was the first occasion that the appearance of Kelvin–Helmholtz waves at the magnetopause had been displayed at high latitude downward orientation of the IMF. These waves are being seen in unforeseen places under solar wind conditions that were formerly believed to be undesired for their generation. These discoveries show how Earth's magnetosphere can be penetrated by solar particles under specific IMF circumstances. The findings are also relevant to studies of magnetospheric progressions around other planetary bodies. This study suggests that Kelvin–Helmholtz waves can be a somewhat common, and possibly constant, instrument for the entrance of solar wind into terrestrial magnetospheres under various IMF orientations.\n",
      "-The magnetopause is the abrupt boundary between a magnetosphere and the surrounding plasma. For planetary science, the magnetopause is the boundary between the planet's magnetic field and the solar wind. The location of the magnetopause is determined by the balance between the pressure of the dynamic planetary magnetic field and the dynamic pressure of the solar wind. As the solar wind pressure increases and decreases, the magnetopause moves inward and outward in response. Waves (ripples and flapping motion) along the magnetopause move in the direction of the solar wind flow in response to small-scale variations in the solar wind pressure and to Kelvin–Helmholtz instability.\n",
      "-Scientific research into the exact nature of the magnetosheath has been limited due to a longstanding misconception that it was a simple byproduct of the bow shock/magnetopause interaction and had no inherently important properties of its own. Recent studies indicate, however, that the magnetosheath is a dynamic region of turbulent plasma flow that may play an important role in the structure of the bow shock and the magnetopause, and may help to dictate the flow of energetic particles across those boundaries. Kinetic plasma instabilities may cause further complexity by generating plasma waves and energetic particle beams in the magnetosheath and foreshock regions.The Earth's magnetosheath typically occupies the region of space approximately 10 Earth radii on the upwind (Sun-facing) side of the planet, extending significantly farther out on the downwind side due to the pressure of the solar wind. The exact location and width of the magnetosheath depends on variables such as solar activity.\n",
      "-Magnetosphere Earth's magnetosphere is shaped by the impact of the solar wind on Earth's magnetic field. This forms an obstacle to the flow, diverting it, at an average distance of about 70,000 km (11 Earth radii or Re), producing a bow shock 12,000 km to 15,000 km (1.9 to 2.4 Re) further upstream. The width of the magnetosphere abreast of Earth is typically 190,000 km (30 Re), and on the night side a long \"magnetotail\" of stretched field lines extends to great distances (> 200 Re).\n",
      "\n",
      "\n",
      "\n",
      "What is the significance of the high degree of fatty-acyl disorder in the thylakoid membranes of plants?\n",
      "-Phase transitions in biological systems Phase transitions play many important roles in biological systems. Examples include the lipid bilayer formation, the coil-globule transition in the process of protein folding and DNA melting, liquid crystal-like transitions in the process of DNA condensation, and cooperative ligand binding to DNA and proteins with the character of phase transition.In biological membranes, gel to liquid crystalline phase transitions play a critical role in physiological functioning of biomembranes. In gel phase, due to low fluidity of membrane lipid fatty-acyl chains, membrane proteins have restricted movement and thus are restrained in exercise of their physiological role. Plants depend critically on photosynthesis by chloroplast thylakoid membranes which are exposed cold environmental temperatures. Thylakoid membranes retain innate fluidity even at relatively low temperatures because of high degree of fatty-acyl disorder allowed by their high content of linolenic acid, 18-carbon chain with 3-double bonds. Gel-to-liquid crystalline phase transition temperature of biological membranes can be determined by many techniques including calorimetry, fluorescence, spin label electron paramagnetic resonance and NMR by recording measurements of the concerned parameter by at series of sample temperatures. A simple method for its determination from 13-C NMR line intensities has also been proposed.It has been proposed that some biological systems might lie near critical points. Examples include neural networks in the salamander retina, bird flocks gene expression networks in Drosophila, and protein folding. However, it is not clear whether or not alternative reasons could explain some of the phenomena supporting arguments for criticality. It has also been suggested that biological organisms share two key properties of phase transitions: the change of macroscopic behavior and the coherence of a system at a critical point. Phase transitions are prominent feature of motor behavior in biological systems. Spontaneous gait transitions, as well as fatigue-induced motor task disengagements, show typical critical behavior as an intimation of the sudden qualitative change of the previously stable motor behavioral pattern.\n",
      "-Plant thylakoid membranes maintain high fluidity, even at relatively cold environmental temperatures, due to the abundance of 18-carbon fatty acyl chains with three double bonds, linolenic acid, as has been revealed by 13-C NMR studies.\n",
      "-Temperature In response to extremes of temperature, plants can produce various proteins. These protect them from the damaging effects of ice formation and falling rates of enzyme catalysis at low temperatures, and from enzyme denaturation and increased photorespiration at high temperatures. As temperatures fall, production of antifreeze proteins and dehydrins increases. As temperatures rise, production of heat shock proteins increases. Metabolic imbalances associated with temperature extremes result in the build-up of reactive oxygen species, which can be countered by antioxidant systems. Cell membranes are also affected by changes in temperature and can cause the membrane to lose its fluid properties and become a gel in cold conditions or to become leaky in hot conditions. This can affect the movement of compounds across the membrane. To prevent these changes, plants can change the composition of their membranes. In cold conditions, more unsaturated fatty acids are placed in the membrane and in hot conditions, more saturated fatty acids are inserted.\n",
      "-Temperature Plastic responses to temperature are essential among ectothermic organisms, as all aspects of their physiology are directly dependent on their thermal environment. As such, thermal acclimation entails phenotypic adjustments that are found commonly across taxa, such as changes in the lipid composition of cell membranes. Temperature change influences the fluidity of cell membranes by affecting the motion of the fatty acyl chains of glycerophospholipids. Because maintaining membrane fluidity is critical for cell function, ectotherms adjust the phospholipid composition of their cell membranes such that the strength of van der Waals forces within the membrane is changed, thereby maintaining fluidity across temperatures.\n",
      "-Cholesterol is normally found dispersed in varying degrees throughout cell membranes, in the irregular spaces between the hydrophobic tails of the membrane lipids, where it confers a stiffening and strengthening effect on the membrane. Additionally, the amount of cholesterol in biological membranes varies between organisms, cell types, and even in individual cells. Cholesterol, a major component of plasma membranes, regulates the fluidity of the overall membrane, meaning that cholesterol controls the amount of movement of the various cell membrane components based on its concentrations. In high temperatures, cholesterol inhibits the movement of phospholipid fatty acid chains, causing a reduced permeability to small molecules and reduced membrane fluidity. The opposite is true for the role of cholesterol in cooler temperatures. Cholesterol production, and thus concentration, is up-regulated (increased) in response to cold temperature. At cold temperatures, cholesterol interferes with fatty acid chain interactions. Acting as antifreeze, cholesterol maintains the fluidity of the membrane. Cholesterol is more abundant in cold-weather animals than warm-weather animals. In plants, which lack cholesterol, related compounds called sterols perform the same function as cholesterol.\n",
      "\n",
      "\n",
      "\n",
      "What is the explanation for the effective supersymmetry in quark-diquark models?\n",
      "-Quarks carry not only electric charge, but also charges such as color charge and weak isospin. Because of a phenomenon known as color confinement, a hadron cannot have a net color charge; that is, the total color charge of a particle has to be zero (\"white\"). A quark can have one of three \"colors\", dubbed \"red\", \"green\", and \"blue\"; while an antiquark may be either \"anti-red\", \"anti-green\" or \"anti-blue\".For normal hadrons, a white color can thus be achieved in one of three ways:  A quark of one color with an antiquark of the corresponding anticolor, giving a meson with baryon number 0, Three quarks of different colors, giving a baryon with baryon number +1, Three antiquarks of different anticolors, giving an antibaryon with baryon number −1.The baryon number was defined long before the quark model was established, so rather than changing the definitions, particle physicists simply gave quarks one third the baryon number. Nowadays it might be more accurate to speak of the conservation of quark number.\n",
      "-The quarks are bound together by the strong force, which acts in such a way as to cancel the colour charges within the particle. In a meson, this means a quark is partnered with an antiquark with an opposite colour charge – blue and antiblue, for example – while in a baryon, the three quarks have between them all three colour charges – red, blue, and green. In a pentaquark, the colours also need to cancel out, and the only feasible combination is to have one quark with one colour (e.g. red), one quark with a second colour (e.g. green), two quarks with the third colour (e.g. blue), and one antiquark to counteract the surplus colour (e.g. antiblue).The binding mechanism for pentaquarks is not yet clear. They may consist of five quarks tightly bound together, but it is also possible that they are more loosely bound and consist of a three-quark baryon and a two-quark meson interacting relatively weakly with each other via pion exchange (the same force that binds atomic nuclei) in a \"meson-baryon molecule\".\n",
      "-In particle physics, a diquark, or diquark correlation/clustering, is a hypothetical state of two quarks grouped inside a baryon (that consists of three quarks) (Lichtenberg 1982). Corresponding models of baryons are referred to as quark–diquark models. The diquark is often treated as a single subatomic particle with which the third quark interacts via the strong interaction. The existence of diquarks inside the nucleons is a disputed issue, but it helps to explain some nucleon properties and to reproduce experimental data sensitive to the nucleon structure. Diquark–antidiquark pairs have also been advanced for anomalous particles such as the X(3872).\n",
      "-The forces between the two quarks in a diquark is attractive when both the colors and spins are antisymmetric. When both quarks are correlated in this way they tend to form a very low energy configuration. This low energy configuration has become known as a diquark.\n",
      "-Nonetheless, color-charged particles may combine to form color neutral composite particles called hadrons. A quark may pair up with an antiquark: the quark has a color and the antiquark has the corresponding anticolor. The color and anticolor cancel out, forming a color neutral meson. Alternatively, three quarks can exist together, one quark being \"red\", another \"blue\", another \"green\". These three colored quarks together form a color-neutral baryon. Symmetrically, three antiquarks with the colors \"antired\", \"antiblue\" and \"antigreen\" can form a color-neutral antibaryon.\n",
      "\n",
      "\n",
      "\n",
      "What is the relationship between the complete electromagnetic Hamiltonian of a molecule and the parity operation?\n",
      "-Molecules The complete (rotational-vibrational-electronic-nuclear spin) electromagnetic Hamiltonian of any molecule commutes with (or is invariant to) the parity operation P (or E*, in the notation introduced by Longuet-Higgins) and its eigenvalues can be given the parity symmetry label + or - as they are even or odd, respectively. The parity operation involves the inversion of electronic and nuclear spatial coordinates at the molecular center of mass.\n",
      "-To show that quantum electrodynamics is invariant under parity, we have to prove that the action is invariant and the quantization is also invariant. For simplicity we will assume that canonical quantization is used; the vacuum state is then invariant under parity by construction. The invariance of the action follows from the classical invariance of Maxwell's equations. The invariance of the canonical quantization procedure can be worked out, and turns out to depend on the transformation of the annihilation operator: where  p denotes the momentum of a photon and  ± refers to its polarization state. This is equivalent to the statement that the photon has odd intrinsic parity. Similarly all vector bosons can be shown to have odd intrinsic parity, and all axial-vectors to have even intrinsic parity.\n",
      "-All fundamental interactions of elementary particles, with the exception of the weak interaction, are symmetric under parity. The weak interaction is chiral and thus provides a means for probing chirality in physics. In interactions that are symmetric under parity, such as electromagnetism in atomic and molecular physics, parity serves as a powerful controlling principle underlying quantum transitions.\n",
      "A matrix representation of P (in any number of dimensions) has determinant equal to −1, and hence is distinct from a rotation, which has a determinant equal to 1. In a two-dimensional plane, a simultaneous flip of all coordinates in sign is not a parity transformation; it is the same as a 180° rotation.\n",
      "In quantum mechanics, wave functions that are unchanged by a parity transformation are described as even functions, while those that change sign under a parity transformation are odd functions.\n",
      "-Possible eigenvalues In quantum mechanics, spacetime transformations act on quantum states. The parity transformation,  P^ , is a unitary operator, in general acting on a state  ψ as follows:  P^ψ(r)=eiϕ/2ψ(−r) One must then have  P^2ψ(r)=eiϕψ(r) , since an overall phase is unobservable. The operator  P^2 , which reverses the parity of a state twice, leaves the spacetime invariant, and so is an internal symmetry which rotates its eigenstates by phases  eiϕ . If  P^2 is an element  eiQ of a continuous U(1) symmetry group of phase rotations, then  e−iQ is part of this U(1) and so is also a symmetry. In particular, we can define  P^′≡P^e−iQ/2 , which is also a symmetry, and so we can choose to call  P^′ our parity operator, instead of  P^ . Note that  P^′2=1 and so  P^′ has eigenvalues  ±1 . Wave functions with eigenvalue +1 under a parity transformation are even functions, while eigenvalue −1 corresponds to odd functions. However, when no such symmetry group exists, it may be that all parity transformations have some eigenvalues which are phases other than  ±1 For electronic wavefunctions, even states are usually indicated by a subscript g for gerade (German: even) and odd states by a subscript u for ungerade (German: odd). For example, the lowest energy level of the hydrogen molecule ion (H2+) is labelled  1σg and the next-closest (higher) energy level is labelled  1σu .The wave functions of a particle moving into an external potential, which is centrosymmetric (potential energy invariant with respect to a space inversion, symmetric to the origin), either remain invariable or change signs: these two possible states are called the even state or odd state of the wave functions.The law of conservation of parity of particles states that, if an isolated ensemble of particles has a definite parity, then the parity remains invariable in the process of ensemble evolution. However this is not true for the beta decay of nuclei) because the weak nuclear interaction violates parity.The parity of the states of a particle moving in a spherically symmetric external field is determined by the angular momentum, and the particle state is defined by three quantum numbers: total energy, angular momentum and the projection of angular momentum.\n",
      "-Consequences of parity symmetry When parity generates the Abelian group ℤ2, one can always take linear combinations of quantum states such that they are either even or odd under parity (see the figure). Thus the parity of such states is ±1. The parity of a multiparticle state is the product of the parities of each state; in other words parity is a multiplicative quantum number.\n",
      "\n",
      "\n",
      "\n",
      "What is the difference between active and passive transport in cells?\n",
      "-There are different ways through which cells can transport substances across the cell membrane. The two main pathways are passive transport and active transport. Passive transport is more direct and does not require the use of the cell's energy. It relies on an area that maintains a high-to-low concentration gradient. Active transport uses adenosine triphosphate (ATP) to transport a substance that moves against its concentration gradient.\n",
      "-In cellular biology, active transport is the movement of molecules or ions across a cell membrane from a region of lower concentration to a region of higher concentration—against the concentration gradient. Active transport requires cellular energy to achieve this movement. There are two types of active transport: primary active transport that uses adenosine triphosphate (ATP), and secondary active transport that uses an electrochemical gradient. This process is in contrast to passive transport, which allows molecules or ions to move down their concentration gradient, from an area of high concentration to an area of low concentration, without energy.\n",
      "-Unlike passive transport, which uses the kinetic energy and natural entropy of molecules moving down a gradient, active transport uses cellular energy to move them against a gradient, polar repulsion, or other resistance. Active transport is usually associated with accumulating high concentrations of molecules that the cell needs, such as ions, glucose and amino acids. Examples of active transport include the uptake of glucose in the intestines in humans and the uptake of mineral ions into root hair cells of plants.\n",
      "-Passive transport is a type of membrane transport that does not require energy to move substances across cell membranes. Instead of using cellular energy, like active transport, passive transport relies on the second law of thermodynamics to drive the movement of substances across cell membranes. Fundamentally, substances follow Fick's first law, and move from an area of high concentration to one of low concentration because this movement increases the entropy of the overall system. The rate of passive transport depends on the permeability of the cell membrane, which, in turn, depends on the organization and characteristics of the membrane lipids and proteins. The four main kinds of passive transport are simple diffusion, facilitated diffusion, filtration, and/or osmosis.\n",
      "-Specialized transmembrane proteins recognize the substance and allow it to move across the membrane when it otherwise would not, either because the phospholipid bilayer of the membrane is impermeable to the substance moved or because the substance is moved against the direction of its concentration gradient. There are two forms of active transport, primary active transport and secondary active transport. In primary active transport, the proteins involved are pumps that normally use chemical energy in the form of ATP. Secondary active transport, however, makes use of potential energy, which is usually derived through exploitation of an electrochemical gradient. The energy created from one ion moving down its electrochemical gradient is used to power the transport of another ion moving against its electrochemical gradient. This involves pore-forming proteins that form channels across the cell membrane. The difference between passive transport and active transport is that the active transport requires energy, and moves substances against their respective concentration gradient, whereas passive transport requires no cellular energy and moves substances in the direction of their respective concentration gradient.In an antiporter, one substrate is transported in one direction across the membrane while another is cotransported in the opposite direction. In a symporter, two substrates are transported in the same direction across the membrane. Antiport and symport processes are associated with secondary active transport, meaning that one of the two substances is transported against its concentration gradient, utilizing the energy derived from the transport of another ion (mostly Na+, K+ or H+ ions) down its concentration gradient.\n",
      "\n",
      "\n",
      "\n",
      "What is the Heisenberg uncertainty principle and how does it relate to angular momentum in quantum mechanics?\n",
      "-In quantum mechanics, angular momentum (like other quantities) is expressed as an operator, and its one-dimensional projections have quantized eigenvalues. Angular momentum is subject to the Heisenberg uncertainty principle, implying that at any time, only one projection (also called \"component\") can be measured with definite precision; the other two then remain uncertain. Because of this, the axis of rotation of a quantum particle is undefined. Quantum particles do possess a type of non-orbital angular momentum called \"spin\", but this angular momentum does not correspond to a spinning motion. In relativistic quantum mechanics the above relativistic definition becomes a tensorial operator.\n",
      "-Uncertainty In the definition  L=r×p , six operators are involved: The position operators  rx ,  ry ,  rz , and the momentum operators  px ,  py ,  pz . However, the Heisenberg uncertainty principle tells us that it is not possible for all six of these quantities to be known simultaneously with arbitrary precision. Therefore, there are limits to what can be known or measured about a particle's angular momentum. It turns out that the best that one can do is to simultaneously measure both the angular momentum vector's magnitude and its component along one axis.\n",
      "-The uncertainty is closely related to the fact that different components of an angular momentum operator do not commute, for example  LxLy≠LyLx . (For the precise commutation relations, see angular momentum operator.) Total angular momentum as generator of rotations As mentioned above, orbital angular momentum L is defined as in classical mechanics:  L=r×p , but total angular momentum J is defined in a different, more basic way: J is defined as the \"generator of rotations\". More specifically, J is defined so that the operator is the rotation operator that takes any system and rotates it by angle  ϕ about the axis  n^ . (The \"exp\" in the formula refers to operator exponential.) To put this the other way around, whatever our quantum Hilbert space is, we expect that the rotation group SO(3) will act on it. There is then an associated action of the Lie algebra so(3) of SO(3); the operators describing the action of so(3) on our Hilbert space are the (total) angular momentum operators.\n",
      "-For a particle of spin- j the following uncertainty relation holds where  Jl are angular momentum components. The relation can be derived from and The relation can be strengthened as where  FQ[ϱ,Jz] is the quantum Fisher information.\n",
      "-Uncertainty principle One consequence of the basic quantum formalism is the uncertainty principle. In its most familiar form, this states that no preparation of a quantum particle can imply simultaneously precise predictions both for a measurement of its position and for a measurement of its momentum. Both position and momentum are observables, meaning that they are represented by Hermitian operators. The position operator  X^ and momentum operator  P^ do not commute, but rather satisfy the canonical commutation relation: [X^,P^]=iℏ.\n",
      "\n",
      "\n",
      "\n",
      "What is the difference between natural convection and forced convection?\n",
      "-Natural convection will be more likely and more rapid with a greater variation in density between the two fluids, a larger acceleration due to gravity that drives the convection or a larger distance through the convecting medium. Natural convection will be less likely and less rapid with more rapid diffusion (thereby diffusing away the thermal gradient that is causing the convection) or a more viscous (sticky) fluid.\n",
      "-Free, or natural, convection occurs when bulk fluid motions (streams and currents) are caused by buoyancy forces that result from density variations due to variations of temperature in the fluid. Forced convection is a term used when the streams and currents in the fluid are induced by external means—such as fans, stirrers, and pumps—creating an artificially induced convection current.\n",
      "-Convection can be \"forced\" by movement of a fluid by means other than buoyancy forces (for example, a water pump in an automobile engine). Thermal expansion of fluids may also force convection. In other cases, natural buoyancy forces alone are entirely responsible for fluid motion when the fluid is heated, and this process is called \"natural convection\". An example is the draft in a chimney or around any fire. In natural convection, an increase in temperature produces a reduction in density, which in turn causes fluid motion due to pressures and forces when fluids of different densities are affected by gravity (or any g-force). For example, when water is heated on a stove, hot water from the bottom of the pan is displaced (or forced up) by the colder denser liquid, which falls. After heating has stopped, mixing and conduction from this natural convection eventually result in a nearly homogeneous density, and even temperature. Without the presence of gravity (or conditions that cause a g-force of any type), natural convection does not occur, and only forced-convection modes operate.\n",
      "-Forced convection: when a fluid is forced to flow over the surface by an internal source such as fans, by stirring, and pumps, creating an artificially induced convection current.In many real-life applications (e.g. heat losses at solar central receivers or cooling of photovoltaic panels), natural and forced convection occur at the same time (mixed convection).Internal and external flow can also classify convection. Internal flow occurs when a fluid is enclosed by a solid boundary such as when flowing through a pipe. An external flow occurs when a fluid extends indefinitely without encountering a solid surface. Both of these types of convection, either natural or forced, can be internal or external because they are independent of each other. The bulk temperature, or the average fluid temperature, is a convenient reference point for evaluating properties related to convective heat transfer, particularly in applications related to flow in pipes and ducts.\n",
      "-Convection The flow of fluid may be forced by external processes, or sometimes (in gravitational fields) by buoyancy forces caused when thermal energy expands the fluid (for example in a fire plume), thus influencing its own transfer. The latter process is often called \"natural convection\". All convective processes also move heat partly by diffusion, as well. Another form of convection is forced convection. In this case the fluid is forced to flow by using a pump, fan or other mechanical means.\n",
      "\n",
      "\n",
      "\n",
      "What is magnetic susceptibility?\n",
      "-Volume susceptibility Magnetic susceptibility is a dimensionless proportionality constant that indicates the degree of magnetization of a material in response to an applied magnetic field. A related term is magnetizability, the proportion between magnetic moment and magnetic flux density. A closely related parameter is the permeability, which expresses the total magnetization of material and volume.\n",
      "-In electromagnetism, the magnetic susceptibility (from Latin susceptibilis 'receptive'; denoted χ, chi) is a measure of how much a material will become magnetized in an applied magnetic field. It is the ratio of magnetization M (magnetic moment per unit volume) to the applied magnetizing field intensity H. This allows a simple classification, into two categories, of most materials' responses to an applied magnetic field: an alignment with the magnetic field, χ > 0, called paramagnetism, or an alignment against the field, χ < 0, called diamagnetism.\n",
      "-Magnetic susceptibility indicates whether a material is attracted into or repelled out of a magnetic field. Paramagnetic materials align with the applied field and are attracted to regions of greater magnetic field. Diamagnetic materials are anti-aligned and are pushed away, toward regions of lower magnetic fields. On top of the applied field, the magnetization of the material adds its own magnetic field, causing the field lines to concentrate in paramagnetism, or be excluded in diamagnetism. Quantitative measures of the magnetic susceptibility also provide insights into the structure of materials, providing insight into bonding and energy levels. Furthermore, it is widely used in geology for paleomagnetic studies and structural geology.The magnetizability of materials comes from the atomic-level magnetic properties of the particles of which they are made. Usually, this is dominated by the magnetic moments of electrons. Electrons are present in all materials, but without any external magnetic field, the magnetic moments of the electrons are usually either paired up or random so that the overall magnetism is zero (the exception to this usual case is ferromagnetism). The fundamental reasons why the magnetic moments of the electrons line up or do not are very complex and cannot be explained by classical physics. However, a useful simplification is to measure the magnetic susceptibility of a material and apply the macroscopic form of Maxwell's equations. This allows classical physics to make useful predictions while avoiding the underlying quantum mechanical details.\n",
      "-Using SI units, the magnetic induction B is related to H by the relationship where μ0 is the vacuum permeability (see table of physical constants), and (1 + χv) is the relative permeability of the material. Thus the volume magnetic susceptibility χv and the magnetic permeability μ are related by the following formula: Sometimes an auxiliary quantity called intensity of magnetization I (also referred to as magnetic polarisation J) and with unit teslas, is defined as This allows an alternative description of all magnetization phenomena in terms of the quantities I and B, as opposed to the commonly used M and H.\n",
      "-A variety of methods are available for the measurement of magnetic susceptibility.\n",
      "\n",
      "\n",
      "\n",
      "What is a transient condensation cloud, also known as a Wilson cloud?\n",
      "-A transient condensation cloud, also called a Wilson cloud, is observable surrounding large explosions in humid air.\n",
      "-The lifetime of the Wilson cloud during nuclear air bursts can be shortened by the thermal radiation from the fireball, which heats the cloud above to the dew point and evaporates the droplets.\n",
      "Non-nuclear explosions Any sufficiently large explosion, such as one caused by a large quantity of conventional explosives or a volcanic eruption, can create a condensation cloud, as seen in Operation Sailor Hat or in the 2020 Beirut explosion, where a very large Wilson cloud expanded outwards from the blast.\n",
      "-When a nuclear weapon or a large amount of a conventional explosive is detonated in sufficiently humid air, the \"negative phase\" of the shock wave causes a rarefaction of the air surrounding the explosion, but not contained within it. This rarefaction results in a temporary cooling of that air, which causes a condensation of some of the water vapor contained in it. When the pressure and the temperature return to normal, the Wilson cloud dissipates.\n",
      "-The shape of the shock wave, influenced by different speed in different altitudes, and the temperature and humidity of different atmospheric layers determines the appearance of the Wilson clouds. During nuclear tests, condensation rings around or above the fireball are commonly observed. Rings around the fireball may become stable and form rings around the rising stem of the mushroom cloud.\n",
      "-Condensation effects Nuclear mushroom clouds are often accompanied by short-lived vapour clouds, known variously as \"Wilson clouds\", condensation clouds, or vapor rings. The \"negative phase\" following the positive overpressure behind a shock front causes a sudden rarefaction of the surrounding medium. This low pressure region causes an adiabatic drop in temperature, causing moisture in the air to condense in an outward moving shell surrounding the explosion. When the pressure and temperature return to normal, the Wilson cloud dissipates. Scientists observing the Operation Crossroads nuclear tests in 1946 at Bikini Atoll named that transitory cloud a \"Wilson cloud\" because of its visual similarity to a Wilson cloud chamber; the cloud chamber uses condensation from a rapid pressure drop to mark the tracks of electrically charged subatomic particles. Analysts of later nuclear bomb tests used the more general term \"condensation cloud\" in preference to \"Wilson cloud\".\n",
      "\n",
      "\n",
      "\n",
      "What is a uniform tiling in the hyperbolic plane?\n",
      "-In hyperbolic geometry, a uniform hyperbolic tiling (or regular, quasiregular or semiregular hyperbolic tiling) is an edge-to-edge filling of the hyperbolic plane which has regular polygons as faces and is vertex-transitive (transitive on its vertices, isogonal, i.e. there is an isometry mapping any vertex onto any other). It follows that all vertices are congruent, and the tiling has a high degree of rotational and translational symmetry.\n",
      "-Tessellations in non-Euclidean geometries It is possible to tessellate in non-Euclidean geometries such as hyperbolic geometry. A uniform tiling in the hyperbolic plane (that may be regular, quasiregular, or semiregular) is an edge-to-edge filling of the hyperbolic plane, with regular polygons as faces; these are vertex-transitive (transitive on its vertices), and isogonal (there is an isometry mapping any vertex onto any other).A uniform honeycomb in hyperbolic space is a uniform tessellation of uniform polyhedral cells. In three-dimensional (3-D) hyperbolic space there are nine Coxeter group families of compact convex uniform honeycombs, generated as Wythoff constructions, and represented by permutations of rings of the Coxeter diagrams for each family.\n",
      "-In geometry, a uniform tiling is a tessellation of the plane by regular polygon faces with the restriction of being vertex-transitive.\n",
      "Uniform tilings can exist in both the Euclidean plane and hyperbolic plane. Uniform tilings are related to the finite uniform polyhedra which can be considered uniform tilings of the sphere.\n",
      "Most uniform tilings can be made from a Wythoff construction starting with a symmetry group and a singular generator point inside of the fundamental domain. A planar symmetry group has a polygonal fundamental domain and can be represented by the group name represented by the order of the mirrors in sequential vertices.\n",
      "A fundamental domain triangle is (p q r), and a right triangle (p q 2), where p, q, r are whole numbers greater than 1. The triangle may exist as a spherical triangle, a Euclidean plane triangle, or a hyperbolic plane triangle, depending on the values of p, q and r.\n",
      "-In geometry, the hexaoctagonal tiling is a uniform tiling of the hyperbolic plane.\n",
      "-There are an infinite number of dual uniform tilings in hyperbolic plane with isogonal irregular pentagonal faces. They have face configurations as V3.3.p.3.q.\n",
      "The binary tiling can be made into a pentagonal tiling if one replaces the horocyclic edges by line segments.\n",
      "\n",
      "\n",
      "\n",
      "What is the relation between the three moment theorem and the bending moments at three successive supports of a continuous beam?\n",
      "-Mohr's first theorem The change in slope of a deflection curve between two points of a beam is equal to the area of the M/EI diagram between those two points.(Figure 02) Mohr's second theorem Consider two points k1 and k2 on a beam. The deflection of k1 and k2 relative to the point of intersection between tangent at k1 and k2 and vertical through k1 is equal to the moment of M/EI diagram between k1 and k2 about k1.(Figure 03) The three moment equation expresses the relation between bending moments at three successive supports of a continuous beam, subject to a loading on a two adjacent span with or without settlement of the supports.\n",
      "-In civil engineering and structural analysis Clapeyron's theorem of three moments is a relationship among the bending moments at three consecutive supports of a horizontal beam.\n",
      "Let A,B,C-D be the three consecutive points of support, and denote by- l the length of AB and  l′ the length of BC, by w and  w′ the weight per unit of length in these segments. Then the bending moments  MA,MB,MC at the three points are related by: MAl+2MB(l+l′)+MCl′=14wl3+14w′(l′)3.\n",
      "-This equation can also be written as  MAl+2MB(l+l′)+MCl′=6a1x1l+6a2x2l′ where a1 is the area on the bending moment diagram due to vertical loads on AB, a2 is the area due to loads on BC, x1 is the distance from A to the centroid of the bending moment diagram of beam AB, x2 is the distance from C to the centroid of the area of the bending moment diagram of beam BC.\n",
      "-The moment-area theorem is an engineering tool to derive the slope, rotation and deflection of beams and frames. This theorem was developed by Mohr and later stated namely by Charles Ezra Greene in 1873. This method is advantageous when we solve problems involving beams, especially for those subjected to a series of concentrated loadings or having segments with different moments of inertia.\n",
      "-The statically indeterminate beam shown in the figure is to be analysed.\n",
      "The beam is considered to be three separate members, AB, BC, and CD, connected by fixed end (moment resisting) joints at B and C.\n",
      "Members AB, BC, CD have the same span  10 m Flexural rigidities are EI, 2EI, EI respectively.\n",
      "Concentrated load of magnitude  10 kN acts at a distance  a=3m from the support A.\n",
      "Uniform load of intensity  q=1kN/m acts on BC.\n",
      "Member CD is loaded at its midspan with a concentrated load of magnitude  10 kN .In the following calculations, clockwise moments are positive.\n",
      "\n",
      "\n",
      "\n",
      "What is the throttling process, and why is it important?\n",
      "-Throttling One of the simple applications of the concept of enthalpy is the so-called throttling process, also known as Joule–Thomson expansion. It concerns a steady adiabatic flow of a fluid through a flow resistance (valve, porous plug, or any other type of flow resistance) as shown in the figure. This process is very important, since it is at the heart of domestic refrigerators, where it is responsible for the temperature drop between ambient temperature and the interior of the refrigerator. It is also the final stage in many types of liquefiers.\n",
      "-Compressor refrigerators A vapor compression cycle is used in most household refrigerators, refrigerator–freezers and freezers. In this cycle, a circulating refrigerant such as R134a enters a compressor as low-pressure vapor at or slightly below the temperature of the refrigerator interior. The vapor is compressed and exits the compressor as high-pressure superheated vapor. The superheated vapor travels under pressure through coils or tubes that make up the condenser; the coils or tubes are passively cooled by exposure to air in the room. The condenser cools the vapor, which liquefies. As the refrigerant leaves the condenser, it is still under pressure but is now only slightly above room temperature. This liquid refrigerant is forced through a metering or throttling device, also known as an expansion valve (essentially a pin-hole sized constriction in the tubing) to an area of much lower pressure. The sudden decrease in pressure results in explosive-like flash evaporation of a portion (typically about half) of the liquid. The latent heat absorbed by this flash evaporation is drawn mostly from adjacent still-liquid refrigerant, a phenomenon known as auto-refrigeration. This cold and partially vaporized refrigerant continues through the coils or tubes of the evaporator unit. A fan blows air from the compartment (\"box air\") across these coils or tubes and the refrigerant completely vaporizes, drawing further latent heat from the box air. This cooled air is returned to the refrigerator or freezer compartment, and so keeps the box air cold. Note that the cool air in the refrigerator or freezer is still warmer than the refrigerant in the evaporator. Refrigerant leaves the evaporator, now fully vaporized and slightly heated, and returns to the compressor inlet to continue the cycle.\n",
      "-The gas-cooling throttling process is commonly exploited in refrigeration processes such as liquefiers in air separation industrial process. In hydraulics, the warming effect from Joule–Thomson throttling can be used to find internally leaking valves as these will produce heat which can be detected by thermocouple or thermal-imaging camera. Throttling is a fundamentally irreversible process. The throttling due to the flow resistance in supply lines, heat exchangers, regenerators, and other components of (thermal) machines is a source of losses that limits their performance.\n",
      "-Another aspect of breathing performance is demand regulator performance in cold water, where a high flow rate may cause chilling sufficient to lock up the mechanism with ice, which usually causes a severe free-flow with consequent loss of breathing gas, which can only be stopped by shutting off the cylinder valve.\n",
      "-In comparison, a compressor based heat pump works by pumping refrigerant gas from an evaporator to a condenser. This reduces the pressure and boiling temperature in the evaporator and increases the pressure and condensing temperature in the condenser. Energy from an electric motor or internal combustion engine is required to operate the compressor pump. Compressing the refrigerant uses this energy to do work on the gas, increasing its temperature. The warm, high pressure gas then enters the condenser where it undergoes a phase change to a liquid, releasing heat to the condenser's surroundings. Warm liquid refrigerant moves from the high pressure condenser to the low pressure evaporator via an expansion valve, also known as a throttling valve or a Joule-Thompson valve. The expansion valve partially vaporizes the refrigerant cooling it via evaporative cooling and the resulting vapor is cooled via expansive cooling. (This is a combination of Joule-Thompson cooling and work done by the expanding gas, both at the expense of the internal energy of the gas) The cold, low pressure liquid refrigerant will now absorb heat from the evaporator's surroundings and vaporize. The resulting gas enters the compressor and the cycle begins again.\n",
      "\n",
      "\n",
      "\n",
      "What happens to excess base metal as a solution cools from the upper transformation temperature towards an insoluble state?\n",
      "-Similarly, a hypoeutectoid alloy has two critical temperatures, called \"arrests\". Between these two temperatures, the alloy will exist partly as the solution and partly as a separate crystallizing phase, called the \"pro eutectoid phase\". These two temperatures are called the upper (A3) and lower (A1) transformation temperatures. As the solution cools from the upper transformation temperature toward an insoluble state, the excess base metal will often be forced to \"crystallize-out\", becoming the pro eutectoid. This will occur until the remaining concentration of solutes reaches the eutectoid level, which will then crystallize as a separate microstructure.\n",
      "-Like oil and water, a molten metal may not always mix with another element. For example, pure iron is almost completely insoluble with copper. Even when the constituents are soluble, each will usually have a saturation point, beyond which no more of the constituent can be added. Iron, for example, can hold a maximum of 6.67% carbon. Although the elements of an alloy usually must be soluble in the liquid state, they may not always be soluble in the solid state. If the metals remain soluble when solid, the alloy forms a solid solution, becoming a homogeneous structure consisting of identical crystals, called a phase. If as the mixture cools the constituents become insoluble, they may separate to form two or more different types of crystals, creating a heterogeneous microstructure of different phases, some with more of one constituent than the other. However, in other alloys, the insoluble elements may not separate until after crystallization occurs. If cooled very quickly, they first crystallize as a homogeneous phase, but they are supersaturated with the secondary constituents. As time passes, the atoms of these supersaturated alloys can separate from the crystal lattice, becoming more stable, and forming a second phase that serves to reinforce the crystals internally.\n",
      "-Hypereutectoid alloys A hypereutectic alloy also has different melting points. However, between these points, it is the constituent with the higher melting point that will be solid. Similarly, a hypereutectoid alloy has two critical temperatures. When cooling a hypereutectoid alloy from the upper transformation temperature, it will usually be the excess solutes that crystallize-out first, forming the pro-eutectoid. This continues until the concentration in the remaining alloy becomes eutectoid, which then crystallizes into a separate microstructure.\n",
      "-The process differs from diffusion bonding, in which diffusion occurs when a melting point represent element from an interlayer moves into lattice and grain boundaries of the substrates at the bonding temperature. Solid state diffusional processes lead to a change of composition at the bond interface and the dissimilar interlayer melts at a lower temperature than the parent materials. Thus a thin layer of liquid spreads along the interface to form a joint at a lower temperature than the melting point of either of the parent materials. This method differs from brazing in that it is \"isothermally solidifying\". While holding the temperature above the filler metal melting point, interdiffusion shifts the composition away from eutectic, so solidification occurs at the process temperature. If sufficient interdiffusion occurs, the joint will remain solid and strong well above the original melt process temperature. This is why it is termed \"transient liquid phase.\" The liquid solidifies before cooling.\n",
      "-In the phase diagram, at three different concentrations, the material will be solid until its heated to its melting point, and then (after adding the heat of fusion) become liquid at that same temperature: the unalloyed extreme left the unalloyed extreme right the dip in the center (the eutectic composition).At other proportions, the material will enter a mushy or pasty phase until it warms up to being completely melted.\n",
      "\n",
      "\n",
      "\n",
      "What is the relationship between mass, force, and acceleration, according to Sir Isaac Newton's laws of motion?\n",
      "-Mass is (among other properties) an inertial property; that is, the tendency of an object to remain at constant velocity unless acted upon by an outside force. Under Sir Isaac Newton's 336-year-old laws of motion and an important formula that sprang from his work, F = ma, an object with a mass, m, of one kilogram accelerates, a, at one meter per second per second (about one-tenth the acceleration due to earth's gravity) when acted upon by a force, F, of one newton.\n",
      "-In more formal terms, Newton's second law of motion states that the force exerted on an object is directly proportional to the acceleration hence acquired by that object, thus: F=ma, where  m represents the mass of the object undergoing an acceleration  a . As a result, the newton may be defined in terms of the kilogram ( kg ), metre ( m ), and second ( s ) as kg ⋅ms2.\n",
      "-In classical mechanics, according to Newton's second law, we say that a body has a mass m if, at any instant of time, it obeys the equation of motion F=ma, where F is the resultant force acting on the body and a is the acceleration of the body's centre of mass. For the moment, we will put aside the question of what \"force acting on the body\" actually means.\n",
      "-Forces and Newton's second law A force in physics is any action that causes an object's velocity to change; that is, to accelerate. A force originates from within a field, such as an electro-static field (caused by static electrical charges), electro-magnetic field (caused by moving charges), or gravitational field (caused by mass), among others.\n",
      "Newton was the first to mathematically express the relationship between force and momentum. Some physicists interpret Newton's second law of motion as a definition of force and mass, while others consider it a fundamental postulate, a law of nature. Either interpretation has the same mathematical consequences, historically known as \"Newton's Second Law\": F=dpdt=d(mv)dt.\n",
      "The quantity mv is called the (canonical) momentum. The net force on a particle is thus equal to the rate of change of the momentum of the particle with time. Since the definition of acceleration is a = dv/dt, the second law can be written in the simplified and more familiar form: F=ma.\n",
      "So long as the force acting on a particle is known, Newton's second law is sufficient to describe the motion of a particle. Once independent relations for each force acting on a particle are available, they can be substituted into Newton's second law to obtain an ordinary differential equation, which is called the equation of motion.\n",
      "As an example, assume that friction is the only force acting on the particle, and that it may be modeled as a function of the velocity of the particle, for example: FR=−λv, where λ is a positive constant, the negative sign states that the force is opposite the sense of the velocity. Then the equation of motion is −λv=ma=mdvdt.\n",
      "-However, this notion of applying \"identical\" forces to different objects brings us back to the fact that we have not really defined what a force is. We can sidestep this difficulty with the help of Newton's third law, which states that if one object exerts a force on a second object, it will experience an equal and opposite force. To be precise, suppose we have two objects of constant inertial masses m1 and m2. We isolate the two objects from all other physical influences, so that the only forces present are the force exerted on m1 by m2, which we denote F12, and the force exerted on m2 by m1, which we denote F21. Newton's second law states that 12 21 =m2a2, where a1 and a2 are the accelerations of m1 and m2, respectively. Suppose that these accelerations are non-zero, so that the forces between the two objects are non-zero. This occurs, for example, if the two objects are in the process of colliding with one another. Newton's third law then states that 12 21 ; and thus m1=m2|a2||a1|.\n",
      "\n",
      "\n",
      "\n",
      "What did Arthur Eddington discover about two of Einstein's types of gravitational waves?\n",
      "-The possibility of gravitational waves was discussed in 1893 by Oliver Heaviside, using the analogy between the inverse-square law of gravitation and the electrostatic force. In 1905, Henri Poincaré proposed gravitational waves, emanating from a body and propagating at the speed of light, as being required by the Lorentz transformations and suggested that, in analogy to an accelerating electrical charge producing electromagnetic waves, accelerated masses in a relativistic field theory of gravity should produce gravitational waves. When Einstein published his general theory of relativity in 1915, he was skeptical of Poincaré's idea since the theory implied there were no \"gravitational dipoles\". Nonetheless, he still pursued the idea and based on various approximations came to the conclusion there must, in fact, be three types of gravitational waves (dubbed longitudinal–longitudinal, transverse–longitudinal, and transverse–transverse by Hermann Weyl).However, the nature of Einstein's approximations led many (including Einstein himself) to doubt the result. In 1922, Arthur Eddington showed that two of Einstein's types of waves were artifacts of the coordinate system he used, and could be made to propagate at any speed by choosing appropriate coordinates, leading Eddington to jest that they \"propagate at the speed of thought\".: 72  This also cast doubt on the physicality of the third (transverse–transverse) type that Eddington showed always propagate at the speed of light regardless of coordinate system. In 1936, Einstein and Nathan Rosen submitted a paper to Physical Review in which they claimed gravitational waves could not exist in the full general theory of relativity because any such solution of the field equations would have a singularity. The journal sent their manuscript to be reviewed by Howard P. Robertson, who anonymously reported that the singularities in question were simply the harmless coordinate singularities of the employed cylindrical coordinates. Einstein, who was unfamiliar with the concept of peer review, angrily withdrew the manuscript, never to publish in Physical Review again. Nonetheless, his assistant Leopold Infeld, who had been in contact with Robertson, convinced Einstein that the criticism was correct, and the paper was rewritten with the opposite conclusion and published elsewhere.: 79ff  In 1956, Felix Pirani remedied the confusion caused by the use of various coordinate systems by rephrasing the gravitational waves in terms of the manifestly observable Riemann curvature tensor.At the time, Pirani's work was overshadowed by the community's focus on a different question: whether gravitational waves could transmit energy. This matter was settled by a thought experiment proposed by Richard Feynman during the first \"GR\" conference at Chapel Hill in 1957. In short, his argument known as the \"sticky bead argument\" notes that if one takes a rod with beads then the effect of a passing gravitational wave would be to move the beads along the rod; friction would then produce heat, implying that the passing wave had done work. Shortly after, Hermann Bondi, published a detailed version of the \"sticky bead argument\". This later lead to a series of articles (1959 to 1989)  by Bondi and Pirani that established the existence of plane wave solutions for gravitational waves.After the Chapel Hill conference, Joseph Weber started designing and building the first gravitational wave detectors now known as Weber bars. In 1969, Weber claimed to have detected the first gravitational waves, and by 1970 he was \"detecting\" signals regularly from the Galactic Center; however, the frequency of detection soon raised doubts on the validity of his observations as the implied rate of energy loss of the Milky Way would drain our galaxy of energy on a timescale much shorter than its inferred age. These doubts were strengthened when, by the mid-1970s, repeated experiments from other groups building their own Weber bars across the globe failed to find any signals, and by the late 1970s consensus was that Weber's results were spurious.In the same period, the first indirect evidence of gravitational waves was discovered. In 1974, Russell Alan Hulse and Joseph Hooton Taylor, Jr. discovered the first binary pulsar, which earned them the 1993 Nobel Prize in Physics. Pulsar timing observations over the next decade showed a gradual decay of the orbital period of the Hulse–Taylor pulsar that matched the loss of energy and angular momentum in gravitational radiation predicted by general relativity.This indirect detection of gravitational waves motivated further searches, despite Weber's discredited result. Some groups continued to improve Weber's original concept, while others pursued the detection of gravitational waves using laser interferometers. The idea of using a laser interferometer for this seems to have been floated independently by various people, including M. E. Gertsenshtein and V. I. Pustovoit in 1962, and Vladimir B. Braginskiĭ in 1966. The first prototypes were developed in the 1970s by Robert L. Forward and Rainer Weiss. In the decades that followed, ever more sensitive instruments were constructed, culminating in the construction of GEO600, LIGO, and Virgo.After years of producing null results, improved detectors became operational in 2015. On 11 February 2016, the LIGO-Virgo collaborations announced the first observation of gravitational waves, from a signal (dubbed GW150914) detected at 09:50:45 GMT on 14 September 2015 of two black holes with masses of 29 and 36 solar masses merging about 1.3 billion light-years away. During the final fraction of a second of the merger, it released more than 50 times the power of all the stars in the observable universe combined. The signal increased in frequency from 35 to 250 Hz over 10 cycles (5 orbits) as it rose in strength for a period of 0.2 second. The mass of the new merged black hole was 62 solar masses. Energy equivalent to three solar masses was emitted as gravitational waves. The signal was seen by both LIGO detectors in Livingston and Hanford, with a time difference of 7 milliseconds due to the angle between the two detectors and the source. The signal came from the Southern Celestial Hemisphere, in the rough direction of (but much farther away than) the Magellanic Clouds. The confidence level of this being an observation of gravitational waves was 99.99994%.A year earlier, the BICEP2 collaboration claimed that they had detected the imprint of gravitational waves in the cosmic microwave background. However, they were later forced to retract this result.In 2017, the Nobel Prize in Physics was awarded to Rainer Weiss, Kip Thorne and Barry Barish for their role in the detection of gravitational waves.In 2023, NANOGrav, EPTA, PPTA, and IPTA announced that they found evidence of a universal gravitational wave background. North American Nanohertz Observatory for Gravitational Waves states, that they were created over cosmological time scales by supermassive black holes, identifying the distinctive Hellings-Downs curve in 15 years of radio observations of 25 pulsars.\n",
      "-Leopold Infeld, who arrived at Princeton University at this time, later remembered his utter astonishment on hearing of this development, since radiation is such an essential element for any classical field theory worthy of the name. Infeld expressed his doubts to a leading expert on general relativity: H. P. Robertson, who had just returned from a visit to Caltech. Going over the argument as Infeld remembered it, Robertson was able to show Infeld the mistake: locally, the Einstein–Rosen waves are gravitational plane waves. Einstein and Rosen had correctly shown that a cloud of test particles would, in sinusoidal plane waves, form caustics, but changing to another chart (essentially the Brinkmann coordinates) shows that the formation of the caustic is not a contradiction at all, but in fact just what one would expect in this situation. Infeld then approached Einstein, who concurred with Robertson's analysis (still not knowing it was he who reviewed the Physical Review submission).\n",
      "-In 1922, Arthur Stanley Eddington wrote a paper expressing (apparently for the first time) the view that gravitational waves are in essence ripples in coordinates, and have no physical meaning. He did not appreciate Einstein's arguments that the waves are real.In 1936, together with Nathan Rosen, Einstein rediscovered the Beck vacuums, a family of exact gravitational wave solutions with cylindrical symmetry (sometimes also called Einstein–Rosen waves). While investigating the motion of test particles in these solutions, Einstein and Rosen became convinced that gravitational waves were unstable to collapse. Einstein reversed himself and declared that gravitational radiation was not after all a prediction of his theory. Einstein wrote to his friend Max Born Together with a young collaborator, I arrived at the interesting result that gravitational waves do not exist, though they had been assumed a certainty to the first approximation. This shows that the nonlinear field equations can show us more, or rather limit us more, than we have believed up till now.\n",
      "-The Eddington experiment was an observational test of general relativity, organised by the British astronomers Frank Watson Dyson and Arthur Stanley Eddington in 1919. The observations were of the total solar eclipse of 29 May 1919 and were carried out by two expeditions, one to the West African island of Príncipe, and the other to the Brazilian town of Sobral. The aim of the expeditions was to measure the gravitational deflection of starlight passing near the Sun. The value of this deflection had been predicted by Albert Einstein in a 1911 paper; however, this initial prediction turned out not to be correct because it was based on an incomplete theory of general relativity. Einstein later improved his prediction after finalizing his theory in 1915 and obtaining the solution to his equations by Karl Schwarzschild. Following the return of the expeditions, the results were presented by Eddington to the Royal Society of London and, after some deliberation, were accepted. Widespread newspaper coverage of the results led to worldwide fame for Einstein and his theories.\n",
      "-Gravity’s Century: From Einstein’s Eclipse to Images of Black Holes (2019) - Ron Cowen \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 뽑힌 context의 내용 확인하기 \n",
    "\n",
    "context=df['context'].tolist()\n",
    "\n",
    "for idx,q in enumerate(df['prompt'].tolist()):\n",
    "    print (q)\n",
    "    print(context[idx])\n",
    "    print()\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b9ce42b-7bdd-446c-bd79-2420db3254b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8194, 3445, 5457, 4509, 3363, 3681, 3733, 2815, 4696, 3033, 8755, 3588, 3971, 3179, 5189, 3361, 3181, 3096, 4139, 2319, 3507, 2205, 2394, 4869, 2482, 4139, 3025, 3311, 5814, 3741, 4304, 3214, 4307, 4813, 2964, 1916, 5524, 4514, 3228, 4849, 2687, 3363, 4806, 7207, 5749, 2638, 3201, 3749, 7376, 4750, 4073, 2719, 2282, 7790, 4060, 3720, 1851, 5461, 5800, 6631, 3443, 5448, 4656, 3081, 3350, 4710, 2809, 3661, 5519, 2135, 4272, 4303, 4272, 6363, 2026, 5632, 3977, 2943, 3576, 5665, 2900, 4943, 4323, 2355, 2340, 3666, 3271, 2728, 4283, 4428, 1440, 3375, 2252, 3226, 3058, 2543, 2312, 4201, 4460, 3125, 4224, 6313, 2766, 5487, 2607, 2678, 4067, 4273, 3371, 4147, 3306, 2836, 5684, 3359, 2274, 2090, 5050, 5363, 3339, 2210, 2938, 3036, 2808, 2856, 2905, 2338, 1737, 4366, 4141, 2707, 2534, 8359, 6470, 1972, 2026, 14370, 2782, 3395, 4968, 5521, 4270, 7742, 3541, 2796, 3044, 1890, 5299, 6270, 3003, 4359, 6465, 7460, 2748, 1952, 3516, 4564, 4110, 2990, 8481, 3794, 2492, 2568, 2695, 2366, 3640, 2664, 1484, 2762, 3255, 5218, 3725, 4278, 3942, 5668, 3371, 4747, 5868, 3719, 8683, 7226, 4835, 3337, 1833, 1846, 6048, 4647, 5601, 3544, 4817, 4163, 3038, 3381, 3252, 2451, 2481, 2441, 4511, 3947, 3881, 10382]\n",
      "4012.905\n"
     ]
    }
   ],
   "source": [
    "## context의 길이 알아보기 \n",
    "\n",
    "lengths=[]\n",
    "for c in context:\n",
    "    lengths.append(len(c))\n",
    "\n",
    "hap=0\n",
    "for l in lengths:\n",
    "    hap+=l\n",
    "\n",
    "print(lengths)\n",
    "print(hap/200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4985766f-e52d-401c-8ab3-ec75324ee599",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
