{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Platypus2-70B + Wikipedia RAG\n","metadata":{}},{"cell_type":"code","source":"# Installing offline dependencies\n!pip install -U --no-deps /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!pip install -U --no-deps /kaggle/input/datasets-214/datasets-2.14.5-py3-none-any.whl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-29T16:03:48.845755Z","iopub.execute_input":"2023-09-29T16:03:48.846150Z","iopub.status.idle":"2023-09-29T16:04:45.322752Z","shell.execute_reply.started":"2023-09-29T16:03:48.846126Z","shell.execute_reply":"2023-09-29T16:04:45.321414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nimport logging\nfrom time import time\nfrom pathlib import Path\nfrom concurrent.futures import ThreadPoolExecutor\nimport ctypes\nfrom functools import partial\n\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\n# For RAG\nimport faiss\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom datasets import load_from_disk, Dataset\n\nNUM_TITLES = 5\nMAX_SEQ_LEN = 512\nMODEL_PATH = \"/kaggle/input/bge-small-faiss/\"\n\n# For LLM\nfrom transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, AutoModel\nfrom accelerate import init_empty_weights\nfrom accelerate.utils.modeling import set_module_tensor_to_device\nfrom safetensors.torch import load_file\n\nN_BATCHES = 5 \nMAX_CONTEXT = 2750\nMAX_LENGTH = 4096","metadata":{"execution":{"iopub.status.busy":"2023-09-29T16:04:45.325136Z","iopub.execute_input":"2023-09-29T16:04:45.325479Z","iopub.status.idle":"2023-09-29T16:05:07.216874Z","shell.execute_reply.started":"2023-09-29T16:04:45.325449Z","shell.execute_reply":"2023-09-29T16:05:07.215909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to clean RAM & vRAM\ndef clean_memory():\n    gc.collect()\n    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n    torch.cuda.empty_cache()\n\n# Load data\ndf = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\", index_col=\"id\")\n\n# Variable used to avoid running the notebook for 3 hours when submitting. Credit : CPMP\nIS_TEST_SET = len(df) != 200\n\n# Uncomment this to see results on the train set\n# df = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/train.csv\", index_col=\"id\")\n# IS_TEST_SET = True\n# N_BATCHES = 1","metadata":{"execution":{"iopub.status.busy":"2023-09-29T16:05:07.217967Z","iopub.execute_input":"2023-09-29T16:05:07.218264Z","iopub.status.idle":"2023-09-29T16:05:07.264814Z","shell.execute_reply.started":"2023-09-29T16:05:07.218230Z","shell.execute_reply":"2023-09-29T16:05:07.263975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Wikipedia Retrieval Augmented Generation (RAG)\n\nThe following code is adapted from [the notebook of @MGöksu](https://www.kaggle.com/code/mgoksu/0-807-sharing-my-trained-with-context-model) and [the notebook of @MB](https://www.kaggle.com/code/mbanaei/86-2-with-only-270k-articles/notebook). We use the [bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) to embed the Wikipedia dataset.","metadata":{}},{"cell_type":"code","source":"# New SentenceTransformer class similar to the one used in @Mgöksu notebook but relying on the transformers library only\n\nclass SentenceTransformer:\n    def __init__(self, checkpoint, device=\"cuda:0\"):\n        self.device = device\n        self.checkpoint = checkpoint\n        self.model = AutoModel.from_pretrained(checkpoint).to(self.device).half()\n        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n    def transform(self, batch):\n        tokens = self.tokenizer(batch[\"text\"], truncation=True, padding=True, return_tensors=\"pt\", max_length=MAX_SEQ_LEN)\n        return tokens.to(self.device)  \n\n    def get_dataloader(self, sentences, batch_size=32):\n        sentences = [\"Represent this sentence for searching relevant passages: \" + x for x in sentences]\n        dataset = Dataset.from_dict({\"text\": sentences})\n        dataset.set_transform(self.transform)\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n        return dataloader\n\n    def encode(self, sentences, show_progress_bar=False, batch_size=32):\n        dataloader = self.get_dataloader(sentences, batch_size=batch_size)\n        pbar = tqdm(dataloader) if show_progress_bar else dataloader\n\n        embeddings = []\n        for batch in pbar:\n            with torch.no_grad():\n                e = self.model(**batch).pooler_output\n                e = F.normalize(e, p=2, dim=1)\n                embeddings.append(e.detach().cpu().numpy())\n        embeddings = np.concatenate(embeddings, axis=0)\n        return embeddings","metadata":{"execution":{"iopub.status.busy":"2023-09-29T16:05:07.267219Z","iopub.execute_input":"2023-09-29T16:05:07.268047Z","iopub.status.idle":"2023-09-29T16:05:07.277177Z","shell.execute_reply.started":"2023-09-29T16:05:07.267985Z","shell.execute_reply":"2023-09-29T16:05:07.276393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if IS_TEST_SET:\n    # Load embedding model\n    start = time()\n    print(f\"Starting prompt embedding, t={time() - start :.1f}s\")\n    model = SentenceTransformer(MODEL_PATH, device=\"cuda:0\")\n\n    # Get embeddings of prompts\n    f = lambda row : \" \".join([row[\"prompt\"], row[\"A\"], row[\"B\"], row[\"C\"], row[\"D\"], row[\"E\"]])\n    inputs = df.apply(f, axis=1).values # better results than prompt only\n    prompt_embeddings = model.encode(inputs, show_progress_bar=False)\n\n    # Search closest sentences in the wikipedia index \n    print(f\"Loading faiss index, t={time() - start :.1f}s\")\n    faiss_index = faiss.read_index(MODEL_PATH + '/faiss.index')\n    # faiss_index = faiss.index_cpu_to_all_gpus(faiss_index) # causes OOM, and not that long on CPU\n\n    print(f\"Starting text search, t={time() - start :.1f}s\")\n    search_index = faiss_index.search(np.float32(prompt_embeddings), NUM_TITLES)[1]\n\n    print(f\"Starting context extraction, t={time() - start :.1f}s\")\n    dataset = load_from_disk(\"/kaggle/input/all-paraphs-parsed-expanded\")\n    for i in range(len(df)):\n        df.loc[i, \"context\"] = \"-\" + \"\\n-\".join([dataset[int(j)][\"text\"] for j in search_index[i]])\n\n    # Free memory\n    faiss_index.reset()\n    del faiss_index, prompt_embeddings, model, dataset\n    clean_memory()\n    print(f\"Context added, t={time() - start :.1f}s\")","metadata":{"execution":{"iopub.status.busy":"2023-09-29T16:05:07.278363Z","iopub.execute_input":"2023-09-29T16:05:07.279142Z","iopub.status.idle":"2023-09-29T16:05:07.308450Z","shell.execute_reply.started":"2023-09-29T16:05:07.279111Z","shell.execute_reply":"2023-09-29T16:05:07.307639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2: Run Platypus2-70B\n\nTo run such a large model on a single T4 GPU, we run it layer by layer and sample by sample","metadata":{}},{"cell_type":"code","source":"# Create symlinks from kaggle datasets to fake cached model\n\ncheckpoint_path = Path(\"/root/.cache/\")\ncheckpoint_path.mkdir(exist_ok=True, parents=True)\n\nfor part in [1, 2]:\n    source_dir = Path(f\"/kaggle/input/platypus2-70b-instruct-part{part}\")\n    for path in source_dir.glob(\"*\"):\n        try:\n            (checkpoint_path / path.name).symlink_to(path)\n        except:\n            pass","metadata":{"execution":{"iopub.status.busy":"2023-09-29T16:05:07.309641Z","iopub.execute_input":"2023-09-29T16:05:07.310657Z","iopub.status.idle":"2023-09-29T16:05:07.357318Z","shell.execute_reply.started":"2023-09-29T16:05:07.310628Z","shell.execute_reply":"2023-09-29T16:05:07.356552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Class for sharded llama\n\nclass ShardedLlama:\n    def __init__(self, checkpoint_path, device=\"cuda:0\", dtype=torch.float16):\n        \"\"\"\n        Sharded version of LlamaForCausalLM : the model is splitted into layer shards to reduce GPU memory usage.\n        During the forward pass, the inputs are processed layer by layer, and the GPU memory is freed after each layer.\n        To avoid loading the layers multiple times, we could save all the intermediate activations in RAM, but\n        as Kaggle accelerators have more GPU memory than CPU, we simply batch the inputs and keep them on the GPU.\n\n        Parameters\n        ----------\n        checkpoint_path : str or Path\n            path to the checkpoint\n        device : str, optional\n            device, by default \"cuda:0\"\n        dtype : torch.dtype, optional\n            dtype, by default torch.float16\n        \"\"\"\n        \n        # Save parameters\n        self.checkpoint_path = Path(checkpoint_path)\n        self.device = device \n        self.dtype = dtype\n\n        # Create model\n        self.config = AutoConfig.from_pretrained(self.checkpoint_path)\n        # For flash attention when Turing architecture will be supported : https://github.com/Dao-AILab/flash-attention/issues/542\n        # self.config.auto_map = {\"AutoModelForCausalLM\" : \"togethercomputer/LLaMA-2-7B-32K--modeling_flash_llama.LlamaForCausalLM\"} \n        \n        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.tokenizer.padding_side = \"right\"\n        self.init_model()\n        self.layer_names = [\"model.embed_tokens\"] + [f\"model.layers.{i}\" for i in range(len(self.model.model.layers))] + [\"model.norm\", \"lm_head\"]\n\n    def init_model(self):\n    \n        # Load meta model (no memory used)\n        with init_empty_weights():\n            self.model = AutoModelForCausalLM.from_config(self.config, trust_remote_code=True)\n            self.model.tie_weights()\n            \n        self.layers = [self.model.model.embed_tokens] + list(self.model.model.layers) + [self.model.model.norm, self.model.lm_head]\n            \n        # Move buffers to device (not that much GPU memory used)\n        for buffer_name, buffer in self.model.named_buffers():\n            set_module_tensor_to_device(self.model, buffer_name, self.device, value=buffer, dtype=self.dtype)\n\n    def load_layer(self, layer_name):\n        state_dict = load_file(self.checkpoint_path / (layer_name + \".safetensors\"), device=self.device)\n        for param_name, param in state_dict.items():\n            assert param.dtype != torch.int8, \"int8 not supported (need to add fp16_statistics)\"\n            set_module_tensor_to_device(self.model, param_name, self.device, value=param, dtype=self.dtype)\n\n    def __call__(self, inputs, output_token):\n        # inputs = [(prefix, suffix), ...] with prefix.shape[0] = 1 and suffix.shape[0] = 5\n        \n        # Reboot the model to make sure buffers are loaded and memory is clean\n        del self.model\n        clean_memory()\n        self.init_model()\n        \n       # Send batch to device\n        batch = [(prefix.to(self.device), suffix.to(self.device)) for prefix, suffix in inputs]\n        n_suffixes = len(batch[0][1])\n        suffix_eos = [(suffix != self.tokenizer.pad_token_id).sum(1) - 1 for _, suffix in inputs]\n\n        # Create attention mask for the largest input, and position ids to use KV cache\n        attention_mask = torch.finfo(self.dtype).min * torch.ones(MAX_LENGTH, MAX_LENGTH)\n        attention_mask = attention_mask.triu(diagonal=1)[None, None, ...]\n        attention_mask = attention_mask.to(self.device)\n        position_ids = torch.arange(MAX_LENGTH, dtype=torch.long, device=self.device)[None, :]\n\n        with ThreadPoolExecutor() as executor, torch.inference_mode():\n\n            # Load first layer\n            #future = executor.submit(self.load_layer, \"model.embed_tokens\")\n            self.load_layer(\"model.embed_tokens\")\n\n            for i, (layer_name, layer) in tqdm(enumerate(zip(self.layer_names, self.layers)), desc=self.device, total=len(self.layers)):\n\n                # Wait for previous layer to be loaded and load next layer\n                #future.result()\n                if (i + 1) < len(self.layer_names):\n                    #future = executor.submit(self.load_layer, self.layer_names[i + 1])\n                    self.load_layer(self.layer_names[i + 1])\n\n                # Run layer\n                for j, (prefix, suffix) in enumerate(batch):\n                    if layer_name == \"model.embed_tokens\":\n                        batch[j] = (layer(prefix), layer(suffix))\n                    elif layer_name == \"model.norm\":\n                        # Only keep the last token at this point\n                        batch[j] = (None, layer(suffix[torch.arange(n_suffixes), suffix_eos[j]][:, None]))\n                    elif layer_name == \"lm_head\":\n                        batch[j] = layer(suffix)[:, 0, output_token].detach().cpu().numpy()\n                    else:\n                        # Run prefix\n                        len_p, len_s = prefix.shape[1], suffix.shape[1]\n                        new_prefix, (k_cache, v_cache) = layer(prefix, use_cache=True, attention_mask=attention_mask[:, :, -len_p:, -len_p:])\n                        \n                        # Run suffix\n                        pos = position_ids[:, len_p:len_p + len_s].repeat(n_suffixes, 1)\n                        attn = attention_mask[:, :, -len_s:, -len_p - len_s:].repeat(n_suffixes, 1, 1, 1)\n                        kv_cache = (k_cache.repeat(n_suffixes, 1, 1, 1), v_cache.repeat(n_suffixes, 1, 1, 1))\n                        new_suffix = layer(suffix, past_key_value=kv_cache, position_ids=pos, attention_mask=attn)[0]\n                        batch[j] = (new_prefix, new_suffix)\n\n                # Remove previous layer from memory (including buffers)\n                layer.to(\"meta\")\n                clean_memory() # proposed by CPMP\n\n        # Get scores\n        return batch","metadata":{"execution":{"iopub.status.busy":"2023-09-29T16:05:07.358750Z","iopub.execute_input":"2023-09-29T16:05:07.359318Z","iopub.status.idle":"2023-09-29T16:05:07.375523Z","shell.execute_reply.started":"2023-09-29T16:05:07.359288Z","shell.execute_reply":"2023-09-29T16:05:07.374747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run model on the 2 GPUs\n\ndef get_tokens(row, tokenizer):\n        system_prefix = \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_prefix}\"\n        instruction = \"Your task is to analyze the question and answer below. If the answer is correct, respond yes, if it is not correct respond no. As a potential aid to your answer, background context from Wikipedia articles is at your disposal, even if they might not always be relevant.\"\n        input_prefix = f\"Context: {row['context'][:MAX_CONTEXT]}\\nQuestion: {row['prompt']}\\nProposed answer: \"\n        prompt_prefix = system_prefix.format(instruction=instruction, input_prefix=input_prefix)\n        prefix = tokenizer(prompt_prefix, return_tensors=\"pt\", return_attention_mask=False, truncation=True, max_length=MAX_LENGTH)[\"input_ids\"]\n        prompt_suffix = [f\"{row[letter]}\\n\\n### Response:\\n\" for letter in \"ABCDE\"]\n        suffix = tokenizer(prompt_suffix, return_tensors=\"pt\", return_attention_mask=False, truncation=True, max_length=MAX_LENGTH, padding=True)[\"input_ids\"][:, 1:]\n        return prefix, suffix\n\ndef run_model(device, df):\n    model = ShardedLlama(checkpoint_path, device=f\"cuda:{device}\")\n    f = partial(get_tokens, tokenizer=model.tokenizer)\n    inputs = df.apply(f, axis=1).values\n    batches = np.array_split(inputs, N_BATCHES)\n    outputs = []\n    for i, batch in enumerate(batches):\n        # Token #4874 is yes.\n        outputs += model(batch, output_token=4874)\n    return outputs\n\n# Run model\nif IS_TEST_SET: \n    with ThreadPoolExecutor() as executor:\n        outputs = list(executor.map(run_model, [0, 1], np.array_split(df, 2)))\n        outputs = sum(outputs, [])\n        \n    # Save results\n    n = len(df)\n    for i, scores in enumerate(outputs):\n        top3 = np.argsort(scores)[::-1]\n        df.loc[i, \"prediction\"] = \" \".join([\"ABCDE\"[j] for j in top3])\n    \n    # Display performances if train set is used (in this case use IS_TEST_SET=True !)\n    if \"answer\" in df.columns:\n        for i in range(n):\n            df.loc[i, \"top_1\"] = df.loc[i, \"prediction\"][0]\n            df.loc[i, \"top_2\"] = df.loc[i, \"prediction\"][2]\n            df.loc[i, \"top_3\"] = df.loc[i, \"prediction\"][4]\n\n        top_i = [(df[f\"top_{i}\"] == df[\"answer\"]).sum() for i in [1, 2, 3]]\n        print(f\"top1 : {top_i[0]}/{n}, top2 : {top_i[1]}/{n}, top3 : {top_i[2]}/{n} (total={sum(top_i)} / {n})\")\n        print(f\"Accuracy: {100*top_i[0]/n:.1f}%, map3: {100*(top_i[0] + top_i[1]*1/2 + top_i[2]*1/3).sum()/n:.1f}%\")\nelse:\n    df[\"prediction\"] = \"A B C\"\n\ndf[[\"prediction\"]].to_csv(\"submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-09-29T16:05:07.376842Z","iopub.execute_input":"2023-09-29T16:05:07.377256Z","iopub.status.idle":"2023-09-29T16:05:07.408366Z","shell.execute_reply.started":"2023-09-29T16:05:07.377228Z","shell.execute_reply":"2023-09-29T16:05:07.407513Z"},"trusted":true},"execution_count":null,"outputs":[]}]}