{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## t5 모델 올라가는지 테스트"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoTokenizer"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-09-11T08:05:25.249593Z",
     "iopub.execute_input": "2023-09-11T08:05:25.249952Z",
     "iopub.status.idle": "2023-09-11T08:05:25.259177Z",
     "shell.execute_reply.started": "2023-09-11T08:05:25.249923Z",
     "shell.execute_reply": "2023-09-11T08:05:25.258210Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('/kaggle/input/testing-upload-t5-base-model')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-11T08:06:05.516800Z",
     "iopub.execute_input": "2023-09-11T08:06:05.517212Z",
     "iopub.status.idle": "2023-09-11T08:06:08.628719Z",
     "shell.execute_reply.started": "2023-09-11T08:06:05.517178Z",
     "shell.execute_reply": "2023-09-11T08:06:08.627644Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/testing-upload-t5-base-model')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-11T08:06:17.295349Z",
     "iopub.execute_input": "2023-09-11T08:06:17.295780Z",
     "iopub.status.idle": "2023-09-11T08:06:17.537561Z",
     "shell.execute_reply.started": "2023-09-11T08:06:17.295745Z",
     "shell.execute_reply": "2023-09-11T08:06:17.536497Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "text": "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "inputs = [\"generate some interesting story\"]\n",
    "inputs = tokenizer(inputs, max_length=16, truncation=True, return_tensors=\"pt\")\n",
    "output = model.generate(**inputs, num_beams=8, do_sample=True, min_length=16, max_length=64)\n",
    "decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-11T08:08:25.139382Z",
     "iopub.execute_input": "2023-09-11T08:08:25.139747Z",
     "iopub.status.idle": "2023-09-11T08:08:30.766265Z",
     "shell.execute_reply.started": "2023-09-11T08:08:25.139720Z",
     "shell.execute_reply": "2023-09-11T08:08:30.765224Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(decoded_output)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-11T08:08:40.827148Z",
     "iopub.execute_input": "2023-09-11T08:08:40.827805Z",
     "iopub.status.idle": "2023-09-11T08:08:40.832329Z",
     "shell.execute_reply.started": "2023-09-11T08:08:40.827774Z",
     "shell.execute_reply": "2023-09-11T08:08:40.831400Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": "some interesting details about the lu ft han sa rba nk\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 데이터셋 올라가는지 테스트 "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "## train, test csv 데이터셋 읽어오기 테스트\n",
    "df_train = pd.read_csv('/kaggle/input/kaggle-llm-science-exam/train.csv')\n",
    "df_test  = pd.read_csv('/kaggle/input/kaggle-llm-science-exam/test.csv')\n",
    "df_samp = pd.read_csv('/kaggle/input/kaggle-llm-science-exam/sample_submission.csv')\n",
    "df_extra = pd.read_csv('/kaggle/input/additional-train-data-for-llm-science-exam/extra_train_set.csv')\n",
    "df_extra_RO = pd.read_csv('/kaggle/input/additional-train-data-for-llm-science-exam/6000_train_examples.csv')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-11T08:13:05.949535Z",
     "iopub.execute_input": "2023-09-11T08:13:05.950080Z",
     "iopub.status.idle": "2023-09-11T08:13:06.094392Z",
     "shell.execute_reply.started": "2023-09-11T08:13:05.950041Z",
     "shell.execute_reply": "2023-09-11T08:13:06.091950Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "## faiss 깔기 위해서 캐글 다른 노트북에 올라와 있는 것을 가져와서 install 해주는 것이 필요함\n",
    "\n",
    "!pip install -U /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-11T08:15:30.409319Z",
     "iopub.execute_input": "2023-09-11T08:15:30.409712Z",
     "iopub.status.idle": "2023-09-11T08:15:45.421763Z",
     "shell.execute_reply.started": "2023-09-11T08:15:30.409681Z",
     "shell.execute_reply": "2023-09-11T08:15:45.420468Z"
    },
    "trusted": true
   },
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nProcessing /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nInstalling collected packages: faiss-gpu\nSuccessfully installed faiss-gpu-1.7.2\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from faiss import write_index, read_index\n",
    "\n",
    "## wikipedia faiss index 불러오기 테스트\n",
    "sentence_index = read_index(\"/kaggle/input/wikipedia-2023-07-faiss-index/wikipedia_202307.index\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-11T08:48:58.538193Z",
     "iopub.execute_input": "2023-09-11T08:48:58.538571Z",
     "iopub.status.idle": "2023-09-11T08:50:32.647520Z",
     "shell.execute_reply.started": "2023-09-11T08:48:58.538543Z",
     "shell.execute_reply": "2023-09-11T08:50:32.646236Z"
    },
    "trusted": true
   },
   "execution_count": 48,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "## sentence transforemr 설치해주기 \n",
    "## sentence-trasnformers-2.2.2 를 input에 넣어줘야 함\n",
    "## llm-whls 도 같이 넣어줘야함\n",
    "## 그래야 kaggle/working 에 sentencetransformer가 생김\n",
    "\n",
    "!cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers /kaggle/working/sentence-transformers\n",
    "!pip install -U /kaggle/working/sentence-transformers\n",
    "!pip install -U /kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\n",
    "\n",
    "!pip install --no-index --no-deps /kaggle/input/llm-whls/transformers-4.31.0-py3-none-any.whl\n",
    "!pip install --no-index --no-deps /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl\n",
    "!pip install --no-index --no-deps /kaggle/input/llm-whls/datasets-2.14.3-py3-none-any.whl\n",
    "!pip install --no-index --no-deps /kaggle/input/llm-whls/trl-0.5.0-py3-none-any.whl"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-11T08:31:00.454865Z",
     "iopub.execute_input": "2023-09-11T08:31:00.455288Z",
     "iopub.status.idle": "2023-09-11T08:31:37.367713Z",
     "shell.execute_reply.started": "2023-09-11T08:31:00.455254Z",
     "shell.execute_reply": "2023-09-11T08:31:37.366486Z"
    },
    "trusted": true
   },
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nProcessing ./sentence-transformers\n  Preparing metadata (setup.py) ... \u001B[?25ldone\n\u001B[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (4.33.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (4.66.1)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.15.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.23.5)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.11.2)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.1.99)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.16.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.12.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2023.9.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.6.3)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (21.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2023.6.3)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.3.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->sentence-transformers==2.2.2) (1.16.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==2.2.2) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==2.2.2) (3.1.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence-transformers==2.2.2) (9.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2023.7.22)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\nBuilding wheels for collected packages: sentence-transformers\n  Building wheel for sentence-transformers (setup.py) ... \u001B[?25ldone\n\u001B[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=126125 sha256=29c652f24ebc2fea68054e70fc5a90b615188cb74983ea33ca6ff7a2a59b39c9\n  Stored in directory: /root/.cache/pip/wheels/6c/ea/76/d9a930b223b1d3d5d6aff69458725316b0fe205b854faf1812\nSuccessfully built sentence-transformers\nInstalling collected packages: sentence-transformers\n  Attempting uninstall: sentence-transformers\n    Found existing installation: sentence-transformers 2.2.2\n    Uninstalling sentence-transformers-2.2.2:\n      Successfully uninstalled sentence-transformers-2.2.2\nSuccessfully installed sentence-transformers-2.2.2\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\u001B[33mWARNING: Requirement '/kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl' looks like a filename, but the file does not exist\u001B[0m\u001B[33m\n\u001B[0mProcessing /kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\n\u001B[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl'\n\u001B[0m\u001B[31m\n\u001B[0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nProcessing /kaggle/input/llm-whls/transformers-4.31.0-py3-none-any.whl\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.33.0\n    Uninstalling transformers-4.33.0:\n      Successfully uninstalled transformers-4.33.0\nSuccessfully installed transformers-4.31.0\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nProcessing /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl\nInstalling collected packages: peft\nSuccessfully installed peft-0.4.0\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nProcessing /kaggle/input/llm-whls/datasets-2.14.3-py3-none-any.whl\nInstalling collected packages: datasets\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\nSuccessfully installed datasets-2.14.3\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nProcessing /kaggle/input/llm-whls/trl-0.5.0-py3-none-any.whl\nInstalling collected packages: trl\nSuccessfully installed trl-0.5.0\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "## sentencetransformers 모델 불러오기 \n",
    "## (지금은 기존 kaggle 노트북에 있던 것을 가져오기, 나중에는 인코더 따로 학습시키거나 성능 좋은 모델을 가져와도 좋을 것으로 보임)\n",
    "\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "## 기본적인 Hyperparameters 설정\n",
    "SIM_MODEL = '/kaggle/input/sentencetransformers-allminilml6v2/sentence-transformers_all-MiniLM-L6-v2'\n",
    "DEVICE = 0\n",
    "MAX_LENGTH = 384\n",
    "BATCH_SIZE = 32\n",
    "WIKI_PATH = \"/kaggle/input/wikipedia-20230701\"\n",
    "wiki_files = os.listdir(WIKI_PATH)\n",
    "\n",
    "model = SentenceTransformer(SIM_MODEL, device='cuda')\n",
    "model.max_seq_length = MAX_LENGTH\n",
    "model = model.half() ## half를 하는 것은 embedding을 FP16으로 놓고 memory 사용량을 줄이기 위함"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-11T08:32:09.075593Z",
     "iopub.execute_input": "2023-09-11T08:32:09.075977Z",
     "iopub.status.idle": "2023-09-11T08:32:09.376549Z",
     "shell.execute_reply.started": "2023-09-11T08:32:09.075947Z",
     "shell.execute_reply": "2023-09-11T08:32:09.375510Z"
    },
    "trusted": true
   },
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "## 예시 문장 두개만 일단 임베딩 생성해보기 \n",
    "\n",
    "import gc\n",
    "\n",
    "prompt_embeddings = model.encode([\"artificial intelligence\", \"data science\"], batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n",
    "prompt_embeddings = prompt_embeddings.detach().cpu().numpy()\n",
    "_ = gc.collect()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-11T08:50:50.866571Z",
     "iopub.execute_input": "2023-09-11T08:50:50.866955Z",
     "iopub.status.idle": "2023-09-11T08:50:51.315339Z",
     "shell.execute_reply.started": "2023-09-11T08:50:50.866924Z",
     "shell.execute_reply": "2023-09-11T08:50:51.314186Z"
    },
    "trusted": true
   },
   "execution_count": 49,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6938c168746f4da584e59027adf4a7b5"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "## 가장 관련있는 인덱스를 먼저 서치하기\n",
    "## top_6의 문서 인덱스를 찾은 것\n",
    "\n",
    "search_score, search_index = sentence_index.search(prompt_embeddings, 6)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-11T08:50:53.536647Z",
     "iopub.execute_input": "2023-09-11T08:50:53.537031Z",
     "iopub.status.idle": "2023-09-11T08:50:59.856122Z",
     "shell.execute_reply.started": "2023-09-11T08:50:53.537002Z",
     "shell.execute_reply": "2023-09-11T08:50:59.855054Z"
    },
    "trusted": true
   },
   "execution_count": 50,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "## 6개의 인덱스 중 유사도 스코어가 어떻게 되는지 확인해주는 것\n",
    "\n",
    "search_score"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-11T08:51:02.338993Z",
     "iopub.execute_input": "2023-09-11T08:51:02.339457Z",
     "iopub.status.idle": "2023-09-11T08:51:02.346448Z",
     "shell.execute_reply.started": "2023-09-11T08:51:02.339419Z",
     "shell.execute_reply": "2023-09-11T08:51:02.345348Z"
    },
    "trusted": true
   },
   "execution_count": 51,
   "outputs": [
    {
     "execution_count": 51,
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.72897464, 0.74723697, 0.75230414, 0.7558636 , 0.7868048 ,\n        0.8105609 ],\n       [0.4773128 , 0.6345405 , 0.7690647 , 0.7716476 , 0.7855485 ,\n        0.7933025 ]], dtype=float32)"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "## top_6의 인덱스 (인덱스는 문서 vector의 대표 번호라고 생각하면 되는 거 같음. index.add(vector)이렇게 들어간 것으로 보임)\n",
    "\n",
    "search_index"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-11T08:51:04.076963Z",
     "iopub.execute_input": "2023-09-11T08:51:04.077875Z",
     "iopub.status.idle": "2023-09-11T08:51:04.085334Z",
     "shell.execute_reply.started": "2023-09-11T08:51:04.077833Z",
     "shell.execute_reply": "2023-09-11T08:51:04.084175Z"
    },
    "trusted": true
   },
   "execution_count": 52,
   "outputs": [
    {
     "execution_count": 52,
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 368586, 4360770,  368589, 1945029, 3271967, 1086190],\n       [1235496, 1235258, 2609004, 3410896, 5603365, 1235691]])"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "## 메모리 관리를 위해서 필요하지 않은 것들은 미리 지우기\n",
    "\n",
    "import ctypes\n",
    "libc = ctypes.CDLL(\"libc.so.6\")\n",
    "\n",
    "del sentence_index\n",
    "del prompt_embeddings\n",
    "_ = gc.collect()\n",
    "libc.malloc_trim(0)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-11T08:51:05.833779Z",
     "iopub.execute_input": "2023-09-11T08:51:05.835037Z",
     "iopub.status.idle": "2023-09-11T08:51:06.866547Z",
     "shell.execute_reply.started": "2023-09-11T08:51:05.834995Z",
     "shell.execute_reply": "2023-09-11T08:51:06.865379Z"
    },
    "trusted": true
   },
   "execution_count": 53,
   "outputs": [
    {
     "execution_count": 53,
     "output_type": "execute_result",
     "data": {
      "text/plain": "1"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "## parquet 파일 불러오기 (wikipedia 문서 본문을 의미하는 것 같음)\n",
    "## parquet은 열기반 데이터 압축 시스템이라고 볼 수 있고, 필요한 columns을 불러오는 데에 용이하다고 볼 수 있다. \n",
    "\n",
    "df = pd.read_parquet(\"/kaggle/input/wikipedia-20230701/wiki_2023_index.parquet\",\n",
    "                     columns=['id', 'file'])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-11T08:44:02.996005Z",
     "iopub.execute_input": "2023-09-11T08:44:02.996981Z",
     "iopub.status.idle": "2023-09-11T08:44:08.176015Z",
     "shell.execute_reply.started": "2023-09-11T08:44:02.996939Z",
     "shell.execute_reply": "2023-09-11T08:44:08.174970Z"
    },
    "trusted": true
   },
   "execution_count": 41,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(type(zip(search_score, search_index)))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-11T08:53:01.926072Z",
     "iopub.execute_input": "2023-09-11T08:53:01.927095Z",
     "iopub.status.idle": "2023-09-11T08:53:01.935162Z",
     "shell.execute_reply.started": "2023-09-11T08:53:01.927061Z",
     "shell.execute_reply": "2023-09-11T08:53:01.934226Z"
    },
    "trusted": true
   },
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "text": "<class 'zip'>\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import tqdm\n",
    "## tqdm 오류가 나서 일단은 빼놓았음\n",
    "## prompt 별로 관련있는 file이 무엇인지 concat하여 dataframe으로 반환하는 함수\n",
    "\n",
    "wikipedia_file_data = []\n",
    "\n",
    "for i, (scr, idx) in enumerate(zip(search_score, search_index)):\n",
    "    scr_idx = idx\n",
    "    _df = df.loc[scr_idx].copy()\n",
    "    _df['prompt_id'] = i\n",
    "    wikipedia_file_data.append(_df)\n",
    "wikipedia_file_data = pd.concat(wikipedia_file_data).reset_index(drop=True)\n",
    "wikipedia_file_data = wikipedia_file_data[['id', 'prompt_id', 'file']].drop_duplicates().sort_values(['file', 'id']).reset_index(drop=True)\n",
    "\n",
    "## Save memory - delete df since it is no longer necessary\n",
    "del df\n",
    "_ = gc.collect()\n",
    "libc.malloc_trim(0)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-11T08:53:17.123718Z",
     "iopub.execute_input": "2023-09-11T08:53:17.124124Z",
     "iopub.status.idle": "2023-09-11T08:53:17.562002Z",
     "shell.execute_reply.started": "2023-09-11T08:53:17.124070Z",
     "shell.execute_reply": "2023-09-11T08:53:17.560998Z"
    },
    "trusted": true
   },
   "execution_count": 58,
   "outputs": [
    {
     "execution_count": 58,
     "output_type": "execute_result",
     "data": {
      "text/plain": "1"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "wikipedia_file_data"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-11T08:53:38.711318Z",
     "iopub.execute_input": "2023-09-11T08:53:38.711733Z",
     "iopub.status.idle": "2023-09-11T08:53:38.724044Z",
     "shell.execute_reply.started": "2023-09-11T08:53:38.711702Z",
     "shell.execute_reply": "2023-09-11T08:53:38.722983Z"
    },
    "trusted": true
   },
   "execution_count": 59,
   "outputs": [
    {
     "execution_count": 59,
     "output_type": "execute_result",
     "data": {
      "text/plain": "          id  prompt_id       file\n0   33900354          0  a.parquet\n1    5841092          0  a.parquet\n2    2911050          0  c.parquet\n3   35458904          1  d.parquet\n4   49954680          1  d.parquet\n5   65583582          1  d.parquet\n6   50336055          0  g.parquet\n7    7827419          1  j.parquet\n8   10556491          0  m.parquet\n9   54415758          1  m.parquet\n10   6585513          0  o.parquet\n11  49281083          1  t.parquet",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt_id</th>\n      <th>file</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>33900354</td>\n      <td>0</td>\n      <td>a.parquet</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5841092</td>\n      <td>0</td>\n      <td>a.parquet</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2911050</td>\n      <td>0</td>\n      <td>c.parquet</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>35458904</td>\n      <td>1</td>\n      <td>d.parquet</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>49954680</td>\n      <td>1</td>\n      <td>d.parquet</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>65583582</td>\n      <td>1</td>\n      <td>d.parquet</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>50336055</td>\n      <td>0</td>\n      <td>g.parquet</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7827419</td>\n      <td>1</td>\n      <td>j.parquet</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>10556491</td>\n      <td>0</td>\n      <td>m.parquet</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>54415758</td>\n      <td>1</td>\n      <td>m.parquet</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>6585513</td>\n      <td>0</td>\n      <td>o.parquet</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>49281083</td>\n      <td>1</td>\n      <td>t.parquet</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "## 각 파일의 문서 내용을 다시 읽어들이는 함수라고 볼 수 있음\n",
    "\n",
    "wiki_text_data = []\n",
    "\n",
    "for file in wikipedia_file_data.file.unique():\n",
    "    _id = [str(i) for i in wikipedia_file_data[wikipedia_file_data['file']==file]['id'].tolist()]\n",
    "    _df = pd.read_parquet(f\"{WIKI_PATH}/{file}\", columns=['id', 'text'])\n",
    "\n",
    "    _df_temp = _df[_df['id'].isin(_id)].copy()\n",
    "    del _df\n",
    "    _ = gc.collect()\n",
    "    libc.malloc_trim(0)\n",
    "    wiki_text_data.append(_df_temp)\n",
    "wiki_text_data = pd.concat(wiki_text_data).drop_duplicates().reset_index(drop=True)\n",
    "_ = gc.collect()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-11T08:55:05.785624Z",
     "iopub.execute_input": "2023-09-11T08:55:05.786025Z",
     "iopub.status.idle": "2023-09-11T08:57:00.978961Z",
     "shell.execute_reply.started": "2023-09-11T08:55:05.785994Z",
     "shell.execute_reply": "2023-09-11T08:57:00.977890Z"
    },
    "trusted": true
   },
   "execution_count": 60,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "wiki_text_data"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-11T08:58:07.029590Z",
     "iopub.execute_input": "2023-09-11T08:58:07.030030Z",
     "iopub.status.idle": "2023-09-11T08:58:07.044021Z",
     "shell.execute_reply.started": "2023-09-11T08:58:07.029998Z",
     "shell.execute_reply": "2023-09-11T08:58:07.043019Z"
    },
    "trusted": true
   },
   "execution_count": 61,
   "outputs": [
    {
     "execution_count": 61,
     "output_type": "execute_result",
     "data": {
      "text/plain": "          id                                               text\n0    5841092  Artificial Intelligence is a scientific journa...\n1   33900354  The Artificial Intelligence Center is a labora...\n2    2911050  Computational Intelligence is a peer-reviewed ...\n3   49954680  thumb|The institute as seen from the main walk...\n4   35458904  Data science is an interdisciplinary academic ...\n5   65583582  Datacommons.org is an open knowledge graph hos...\n6   50336055  This glossary of artificial intelligence is a ...\n7    7827419  John Yen is Professor of Data Science and Prof...\n8   10556491  Machine Intelligence may refer to: * Artificia...\n9   54415758  A Master of Science in Data Science is an inte...\n10   6585513  The following outline is provided as an overvi...\n11  49281083  The Data Incubator is a data science education...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5841092</td>\n      <td>Artificial Intelligence is a scientific journa...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>33900354</td>\n      <td>The Artificial Intelligence Center is a labora...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2911050</td>\n      <td>Computational Intelligence is a peer-reviewed ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>49954680</td>\n      <td>thumb|The institute as seen from the main walk...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>35458904</td>\n      <td>Data science is an interdisciplinary academic ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>65583582</td>\n      <td>Datacommons.org is an open knowledge graph hos...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>50336055</td>\n      <td>This glossary of artificial intelligence is a ...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7827419</td>\n      <td>John Yen is Professor of Data Science and Prof...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>10556491</td>\n      <td>Machine Intelligence may refer to: * Artificia...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>54415758</td>\n      <td>A Master of Science in Data Science is an inte...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>6585513</td>\n      <td>The following outline is provided as an overvi...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>49281083</td>\n      <td>The Data Incubator is a data science education...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "## 이렇게 하면 위키 텍스트를 불러올 수 있음을 확인할 수 있음\n",
    "\n",
    "wiki_text_data['text'].tolist()[0]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-11T08:59:21.038583Z",
     "iopub.execute_input": "2023-09-11T08:59:21.039637Z",
     "iopub.status.idle": "2023-09-11T08:59:21.047011Z",
     "shell.execute_reply.started": "2023-09-11T08:59:21.039588Z",
     "shell.execute_reply": "2023-09-11T08:59:21.045936Z"
    },
    "trusted": true
   },
   "execution_count": 65,
   "outputs": [
    {
     "execution_count": 65,
     "output_type": "execute_result",
     "data": {
      "text/plain": "'Artificial Intelligence is a scientific journal on artificial intelligence research. It was established in 1970 and is published by Elsevier. The journal is abstracted and indexed in Scopus and Science Citation Index. The 2021 Impact Factor for this journal is 14.05 and the 5-Year Impact Factor is 11.616.Journal Citation Reports 2022, Published by Thomson Reuters ==References== == External links == * Official website Category:Artificial intelligence publications Category:Computer science journals Category:Elsevier academic journals Category:Academic journals established in 1970'"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 여기까지 공부한 것 정리 \n",
    "* 모델 올리는 것은 오른편에 add data 한 다음에 내 파일을 올리면 된다. 거기서 로드해올 수 있는데, 어느 정도 크기까지가 가능할지는 좀 더 실험이 필요할 것으로 보인다.\n",
    "* sentence trasnformer로 query를 임베딩 하는 방법을 터득했다. kaggle에서 제공하는 sentencetransformer를 사용하는 것도 괜찮을 것 같고, 여유가 되면 다른 모델을 가져와서 사용해도 될 것 같다.\n",
    "* faiss wikipedia dataset을 이용하는 방법에 대하여 공부하였다. index를 먼저 서칭하게 된다. index는 parquet으로 되어 있는 데이터셋 중에서 가장 관련있는 번호를 뽑을 수 있게 해준다. 즉, index는 (id, 벡터) 로만 기억하고 있어서 어느 파일에 있는지 찾아갈 수 있게해준다. 이후에 parquet을 읽어서 직접적인 텍스트를 읽어올 수 있게 되면 된다. "
   ],
   "metadata": {}
  }
 ]
}
